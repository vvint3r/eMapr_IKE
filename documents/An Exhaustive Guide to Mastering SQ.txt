An Exhaustive Guide to Mastering SQL 
for Technical Interviews 
… 
Section 1.2: The Anatomy of a Query 
The SELECT statement is the cornerstone of data retrieval in SQL. 
Understanding its components and the logical order in which they are processed is 
fundamental to writing correct and efficient queries. 
Core Syntax and Logical Order of Operations 
While a query is written in a specific order (SELECT, FROM, WHERE, etc.), the database engine 
processes it in a different logical order. 
A solid mental model of this processing order helps in understanding how clauses interact. 
1. FROM / JOIN: Identifies the source tables and combines them. 
2. WHERE: Filters individual rows based on conditions. 
3. GROUP BY: Aggregates the filtered rows into groups. 
4. HAVING: Filters the aggregated groups. 
5. SELECT: Selects the final columns and performs calculations. 
6. DISTINCT: Removes duplicate rows from the result. 
7. ORDER BY: Sorts the final result set. 
8. LIMIT / OFFSET: Restricts the number of rows returned. 
Clauses & Operators 
● SELECT: Specifies the columns to be returned. Using * selects all columns, but it is a best 
practice in production code to explicitly name the required columns to improve clarity 
and performance.8 
● DISTINCT: Used within the SELECT clause to return only unique values, eliminating 
duplicate rows from the result set.13 
● WHERE: Filters the result set based on one or more conditions applied to individual rows 
before any grouping occurs.12 
● ORDER BY: Sorts the final result set in ascending (ASC, default) or descending (DESC) 
order based on one or more columns.11 
● LIMIT / OFFSET: LIMIT restricts the number of rows returned, while OFFSET skips a 
specified number of rows before starting to return results. These are crucial for 
implementing pagination.13 
● Operators: 
○ Comparison: =, <>, !=, >, <, >=, <= are used for comparing values.13 
○ Logical: AND, OR, NOT are used to combine multiple conditions in a WHERE clause.13 
○ Pattern Matching & Lists: 
■ LIKE: Used with wildcards (% for zero or more characters, _ for a single 
character) for pattern matching in strings.11 
■ IN: Checks if a value exists within a specified list of values.12 
■ BETWEEN: Selects values within a given range (inclusive).13 
■ IS NULL / IS NOT NULL: Specifically used to check for the presence or absence of 
a value, as NULL cannot be compared using standard operators like = or <>.14 
Practice Questions 
● Question: Write a query to find employees whose names start with ‘Int’. 
○ Answer: SELECT * FROM employees WHERE employee_name LIKE 'Int%';.13 
● Question: Write a query to find orders where the order amount exists between 1000 and 
5000. 
○ Answer: SELECT * FROM orders WHERE order_amount BETWEEN 1000 AND 5000;.12 
● Question: Explain the difference between BETWEEN and IN. 
○ Answer: BETWEEN is used to select values within a continuous range (e.g., numbers, 
dates), and it is inclusive of the start and end values. IN is used to check if a value 
matches any value in a discrete list of specified values.12 
● Question: Are NULL values equal to zero or a blank space? Explain. 
○ Answer: No. A NULL value represents the absence of data or an unknown value. It is 
distinct from zero, which is a specific number, and a blank space or empty string, 
which is a character value of length zero. NULL cannot be compared with any other 
value using standard comparison operators; one must use IS NULL or IS NOT NULL.12 
Section 1.3: Data Definition and Manipulation (DDL, DML, DCL, TCL) 
SQL commands are categorized into functional subsets. 
A clear understanding of these categories is often tested in foundational interview questions. 
● Data Definition Language (DDL): Defines and manages the structure of database 
objects. 
○ Commands: CREATE, ALTER, DROP, TRUNCATE.1 
● Data Manipulation Language (DML): Used to manage the data within the schema 
objects. 
○ Commands: INSERT, UPDATE, DELETE, SELECT.1 
● Data Control Language (DCL): Manages access rights and permissions to the 
database. 
○ Commands: GRANT, REVOKE.1 
● Transaction Control Language (TCL): Manages transactions within the database. 
○ Commands: COMMIT, ROLLBACK, SAVEPOINT.13 
● Data Query Language (DQL): Primarily consists of the SELECT statement for retrieving 
data.1 
The DELETE vs. TRUNCATE vs. DROP Debate 
This is a classic interview question that tests a candidate's understanding of how the 
database handles data removal at different levels. 
● DELETE: A DML command that removes rows from a table one by one. It can be used with 
a WHERE clause to remove specific rows. Because it is a logged operation, it is generally 
slower and can be rolled back within a transaction. It also fires any DELETE triggers 
associated with the table.11 
● TRUNCATE: A DDL command that removes all rows from a table by deallocating the data 
pages. It is much faster than DELETE for large tables as it is minimally logged. It cannot 
be used with a WHERE clause and typically cannot be rolled back (though behavior can 
vary by RDBMS within an explicit transaction). It does not fire DELETE triggers.11 
● DROP: A DDL command that completely removes the table itself, including its structure, 
data, indexes, constraints, and permissions. The action is irreversible without a backup.11 
Practice Questions 
● Question: What is the difference between DELETE and TRUNCATE? Can you roll back a 
TRUNCATE statement? 
○ Answer: DELETE is a DML command that removes rows individually and is logged, 
making it slower but allowing for rollback and firing triggers. TRUNCATE is a DDL 
command that deallocates all data pages at once, making it faster but generally not 
rollbackable (with some exceptions like SQL Server within a BEGIN TRAN...ROLLBACK 
block) and it does not fire triggers.11 
● Question: How would you add a column to an existing table? 
○ Answer: Using the ALTER TABLE command with the ADD COLUMN clause. For 
example: ALTER TABLE employees ADD email VARCHAR(100);.14 
● Question: How can you copy data from one table to another? 
○ Answer: Using the INSERT INTO... SELECT... statement. For example, to copy all 
records: INSERT INTO new_table SELECT * FROM old_table;.13 To create a new table 
with the same structure and data, one can use 
SELECT * INTO new_table FROM old_table; in some SQL dialects.11 
Section 1.4: Database Normalization 
Normalization is a systematic approach to designing a database schema to minimize data 
redundancy and improve data integrity. 
It involves dividing larger tables into smaller, well-structured tables and defining relationships 
between them.11 
Normal Forms (1NF, 2NF, 3NF, BCNF) 
● First Normal Form (1NF): A table is in 1NF if all its columns contain atomic (indivisible) 
values, and each row is unique. This means no repeating groups or multi-valued 
columns.11 
● Second Normal Form (2NF): A table must be in 1NF and all of its non-key attributes must 
be fully functionally dependent on the entire primary key. This rule applies to tables with 
composite primary keys and aims to eliminate partial dependencies, where a non-key 
attribute depends on only part of the composite primary key.11 
● Third Normal Form (3NF): A table must be in 2NF and all its attributes must be 
dependent only on the primary key. This eliminates transitive dependencies, where a 
non-key attribute is dependent on another non-key attribute.11 
● Boyce-Codd Normal Form (BCNF): A stricter version of 3NF. A table is in BCNF if for 
every non-trivial functional dependency X→Y, X is a superkey. In simpler terms, every 
determinant must be a candidate key.13 
Denormalization 
Denormalization is the process of intentionally introducing redundancy into a normalized 
database design to improve query performance. 
By combining tables and reducing the number of required JOINs, read operations can be 
made significantly faster. 
This is a common strategy in data warehousing and reporting databases (OLAP systems) 
where read performance is more critical than write efficiency.11 
Practice Questions 
● Question: Explain the different normal forms (1NF, 2NF, 3NF). 
○ Answer: 1NF ensures atomic values in columns. 2NF builds on 1NF and removes 
partial dependencies on composite keys. 3NF builds on 2NF and removes transitive 
dependencies, where non-key attributes depend on other non-key attributes.13 
● Question: What is denormalization and when would you use it? 
○ Answer: Denormalization is the strategic introduction of redundancy to a database 
to improve read performance by reducing the need for complex joins. It is often used 
in analytical systems (OLAP) or high-traffic applications where query speed is a 
primary concern, and the trade-off of increased storage and more complex updates 
is acceptable.11 
Part II: Intermediate SQL - Aggregating and 
Combining Data 
This section transitions from single-table operations to the core analytical tasks of 
summarizing and merging data from multiple sources. 
The progression from simple JOINs to subqueries and then to Common Table Expressions 
(CTEs) reflects a growing sophistication in a developer's approach to problem-solving. 
Interviews often test this maturity curve. 
A candidate who can solve a problem with a JOIN is competent; one who recognizes when a 
CTE offers superior clarity and efficiency demonstrates a higher level of expertise. 
Section 2.1: Mastering Aggregation 
Aggregation is the process of transforming detailed, row-level data into summarized, 
meaningful information. 
Aggregate Functions 
These functions operate on a set of values to return a single, summary value. 
They are essential for calculating metrics. 
● COUNT(): Returns the number of rows. COUNT(*) counts all rows, while 
COUNT(column_name) counts non-NULL values in that column.13 
● SUM(): Calculates the total sum of a numeric column.13 
● AVG(): Calculates the average value of a numeric column.13 
● MIN(): Returns the minimum value in a column.13 
● MAX(): Returns the maximum value in a column.13 
Grouping Data with GROUP BY 
The GROUP BY clause is used with aggregate functions to group rows that have the same 
values in specified columns into summary rows. 
For each group, the aggregate function calculates a summary value.13 
Filtering Groups with WHERE vs. HAVING 
This distinction is a fundamental and frequently asked interview question. 
● WHERE clause filters individual rows before they are passed to the GROUP BY clause and 
aggregate functions. It operates on row-level data.12 
● HAVING clause filters groups after the GROUP BY clause has been applied and the 
aggregations have been calculated. It operates on the aggregated results.12 
Practice Questions 
● Question: Find the average salary for each department. 
○ Answer: SELECT department, AVG(salary) AS avg_salary FROM employees GROUP 
BY department;.13 
● Question: List departments with more than 10 employees. 
○ Answer: SELECT department, COUNT(*) FROM employees GROUP BY department 
HAVING COUNT(*) > 10;.14 
● Question: Explain, with an example, the difference between the WHERE and HAVING 
clauses. 
○ Answer: WHERE filters rows before aggregation, while HAVING filters groups after 
aggregation. For example, to find the total sales for products with a price over $10, 
but only for categories with total sales exceeding $1000, you would use WHERE price 
> 10 to filter products first, then GROUP BY category, and finally HAVING SUM(sales) 
> 1000 to filter the resulting categories.12 
Section 2.2: The Art of the JOIN 
JOIN clauses are the primary mechanism for combining data from two or more tables based 
on a related column. 
Core JOIN Types 
● INNER JOIN: Returns only the rows that have matching values in both tables. It is the 
most common type of join.11 
● LEFT JOIN (or LEFT OUTER JOIN): Returns all rows from the left table and the matched 
rows from the right table. If there is no match, NULL is returned for the columns from the 
right table. This is useful for finding entities that may not have a corresponding entry in 
another table (e.g., customers who have never placed an order).11 
● RIGHT JOIN (or RIGHT OUTER JOIN): The inverse of a LEFT JOIN. It returns all rows from 
the right table and the matched rows from the left table. NULL is returned for left table 
columns where there is no match.11 
● FULL OUTER JOIN: Returns all rows when there is a match in either the left or the right 
table. It effectively combines the results of both LEFT and RIGHT joins. If there is no 
match for a given row, the columns from the other table will contain NULL.11 
Advanced JOIN Types 
● CROSS JOIN: Returns the Cartesian product of the two tables, meaning every row from 
the first table is combined with every row from the second table. It is used less frequently 
but can be useful for generating all possible combinations of data.11 
● SELF JOIN: This is a regular join, but the table is joined with itself. It is used to query 
hierarchical data or to compare rows within the same table. Table aliases are required to 
distinguish between the two instances of the table in the query.11 
Practice Questions 
● Question: Write a query to join 3 tables. 
○ Answer: SELECT * FROM table1 t1 JOIN table2 t2 ON t1.id = t2.t1_id JOIN table3 t3 
ON t2.id = t3.t2_id;.13 
● Question: Get all employees and their project names, showing NULL if an employee is 
not assigned a project. 
○ Answer: This requires a LEFT JOIN from the employees table to the projects table. 
SELECT e.name, p.project_name FROM employees e LEFT JOIN projects p ON e.id = 
p.employee_id;.13 
● Question: Write a query to find pairs of employees who have the same salary. 
○ Answer: This is a classic SELF JOIN problem. SELECT e1.name, e2.name, e1.salary 
FROM employees e1 JOIN employees e2 ON e1.salary = e2.salary AND e1.id < e2.id; 
The e1.id < e2.id condition is crucial to avoid listing the same pair twice (e.g., A-B and 
B-A) and an employee with themselves.13 
● Question: Find all salespeople and customers who live in the same city. 
○ Answer: This requires an INNER JOIN between the two tables on the city column. 
SELECT s.name, c.name, s.city FROM salespeople s INNER JOIN customers c ON 
s.city = c.city;.12 
Section 2.3: Advanced Data Combination 
Beyond JOINs, SQL provides other powerful tools for combining and filtering datasets. 
Set Operators 
Set operators combine the results of two or more SELECT statements. 
● UNION vs. UNION ALL: This is a very common interview topic. 
○ UNION: Combines the result sets and removes duplicate rows. This deduplication 
step requires extra processing, making it slower.11 
○ UNION ALL: Combines the result sets but includes all rows, including duplicates. It is 
significantly more performant because it does not need to check for duplicates.11 
● INTERSECT: Returns only the rows that appear in both result sets.12 
● EXCEPT (or MINUS in Oracle): Returns the rows from the first result set that do not 
appear in the second result set.13 
Subqueries (Nested Queries) 
A subquery is a SELECT statement nested inside another statement. 
They allow for complex, multi-step logic where the result of an inner query is used to guide 
the outer query. 
● Placement: Subqueries can be used in the SELECT list, the FROM clause (where they are 
often called derived tables), and most commonly, the WHERE clause.11 
● Correlated vs. Non-Correlated Subqueries: 
○ Non-Correlated: The inner query can be run independently of the outer query. Its 
result is calculated once and then used by the outer query.11 
○ Correlated: The inner query depends on the outer query for its values. It is evaluated 
once for each row processed by the outer query. Correlated subqueries can be less 
efficient than other methods like JOINs or CTEs.11 
Practice Questions 
● Question: What is the difference between UNION and UNION ALL? Which is more 
performant and why? 
○ Answer: UNION combines results and removes duplicates, while UNION ALL 
combines results and keeps duplicates. UNION ALL is more performant because it 
avoids the computationally expensive sort or hash operation required to identify and 
remove duplicates.11 
● Question: Write a query to fetch employees who earn more than the average salary in 
the entire company. 
○ Answer: This is a classic use case for a subquery in the WHERE clause. SELECT 
name, salary FROM employees WHERE salary > (SELECT AVG(salary) FROM 
employees);.13 
● Question: Explain the difference between a correlated and a non-correlated subquery. 
○ Answer: A non-correlated subquery is self-contained and executes once, passing its 
result to the outer query. A correlated subquery references columns from the outer 
query and thus must be re-executed for each row processed by the outer query, 
which can lead to performance issues.11 
Part III: Advanced SQL for Modern Data Analysis 
This section delves into the modern SQL features that are now standard expectations in 
data-centric interviews. 
Mastery of window functions and Common Table Expressions (CTEs) is what separates 
top-tier candidates from the rest. 
These features are not merely individual topics to learn; they form a synergistic toolset. 
The combination of CTEs and window functions is a modern SQL "power combo" capable of 
elegantly solving most complex analytical questions. 
CTEs provide the framework for readable, modular logic, while window functions provide the 
engine for sophisticated calculations. 
Section 3.1: Unleashing the Power of Window Functions 
Window functions perform a calculation across a set of table rows that are related to the 
current row. 
Unlike GROUP BY aggregations, they do not collapse the result set; they return a value for 
each row, preserving the original row's identity.9 
This capability is essential for tasks like ranking, calculating running totals, and comparing 
values between rows. 
The core of a window function is the OVER() clause, which defines the "window" or set of rows 
the function operates on. 
It has two main components: 
● PARTITION BY: Divides the rows into groups (partitions). The window function is applied 
independently to each partition. This is conceptually similar to GROUP BY but does not 
collapse the rows. 
● ORDER BY: Orders the rows within each partition. This is crucial for functions that depend 
on sequence, such as RANK() or LAG(). 
Key Categories of Window Functions 
1. Ranking Functions: Used to assign a rank to each row within a partition based on a 
specified order. The differences in how they handle tied values are a critical interview 
topic. 
○ ROW_NUMBER(): Assigns a unique, sequential integer to each row, regardless of 
ties.15 
○ RANK(): Assigns the same rank to tied values but leaves gaps in the subsequent 
ranks (e.g., 1, 2, 2, 4).14 
○ DENSE_RANK(): Assigns the same rank to tied values but does not leave gaps (e.g., 1, 
2, 2, 3). This is often the most useful ranking function in interviews.12 
○ NTILE(n): Distributes the rows in an ordered partition into a specified number of 
ranked groups (buckets).23 
2. Navigation/Offset Functions: Used to access data from a different row relative to the 
current row within the same result set. 
○ LAG(column, offset, default): Accesses data from a previous row in the partition.9 
○ LEAD(column, offset, default): Accesses data from a subsequent row in the 
partition.9 
3. Aggregate Window Functions: Apply standard aggregate functions (SUM, AVG, COUNT, 
MIN, MAX) over a defined window. 
○ Frame Clause (ROWS BETWEEN...): This clause provides fine-grained control over 
the window frame. For example, ROWS BETWEEN UNBOUNDED PRECEDING AND 
CURRENT ROW defines a window that includes all rows from the start of the partition 
up to the current row, which is essential for calculating running totals.13 
The following table provides a quick reference for these essential functions. 
Function Syntax Example Purpose Common Interview 
Use Case 
ROW_NUMBER() ROW_NUMBER() 
OVER (ORDER BY 
salary DESC) 
Assigns a unique 
sequential integer 
to each row. 
Arbitrarily breaking 
ties; assigning a 
unique ID to a 
result set. 
RANK() RANK() OVER 
(PARTITION BY 
dept ORDER BY 
salary DESC) 
Ranks rows, with 
gaps after ties. 
General ranking 
scenarios where 
gaps are 
acceptable. 
DENSE_RANK() DENSE_RANK() 
OVER (PARTITION 
BY dept ORDER BY 
salary DESC) 
Ranks rows, with no 
gaps after ties. 
Finding "Top N per 
group" (e.g., top 3 
salaries in each 
department). 
NTILE(n) NTILE(4) OVER 
(ORDER BY sales 
DESC) 
Divides rows into n 
ranked groups 
(e.g., quartiles). 
Segmenting 
customers into 
sales quartiles or 
performance 
buckets. 
LAG() LAG(sales, 1) OVER 
(PARTITION BY 
product ORDER BY 
month) 
Accesses a value 
from a previous 
row. 
Calculating 
period-over-period 
growth (e.g., 
month-over-month 
sales change). 
LEAD() LEAD(event_time, 1) 
OVER (PARTITION 
BY user_id ORDER 
BY event_time) 
Accesses a value 
from a subsequent 
row. 
Calculating the 
duration until the 
next event for 
sessionization. 
SUM() OVER() SUM(sales) OVER 
(PARTITION BY year 
ORDER BY month 
ROWS 
UNBOUNDED 
PRECEDING) 
Calculates a sum 
over a window. 
Calculating a 
running total or 
cumulative sum 
(e.g., year-to-date 
sales). 
AVG() OVER() AVG(price) OVER 
(ORDER BY date 
ROWS BETWEEN 6 
PRECEDING AND 
CURRENT ROW) 
Calculates an 
average over a 
window. 
Calculating a 
moving average 
(e.g., 7-day rolling 
average price). 
 
Section 3.2: Simplifying Complexity with Common Table Expressions 
(CTEs) 
 
A Common Table Expression, defined using the WITH clause, creates a temporary, named 
result set that can be referenced within a single SELECT, INSERT, UPDATE, or DELETE 
statement. 
CTEs are indispensable for breaking down complex queries into logical, readable steps, 
making them far superior to deeply nested subqueries.1 
 
Recursive CTEs 
A powerful feature of CTEs is their ability to be recursive, meaning a CTE can reference itself. 
This is the standard SQL method for querying hierarchical data, such as organizational charts, 
bill of materials, or network graphs.13 
A recursive CTE has two parts: an anchor member that returns the base result, and a 
recursive member that references the CTE itself, joined with the anchor. 
The recursion stops when the recursive member returns no more rows. 
Practice Questions 
● Question: Rewrite a complex query containing multiple nested subqueries to use CTEs 
for improved readability. 
○ Answer: This involves identifying each subquery's purpose, converting it into a 
named CTE using the WITH clause, and then joining these CTEs in the final SELECT 
statement. 
● Question: Given an employees table with employee_id and manager_id, write a query to 
f
 ind the entire reporting hierarchy for a specific employee. 
○ Answer: This is a classic recursive CTE problem. The anchor member selects the 
starting employee. The recursive member repeatedly joins the employees table to the 
CTE to find the next level of direct reports, continuing until all levels of the hierarchy 
are traversed.13 
● Question: Use a CTE to rank customers by total purchase amount and return the top 10. 
○ Answer: A first CTE would calculate the total purchase amount for each customer 
using SUM() and GROUP BY. A second CTE would then use DENSE_RANK() on the 
result of the first CTE. The final query would select from the second CTE where the 
rank is less than or equal to 10.13 
Section 3.3: Data Transformation Techniques 
Beyond selecting and joining data, SQL provides a rich set of functions for transforming data 
within a query. 
● Conditional Logic with CASE: The CASE statement provides if-then-else logic, allowing 
for the creation of new columns or conditional aggregations based on specified rules. It 
is incredibly versatile for tasks like bucketing data, creating flags, or pivoting.6 
● Handling NULLs: 
○ COALESCE(val1, val2,...): Returns the first non-NULL value from a list of expressions. 
It is commonly used to substitute a default value for NULLs.12 
○ NULLIF(expr1, expr2): Returns NULL if the two expressions are equal; otherwise, it 
returns the first expression. Useful for preventing division-by-zero errors by 
converting a zero denominator to NULL.25 
● Date/Time Manipulation: Nearly every analytical query involves dates. Common 
functions include EXTRACT() or DATE_PART() to get components like year or month, 
DATEDIFF() to find the interval between two dates, and various formatting functions.9 
● String Manipulation: Functions like CONCAT() (or ||), SUBSTRING(), UPPER(), LOWER(), 
and TRIM() are essential for cleaning and formatting text data.12 
● Pivoting Data: This involves transforming data from a row-based format to a 
column-based format. While some databases have a native PIVOT function, the universal 
method involves using an aggregate function with a CASE statement for each desired 
new column.5 
Practice Questions 
● Question: Write a query to swap gender values ('M' to 'F' and 'F' to 'M') in a table. 
○ Answer: UPDATE employees SET gender = CASE WHEN gender = 'M' THEN 'F' ELSE 
'M' END;.13 
● Question: Calculate the number of days an employee has been with the company. 
○ Answer: SELECT name, DATEDIFF(CURDATE(), joining_date) AS days_with_company 
FROM employees; (Syntax may vary by SQL dialect).13 
● Question: Pivot a table of sales data to show total sales for each product category 
('Electronics', 'Clothing', 'Home Goods') as separate columns for each year. 
○ Answer: SELECT year, SUM(CASE WHEN category = 'Electronics' THEN sales ELSE 0 
END) AS electronics_sales, SUM(CASE WHEN category = 'Clothing' THEN sales ELSE 
0 END) AS clothing_sales, SUM(CASE WHEN category = 'Home Goods' THEN sales 
ELSE 0 END) AS home_goods_sales FROM sales_table GROUP BY year; 
Part IV: Cracking the Code - SQL Interview Patterns 
and Puzzles 
 
Moving from syntax to strategy is the most critical step in interview preparation. 
Technical interviews are not just about knowing commands; they are about recognizing a 
problem's underlying structure and applying the most efficient solution pattern. 
Candidates who struggle often do so because they treat each problem as unique, attempting 
to invent a solution from scratch under pressure. 
Successful candidates, however, have developed a mental library of these patterns, allowing 
them to quickly classify a problem and deploy a proven technique. 
The most challenging problems on platforms like LeetCode are frequently combinations of 
two or more fundamental patterns. 
For instance, a "Hard" problem might first require sessionizing user activity (a LAG-based 
pattern) and then finding the top N longest sessions (a DENSE_RANK pattern). 
This section deconstructs these complex problems into their core building blocks, teaching 
not just the patterns themselves but also how to combine them. 
The following matrix provides a high-level map, linking common question types to their 
primary solution techniques. 
This framework should guide the approach to any new problem encountered. 
 
Problem Pattern / 
Question Type 
Primary SQL 
Technique(s) 
Key Functions Example 
LeetCode/DataLem
 ur Problem 
Nth Highest Salary Window Function 
or Subquery/LIMIT 
DENSE_RANK(), 
LIMIT, OFFSET 
LeetCode 176, 177 
24 
Top N per Group Window Function 
with Partitioning 
DENSE_RANK(), 
PARTITION BY 
LeetCode 185 24 
Finding Duplicates Aggregation or 
Window Function 
GROUP BY, HAVING 
COUNT(*) > 1, 
ROW_NUMBER() 
LeetCode 182, 196 
24 
Rolling 
Average/Sum 
Aggregate Window 
Function with 
Frame 
AVG(), SUM(), 
ROWS BETWEEN 
Tweets' Rolling 
Averages 29 
Period-over-Period Navigation Window 
Function 
LAG() Y-on-Y Growth Rate 
29 
Consecutive Events 
/ Streaks 
Window Functions 
to create groups 
ROW_NUMBER(), 
Date Arithmetic 
LeetCode 180 24 
Gaps and Islands Window Functions 
to identify groups 
ROW_NUMBER(), 
LAG() 
Human Traffic of 
Stadium 24 
Analyzing Pairs Self-Join JOIN table AS a 
JOIN table AS b 
Analyze Pairs of 
Things 7 
Hierarchy Traversal Recursive Common 
Table Expression 
WITH RECURSIVE Tree Node 24 
Sessionization Navigation Window 
Function 
LAG(), LEAD() Calculating User 
Activity 23 
 
Section 4.1: Foundational Patterns 
 
These patterns represent the most common tasks and are frequently asked in interviews for 
all data roles. 
 
Top-N / Nth-Highest 
 
This pattern involves finding a specific rank or the top N records, either across an entire table 
or within distinct groups. 
● Technique: The most robust and modern solution is to use window functions like 
DENSE_RANK() or RANK() partitioned by the grouping column. An outer query or CTE is 
then used to filter on the calculated rank. For simpler cases (Nth highest overall), a 
subquery or LIMIT with OFFSET can also be used.7 
● Example Problem: Find the top 3 salaries in each department. 
● Logic: 
1. Use DENSE_RANK() to assign a salary rank for each employee, restarting the rank for 
each department. 
2. The OVER clause will be (PARTITION BY department_id ORDER BY salary DESC). 
3. Use a CTE to store this ranked result. 
4. Select from the CTE where the rank is less than or equal to 3.24 
Finding Duplicates 
This involves identifying rows that share the same values across one or more specified 
columns. 
● Technique 1 (GROUP BY): Group the data by the columns that define a duplicate and 
use HAVING COUNT(*) > 1 to filter for the groups with more than one entry.9 
● Technique 2 (Window Function): Use ROW_NUMBER() OVER (PARTITION BY col1, col2... 
ORDER BY some_column) to assign a sequence number to each row within a group of 
duplicates. Any row with a ROW_NUMBER > 1 is a duplicate that can be filtered or 
deleted.11 This method is particularly useful for deleting duplicates while keeping one 
instance. 
● Example Problem: Find all duplicate emails in the Person table. 
● Logic (GROUP BY): 
1. GROUP BY Email. 
2. Filter the groups with HAVING COUNT(Email) > 1.24 
Existence Checks 
This pattern addresses questions about finding records in one table that do or do not have a 
corresponding record in another table. 
● Technique 1 (LEFT JOIN): LEFT JOIN from the primary table to the secondary table. 
Where a match is not found, the columns from the secondary table will be NULL. Filtering 
WHERE secondary_table.key IS NULL will find all records from the primary table that have 
no match.13 
● Technique 2 (NOT IN): Use a subquery to select all keys from the secondary table and 
f
 ilter the primary table WHERE primary_table.key NOT IN (...). Caution: This method can 
produce unexpected empty results if the subquery's result set contains any NULL 
values.20 
● Technique 3 (NOT EXISTS): Use a correlated subquery with NOT EXISTS. This is often 
more performant and safer with NULLs than NOT IN.9 
● Example Problem: Find all customers who have never placed an order. 
● Logic (LEFT JOIN): 
1. LEFT JOIN the Customers table to the Orders table on customer_id. 
2. Filter the results WHERE Orders.order_id IS NULL.24 
Section 4.2: Analytical Patterns 
These patterns are central to business intelligence and data science, focusing on time-series 
analysis and trend calculation. 
Rolling Metrics (Moving Averages) 
This pattern involves calculating an aggregate (like an average or sum) over a moving window 
of time (e.g., a 7-day rolling average). 
● Technique: Use an aggregate window function (AVG(), SUM()) with an ORDER BY clause 
to define the sequence and a frame clause (ROWS BETWEEN N PRECEDING AND 
CURRENT ROW) to define the size of the window.6 
● Example Problem: Calculate the 7-day rolling average of daily sales. 
● Logic: 
1. First, ensure you have a table with daily total sales. 
2. If not, create one using a CTE with SUM() and GROUP BY date. 
3. On this daily sales table, apply the window function AVG(daily_sales) OVER (ORDER 
BY date ROWS BETWEEN 6 PRECEDING AND CURRENT ROW). 
Cumulative Metrics (Running Totals) 
This pattern involves calculating the cumulative sum or count of a metric over time. 
● Technique: Use an aggregate window function (SUM(), COUNT()) with an ORDER BY 
clause and the frame clause ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT 
ROW.13 
● Example Problem: For each player, report the cumulative games played so far by date. 
● Logic: 
1. Use the window function SUM(games_played) OVER (PARTITION BY player_id ORDER 
BY event_date). 
2. The default frame for ORDER BY is RANGE BETWEEN UNBOUNDED PRECEDING AND 
CURRENT ROW, which achieves the running total.24 
Period-over-Period Analysis 
This involves comparing a metric from the current period to the same metric from a previous 
period (e.g., month-over-month, year-over-year). 
● Technique: The LAG() window function is the perfect tool. It allows you to pull a value 
from a previous row into the current row, making direct comparison and calculation 
possible.23 
● Example Problem: Calculate the month-over-month percentage growth in sales. 
● Logic: 
1. Create a CTE that aggregates sales by month. 
2. In a second query on the CTE, use LAG(monthly_sales, 1) OVER (ORDER BY month) to 
get the previous month's sales. 
3. Calculate the growth using the formula: 
(current_sales−previous_sales)/previous_sales. 
Section 4.3: Advanced Patterns & Puzzles 
These patterns often appear in interviews for senior roles or at companies with complex data 
challenges. 
They test a candidate's ability to solve non-obvious problems with creative applications of 
SQL features. 
Consecutive Events / Streaks (Gaps and Islands) 
This pattern involves identifying continuous sequences of events (islands) separated by 
breaks (gaps). 
A common variant is finding N consecutive events. 
● Technique: The core trick is to create a grouping identifier for each "island." This is done 
by subtracting a sequence number generated by ROW_NUMBER() from the actual date or 
ID. For consecutive items, this difference will be constant. 
● Example Problem: Find all numbers that appear at least three times consecutively in a 
Logs table. 
● Logic: 
1. A simple solution uses self-joins: SELECT l1.Num FROM Logs l1 JOIN Logs l2 ON l1.Id 
= l2.Id - 1 AND l1.Num = l2.Num JOIN Logs l3 ON l1.Id = l3.Id - 2 AND l1.Num = 
l3.Num.24 
2. A more robust "Gaps and Islands" solution: 
■ Use a CTE to calculate ROW_NUMBER() OVER (ORDER BY id) and 
ROW_NUMBER() OVER (PARTITION BY Num ORDER BY id). 
■ The difference between these two row numbers will be constant for any 
consecutive block of the same number. 
■ Group by this difference and the number itself, and use HAVING COUNT(*) >= 3. 
Sessionization 
This involves grouping a stream of user events into distinct sessions, typically defined by a 
period of inactivity. 
● Technique: Use the LAG() function to get the timestamp of the previous event for a user. 
Calculate the time difference between the current event and the previous one. A CASE 
statement can then create a flag (e.g., set to 1) whenever this difference exceeds the 
inactivity threshold (e.g., 30 minutes). A cumulative sum of this flag then serves as a 
unique session_id for each group of events.23 
● Example Problem: Group user clicks into sessions, where a new session starts after 30 
minutes of inactivity. 
● Logic: 
1. Use a CTE to calculate LAG(event_timestamp, 1) OVER (PARTITION BY user_id ORDER 
BY event_timestamp) to get the previous_event_time. 
2. Use a second CTE to calculate the time difference and set a new_session_flag using 
a CASE statement. 
3. Use a third CTE to create a session_id by calculating SUM(new_session_flag) OVER 
(PARTITION BY user_id ORDER BY event_timestamp). 
4. Finally, group by user_id and session_id to analyze each session. 
Section 4.4: A Curated Gauntlet of Practice Problems 
This section provides a comprehensive set of practice problems, categorized by pattern and 
difficulty, to solidify the concepts learned. 
Each problem includes a detailed breakdown of the logic and the final query. 
(Note: Due to the exhaustive nature of a full problem set, a representative selection is 
provided below. A complete interview preparation would involve working through dozens of 
such problems from the provided resources.) 
Easy Difficulty 
● Problem: LeetCode 182: Duplicate Emails 24 
○ Pattern: Finding Duplicates 
○ Statement: Write a SQL query to find all duplicate emails in a table named Person. 
○ Schema: Person (Id INT, Email VARCHAR) 
○ Logic: This is a direct application of the GROUP BY and HAVING pattern. Group the 
rows by the Email column and then use the HAVING clause to filter for groups where 
the count of rows is greater than 1. 
○ Query: 
SQL 
SELECT Email 
FROM Person 
GROUP BY Email 
HAVING COUNT(Email) > 1; 
● Problem: LeetCode 181: Employees Earning More Than Their Managers 24 
○ Pattern: Analyzing Pairs (Self-Join) 
○ Statement: Given an Employee table with Id, Name, Salary, and ManagerId, find 
employees who earn more than their managers. 
○ Schema: Employee (Id INT, Name VARCHAR, Salary INT, ManagerId INT) 
○ Logic: This requires comparing rows within the same table. A SELF JOIN is the ideal 
pattern. Join the Employee table to itself, aliasing one instance as E (for employee) 
and the other as M (for manager). The join condition will be E.ManagerId = M.Id. The 
WHERE clause then filters for rows where E.Salary > M.Salary. 
○ Query: 
SQL 
SELECT E.Name AS Employee 
FROM Employee E 
JOIN Employee M ON E.ManagerId = M.Id 
WHERE E.Salary > M.Salary; 
Medium Difficulty 
● Problem: LeetCode 176: Second Highest Salary 24 
○ Pattern: Nth-Highest 
○ Statement: Write a SQL query to get the second highest salary from the Employee 
table. If there is no second highest salary, the query should return null. 
○ Schema: Employee (Id INT, Salary INT) 
○ Logic (Subquery): Find the maximum salary in the table. Then, find the maximum 
salary that is less than the overall maximum salary. Using MAX() on a filtered set 
handles cases with ties for the highest salary correctly. 
○ Query (Subquery): 
SQL 
SELECT MAX(Salary) AS SecondHighestSalary 
FROM Employee 
WHERE Salary < (SELECT MAX(Salary) FROM Employee); 
○ Logic (Window Function): Use DENSE_RANK() to rank salaries in descending order. 
Then select the salary where the rank is 2. This approach is more generalizable for 
f
 inding the Nth highest salary. 
○ Query (Window Function): 
SQL 
WITH RankedSalaries AS ( 
SELECT Salary, DENSE_RANK() OVER (ORDER BY Salary DESC) as rnk 
FROM Employee 
) 
SELECT Salary 
FROM RankedSalaries 
WHERE rnk = 2; 
 
● Problem: LeetCode 184: Department Highest Salary 24 
○ Pattern: Top-N per Group (where N=1) 
○ Statement: Find employees who have the highest salary in each department. 
○ Schema: Employee (Id, Name, Salary, DepartmentId), Department (Id, Name) 
○ Logic: This requires finding the maximum salary for each department and then 
finding the employee(s) who match that salary in that department. A window function 
is the cleanest approach. 
○ Query (Window Function): 
SQL 
WITH RankedEmployees AS ( 
    SELECT 
        D.Name AS Department, 
        E.Name AS Employee, 
        E.Salary, 
        RANK() OVER (PARTITION BY E.DepartmentId ORDER BY E.Salary DESC) as rnk 
    FROM Employee E 
    JOIN Department D ON E.DepartmentId = D.Id 
) 
SELECT Department, Employee, Salary 
FROM RankedEmployees 
WHERE rnk = 1; 
 
 
Hard Difficulty 
 
● Problem: LeetCode 185: Department Top Three Salaries 24 
○ Pattern: Top-N per Group 
○ Statement: Find employees who earn the top three salaries in each department. 
○ Schema: Employee (Id, Name, Salary, DepartmentId), Department (Id, Name) 
○ Logic: This is a direct extension of the previous problem. The key is to use 
DENSE_RANK() instead of RANK() to handle ties correctly (e.g., if two employees 
share the 2nd highest salary, the next salary is still the 3rd highest). 
○ Query: 
SQL 
WITH RankedSalaries AS ( 
    SELECT 
        E.*, 
        D.Name as DepartmentName, 
        DENSE_RANK() OVER (PARTITION BY E.DepartmentId ORDER BY E.Salary DESC) as 
rnk 
    FROM Employee E 
    JOIN Department D ON E.DepartmentId = D.Id 
) 
SELECT 
    DepartmentName AS Department, 
    Name AS Employee, 
    Salary 
FROM RankedSalaries 
WHERE rnk <= 3; 
 
● Problem: LeetCode 262: Trips and Users 24 
○ Pattern: Conditional Aggregation 
○ Statement: Find the cancellation rate of requests with unbanned users for each day 
between '2013-10-01' and '2013-10-03'. 
○ Schema: Trips (Id, Client_Id, Driver_Id, Status, Request_at), Users (Users_Id, Banned, 
Role) 
○ Logic: This complex problem requires several steps: 
1. Filter the Trips table for the specified date range. 
2. Filter out trips involving banned clients or banned drivers. This can be done with 
a WHERE clause and subqueries on the Users table. 
3. Group the results by Request_at date. 
4. For each day, calculate the total number of valid requests (COUNT(*)). 
5. For each day, calculate the number of canceled requests using conditional 
aggregation: SUM(CASE WHEN Status LIKE 'cancelled%' THEN 1 ELSE 0 END). 
6. Divide the canceled count by the total count and round to two decimal places. 
○ Query: 
SQL 
SELECT 
    T.Request_at AS Day, 
    ROUND( 
        SUM(CASE WHEN T.Status IN ('cancelled_by_driver', 'cancelled_by_client') THEN 1.0 
ELSE 0.0 END) / COUNT(T.Id), 2 
    ) AS "Cancellation Rate" 
FROM Trips T 
JOIN Users C ON T.Client_Id = C.Users_Id 
JOIN Users D ON T.Driver_Id = D.Users_Id 
WHERE C.Banned = 'No' AND D.Banned = 'No' 
AND T.Request_at BETWEEN '2013-10-01' AND '2013-10-03' 
GROUP BY T.Request_at; 
Part V: Beyond the Query - Performance, 
Optimization, and Architecture 
In interviews for senior roles, writing a correct query is merely the first step. 
The follow-up questions often transition into performance, scalability, and system 
architecture. 
These questions are designed to gauge a candidate's real-world experience and engineering 
mindset. 
A junior candidate can write a query that works; a senior candidate understands why it works, 
how it will perform at scale, and its impact on the broader system. 
Demonstrating this deeper level of thinking can significantly influence the outcome of an 
interview and the level of the subsequent offer.8 
Section 5.1: Writing Performant SQL 
Optimizing a slow-running query is a common task and a frequent interview scenario. 
A structured approach to diagnosis is key. 
Indexes: The Key to Fast Lookups 
An index is a database object that provides a fast lookup path to data in a table, much like an 
index in a book. 
Without an index, the database must perform a full table scan, reading every row to find the 
ones that match the query's conditions. 
With an index, it can directly seek the relevant data pages.12 
● Clustered vs. Non-Clustered Index: 
○ Clustered Index: Determines the physical order of data in a table. Because the data 
can only be physically sorted in one way, a table can have only one clustered index. It 
is highly efficient for range queries.11 
○ Non-Clustered Index: Has a structure separate from the data rows. It contains the 
indexed values and pointers to the actual data rows. A table can have multiple 
non-clustered indexes. They are efficient for specific lookups.11 
● Trade-offs: While indexes dramatically speed up SELECT queries, they slow down data 
modification operations (INSERT, UPDATE, DELETE) because the index itself must also be 
updated.14 
Execution Plans 
An execution plan (or query plan) is the sequence of steps the database query optimizer 
generates to execute a SQL statement. 
Analyzing the execution plan using a command like EXPLAIN is the primary method for 
diagnosing performance bottlenecks. 
It reveals whether indexes are being used effectively, identifies costly operations like full table 
scans or inefficient join methods, and provides cost estimates for each step of the query.13 
Common Optimization Techniques 
● SELECT Specific Columns: Avoid using SELECT *. Specifying only the necessary 
columns reduces the amount of data that needs to be processed and transferred.7 
● Filter Early: Apply WHERE clauses as early as possible to reduce the size of the dataset 
that subsequent operations (like joins and aggregations) have to work with.7 
● Use JOINs Efficiently: Ensure that join conditions are on indexed columns. Understand 
the different join algorithms (e.g., Nested Loop, Hash Join, Merge Join) and how the 
optimizer chooses them. 
● Prefer UNION ALL: If duplicate rows are acceptable or known not to exist, UNION ALL is 
always more performant than UNION because it skips the deduplication step.11 
● Avoid Unnecessary Subqueries: While useful, subqueries (especially correlated ones) 
can sometimes be rewritten as more efficient JOINs or CTEs.7 
Practice Questions 
● Question: Your query is running slow. What are the first steps you would take to 
diagnose the problem? 
○ Answer: 1) Analyze the query execution plan (EXPLAIN) to identify bottlenecks like 
full table scans or expensive joins. 2) Verify that appropriate indexes exist on columns 
used in WHERE clauses and JOIN conditions. 3) Check table statistics to ensure the 
query optimizer has accurate information. 4) Evaluate the query logic for potential 
improvements, such as rewriting subqueries or simplifying complex conditions.8 
● Question: Explain the difference between a clustered and a non-clustered index. 
○ Answer: A clustered index dictates the physical storage order of rows in a table; 
there can be only one. A non-clustered index is a separate structure with pointers to 
the data rows; a table can have many. Clustered indexes are generally faster for 
range scans, while non-clustered indexes are better for point lookups.11 
Section 5.2: Database Architecture and System Design Concepts 
For senior roles, questions may touch upon higher-level database architecture and design 
principles. 
● OLTP vs. OLAP: 
○ Online Transaction Processing (OLTP): These systems are designed to handle a 
large number of short, atomic transactions (e.g., e-commerce order entry, banking 
transactions). They are optimized for fast writes and updates, with highly normalized 
schemas to ensure data integrity.11 
○ Online Analytical Processing (OLAP): These systems are designed for complex 
queries and analysis on large volumes of data (e.g., data warehouses). They are 
optimized for fast reads, often using denormalized schemas (like star or snowflake 
schemas) to minimize joins and speed up aggregations.11 
● ACID Properties: These are a set of four properties that guarantee the reliability of 
database transactions. 
○ Atomicity: Ensures that a transaction is an "all or nothing" operation. Either all of its 
operations complete successfully, or none of them do.11 
○ Consistency: Ensures that a transaction brings the database from one valid state to 
another, upholding all integrity constraints.11 
○ Isolation: Ensures that concurrent transactions do not interfere with each other, 
producing the same result as if they were executed sequentially.11 
○ Durability: Guarantees that once a transaction has been committed, it will remain so, 
even in the event of a system failure.11 
● Database Partitioning: The process of dividing a very large table into smaller, more 
manageable pieces (partitions) while still treating it as a single table logically. This can 
dramatically improve query performance and manageability. 
○ Horizontal Partitioning: Divides a table by rows (e.g., partitioning sales data by 
month or year).13 
○ Vertical Partitioning: Divides a table by columns (e.g., separating frequently 
accessed columns from rarely accessed large text columns).13 
Practice Questions 
● Question: Explain the ACID properties. 
○ Answer: ACID stands for Atomicity (all or nothing), Consistency (database remains in 
a valid state), Isolation (concurrent transactions don't interfere), and Durability 
(committed changes are permanent). Together, they ensure transactions are 
processed reliably.11 
● Question: What is the difference between OLTP and OLAP databases? 
○ Answer: OLTP systems are for managing day-to-day transactions, optimized for 
writes, and highly normalized (e.g., an e-commerce site's live database). OLAP 
systems are for analysis and business intelligence, optimized for reads, and often 
denormalized (e.g., a data warehouse for reporting).11 
Conclusion and Final Recommendations 
This guide has synthesized a vast array of resources into a structured, comprehensive 
curriculum for SQL interview preparation. 
The journey from foundational syntax to advanced analytical patterns and architectural 
considerations reflects the evolving demands of the modern data industry. 
Success in a SQL interview is not merely a function of memorizing answers but of developing 
a deep, intuitive understanding of how to model problems and apply the most effective tools 
to solve them. 
The key takeaways are twofold: 
1. Master the Fundamentals, but Differentiate with the Advanced: A flawless command 
of basic JOINs, aggregations, and filtering is the price of entry. The ability to fluently 
deploy window functions and CTEs to solve complex analytical puzzles is what 
distinguishes a top-tier candidate. A significant portion of preparation time should be 
dedicated to mastering the concepts in Part III and recognizing the patterns in Part IV. 
2. Think in Patterns, Not Just Problems: The most effective preparation strategy is to 
move beyond solving individual, disconnected problems and instead focus on identifying 
and mastering the underlying patterns. Recognizing a question as a "Top-N-per-Group" 
problem or a "Gaps-and-Islands" puzzle immediately narrows the solution space and 
provides a clear path forward, which is a critical advantage under the pressure of an 
interview. 
Actionable Recommendations for the Candidate: 
● Structured Practice: Work through the practice questions in this guide sequentially, 
ensuring a solid grasp of one section before moving to the next. Supplement this with 
active problem-solving on platforms like LeetCode and DataLemur, but always with the 
goal of categorizing each new problem into one of the patterns discussed. 
● Verbalize Your Logic: For every practice problem, articulate the step-by-step logic 
before writing the code. Explain the chosen pattern, the function of each clause, and any 
assumptions made. This practice is invaluable for the live interview setting, where 
communicating the thought process is as important as the final query. 
● Focus on Performance: After solving a problem correctly, always ask the follow-up 
question: "How can I make this more efficient?" Consider indexing strategies, alternative 
query structures (e.g., JOIN vs. EXISTS), and the potential impact of large data volumes. 
Being prepared to discuss optimization demonstrates seniority and real-world 
experience. 
By adopting this structured, pattern-oriented approach, the candidate will be well-equipped 
to not only answer the questions asked but also to demonstrate the analytical rigor and deep 
technical understanding that top companies seek in their data professionals. 
SQL Interview Questions: Complete Study 
Guide 
Based on my research of current SQL interview practices, I've compiled a comprehensive guide 
covering the most common SQL interview questions by syntax and question types. The field of 
SQL interviewing has evolved significantly, with companies now focusing on practical 
problem-solving skills rather than just theoretical knowledge. 
Most Common SQL Interview Question Categories 
1. Basic SQL Fundamentals (Entry Level) 
These questions test your understanding of core database concepts:ccslearningacademy+2 
Database Concepts: 
● What is SQL and its purpose? 
● Difference between SQL and NoSQL databases 
● Understanding of tables, columns, rows, and relationships 
● Primary keys vs foreign keys vs unique keys 
Basic Query Operations: 
● SELECT statements with filtering (WHERE clause) 
● Sorting data (ORDER BY) 
● Basic aggregate functions (COUNT, SUM, AVG, MIN, MAX) 
● Data insertion, updating, and deletion 
2. SQL Joins (Most Critical Topic) 
Joins are considered the most important SQL interview topic across all experience 
levels:datainterview+2 
Join Types You Must Master: 
● INNER JOIN - Returns only matching records 
● LEFT JOIN - Returns all left table records plus matches 
● RIGHT JOIN - Returns all right table records plus matches 
● FULL OUTER JOIN - Returns all records from both tables 
● CROSS JOIN - Cartesian product 
● SELF JOIN - Joining table with itself 
Common Join Interview Questions: 
● Write queries joining 2-3 tables 
● Explain when to use each join type 
● Handle NULL values in joins 
● Optimize join performancedatainterview 
3. Aggregate Functions and GROUP BY 
These questions test your ability to summarize and analyze data:simplilearn+1 
Key Concepts: 
● Using COUNT, SUM, AVG, MIN, MAX effectively 
● GROUP BY for data aggregation 
● HAVING vs WHERE clause differences 
● Combining multiple aggregate functions 
Typical Questions: 
● Find department-wise employee counts 
● Calculate running totals and moving averages 
● Group data by multiple columns 
● Filter grouped results with HAVING 
4. Window Functions (Advanced Level) 
Window functions are increasingly common in technical interviews, especially for senior 
roles:datalemur+2 
Essential Window Functions: 
● ROW_NUMBER() - Assigns sequential numbers 
● RANK() and DENSE_RANK() - Ranking with/without gaps 
● LAG() and LEAD() - Access previous/next row values 
● PARTITION BY - Create data subsets for calculations 
Common Window Function Problems: 
● Find Nth highest salary 
● Calculate year-over-year growth 
● Identify top N records per group 
● Create running totals and moving averagesdatalemur 
5. Subqueries and CTEs 
These test your ability to write complex, nested queries:sqlpad+1 
Subquery Types: 
● Scalar subqueries - Return single values 
● Correlated subqueries - Reference outer query 
● EXISTS and NOT EXISTS clauses 
● Common Table Expressions (CTEs) for readable complex queries 
Typical Applications: 
● Find records above/below averages 
● Complex filtering conditions 
● Multi-step data transformations 
6. Data Types and Constraints 
Understanding data integrity and table design:reddit+2 
Key Constraints: 
● NOT NULL - Prevent empty values 
● UNIQUE - Ensure distinctness 
● PRIMARY KEY - Table identifiers 
● FOREIGN KEY - Referential integrity 
● CHECK - Custom validation rules 
● DEFAULT - Automatic value assignment 
7. Performance Optimization 
Critical for senior-level positions:interviewbit+1youtube 
Optimization Techniques: 
● Index usage and types (clustered vs non-clustered) 
● Query execution plans analysis 
● Avoiding SELECT * statements 
● Proper WHERE clause ordering 
● JOIN optimization strategies 
Common Performance Questions: 
● How to identify slow queries? 
● When and how to create indexes? 
● Query optimization best practices 
● Database design impact on performance 
Classic SQL Problem Types 
1. Ranking Problems 
● Find 2nd/3rd highest salary 
● Top N customers by revenue 
● Rank employees within departments 
2. Duplicate Handling 
● Identify duplicate records 
● Remove duplicates efficiently 
● Find unique vs distinct values 
3. Date and Time Analysis 
● Extract date parts (year, month, day) 
● Calculate date differences 
● Time-based aggregations 
● Sequence and gap analysis 
4. String Manipulation 
● Text processing functions 
● Pattern matching with LIKE 
● String concatenation and extraction 
Interview Preparation Strategy 
By Experience Level: 
Entry Level (0-2 years):codesignal+1 
● Master basic SELECT, WHERE, ORDER BY 
● Understand all join types thoroughly 
● Practice aggregate functions 
● Learn constraint basics 
Mid-Level (2-5 years):dataquest+1 
● Complex multi-table queries 
● Window functions proficiency 
● Subquery optimization 
● Basic performance concepts 
Senior Level (5+ years):stratascratch+1 
● Advanced query optimization 
● Database design principles 
● Complex analytical queries 
● Scalability considerations 
Practice Approach: 
1. Start with fundamentals - Ensure solid grasp of basic concepts 
2. Progress systematically - Build complexity gradually 
3. Practice real scenarios - Use business-like problems 
4. Focus on problem-solving - Explain your thinking process 
5. Optimize solutions - Consider performance implications 
The key to SQL interview success is consistent practice with increasingly complex problems, 
understanding the reasoning behind different approaches, and being able to explain your 
solutions clearly to interviewers. Focus on mastering joins first, as they appear in virtually 
every SQL interview, then build proficiency in window functions and performance optimization 
for more advanced positions.dataquest+1 