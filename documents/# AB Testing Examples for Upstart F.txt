# A/B Testing Examples for Upstart FinTech Interview Preparation

## Example 1: Determining Sample Size for a Loan Application Page Test

**Scenario:** Upstart wants to test two versions of their personal loan application page to see which leads to higher completed applications.

**Current metrics:**
- Current completion rate: 12%
- Minimum important effect to detect: 1.5 percentage point increase (to 13.5%)
- Desired confidence level: 95% (α = 0.05)
- Desired statistical power: 80%

**Problem:** Calculate the required sample size per variant.

**Solution:**
For a two-sided test comparing proportions:

```
n = 2 × (Z₁₋ₐ/₂ + Z₁₋ᵦ)² × p × (1-p) / (p₁ - p₂)²

Where:
- Z₁₋ₐ/₂ = 1.96 (for 95% confidence)
- Z₁₋ᵦ = 0.84 (for 80% power)
- p = (p₁ + p₂)/2 = (0.12 + 0.135)/2 = 0.1275
- (p₁ - p₂)² = (0.135 - 0.12)² = 0.015²
```

Plugging in the values:
```
n = 2 × (1.96 + 0.84)² × 0.1275 × (1-0.1275) / (0.015)²
n = 2 × (2.8)² × 0.1275 × 0.8725 / 0.000225
n = 2 × 7.84 × 0.1275 × 0.8725 / 0.000225
n = 2 × 7.84 × 0.11124375 / 0.000225
n = 2 × 7.84 × 494.4172222
n = 2 × 3876.231
n = 7,752.46
```

Therefore, Upstart would need approximately 7,753 visitors per variant, or a total of 15,506 visitors for the entire test.

## Example 2: Determining Test Duration

**Scenario:** Upstart is testing a new underwriting model against the current model to see if it improves loan approval rates while maintaining acceptable default rates.

**Given information:**
- Average daily loan applications: 1,200
- Required sample size (from prior calculation): 24,000 applications per variant
- Total required sample: 48,000 applications

**Problem:** How long should Upstart run this test?

**Solution:**
```
Test duration = Total required sample / Daily applications
Test duration = 48,000 / 1,200
Test duration = 40 days
```

However, this simple calculation doesn't account for:

1. **Seasonal effects**: Loan application volumes fluctuate throughout the month (higher near paydays) and year (holiday seasons). A 40-day test spanning different parts of months might introduce bias.

2. **External factors**: Major economic news, marketing campaigns, or competitor actions can affect application volume.

**Adjusted recommendation:**
Run the test for a full 6 weeks (42 days) to ensure:
- Complete weekly cycles are captured
- Adequate sample size is achieved
- Beginning and end dates fall on the same day of the week
- Test encompasses a full monthly cycle of applicant behavior

## Example 3: Dealing with Traffic Imbalance Issues

**Scenario:** Upstart is running an A/B test on their landing page for personal loans, but they notice significant traffic differences between weekdays and weekends.

**Problem:** How should Upstart account for this traffic imbalance to ensure valid test results?

**Solution:**
1. **Stratified Sampling**: Ensure each variant receives the same proportion of weekday and weekend traffic.

2. **Time-based Analysis**: Analyze test results separately for different time periods:
```
# Pseudocode for time-based segmentation
for each variant (A and B):
    weekday_conversion_rate = weekday_conversions / weekday_visitors
    weekend_conversion_rate = weekend_conversions / weekend_visitors
    
    # Weighted average based on typical traffic distribution
    overall_rate = (weekday_conversion_rate * 0.75) + (weekend_conversion_rate * 0.25)
```

3. **CUPED Implementation**: Use pre-experiment data to reduce variance:
```
# Simplified CUPED implementation
adjusted_metric = current_conversion_rate - θ * (pre_experiment_metric - average_pre_experiment_metric)

Where θ is the correlation coefficient between pre-experiment and experiment metrics
```

## Example 4: Handling Common A/B Testing Issues in FinTech

**Scenario:** Upstart is testing two different interest rate display methods on their loan offer page. They've encountered several issues during the test.

**Problem 1: Sample Ratio Mismatch**

The test shows 52% of users in variant A and 48% in variant B, despite a planned 50/50 split.

**Solution:**
1. Calculate the Sample Ratio Mismatch (SRM) p-value:
```
# Assuming 10,000 total visitors
# Expected: 5,000 in each variant
# Actual: 5,200 in A, 4,800 in B

# Chi-square test
chi_sq = ((5200-5000)² / 5000) + ((4800-5000)² / 5000)
chi_sq = (40000 / 5000) + (40000 / 5000)
chi_sq = 8 + 8
chi_sq = 16

# With 1 degree of freedom, this gives p < 0.001
```

2. Investigate tracking issues or technical problems causing the imbalance
3. If systemic, restart the test after fixing the issue

**Problem 2: Different Conversion Paths**

Upstart notices some users take multiple sessions to complete a loan application, complicating attribution.

**Solution:**
1. Implement cookie-based tracking to maintain variant assignment across sessions
2. Use a "time to event" analysis (survival analysis) instead of simple conversion rates:
```
# Kaplan-Meier Estimator for survival analysis
S(t) = ∏(nᵢ - dᵢ) / nᵢ

Where:
- S(t) is the survival function
- nᵢ is the number at risk at time i
- dᵢ is the number of events at time i
```

**Problem 3: Simpson's Paradox**

Overall, variant B shows better approval rates, but when segmented by credit score bands, variant A performs better in each segment.

**Solution:**
1. Perform stratified analysis across important segments:
```
# Example credit score bands
segments = ['300-579', '580-669', '670-739', '740-799', '800-850']

for segment in segments:
    variant_A_rate = A_approvals_in_segment / A_applications_in_segment
    variant_B_rate = B_approvals_in_segment / B_applications_in_segment
    print(f"Segment {segment}: A = {variant_A_rate}, B = {variant_B_rate}")
```

2. Investigate if variant assignment correlates with user characteristics
3. Use segmented results for decision-making if a Simpson's Paradox is confirmed

## Example 5: Calculating Expected Value of an A/B Test

**Scenario:** Upstart is testing a new risk model that might increase loan approval rates.

**Given information:**
- Current approval rate: 25%
- Variant B approval rate: 27% (+2 percentage points)
- Average loan amount: $12,000
- Average profit per loan: $800
- Average daily applications: 1,200

**Problem:** Calculate the expected annual value of implementing variant B.

**Solution:**
```
Additional approvals = Total applications × Approval rate increase
Additional approvals = 1,200 × 365 × 0.02
Additional approvals = 8,760 loans per year

Additional profit = Additional approvals × Profit per loan
Additional profit = 8,760 × $800
Additional profit = $7,008,000 per year
```

However, we must consider the increased risk:
```
# If the new model increases default rate by 0.3 percentage points
Additional defaults = Additional approvals × Default rate increase
Additional defaults = 8,760 × 0.003
Additional defaults = 26.28 additional defaults

Cost of defaults = Additional defaults × Average loan amount
Cost of defaults = 26.28 × $12,000
Cost of defaults = $315,360

Net annual value = Additional profit - Cost of defaults
Net annual value = $7,008,000 - $315,360
Net annual value = $6,692,640
```

The expected value of implementing variant B is approximately $6.7 million annually, assuming the test results scale proportionally to the full user base.

## Conclusion

These examples cover fundamental A/B testing concepts specifically related to Upstart's personal loan business. Understanding these principles will help you demonstrate not just statistical knowledge, but how to apply that knowledge to make business decisions in a FinTech context. Remember to emphasize both the statistical rigor and business implications in your interview answers.

# More A/B Testing Scenarios for Upstart

Let me walk you through additional A/B testing examples that would be relevant for Upstart's personal loan business. I'll include detailed scenarios with specific numbers to demonstrate how these tests might be analyzed and interpreted.

## Example 1: Testing Loan Term Presentation

**Scenario:** Upstart wants to test whether showing monthly payment amounts more prominently than total interest impacts loan term selection.

**Test Setup:**
- Control (A): Standard display with equal prominence to loan term, APR, and monthly payment
- Variant (B): Monthly payment displayed in larger font, with a comparison table showing payment differences between terms

**Current metrics:**
- 60% of approved customers choose 3-year terms (36 months)
- 40% of approved customers choose 5-year terms (60 months)
- Average loan amount: $15,000
- Average APR: 12% for 3-year terms, 15% for 5-year terms

**Test Results (after 4 weeks):**
- Control (A): 8,500 customers
  - 5,100 chose 3-year terms (60%)
  - 3,400 chose 5-year terms (40%)
- Variant (B): 8,450 customers
  - 4,395 chose 3-year terms (52%)
  - 4,055 chose 5-year terms (48%)

**Analysis:**
```
# Testing if the difference in proportions is significant
p₁ = 0.60 (proportion choosing 3-year in Control)
p₂ = 0.52 (proportion choosing 3-year in Variant)
p = (5100 + 4395) / (8500 + 8450) = 9495/16950 = 0.56
n₁ = 8500
n₂ = 8450

z = (p₁ - p₂) / √[p(1-p)(1/n₁ + 1/n₂)]
z = (0.60 - 0.52) / √[0.56(0.44)(1/8500 + 1/8450)]
z = 0.08 / √[0.2464(0.000118 + 0.000118)]
z = 0.08 / √[0.2464 × 0.000236]
z = 0.08 / √0.0000582
z = 0.08 / 0.00762
z = 10.5

# z = 10.5 corresponds to p < 0.0001
```

**Business Impact Analysis:**
```
# Current revenue calculation
A_revenue = (5100 × $15,000 × 0.12 × 3) + (3400 × $15,000 × 0.15 × 5)
A_revenue = $27,540,000 + $38,250,000 = $65,790,000

# Variant revenue calculation
B_revenue = (4395 × $15,000 × 0.12 × 3) + (4055 × $15,000 × 0.15 × 5)
B_revenue = $23,733,000 + $45,618,750 = $69,351,750

# Difference
Increased revenue = $69,351,750 - $65,790,000 = $3,561,750
```

**Recommendation:** Implement variant B as it leads to an 8% shift toward 5-year terms, potentially increasing overall revenue by approximately $3.5 million while giving customers lower monthly payments.

## Example 2: Testing Risk-Based Pricing Algorithm

**Scenario:** Upstart is testing a new AI-driven pricing algorithm against their current model.

**Test Setup:**
- Control (A): Current underwriting model
- Variant (B): New AI model incorporating additional behavioral features

**Metrics to track:**
- Approval rate
- Average APR
- Take-rate (% of approved applicants who accept the loan)
- Expected default rate (based on historical data)

**Test Results (after 5,000 applicants per variant):**

| Metric | Control (A) | Variant (B) | Difference |
|--------|-------------|-------------|------------|
| Approval Rate | 32.0% | 35.5% | +3.5% |
| Average APR | 14.2% | 13.8% | -0.4% |
| Take-rate | 70.5% | 74.8% | +4.3% |
| Est. Default Rate | 5.4% | 5.5% | +0.1% |

**Statistical Analysis:**
```
# Approval rate comparison
p₁ = 0.32 (Control)
p₂ = 0.355 (Variant)
p = (1600 + 1775) / 10000 = 0.3375
n₁ = n₂ = 5000

z = (0.355 - 0.32) / √[0.3375(0.6625)(1/5000 + 1/5000)]
z = 0.035 / √[0.2236(0.0004)]
z = 0.035 / √0.0000894
z = 0.035 / 0.00946
z = 3.70 (p < 0.001)

# Similar calculations show statistical significance for take-rate difference
```

**Expected Value Calculation:**
```
# For Control (A):
Applications = 100,000
Approvals = 100,000 × 0.32 = 32,000
Accepted loans = 32,000 × 0.705 = 22,560
Average loan amount = $14,000
Total loan volume = 22,560 × $14,000 = $315,840,000
Revenue (APR) = $315,840,000 × 0.142 = $44,849,280
Defaults = 22,560 × 0.054 = 1,218 loans
Default cost = 1,218 × $14,000 = $17,052,000
Net revenue = $44,849,280 - $17,052,000 = $27,797,280

# For Variant (B):
Applications = 100,000
Approvals = 100,000 × 0.355 = 35,500
Accepted loans = 35,500 × 0.748 = 26,554
Total loan volume = 26,554 × $14,000 = $371,756,000
Revenue (APR) = $371,756,000 × 0.138 = $51,302,328
Defaults = 26,554 × 0.055 = 1,460 loans
Default cost = 1,460 × $14,000 = $20,440,000
Net revenue = $51,302,328 - $20,440,000 = $30,862,328

# Difference in net revenue
Improvement = $30,862,328 - $27,797,280 = $3,065,048
```

**Recommendation:** The new AI model shows a statistically significant improvement in approval rate and take-rate, with only a slight increase in default rate. The expected annual net revenue improvement is over $3 million, justifying implementation of the new model.

## Example 3: Testing Income Verification Methods

**Scenario:** Upstart is testing a simplified income verification process against their traditional method.

**Test Setup:**
- Control (A): Traditional method requiring paystubs or W-2 forms
- Variant (B): Bank transaction analysis with optional document upload

**Current metrics:**
- Income verification completion rate: 78%
- Average verification time: 36 hours
- Average application-to-funding time: 5.2 days
- Verification error rate (discrepancy >10% from stated income): 8.5%

**Test Results (4,000 applications per variant):**

| Metric | Control (A) | Variant (B) | Difference |
|--------|-------------|-------------|------------|
| Completion Rate | 77.8% | 89.5% | +11.7% |
| Avg. Verification Time | 35.2 hrs | 5.8 hrs | -29.4 hrs |
| App-to-Funding Time | 5.1 days | 3.8 days | -1.3 days |
| Verification Error Rate | 8.3% | 9.1% | +0.8% |

**Statistical Analysis for Completion Rate:**
```
p₁ = 0.778 (Control)
p₂ = 0.895 (Variant)
p = (3112 + 3580) / 8000 = 0.8365
n₁ = n₂ = 4000

z = (0.895 - 0.778) / √[0.8365(0.1635)(1/4000 + 1/4000)]
z = 0.117 / √[0.1368(0.0005)]
z = 0.117 / √0.0000684
z = 0.117 / 0.00827
z = 14.15 (p < 0.0001)
```

**Impact Analysis:**
```
# Assuming 200,000 annual applications
Additional completed verifications = 200,000 × 0.117 = 23,400
Average loan amount = $16,000
Conversion rate (verified to funded) = 60%
Additional funded loans = 23,400 × 0.6 = 14,040
Additional loan volume = 14,040 × $16,000 = $224,640,000
Average profit margin = 6%
Additional profit = $224,640,000 × 0.06 = $13,478,400

# Operational savings
Time saved per verification = 29.4 hours
Cost per hour of verification = $25
Savings per verification = 29.4 × $25 = $735
Total verifications with new system = 200,000 × 0.895 = 179,000
Total operational savings = 179,000 × $735 = $13,156,500

# Potential cost of increased verification errors
Additional errors = 200,000 × 0.895 × (0.091 - 0.083) = 1,432
Average cost per error = $500
Total error cost = 1,432 × $500 = $716,000

# Net benefit
Total benefit = $13,478,400 (additional profit) + $13,156,500 (operational savings) - $716,000 (error cost) = $25,918,900
```

**Recommendation:** Implement the new verification system as it significantly reduces friction in the application process, leading to higher completion rates and faster funding times. The estimated annual benefit of over $25 million far outweighs the slight increase in verification errors.

## Example 4: Testing Application Form Length

**Scenario:** Upstart is testing a shortened application form against their standard form.

**Test Setup:**
- Control (A): Standard 18-field application form
- Variant (B): Streamlined 12-field form, with some fields moved to post-approval

**Current metrics:**
- Application start-to-submission rate: 62%
- Average time to complete: 8.5 minutes
- Application bounce rate: 28%

**Test Results (10,000 visitors per variant):**

| Metric | Control (A) | Variant (B) | Difference |
|--------|-------------|-------------|------------|
| Start-to-submission | 61.5% | 73.2% | +11.7% |
| Avg. Completion Time | 8.3 mins | 5.6 mins | -2.7 mins |
| Bounce Rate | 29.1% | 21.8% | -7.3% |
| Approval Rate | 34.2% | 33.8% | -0.4% |

**Statistical Analysis:**
```
# Start-to-submission comparison
p₁ = 0.615 (Control)
p₂ = 0.732 (Variant)
p = (6150 + 7320) / 20000 = 0.6735
n₁ = n₂ = 10000

z = (0.732 - 0.615) / √[0.6735(0.3265)(1/10000 + 1/10000)]
z = 0.117 / √[0.2199(0.0002)]
z = 0.117 / √0.000044
z = 0.117 / 0.00663
z = 17.65 (p < 0.0001)
```

**Funnel Analysis:**
```
# For Control (A):
Visitors: 10,000
Started applications: 7,090 (100% - 29.1% bounce)
Completed applications: 6,150 (61.5% of total)
Approved loans: 2,103 (34.2% of completed)

# For Variant (B):
Visitors: 10,000
Started applications: 7,820 (100% - 21.8% bounce)
Completed applications: 7,320 (73.2% of total)
Approved loans: 2,474 (33.8% of completed)

# Conversion improvement
Increase in approved loans = 2,474 - 2,103 = 371 (+17.6%)
```

**ROI Calculation:**
```
# Assuming 500,000 annual visitors
Additional approved loans = 500,000 × (2,474 - 2,103) / 10,000 = 18,550
Average loan amount = $18,000
Average profit per loan = $1,080 (6% of loan amount)
Additional annual profit = 18,550 × $1,080 = $20,034,000

# Implementation cost
Development and testing: $150,000
Risk analysis: $50,000
Total cost: $200,000

# ROI
First year ROI = ($20,034,000 - $200,000) / $200,000 = 99.17x
```

**Recommendation:** Implement the streamlined application form immediately. The shorter form substantially increases completion rates with only a minimal decrease in approval rate, leading to a significant net increase in approved loans. The projected ROI is extremely high, and the improved user experience aligns with Upstart's goal of making credit more accessible.

## Example 5: Testing Credit Score Display and Education

**Scenario:** Upstart is testing whether showing applicants their credit score with educational content affects loan performance.

**Test Setup:**
- Control (A): Standard process with no credit score disclosure
- Variant (B): Show applicants their credit score with personalized tips

**Test Duration:** 6 months with 3-month performance tracking

**Test Results (7,500 loans per variant):**

| Metric | Control (A) | Variant (B) | Difference |
|--------|-------------|-------------|------------|
| 30-day Delinquency | 2.8% | 2.4% | -0.4% |
| 90-day Delinquency | 1.5% | 1.2% | -0.3% |
| Customer Support Contacts | 0.8/loan | 0.6/loan | -0.2/loan |
| Repeat Application Rate | 15.2% | 18.7% | +3.5% |

**Statistical Analysis for 30-day Delinquency:**
```
p₁ = 0.028 (Control)
p₂ = 0.024 (Variant)
p = (210 + 180) / 15000 = 0.026
n₁ = n₂ = 7500

z = (0.028 - 0.024) / √[0.026(0.974)(1/7500 + 1/7500)]
z = 0.004 / √[0.02532(0.000267)]
z = 0.004 / √0.00000677
z = 0.004 / 0.0026
z = 1.54 (p = 0.12)
```

This doesn't reach statistical significance at α = 0.05.

**For 90-day Delinquency:**
```
p₁ = 0.015 (Control)
p₂ = 0.012 (Variant)
p = (112.5 + 90) / 15000 = 0.0135
n₁ = n₂ = 7500

z = (0.015 - 0.012) / √[0.0135(0.9865)(1/7500 + 1/7500)]
z = 0.003 / √[0.01332(0.000267)]
z = 0.003 / √0.00000356
z = 0.003 / 0.00189
z = 1.59 (p = 0.11)
```

Also not statistically significant at α = 0.05.

**Impact Analysis:**
```
# Assuming the observed differences would become significant with larger sample
Average loan amount = $20,000
Portfolio size = 100,000 loans
Annual value of 30-day delinquency reduction:
- Reduction in delinquent loans = 100,000 × 0.004 = 400 loans
- Average collections cost per delinquent loan = $120
- Total savings = 400 × $120 = $48,000

Annual value of 90-day delinquency reduction:
- Reduction in seriously delinquent loans = 100,000 × 0.003 = 300 loans
- Average loss given default = $8,000 per loan
- Total savings = 300 × $8,000 = $2,400,000

Annual value of increased repeat applications:
- Additional repeat applications = 100,000 × 0.035 = 3,500
- Conversion rate for repeat applications = 50%
- Average profit per repeat loan = $1,200
- Total additional profit = 3,500 × 0.5 × $1,200 = $2,100,000

Customer support savings:
- Reduction in contacts = 100,000 × 0.2 = 20,000 contacts
- Average cost per contact = $15
- Total savings = 20,000 × $15 = $300,000

Total annual benefit = $48,000 + $2,400,000 + $2,100,000 + $300,000 = $4,848,000
```

**Recommendation:** While the delinquency reductions haven't reached statistical significance yet, the combined benefits of increased repeat business and reduced customer support contacts make this a promising feature. Continue monitoring delinquency metrics while implementing the credit score display and education feature, as the potential upside ($4.8 million annually) far outweighs the implementation costs.

These examples demonstrate various A/B testing scenarios specifically relevant to Upstart's personal loan business. They cover key aspects like user experience, underwriting models, operational efficiency, and customer education—all critical components for a successful FinTech company.
