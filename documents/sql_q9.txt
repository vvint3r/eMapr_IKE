11 Airbnb SQL Interview Questions - Can You Solve Them?
ByNick Singh
(Ex-Facebook & Best-Selling Data Science Author)
Nick’s previously held Software & Data roles at Facebook, Google, & SafeGraph (a geospatial analytics startup).
Currently, he’s the best-selling author of Ace the Data Science Interview, and Founder & CEO of DataLemur.
Nick’s also active on LinkedIn, where he shares career tips with his 160,000+ followers.
Learn More About Nick SinghUpdated on
April 30, 2025
At Airbnb, SQL is used day-to-day for analyzing customer behavior to improve property recommendations and monitoring system performance for seamless booking experiences. Unsurprisingly this is why Airbnb LOVES to ask SQL problems in interviews for Data Analyst, Data Science, and BI jobs.
In case you want to ace the SQL Assessment, we've curated 11 Airbnb SQL interview questions to practice, which are similar to recently asked questions at Airbnb – able to answer them all?
11 Airbnb SQL Interview Questions
SQL Question 1: Booking Referral Source
The strategy team in Airbnb is trying to analyze the impact of Covid-19 during 2021. To do so, they need you to write a query that outputs the average vacant days across the AirBnbs in 2021. Some properties have gone out of business, so you should only analyze rentals that are currently active. Round the results to a whole number.
Assumptions:
1 field equals to 1 when the property is active, and 0 otherwise.
2 In cases where the check-in or check-out date is in another year other than 2021, limit the calculation to the beginning or end of the year 2021 respectively.
3 Listing can be active even if there are no bookings throughout the year.
Table:
Column NameTypelisting_id integer checkin_date date checkout_date date
Example Input:
listing_id checkin_date checkout_date 1 08/17/2021 00:00:00 08/19/2021 00:00:00 1 08/19/2021 00:00:00 08/25/2021 00:00:00 2 08/19/2021 00:00:00 09/22/2021 00:00:00 3 12/23/2021 00:00:00 01/05/2022 00:00:00
Table:
Column NameTypelisting_id integer is_active integer
Example Input:
listing_id is_active 1 1 2 0 3 1
Example Output:
avg_vacant_days 357
Explanation:
1 Property 1 was rented for 8 days, thus the property has 365 - 8 = 357 vacant days.
2 Property 2 is excluded as it is not active.
3 Property 3 was rented out for 12 days, thus the property as 365 - 12 = 353 vacant days.
Average vacant days are 355 days. (357 + 353 / 2).
The dataset you are querying against may have different input & output - this is just an example!
Answer:
To solve this question join DataLemur Premium to try this Airbnb SQL interview question:
SQL Question 2: Analyzing Monthly Average Ratings of Airbnb Property Listings
Given the table with columns: , , , , , write a SQL query to get the average rating of each Airbnb property listing per month. The column represents when the review was submitted. The column represents the unique ID of the Airbnb property, and represents the rating given by the user where 1 is the lowest and 5 is the highest rating.
Example Input:
review_iduser_idsubmit_datelisting_idstars6171 123 01/02/2022 00:00:00 50001 4 7802 265 01/15/2022 00:00:00 69852 4 5293 362 01/22/2022 00:00:00 50001 3 6352 192 02/05/2022 00:00:00 69852 3 4517 981 02/10/2022 00:00:00 69852 2
Answer:
This SQL query will first group the data by month and listing_id. For each group, it calculates the average rating. The function is used to get the month from . The function is used to calculate the average stars rating. In the end, it orders the results by and .
Example Output:
mthlisting_idavg_stars1 50001 3.50 1 69852 4.00 2 69852 2.50
p.s. Window functions show up super frequently during SQL interviews, so practice the 27+ window function questions on DataLemur
SQL Question 3: Average Vacant Days
The strategy team in Airbnb is trying to analyze the impact of Covid-19 during 2021. To do so, they need you to write a query that outputs the average vacant days across the AirBnbs in 2021. Some properties have gone out of business, so you should only analyze rentals that are currently active. Round the results to a whole number.
Assumptions:
1 field equals to 1 when the property is active, and 0 otherwise.
2 In cases where the check-in or check-out date is in another year other than 2021, limit the calculation to the beginning or end of the year 2021 respectively.
3 Listing can be active even if there are no bookings throughout the year.
Table:
Column NameTypelisting_id integer checkin_date date checkout_date date
Example Input:
listing_id checkin_date checkout_date 1 08/17/2021 00:00:00 08/19/2021 00:00:00 1 08/19/2021 00:00:00 08/25/2021 00:00:00 2 08/19/2021 00:00:00 09/22/2021 00:00:00 3 12/23/2021 00:00:00 01/05/2022 00:00:00
Table:
Column NameTypelisting_id integer is_active integer
Example Input:
listing_id is_active 1 1 2 0 3 1
Example Output:
avg_vacant_days 357
Solution:
To solve this question on DataLemur's free interactive site, try this Airbnb SQL interview question:
SQL Question 4: Retrieve Housing Data from Specific Cities
You're a data analyst at Airbnb and you've been tasked with retrieving housing data from specific cities. You want to find all Airbnb listings in San Francisco and New York that have at least 10 reviews and an average rating equal to or above 4.5.
Assume you have two tables: a table with the ID of the housing, its name, city, and the total number of reviews; and a table with the ID of the listing, the review ID, the rating, and the date submitted.
Example Input:
listing_idnamecityreviews_count10001 "Central Loft" "San Francisco" 15 10002 "Cozy Apartment" "New York" 20 10003 "Sunny Studio" "San Francisco" 8 10004 "Stylish Suite" "Las Vegas" 13 10005 "Dreamy Duplex" "New York" 5
Example Input:
listing_idreview_idstarssubmit_date10001 15001 4.5 2022-06-08 10001 15002 5.0 2022-06-10 10002 15003 4.0 2022-06-18 10002 15004 5.0 2022-07-26 10003 15005 3.5 2022-07-05 10004 15006 4.5 2022-06-08 10005 15007 3.0 2022-06-10
Answer:
The above query works as follows:
1 We join the table (alias 'l') with the table (alias 'r') using the as the common column between the two tables.
2 In the WHERE clause, we filter the cities to "San Francisco" and "New York", and the to be at least 10.
3 In the GROUP BY clause, we group by . This allows us to calculate the average rating for each listing.
4 In the HAVING clause, we filter the average rating to be at least 4.5.
By joining the tables, filtering, grouping, and running the average function, we're able to get the desired listings in those cities with the specific review conditions.
SQL Question 5: What is the purpose of the SQL constraint ?
The UNIQUE constraint is used to ensure the uniqueness of the data in a column or set of columns in a table. It prevents the insertion of duplicate values in the specified column or columns and helps to ensure the integrity and reliability of the data in the database.
For example, say you were on the Marketing Analytics team at Airbnb and were doing some automated keyword research:
Your keyword database might store SEO data like this:
In this example, the UNIQUE constraint is applied to the "keyword" field to ensure that each keyword is unique. This helps to ensure the integrity of the data in the database and prevents errors that could occur if two rows had the same keyword.
SQL Question 6: Find the Average Number of Guests per Booking in Each City for Airbnb
As an analyst at Airbnb, one of the most useful insights you could provide would be to understand the average number of guests per booking across locations. For this question, we would like you to write a SQL query that will find the average number of guests per booking in each city.
Example Input:
booking_idproperty_idguestsbooking_date101 4523 3 01/01/2022 102 9871 2 01/05/2022 103 4523 4 02/10/2022 104 7452 1 02/20/2022 105 9871 3 03/01/2022
Example Input:
property_idcity4523 New York 9871 Los Angeles 7452 Chicago
Example Output:
cityaverage_guestsNew York 3.5 Los Angeles 2.5 Chicago 1.0
Answer:
In this query, we first join the and tables on , allowing us to access both the and columns in the same query. We then group by , so we get a separate row for each city in our output. For each group, we calculate the average number of guests.
To practice a very similar question try this interactive Robinhood Cities With Completed Trades Question which is similar for requiring SQL analysis grouped by city or this Amazon Average Review Ratings Question which is similar for needing an average calculation.
SQL Question 7: What's the difference between relational and non-relational databases?
While both types of databases are used to store data (no duh!), relational databases and non-relational (also known as NoSQL databases) differ in a few important ways, most importantly on the way data is stored. Relational databases use a data model consisting of tables and rows, while NoSQL databases use a variety of data models, including document, key-value, columnar, and graph storage formats.
This added flexibilty makes NoSQL databases great for non-tabular data (like hierarchal data or JSON data), or data where the type/format is constantly evolving. With this added flexibility, comes one big weakness – you won't get ACID-compliance. That means, unlike relational databases which are typically adhere to the ACID properties (atomic, consistent, isolated, and durable), you don't get as strong guarentees with most non-relational databases.
SQL Question 8: Analyzing click-through rates for Airbnb Listing Views and Bookings
The scenario is that Airbnb wants to analyze the click-through conversion rates (CTRs) of their listings. The CTR is calculated by dividing the number of bookings by the number of listing views, giving a proportion of views that resulted in a booking.
Consider you have two tables: one showing all the views for a listing ( ) and another one showing all bookings ( ).
Example Input:
view_iduser_idvisit_datelisting_id101 10 7/08/2022 1001 102 12 7/08/2022 1002 103 14 7/09/2022 1001 104 10 7/10/2022 1003 105 13 7/11/2022 1002
Example Input:
booking_iduser_idbooking_datelisting_id201 10 7/09/2022 1001 202 12 7/10/2022 1002 203 15 7/12/2022 1003 204 13 7/13/2022 1002 205 12 7/14/2022 1001
The question is to write a SQL query to find the CTR for every unique listing in July 2022.
Answer:
Here is a PostgreSQL query answer:
The query utilises two subqueries ( and ) to calculate the counts of views and bookings respectively for each listing for the month of July. The final query joins these subqueries via a LEFT JOIN (to include those listings which were viewed but never booked), and calculates the CTR as . The function is used to avoid division by zero error.
To practice a similar SQL problem on DataLemur's free online SQL coding environment, solve this Meta SQL interview question:
SQL Question 9: The Most Popular City for Airbnb Stays
As a data analyst for Airbnb, you've been asked to determine the city that has had the most bookings (reservations) in the past year. You are given two tables - a 'bookings' table with booking IDs, user IDs, listing IDs, and booking dates, and a 'listings' table with listing IDs, city locations, and host IDs.
Provide a SQL query that returns the city with the maximum number of bookings, along with the number of bookings.
Example Input:
booking_iduser_idlisting_idbooking_date101 123 50001 06/08/2022 102 265 69852 06/10/2022 103 362 50001 06/18/2022 104 192 69852 07/26/2022 105 981 69852 07/05/2022
Example Input:
listing_idcityhost_id50001 Amsterdam 876 69852 Barcelona 974
Answer:
This query works by joining the 'bookings' table with the 'listings' table on the shared 'listing_id'. After joining, the 'city' in the 'listings' table is grouped by, and for each city, the number of bookings is counted (by counting the 'booking_id' in the 'bookings' table). The results are then ordered by the number of bookings in descending order, and finally, the top result (the city with the most bookings) is selected. undefined
SQL Question 10: What's the SQL command do, and when would you use it?
The SQL command merges the results of multiple statements and keeps only those rows that are present in all sets.
For example, say you were doing an HR Analytics project for Airbnb, and had access to Airbnb's employees and contractors data. Assume that some employees were previously contractors, and vice versa, and thus would show up in both tables. You could use operator to find all contractors who also show up in the employees table:
SQL Question 11: Analyze Host Listings and Booking Transactions
As an Airbnb data analyst, you have been asked to analyze the performance of hosts' listings in the past year. Your task is to identify the top 10 listings with the most bookings.
There are two tables involved. One is the 'hosts' table that provides details about each host and its respective listing. The other one is 'bookings' table that records each booking transaction.
The 'hosts' table:
host_idlisting_idlisting_namecity101 201 Penthouse New York 102 202 Ocean View San Francisco 103 203 Country House Austin
The 'bookings' table:
booking_iduser_idlisting_idbooking_date301 401 201 06/08/2022 302 402 202 06/10/2022 303 403 202 07/10/2022 304 404 203 08/08/2022 305 405 201 09/08/2022
Remarks:
Both tables can be joined on the 'listing_id' field.
Answer:
This query first joins the 'hosts' and 'bookings' tables based on the 'listing_id' field. It then groups the result by 'listing_id' and 'listing_name' from the 'hosts' table. For each group, it calculates the total number of bookings, and finally sorts the groups based on this count in descending order. It returns the top 10 listings with the most bookings.

---

28 SQL interview questions and answers from beginner to senior level
By Team CodeSignalWhether you’re just starting your career as a developer, data scientist, or business analyst—or you have a few years of experience under your belt—using structured query language (SQL) is a core skill for a wide range of roles that involve database management, data analysis, and back-end development. And, it pays to have mastery in this querying language: SQL developers in the US earn an average salary of $116,000 per year, according to Glassdoor. With practice and preparation, you can showcase strong SQL skills in your coding interview and stand out to potential employers.
This guide is designed to help you prepare for SQL technical interviews by providing 28 example questions that cover a wide range of topics, from common SQL functions to complex query optimization. These questions mimic the types of challenges you’ll face in a technical assessment or a live coding interview, giving you the practice you need to perform your best in a high-stakes environment.
To take your interview prep to the next level, try using CodeSignal Learn—a practice-based learning platform that helps you prepare for interviews and build technical skills, including SQL, with support from a friendly AI tutor. By reviewing the questions in this guide alongside practicing skills in CodeSignal Learn, you’ll be well-equipped to tackle your next interview with confidence and secure the role you’ve been working towards.
Jump to a section:
1 How to use this guide to prepare for your SQL coding interview
2 What to expect from an SQL technical screening
3 Basic SQL interview questions for beginners (0 to 2 years of experience)
4 Intermediate SQL interview questions (2 to 5 years of experience)
5 Advanced SQL interview questions (5 years experience or more)
6 Hard SQL server interview questions for senior developers (10+ years of experience)
7 SQL performance tuning interview questions
8 Role-based SQL interview questions
9 Scenario-based SQL interview questions
10 Common SQL interview questions (if you have limited time to practice)
11 Next steps & resources
How to use this guide to prepare for your SQL coding interview
You can use this guide of 28 example SQL interview questions and answers as a tool to prepare for your upcoming coding interview. Start by setting clear goals for your interview prep and identify specific areas where you need to improve. Use these questions to assess your current SQL skills, and then implement focused practice strategies to strengthen any weak areas. SQL interviews often differ from other coding interviews by emphasizing data management and query optimization, so tailor your preparation accordingly.
What you will need to start practicing these SQL interview questions
To start practicing these SQL interview questions effectively, you’ll need a few key resources and strategies. Here’s what you should have in place:
1 SQL tutorial resources: Use online tutorials and courses to refresh your knowledge of essential SQL concepts.
2 Practice SQL environments: Set up a local database or use online platforms that allow you to write and test SQL queries.
3 SQL reference materials: Keep a handy guide or documentation to quickly look up SQL syntax and functions as you practice.
4 Time management: Allocate specific times in your schedule for focused SQL practice sessions.
5 Feedback mechanisms: Seek feedback from peers, mentors, or use automated tools to review your SQL query performance and identify areas for improvement.
What to expect from an SQL technical screening
During an SQL technical screening, you can expect a format that tests your ability to handle common SQL tasks like writing queries, optimizing database performance, and ensuring data integrity. The technical interviewer will be looking for you to take a clear SQL problem-solving approach that demonstrates both your technical skills and your understanding of best practices. You’ll be evaluated based on your accuracy, efficiency, and ability to explain your thought process, so it’s important to be prepared to discuss your reasoning.
Basic SQL interview questions for beginners (0 to 2 years of experience)
Basic SQL data types and simple SELECT query
Question: Write a SQL query that retrieves the `first_name`, `last_name`, and `email` columns from a table named `users`, where the `email` domain is “example.com”. Assume that `email` is a `VARCHAR` type.
Example Answer:
SELECT first_name, last_name, email FROM users WHERE email LIKE '%@example.com';
Explanation: This query selects the `first_name`, `last_name`, and `email` columns from the `users` table and filters the rows to include only those with an email domain of “example.com”. The `LIKE` operator is used with a wildcard (`%`) to match any characters before “@example.com”.
SQL joins and relationships
Question: Write a SQL query to retrieve the `order_id` and `order_date` from an `orders` table and the `product_name` from a `products` table for all orders. Assume that the `orders` table has a `product_id` foreign key that references the `product_id` in the `products` table.
Example Answer:
SELECT o.order_id, o.order_date, p.product_name FROM orders o JOIN products p ON o.product_id = p.product_id;
Explanation: This query retrieves data from both the `orders` and `products` tables using an `INNER JOIN`. The `JOIN` is performed on the `product_id` column, which is common between the two tables, allowing the query to combine rows from each table where there is a matching `product_id`.
Basic data manipulation
Question: Write a SQL query to update the `salary` column in the `employees` table, increasing it by 10% for all employees who work in the “Sales” department. Assume the `department` column is of type `VARCHAR`.
Example Answer:
UPDATE employees SET salary = salary * 1.10 WHERE department = 'Sales';
Explanation: This query updates the `salary` field in the `employees` table by multiplying the current salary by 1.10 (a 10% increase) for all employees in the “Sales” department. The `WHERE` clause ensures that only rows where the `department` is “Sales” are affected.
Learning tip: Want to review SQL basics before your next interview?Journey into SQL with Taylor Swiftis a fun and accessible learning path in CodeSignal Learn where you’ll practice key querying skills using Taylor Swift’s discography as your database.
Intermediate SQL interview questions (2 to 5 years of experience)
Complex SQL queries and subqueries
Question: Write a SQL query to find the top 3 customers with the highest total `order_amount` from the `orders` table. Assume that each order is linked to a customer via a `customer_id` column, and the `order_amount` is a numeric column.
Example Answer:
SELECT customer_id, SUM(order_amount) AS total_spent FROM orders GROUP BY customer_id ORDER BY total_spent DESC LIMIT 3;
Explanation: This query calculates the total `order_amount` spent by each customer using the `SUM()` function and groups the results by `customer_id`. The `ORDER BY` clause sorts the results in descending order of total spent, and the `LIMIT` clause restricts the output to the top 3 customers. This type of query is essential for analyzing customer behavior and identifying high-value customers.
Subqueries and data integrity
Question: Write a SQL query to find all employees in the `employees` table whose `salary` is greater than the average salary in their department. Assume that the table has `employee_id`, `department_id`, and `salary` columns.
Example Answer:
SELECT employee_id, department_id, salary FROM employees e WHERE salary > ( SELECT AVG(salary) FROM employees WHERE department_id = e.department_id );
Explanation: This query uses a subquery to calculate the average salary within each department. The main query then selects employees whose salary exceeds the average salary of their respective department. The use of correlated subqueries (where the subquery references a column from the outer query) is a powerful technique for comparing data within grouped contexts.
Indexes, performance, and transaction control
Question: Suppose you need to delete a large number of records from the `transactions` table where the `transaction_date` is older than one year. Write a SQL script that includes steps to ensure the deletion is efficient and doesn’t affect the performance of the database during the operation. Assume an index exists on the `transaction_date` column.
Example Answer:
BEGIN; SET TRANSACTION ISOLATION LEVEL READ COMMITTED; DELETE FROM transactions WHERE transaction_date < NOW() - INTERVAL '1 year'; COMMIT;
Explanation: This script begins with a `BEGIN` statement to start a transaction. The `SET TRANSACTION ISOLATION LEVEL` command ensures that the operation uses the appropriate isolation level to prevent reading data that has been modified but not committed by other transactions (dirty reads), improving performance during the deletion. The `DELETE` operation then removes records older than one year, leveraging the existing index on `transaction_date` for faster execution. Finally, the `COMMIT` statement ensures that all changes are saved permanently, maintaining data integrity and consistency.
Learning tip: Refresh your SQL scripting skills before your next interview or assessment with the Learning SQL Scripting with Leo Messi learning path in CodeSignal Learn. Practice joins, functions, conditional logic, and more using stats from soccer star Lionel Messi’s career as your database.
Advanced SQL interview questions (5 years experience or more)
SQL optimization techniques and handling large datasets
Question: You have a table `large_sales` with millions of rows and a composite index on `(customer_id, sale_date) named `idx_customer_date`. Write an optimized SQL query to retrieve the total sales amount for each `customer_id` in the year 2023, considering the potential performance impact due to the dataset size.
Example Answer:
SELECT customer_id, SUM(sale_amount) AS total_sales FROM large_sales WHERE sale_date BETWEEN '2023-01-01' AND '2023-12-31' GROUP BY customer_id USE INDEX (idx_customer_date);
Explanation: This query retrieves the total sales amount for each `customer_id` for the year 2023 from a very large dataset. By specifying the `USE INDEX` hint, the query explicitly directs the database to utilize the composite index on `(customer_id, sale_date)` to optimize the filtering and grouping operations instead of an index on just `sale_date`. This is crucial for maintaining performance when dealing with large datasets, as it minimizes the amount of data scanned.
Advanced data modeling and stored procedures
Question: Design a stored procedure named `UpdateEmployeeDepartment` that transfers an employee to a new department while ensuring that the new department’s `budget` is not exceeded. Assume that `employees` and `departments` tables exist, with `employees` containing `employee_id`, `department_id`, and `salary`, and `departments` containing `department_id`, `budget`, and `current_expenditure`.
Example Answer:
DELIMITER // CREATE PROCEDURE UpdateEmployeeDepartment(IN emp_id INT, IN new_dept_id INT) BEGIN DECLARE emp_salary DECIMAL(10,2); DECLARE current_expenditure DECIMAL(10,2); DECLARE dept_budget DECIMAL(10,2); SELECT salary INTO emp_salary FROM employees WHERE employee_id = emp_id; SELECT current_expenditure, budget INTO current_expenditure, dept_budget FROM departments WHERE department_id = new_dept_id; IF current_expenditure + emp_salary <= dept_budget THEN UPDATE employees SET department_id = new_dept_id WHERE employee_id = emp_id; UPDATE departments SET current_expenditure = current_expenditure + emp_salary WHERE department_id = new_dept_id; ELSE SIGNAL SQLSTATE '45000' SET MESSAGE_TEXT = 'Budget exceeded for the new department'; END IF; END // DELIMITER ;
Explanation: This stored procedure first retrieves the salary of the employee being transferred and the budget and current expenditure of the target department. It then checks if adding the employee’s salary to the department’s current expenditure would exceed the department’s budget. If not, the employee is transferred, and the department’s expenditure is updated. If the budget would be exceeded, the procedure raises an error, ensuring budget constraints are respected. This approach demonstrates advanced data modeling by handling complex relationships between entities in the database.
Database architecture considerations and triggers
Question: Write a trigger named `CheckInventoryBeforeInsert` that prevents the insertion of a new order in the `orders` table if the total quantity of items ordered exceeds the available stock in the `inventory` table. Assume the `orders` table has `product_id` and `quantity` columns, and the `inventory` table has `product_id` and `stock_quantity` columns.
Example Answer:
DELIMITER // CREATE TRIGGER CheckInventoryBeforeInsert BEFORE INSERT ON orders FOR EACH ROW BEGIN DECLARE available_stock INT; SELECT stock_quantity INTO available_stock FROM inventory WHERE product_id = NEW.product_id; IF NEW.quantity > available_stock THEN SIGNAL SQLSTATE '45000' SET MESSAGE_TEXT = 'Insufficient stock for the product'; END IF; END // DELIMITER ;
Explanation: This trigger executes before a new order is inserted into the `orders` table. It checks if the quantity being ordered exceeds the available stock in the `inventory` table. If the order quantity is greater than the available stock, the trigger prevents the insert operation by raising an error. This ensures that the database maintains data integrity and consistency, crucial for systems where inventory management is critical. It also reflects an understanding of how triggers can enforce business rules at the database level, which is a key consideration in robust database architecture.
Hard SQL server interview questions for senior developers (10+ years of experience)
High-availability solutions and disaster recovery strategies
Question: Can you describe a high-availability solution for an SQL Server environment, and how you would implement a disaster recovery plan to minimize downtime and data loss?
Example Answer: I would use Always On Availability Groups for high availability, setting up primary and secondary replicas across different servers, ideally in separate geographic locations. The primary replica handles transactions, while secondary replicas are kept in sync.
For disaster recovery, I’d configure a secondary replica in a remote data center with automatic failover. This setup ensures minimal downtime and no data loss if the primary server fails. I’d also establish regular backups and test the failover process to ensure reliability.
Performance tuning complex systems
Question: Can you walk me through your approach to diagnosing and resolving performance issues in a complex SQL Server system with multiple large databases?
Example Answer: I start by analyzing wait statistics to find bottlenecks like CPU or I/O issues. Then, I examine query execution plans to spot inefficiencies, such as unnecessary table scans.
For optimization, I may tune indexes, rewrite queries, or partition large tables. I also check system configurations, such as memory and I/O settings, and ensure regular maintenance tasks like index rebuilding are in place to keep performance stable.
Security best practices in SQL server management
Question: What are some of the security best practices you follow when setting up and managing SQL Server databases?
Example Answer: I follow the principle of least privilege, assigning minimal permissions needed for tasks. I integrate SQL Server with Active Directory for secure authentication and use encryption for sensitive data with tools like Transparent Data Encryption (TDE).
I also ensure SQL Server is regularly patched and perform security audits to monitor for unauthorized access. Regular reviews of activity logs help me quickly detect and respond to any security issues.
SQL performance tuning interview questions
Query optimization and execution plans analysis
Question: How do you approach optimizing a slow-running query in SQL Server, and what role do execution plans play in this process?
Example Answer: When optimizing a slow query, I start by analyzing its execution plan to identify bottlenecks like full table scans or expensive joins. The execution plan shows how SQL Server processes the query, helping me spot inefficiencies.
Based on the plan, I might rewrite the query, add or modify indexes, or adjust the query structure to reduce processing time. I continually review the updated execution plan to ensure the changes improve performance.
Index management and query optimization
Question: Can you explain your process for managing indexes to ensure efficient query performance in SQL Server?
Example Answer: I regularly monitor index usage to identify underutilized or missing indexes. If a query is slow, I check the execution plan to see if an index could improve performance.
I also evaluate existing indexes to ensure they are not redundant or overlapping, which could cause unnecessary overhead. Periodically, I perform index maintenance, such as rebuilding or reorganizing fragmented indexes, to keep the database performing optimally.
SQL server profiler and database tuning advisor
Question: How do you use SQL Server Profiler and Database Tuning Advisor to enhance database performance?
Example Answer: I use SQL Server Profiler to capture and analyze slow-running queries or resource-intensive operations. The trace data helps me identify patterns and specific queries that need optimization.
Then, I run these queries through the Database Tuning Advisor, which provides recommendations for indexing, partitioning, and other optimizations. This combination allows me to make data-driven decisions to enhance performance while avoiding guesswork.
Role-based SQL interview questions
SQL developer interview questions
Development environment setup and debugging SQL scripts
Question: Write a SQL script that sets up a development environment by creating a new schema named `dev_environment`, and within that schema, create a table `test_data` with columns `id` (INT, primary key) and `value` (VARCHAR). Then, include a statement to debug by inserting a sample record into the `test_data` table and verifying that the record was correctly inserted.
Example Answer:
CREATE SCHEMA dev_environment; CREATE TABLE dev_environment.test_data ( id INT PRIMARY KEY, value VARCHAR(100) ); INSERT INTO dev_environment.test_data (id, value) VALUES (1, 'Sample Data'); -- Debugging step: Check the inserted record SELECT * FROM dev_environment.test_data WHERE id = 1;
Explanation: This script first creates a new schema named `dev_environment` to organize the development environment. It then creates a `test_data` table within that schema with an `id` column as the primary key and a `value` column for storing text data. The script includes a sample `INSERT` statement to add a record to the `test_data` table and a `SELECT` statement to verify that the insertion was successful. This approach helps in setting up a consistent development environment while also incorporating basic debugging practices.
Code versioning in SQL and best practices in database schema design
Question: Write a SQL script to create a version-controlled stored procedure that adds a new column `email` (VARCHAR) to an existing `users` table. Include comments that explain the purpose of the changes and a method to rollback the change if needed.
Example Answer:
-- Version 1.1: Adding an email column to users table -- Purpose: To store email addresses of users ALTER TABLE users ADD email VARCHAR(255); -- Rollback script: Remove the email column if the change needs to be undone -- Version 1.1 Rollback -- Purpose: To rollback the addition of the email column in case of issues -- ALTER TABLE users -- DROP COLUMN email;
Explanation: This script demonstrates best practices in code versioning and schema design. It includes an `ALTER TABLE` statement to add an `email` column to the `users` table, following a versioning format in the comments to track changes. The comments clearly explain the purpose of the update. Additionally, the script provides a rollback mechanism (commented out) to remove the `email` column if the change needs to be undone, promoting safe and controlled schema changes.
SQL interview questions for data analysts
SQL for data extraction and analytical functions in SQL
Question: Write a SQL query that extracts the total sales and calculates the average sales per month for each product in the `sales` table. The table contains `product_id`, `sale_date`, and `sale_amount` columns. Use SQL analytical functions to achieve this.
Example Answer:
WITH monthly_sales AS ( SELECT product_id, EXTRACT(YEAR FROM sale_date) AS sale_year, EXTRACT(MONTH FROM sale_date) AS sale_month, SUM(sale_amount) AS monthly_total_sales FROM sales GROUP BY product_id, EXTRACT(YEAR FROM sale_date), EXTRACT(MONTH FROM sale_date) ) SELECT product_id, SUM(monthly_total_sales) AS total_sales, AVG(monthly_total_sales) AS avg_monthly_sales FROM monthly_sales GROUP BY product_id;
Explanation: This query uses SQL analytical functions to calculate the total sales and the average monthly sales for each product. The `SUM(sale_amount)` function aggregates the sales by `product_id`, month, and year. The `AVG()` function calculates the average of these monthly totals. This allows for a detailed analysis of sales patterns across products on a monthly basis.
Advanced reporting techniques and data visualization with SQL
Question: Write a SQL query to generate a report that shows the cumulative sales by month for the current year for each region. The `sales` table includes `region`, `sale_date`, and `sale_amount` columns. Ensure the report is ordered by region and month.
Example Answer:
SELECT region, EXTRACT(MONTH FROM sale_date) AS sale_month, SUM(sale_amount) AS monthly_sales, SUM(SUM(sale_amount)) OVER (PARTITION BY region ORDER BY EXTRACT(MONTH FROM sale_date)) AS cumulative_sales FROM sales WHERE EXTRACT(YEAR FROM sale_date) = EXTRACT(YEAR FROM CURRENT_DATE) GROUP BY region, EXTRACT(MONTH FROM sale_date) ORDER BY region, sale_month;
Explanation: This query produces an advanced report that shows both monthly and cumulative sales by region for the current year. The `SUM(sale_amount)` function calculates the monthly sales per region. The cumulative sales are calculated using `SUM(SUM(sale_amount)) OVER (PARTITION BY region ORDER BY EXTRACT(MONTH FROM sale_date))`, which sums the monthly totals progressively. The report is ordered by region and then by month, making it useful for visualizations that track sales trends across regions over time.
SQL interview questions for data engineers
ETL processes and data quality + cleaning
Question: Write a SQL script that performs an ETL (Extract, Transform, Load) process to clean and load data from a `raw_sales` table into a `cleaned_sales` table. The `raw_sales` table contains `sale_id`, `sale_date`, `product_id`, `sale_amount`, and `customer_id`, where `sale_amount` may contain null or negative values. Clean the data by removing rows with null or negative `sale_amount`, and load the cleaned data into the `cleaned_sales` table.
Example Answer:
-- Step 1: Extract and Clean Data INSERT INTO cleaned_sales (sale_id, sale_date, product_id, sale_amount, customer_id) SELECT sale_id, sale_date, product_id, sale_amount, customer_id FROM raw_sales WHERE sale_amount IS NOT NULL AND sale_amount > 0; -- Step 2: Optional additional transformations can be applied here
Explanation: This script performs a basic ETL operation by extracting data from the `raw_sales` table, cleaning it by removing rows where `sale_amount` is null or negative, and then loading the cleaned data into the `cleaned_sales` table. This ensures that only valid sales data is stored in the `cleaned_sales` table, improving data quality for further analysis or reporting.
Data warehousing with SQL and SQL in data pipeline design
Question: Design a SQL query that aggregates daily sales data from a `daily_sales` table and loads it into a `monthly_sales_summary` table. The `daily_sales` table contains `sale_date`, `product_id`, and `sale_amount`. The `monthly_sales_summary` table should store `year`, `month`, `product_id`, and `total_sales`.
Example Answer:
-- Step 1: Aggregate Daily Sales into Monthly Totals INSERT INTO monthly_sales_summary (year, month, product_id, total_sales) SELECT EXTRACT(YEAR FROM sale_date) AS year, EXTRACT(MONTH FROM sale_date) AS month, product_id, SUM(sale_amount) AS total_sales FROM daily_sales GROUP BY EXTRACT(YEAR FROM sale_date), EXTRACT(MONTH FROM sale_date), product_id; -- Step 2: This data can now be used for reporting or further analysis
Explanation: This query aggregates daily sales data into monthly totals, which are then stored in the `monthly_sales_summary` table. The `EXTRACT(YEAR FROM sale_date)` and `EXTRACT(MONTH FROM sale_date)` functions are used to group the data by year and month. The `SUM(sale_amount)` function calculates the total sales per product for each month. This process is a common step in data warehousing, where data is aggregated and summarized for more efficient storage and faster querying.
Scenario-based SQL interview questions
Real-world problem-solving with SQL and handling corrupt data
Question: Can you describe how you would handle a situation where you find corrupt data in a critical production table, such as missing or invalid values in key columns?
Example Answer: If I encounter corrupt data in a production table, my first step would be to identify the extent of the corruption by running queries that check for anomalies like nulls in non-nullable columns or invalid data types. Once identified, I would create a backup of the affected data to ensure we have a recovery point.
Next, I’d isolate the problematic records and attempt to correct them, either by referencing backup data, if available, or by applying business rules to regenerate the correct values. If the corruption is widespread, I might consider restoring the table from a backup, followed by reapplying any subsequent valid changes. I would also investigate the root cause to prevent future occurrences, possibly by adding constraints or triggers to enforce data integrity.
Optimizing slow-running queries and simulating concurrency scenarios
Question: How would you approach optimizing a slow-running query in a high-traffic database, especially considering potential concurrency issues?
Example Answer: I would start by analyzing the query execution plan to identify inefficiencies like table scans, missing indexes, or suboptimal join operations. If the issue is related to indexing, I would add or adjust indexes to reduce the query’s execution time. Additionally, I’d consider query refactoring to eliminate unnecessary complexity.
Given the high-traffic environment, I’d also assess the query’s impact on concurrency. For example, I would check for locking or blocking issues that could be slowing down the system and might use techniques like query hints or isolation level adjustments to minimize contention. Finally, I would test the optimized query in a staging environment under simulated load to ensure that it performs well and doesn’t introduce new concurrency issues.
SQL for data migration tasks
Question: Can you walk me through your process for migrating large datasets from one SQL Server to another, ensuring minimal downtime and data integrity?
Example Answer: In a large-scale data migration, my first step is to plan and document the migration process, including identifying dependencies, assessing data volume, and estimating downtime. I usually start by performing a full backup of the source database to ensure we have a recovery point.
To minimize downtime, I’d consider using techniques like log shipping or database mirroring to keep the target database up-to-date with changes made during the migration process. Before the final cutover, I’d perform a series of test migrations on a staging environment to verify that the data is correctly transferred and that the target environment functions as expected.
During the final migration, I’d carefully monitor the process, validating data integrity through checksums or row counts, and ensure that all necessary application connections are redirected to the new server. Post-migration, I’d run thorough tests to confirm everything is working correctly and that there are no data integrity issues.
Learning tip: Practice interview skills for behavioral interviews, recruiter screens, and panel interviews in CodeSignal Learn’s Behavioral Interview Practice for CS Students learning path. Engage in live mock interviews with an advanced AI agent and get immediate feedback on your performance from our AI tutor and guide, Cosmo.
Common SQL interview questions (if you have limited time to practice)
Essential SQL functions
Question: Write a SQL query to calculate the total number of orders and the average order amount from an `orders` table. The table contains columns `order_id`, `order_date`, and `order_amount`.
Example Answer:
SELECT COUNT(order_id) AS total_orders, AVG(order_amount) AS average_order_amount FROM orders;
Explanation: This query uses two essential SQL aggregate functions: `COUNT()` and `AVG()`. The `COUNT(order_id)` function calculates the total number of orders, while `AVG(order_amount)` calculates the average order amount across all orders. These functions are fundamental for summarizing data and generating insights from an SQL table.
SQL debugging
Question: You’ve written a query that doesn’t return the expected results. Describe how you would debug the issue, assuming you are dealing with a simple `SELECT` statement.
Example Answer:
-- Original query SELECT * FROM customers WHERE last_name = 'Smith'; -- Debugging steps -- 1. Check if the condition is too restrictive or misspelled SELECT * FROM customers WHERE last_name LIKE '%Smith%'; -- 2. Verify the data SELECT DISTINCT last_name FROM customers; -- 3. Test a simplified version of the query SELECT * FROM customers WHERE 1 = 1; -- 4. Check for case sensitivity issues (if the database is case-sensitive) SELECT * FROM customers WHERE LOWER(last_name) = 'smith'; -- 5. Ensure there are no leading/trailing spaces SELECT * FROM customers WHERE TRIM(last_name) = 'Smith';
Explanation: The debugging process involves several steps. First, I’d check if the condition might be too restrictive or if there’s a typo by using a broader condition, like `LIKE`. Then, I’d verify the data by querying distinct values to see if the data matches the expected condition. Next, I’d run a simplified version of the query (`WHERE 1 = 1`) to confirm the basic query structure is sound. If your database is case-sensitive, Smith and smith would be treated differently. To avoid case sensitivity issues, you can use LOWER(last_name) = ‘smith’ or UPPER(last_name) = ‘SMITH’. Finally, data might have leading or trailing spaces that affect the match. Using TRIM(last_name) = ‘Smith’ ensures that such spaces are removed before comparison. These steps help quickly identify common issues.
Efficient query writing and key SQL clauses
Question: Write an efficient SQL query to retrieve all unique product names from a `products` table that has a `product_name` column, and ensure the results are sorted alphabetically.
Example Answer:
SELECT DISTINCT product_name FROM products ORDER BY product_name ASC;
Explanation: This query retrieves all unique product names using the `DISTINCT` clause, ensuring that no duplicates appear in the results. The `ORDER BY` clause sorts the product names alphabetically (`ASC`). Using `DISTINCT` in combination with `ORDER BY` is a common practice to write efficient queries that provide meaningful, well-organized results.
Critical performance factors
Question: Given a `sales` table with millions of records, write an optimized SQL query to retrieve the total sales amount for each `region` from the current year. The table includes `sale_id`, `region`, `sale_date`, and `sale_amount` columns.
Example Answer:
SELECT region, SUM(sale_amount) AS total_sales FROM sales WHERE EXTRACT(YEAR FROM sale_date) = EXTRACT(YEAR FROM CURRENT_DATE) GROUP BY region;
Explanation: This query efficiently calculates the total sales amount for each `region` by limiting the dataset to the current year using the `EXTRACT(YEAR FROM sale_date)` function in the `WHERE` clause. The `SUM(sale_amount)` function aggregates the sales for each `region`, and the `GROUP BY` clause organizes the results by region. This approach optimizes performance by reducing the data processed and ensures that the query scales well with large datasets.
Next steps & resources
---

Financial and Legal Services30 Common Upstart Interview Questions & Answers
Prepare for your interview at Upstart with commonly asked interview questions and example answers and advice from experts in the field.
Career CoachPublished Jun 19, 2024
Preparing for an interview at Upstart is crucial for showcasing your skills and aligning yourself with the company’s mission of improving financial access for all. As a pioneering fintech company, Upstart values innovation, critical thinking, and a deep understanding of their technology-driven solutions.
In this article, we’ll explore common interview questions and effective answers tailored for Upstart’s unique culture and goals. By preparing thoroughly, you’ll increase your chances of making a strong impression and securing a position at this forward-thinking organization.
Upstart Overview
Upstart is a financial technology company that leverages artificial intelligence and machine learning to provide personal loans and other credit products. The company aims to improve access to affordable credit by using non-traditional variables, such as education and employment, to assess creditworthiness. Upstart partners with banks and credit unions to offer its services, striving to reduce the risk and cost associated with lending. The platform seeks to provide a more inclusive and efficient lending process, benefiting both borrowers and financial institutions.
Upstart Hiring Process
The hiring process at Upstart typically begins with an initial screening by a recruiter, followed by a series of interviews. Candidates often experience a phone interview, a Zoom interview, and multiple rounds with different team members, including technical screens and behavioral questions.
Many candidates report a structured process with clear communication and support from HR, while others have experienced delays, rescheduling, and lack of follow-up. Technical interviews can include coding challenges, system design questions, and project-based assessments.
Candidates should be prepared for a potentially lengthy process with multiple stages. While some candidates appreciate the thoroughness and professionalism, others have noted issues such as unresponsive recruiters and unprofessional behavior during interviews.
Overall, preparation for both technical and behavioral questions, along with patience for potential delays, will be key for navigating Upstart’s hiring process.
Common Upstart Interview Questions
1. How do you assess the creditworthiness of a potential borrower?
Assessing the creditworthiness of a potential borrower delves into understanding a blend of quantitative and qualitative factors that predict financial behavior. Beyond just looking at traditional credit scores, one must consider income stability, debt-to-income ratios, employment history, and even softer metrics like spending habits and financial goals. This comprehensive approach allows for a more accurate prediction of a borrower’s ability to repay and aligns with modern financial practices, which increasingly prioritize a holistic view over a single metric.
How to Answer: When answering, emphasize your ability to synthesize various data points to form a complete picture of financial reliability. Discuss your experience with advanced analytics or alternative data sources, and how these tools can provide deeper insights into a borrower’s financial profile. For instance, mentioning how Upstart utilizes machine learning models to incorporate non-traditional variables can demonstrate your understanding of innovative credit assessment methods and your readiness to contribute to cutting-edge financial solutions.
Example: “I always start by looking beyond just the traditional credit score. I like to dive deeper into their financial history and look at factors like income stability, debt-to-income ratio, and spending habits. I find that having a conversation with the borrower can also provide a lot of insight—understanding their personal circumstances, future earning potential, and any mitigating factors can paint a much fuller picture.
For instance, when I worked at a small financial firm, we had a client with a mediocre credit score but a strong and consistent freelance income. By analyzing his bank statements and contracts, we could see he had a reliable income stream even though it didn’t fit the traditional mold. This allowed us to extend credit to him confidently, which ended up being a great decision for both parties. This holistic approach ensures we’re making more informed and fair lending decisions.”
2. Describe your approach to debugging a complex software issue.
Addressing how you approach debugging a complex software issue reveals your problem-solving skills, technical expertise, and ability to remain composed under pressure. In a fast-paced, innovation-driven environment like Upstart, the ability to efficiently diagnose and resolve software issues is paramount. This question seeks to understand not only your technical proficiency but also your methodology in breaking down intricate problems, your persistence in finding solutions, and your ability to learn and adapt from each debugging experience.
How to Answer: Articulate a structured approach by outlining how you identify and isolate the problem, mentioning tools or methodologies you use. Discuss collaborative efforts with team members and how you leverage their insights. Highlight your ability to maintain clear documentation throughout the process and your focus on root cause analysis to prevent future issues. By showcasing a thorough, methodical approach, you demonstrate your readiness to handle the complex challenges encountered at Upstart.
Example: “I start by reproducing the issue in a controlled environment to ensure it’s not a fluke. Once I can reliably replicate it, I break down the problem into smaller components and systematically check each one. I use logging and debugging tools to track the flow of data and identify where things go wrong. Communication is key too—I often consult with team members who might have faced similar issues or have deeper knowledge in a specific area.
I remember dealing with a persistent bug in a payment processing system. After isolating the problem to a specific module, I discovered it was a race condition that only occurred under heavy load. By adding proper synchronization and stress-testing the fix, I was able to ensure the system’s stability. The key is to stay methodical and collaborative—complex issues are rarely solved in isolation.”
3. Explain how you would optimize an existing loan approval algorithm.
Optimizing an existing loan approval algorithm involves a deep understanding of both the technical and financial aspects that drive the success of such a system. This question is designed to gauge your ability to balance accuracy with efficiency, ensuring that the algorithm not only minimizes risk for the company but also maximizes customer satisfaction and inclusivity. At a company like Upstart, which leverages artificial intelligence and machine learning to improve credit decisions, the focus is on innovation and continuous improvement. Your response should reflect an awareness of these priorities, demonstrating your ability to utilize data science techniques, understand regulatory compliance, and adapt to evolving market conditions.
How to Answer: Outline a clear, structured approach to optimization. Mention techniques like feature engineering, model tuning, or incorporating alternative data sources to improve predictive accuracy. Discuss the importance of A/B testing and monitoring the algorithm’s performance over time to ensure it meets both business objectives and ethical standards. Highlight any experience you have with similar systems, and emphasize collaboration with cross-functional teams to ensure that the optimized algorithm aligns with broader company goals.
Example: “First, I would start by examining the current performance metrics of the algorithm to identify any bottlenecks or areas for improvement. This means looking at approval rates, processing times, and default rates, among other key indicators. Then, I would gather feedback from the underwriting team to understand any qualitative insights they might have about the algorithm’s performance.
My next step would involve diving into the data set to identify patterns or anomalies that the current algorithm might be missing. For example, if we’re seeing a high default rate in a specific demographic, I would investigate whether additional variables or improved weighting could help refine our risk assessment. I would also consider implementing machine learning techniques to continuously learn from new data and improve accuracy over time.
Once I have a clear understanding of the areas that need improvement, I would run A/B tests with proposed changes to ensure that any modifications lead to better outcomes without introducing new risks. Throughout this process, maintaining clear documentation and open communication with all stakeholders would be crucial to ensure everyone is aligned and informed about the changes being implemented.”
4. How do you handle the prioritization of multiple loan applications with tight deadlines?
Effectively managing the prioritization of multiple loan applications with tight deadlines is crucial in a fast-paced and dynamic environment. This question delves into a candidate’s ability to handle stress, manage time efficiently, and make judicious decisions under pressure. It also touches on their organizational skills and their ability to maintain a high level of accuracy and customer service while juggling multiple priorities. For a company like Upstart, which relies heavily on data-driven decision-making and efficient processing to maintain competitiveness, showcasing these abilities is essential.
How to Answer: Highlight strategies you use to prioritize tasks, such as categorizing applications by urgency or complexity and setting clear, achievable goals for each day. Discuss tools or systems you employ to stay organized and efficient, such as project management software or scheduling techniques. Illustrate your answer with examples from past experiences where you successfully managed competing priorities, demonstrating your ability to stay calm under pressure and deliver quality results on time.
Example: “I rely on a combination of meticulous planning and adaptability. First, I assess all the applications to identify any that have critical, time-sensitive requirements or issues that could cause delays. These get flagged for priority. Then, I use project management tools to create a timeline, breaking down tasks into manageable steps and assigning deadlines.
In a previous role, I had to manage multiple grant applications simultaneously. I consistently communicated with my team and stakeholders to keep everyone on the same page, and I made sure to leave some buffer time for unexpected challenges. If something urgent came up, I was ready to pivot and reallocate resources. This approach ensured that all applications were submitted on time without compromising quality.”
5. Discuss your experience in using Python for data analysis.
Understanding a candidate’s experience with Python for data analysis reveals not only their technical proficiency but also their approach to problem-solving and analytical thinking. Companies like Upstart seek candidates who can efficiently manipulate data, derive meaningful insights, and contribute to predictive models. Python’s versatility in handling large datasets, performing statistical analysis, and visualizing results makes it an essential tool in such environments. Demonstrating a solid grasp of Python shows you can contribute to complex projects and improve operational efficiency through data insights.
How to Answer: Discuss specific projects where you utilized Python for data analysis. Mention the tools and libraries you used, such as Pandas for data manipulation, NumPy for numerical operations, and Matplotlib or Seaborn for visualization. Emphasize the impact of your analysis on the project’s outcomes, such as improved decision-making, increased efficiency, or enhanced predictive accuracy. Providing context around the challenges you faced and how you overcame them can further illustrate your problem-solving skills and adaptability.
Example: “At my previous role, I was part of a team tasked with analyzing customer behavior data to improve our marketing strategies. We used Python extensively for this. One of the most impactful projects I worked on was developing a customer segmentation model using pandas and scikit-learn. We pulled data from our SQL database, cleaned it up using pandas, and then ran clustering algorithms to identify distinct customer groups.
I also created several custom visualizations using matplotlib and seaborn to present our findings to the marketing team, who didn’t have a technical background. This helped them understand which segments were most profitable and how to tailor campaigns to each group. The insights we gained led to a 15% increase in our campaign conversion rates within the first quarter of implementation.”
6. What strategies would you implement to improve customer satisfaction in a fintech environment?
Achieving high levels of customer satisfaction in a fintech environment requires a multifaceted approach that addresses both technological and human elements. Understanding the intricacies of financial technology is crucial; customers often deal with complex, high-stakes transactions that demand accuracy, security, and seamless user experiences. Additionally, fintech companies must navigate stringent regulatory frameworks and evolving consumer expectations. For a company like Upstart, which leverages AI to provide personalized financial solutions, strategies must not only focus on traditional customer service metrics but also on leveraging data analytics to anticipate customer needs and streamline their journey. This involves a deep understanding of customer pain points, proactive communication, and continuous improvement based on feedback.
How to Answer: Highlight strategies that blend technology and human touch, such as implementing advanced AI-driven chatbots for instant support and using customer data to personalize services. Discuss how you would foster a culture of continuous improvement by regularly analyzing customer feedback and metrics to identify areas for enhancement. Emphasize the importance of training customer service teams to handle complex inquiries with empathy and efficiency. Mention any relevant experiences where you successfully boosted customer satisfaction through innovative solutions.
Example: “First, I’d focus on enhancing user experience by simplifying the interface and ensuring that all processes, from loan applications to customer service inquiries, are intuitive and user-friendly. Clear instructions, easy navigation, and quick access to support are crucial.
Next, I’d invest in robust customer support training. Customer service reps should be well-versed in our products and empathetic to customer needs. I’d also implement regular feedback loops where we gather customer insights through surveys and direct feedback, and then use that data to make informed improvements. At a previous company, I spearheaded a similar initiative that not only increased our NPS score but also helped us identify and address pain points more proactively. This multifaceted approach should lead to higher customer satisfaction and loyalty.”
7. Describe a time when you had to analyze large datasets to make business decisions.
Analyzing large datasets to inform business decisions is a crucial skill in data-driven environments like Upstart. This question delves into your ability to not only handle vast amounts of information but also extract actionable insights that drive strategic initiatives. The emphasis is on your analytical thinking, proficiency with data tools, and your approach to problem-solving. Demonstrating a methodical and results-oriented mindset is essential because it reflects your capacity to contribute to data-centric projects and ultimately enhance business performance.
How to Answer: Provide a specific example where you successfully navigated complex datasets to arrive at a significant decision. Highlight the tools and methodologies you employed, the challenges you faced, and the impact of your decision. Illustrate your logical approach, attention to detail, and ability to translate data findings into practical business strategies. This showcases your technical skills, strategic thinking, and business acumen.
Example: “At my last job at a retail analytics firm, I was tasked with identifying trends in customer purchasing behavior to help a client optimize their inventory management. They were dealing with overstock in some categories and stockouts in others, which was affecting their bottom line.
I dove into their sales data and used SQL to pull out patterns and insights. One of the key findings was that certain seasonal items were consistently overstocked because the forecasting model hadn’t been updated in years. I created a more dynamic forecasting model using Python, which incorporated recent purchase data and seasonality factors. Once implemented, the client saw a 15% reduction in overstock and a 10% increase in sales due to better stock availability. This not only improved their inventory management but also increased customer satisfaction by ensuring popular items were always in stock.”
8. How do you ensure compliance with financial regulations in your work?
Ensuring compliance with financial regulations is essential in any role within the financial technology sector, as it not only protects the company from legal repercussions but also builds trust with customers and stakeholders. For a company like Upstart, which innovates in the lending space, adherence to financial regulations is paramount to maintaining credibility and operational integrity. This question delves into your understanding of regulatory landscapes, your attention to detail, and your ability to integrate compliance seamlessly into your daily responsibilities. It also reflects on your proactive measures to stay updated with evolving regulations, demonstrating your commitment to upholding the company’s standards and ethical practices.
How to Answer: Discuss specific protocols and systems you’ve implemented or adhered to in previous roles to ensure compliance. Highlight any experience with regulatory audits, internal controls, and continuous education on financial laws. Mention how you stay informed about changes in regulations and how you communicate these updates within your team. Emphasize your ability to balance innovation with regulatory compliance, showcasing your capability to contribute to the company’s growth while safeguarding its legal and ethical standing.
Example: “For me, ensuring compliance with financial regulations starts with staying updated on all relevant laws and guidelines. I subscribe to industry newsletters, attend webinars, and participate in professional development courses to keep my knowledge current. When I’m working on a project, I create a checklist of key compliance points we need to hit, and I make sure that everyone on the team is aware of these requirements from the get-go.
In a previous role at a fintech startup, I helped establish an internal audit process to regularly review our operations for compliance. This involved collaborating with our legal team to ensure we interpreted regulations correctly and implementing automated tools that flagged potential compliance issues in real time. By fostering a culture of continuous learning and proactive checks, we successfully navigated complex regulations and avoided any compliance issues.”
9. How would you handle a situation where a client disputes a payment transaction?
Handling disputes over payment transactions requires a blend of technical knowledge, customer empathy, and problem-solving skills. Companies like Upstart prioritize these attributes because they directly impact client trust and satisfaction. Resolving such disputes effectively not only ensures compliance with financial regulations but also maintains the integrity of client relationships. The ability to navigate these situations smoothly reflects on the company’s commitment to transparency and customer service excellence, which are crucial for retaining clients in a competitive market.
How to Answer: Emphasize your methodical approach to understanding the client’s concerns, your ability to stay calm under pressure, and your knowledge of relevant policies and procedures. Describe a process that includes listening to the client’s issue, verifying transaction details, consulting relevant documentation, and providing a clear, empathetic resolution. Highlighting any past experience with similar disputes can also demonstrate your capability and reliability in managing such critical situations.
Example: “First, I’d ensure the client felt heard by carefully listening to their concerns without interrupting. Acknowledging their frustration is key to defusing any immediate tension. Then, I would gather all relevant details about the disputed transaction, such as dates, amounts, and any correspondence or documentation they could provide.
Once I had all the information, I’d review our internal records to identify any discrepancies or errors. If the issue stemmed from our side, I’d apologize and promptly correct it, ensuring the client is updated at every step. If the transaction was accurate, I’d explain the findings clearly and calmly, providing any supporting documentation to back up our position. Throughout the process, maintaining a transparent and empathetic approach is crucial to preserving the client relationship and ensuring they feel valued and respected.”
10. Describe your approach to performance tuning in software applications.
Performance tuning in software applications is about more than just optimizing code; it’s about ensuring the entire system operates efficiently under various conditions. At a company like Upstart, where data-driven decisions and real-time processing are crucial, performance tuning becomes a vital aspect of delivering seamless user experiences and maintaining system reliability. Interviewers are looking to understand your depth of knowledge in diagnosing bottlenecks, employing profiling tools, and implementing optimization techniques. They want to see your ability to balance performance with other critical factors such as maintainability, scalability, and security, which are essential for sustaining complex, high-impact applications.
How to Answer: Explain your systematic approach to performance tuning. Discuss methods you use to identify performance issues, such as profiling and monitoring tools, and describe how you prioritize and implement optimizations. Provide examples of past experiences where your tuning efforts led to significant performance gains, and highlight how you collaborated with other team members to ensure that improvements were aligned with overall project goals.
Example: “The first thing I do is establish a baseline by profiling the application to identify bottlenecks. This involves looking at CPU usage, memory consumption, and I/O operations. Once I know where the issues are, I prioritize them based on their impact on overall performance.
In a recent project, I noticed that our database queries were slowing down the entire system. I worked with the team to optimize those queries and implemented indexing where it was missing. We also realized that some of our functions were doing redundant calculations, so I refactored the code to make it more efficient. We saw a significant reduction in response times and an overall improvement in user experience. Finally, I continuously monitor the application to catch any new issues early and ensure that our performance remains optimal.”
11. Explain a time when you had to implement a new feature based on user feedback.
Understanding how candidates handle user feedback is essential. Implementing new features based on user feedback demonstrates not only technical proficiency but also an ability to listen actively and prioritize user needs. It reflects a candidate’s capacity to translate real-world feedback into actionable improvements, ensuring that the product evolves in alignment with user expectations and market demands. This question also reveals a candidate’s problem-solving skills and adaptability, key traits for thriving in an environment that values rapid iteration and continuous improvement.
How to Answer: Focus on a specific example where user feedback led to a meaningful change. Detail the steps you took from gathering the feedback to implementing the new feature, highlighting any challenges faced and how you overcame them. Emphasize collaboration with team members, any data-driven decisions you made, and the positive impact the new feature had on user experience or business outcomes.
Example: “At my previous company, we had a fintech app that allowed users to manage their personal finances. We received several comments that users were finding it difficult to track their recurring expenses. After analyzing the feedback, I proposed adding a feature that would automatically detect and categorize recurring transactions.
I collaborated closely with the data science team to develop an algorithm that could accurately identify these recurring expenses. Once the backend was sorted, I worked with the design team to create an intuitive UI for users to view and manage their recurring expenses. We also ran a beta test with a small group of users to get additional feedback and make necessary adjustments before the full rollout. The new feature was well-received and led to a significant increase in user engagement and satisfaction, validating the importance of listening to and acting on user feedback.”
12. What methods do you use to evaluate the risk of loan default?
Evaluating the risk of loan default is crucial for financial companies to ensure they maintain a healthy balance between lending and risk management. This question delves into your analytical and decision-making skills, specifically your ability to interpret financial data, assess borrower credibility, and predict future behavior. Companies like Upstart are particularly interested in understanding your proficiency with advanced algorithms and data-driven decision-making. They want to gauge your familiarity with both traditional credit assessment methods and innovative approaches that incorporate non-conventional data points, which can lead to more accurate risk predictions.
How to Answer: Highlight your experience with various risk evaluation techniques, such as credit scoring models, income verification, and debt-to-income ratios. Explain how you’ve used data analytics tools and machine learning models to enhance risk assessment accuracy. Emphasize your ability to balance quantitative analysis with qualitative insights, such as understanding the borrower’s financial history and current economic conditions.
Example: “I use a combination of quantitative and qualitative methods to evaluate loan default risk. I start with a thorough analysis of credit scores, debt-to-income ratios, and employment history, which provides a strong initial assessment. I also look at any recent financial behaviors or patterns, such as consistent on-time payments or any recent large purchases, to understand the applicant’s current financial stability.
In addition to these traditional metrics, I like to incorporate alternative data sources where possible. For instance, reviewing social media presence for indicators of financial habits or even using machine learning algorithms to identify potential risk factors that might not be obvious in standard reports. Combining these methods allows for a more holistic view of an applicant’s risk profile, leading to more informed lending decisions.”
13. How do you stay updated with the latest trends in financial technology?
Staying updated with the latest trends in financial technology is crucial for any role within a company like Upstart. This question delves into your commitment to continuous learning and adaptability in an ever-evolving field. It assesses whether you proactively seek out new information and can apply it to improve processes, products, or services. Furthermore, it gauges your ability to anticipate market shifts and integrate new technologies that could drive the company forward.
How to Answer: Discuss strategies you use to stay informed, such as subscribing to industry-leading publications, participating in webinars, attending conferences, or being part of professional networks. Mention any recent trends or developments you’ve followed and how they’ve influenced your thinking or approach to work. This demonstrates your proactive mindset and your readiness to contribute to Upstart’s innovative culture.
Example: “I’m always eager to stay on top of the latest fintech trends because it’s such a dynamic field. I make it a point to follow key industry publications like Finextra and TechCrunch, which provide timely updates and in-depth analyses. I also subscribe to newsletters from thought leaders and influential fintech companies to get their insights directly in my inbox.
Beyond reading, I actively participate in webinars and attend industry conferences, like Money20/20, which offer a chance to hear from experts and network with other professionals. I found that being part of professional networks, both online on platforms like LinkedIn and in-person meetups, keeps me informed about emerging trends and innovative solutions. This multi-faceted approach ensures I’m always aware of the latest developments and can bring fresh, relevant ideas to my work.”
14. Describe your process for conducting a market analysis for new product offerings.
Understanding how candidates approach market analysis reveals their ability to navigate complex data, identify trends, and make informed decisions that align with company goals. For a company like Upstart, this question helps determine whether a candidate can contribute to strategic initiatives and drive growth. A thorough market analysis process demonstrates the candidate’s capability to assess competitive landscapes, understand consumer needs, and anticipate market shifts—key elements that influence product success and overall company performance.
How to Answer: Outline a structured approach that includes identifying target markets, performing SWOT analysis, analyzing competitors, and using data analytics tools to interpret market trends. Emphasize your ability to synthesize data into actionable insights and present findings in a way that supports strategic decision-making. Highlight any relevant experience where your market analysis directly impacted product development or market entry strategies.
Example: “I start by identifying the target audience and their needs, using a mix of customer surveys, focus groups, and analyzing social media trends. I then look at the competitive landscape to see what’s already available and identify any gaps or opportunities for differentiation.
Next, I delve into the data—market size, growth potential, and revenue forecasts, which help prioritize potential products. I also consider regulatory factors and any potential barriers to entry. From there, I compile all this information into a comprehensive report with actionable insights and recommendations. This approach not only ensures that we’re making data-driven decisions but also aligns with our overall business strategy and goals.”
15. How would you design an experiment to test the effectiveness of a new credit model?
Designing an experiment to test the effectiveness of a new credit model involves a deep understanding of both statistical methodologies and practical implementation. This question seeks to evaluate your ability to think critically about experimental design, including control groups, randomization, and metrics for success. Moreover, it assesses your understanding of the credit industry, particularly how different variables can impact creditworthiness and loan performance. At Upstart, where data-driven decision-making is integral, your ability to design a robust experiment can directly influence the company’s ability to innovate and improve its credit models.
How to Answer: Start by defining the objective of the experiment and the hypothesis you aim to test. Explain the importance of selecting a representative sample and ensuring random assignment to control and experimental groups. Discuss the key metrics you would use to measure effectiveness, such as default rates, repayment timelines, and customer satisfaction. Highlight any potential confounding variables and how you would control for them. Conclude by emphasizing the iterative nature of such experiments, where continuous learning and model refinement are essential for sustained improvements.
Example: “To design an experiment testing the effectiveness of a new credit model, I’d start with a clear objective: understanding how this new model impacts credit approval rates and default rates compared to the current model. I’d use an A/B testing framework, where one group of customers would be evaluated using the existing model (control group) and another group using the new model (experimental group).
I’d ensure we have a large, randomized sample of applicants to avoid any biases. We would track key performance indicators like approval rates, default rates, and borrower credit scores over a significant period, probably around six months to a year, to get robust data. Alongside quantitative metrics, I’d also gather qualitative feedback from underwriters and account managers to understand any nuanced impacts. Throughout the experiment, I’d monitor and analyze the data continuously, making adjustments as necessary to ensure the validity and reliability of our results. Finally, we’d conduct a thorough analysis to compare the outcomes and make an informed decision on whether to implement the new model more broadly.”
16. Explain your experience with machine learning techniques in credit scoring.
Understanding the application of machine learning in credit scoring is not just about technical prowess; it’s about demonstrating an ability to leverage data for predictive accuracy and risk management. In the context of a company like Upstart, this question assesses your competence in crafting models that can predict creditworthiness with higher precision. Your response reveals your familiarity with various machine learning techniques, your problem-solving approach, and how you translate complex data into actionable insights that drive business decisions.
How to Answer: Focus on specific machine learning techniques you have employed, such as decision trees, neural networks, or ensemble methods, and discuss the outcomes of your models. Highlight any improvements you achieved in predictive accuracy or risk reduction. Providing concrete examples of past projects, including challenges faced and how you overcame them, will showcase your practical experience and ability to contribute to Upstart’s innovative approach to credit scoring.
Example: “In my last role at a fintech startup, I was part of a team that developed and refined machine learning models to assess credit risk. We primarily used logistic regression and random forests, but we also explored more complex algorithms like gradient boosting machines to improve prediction accuracy.
One of my most significant contributions was integrating alternative data sources, such as social media activity and utility bill payments, into our models. I collaborated with data engineers to clean and preprocess this data, and then fine-tuned our algorithms to incorporate these new features. This not only improved our model’s predictive power but also allowed us to extend credit to individuals who were previously underserved by traditional scoring methods. The result was a more inclusive and accurate credit assessment process, which directly contributed to the company’s growth and customer satisfaction.”
17. Describe a scenario where you improved an operational process through automation.
Efficiency and innovation are at the core of what drives companies like Upstart. Understanding how you have improved operational processes through automation can reveal your ability to identify inefficiencies, leverage technology, and implement solutions that save time and resources. This question delves into your technical prowess, problem-solving skills, and your ability to think critically about how processes can be optimized to enhance overall productivity.
How to Answer: Provide a clear example that outlines the specific problem you identified, the automated solution you implemented, and the measurable impact it had on the organization’s operations. Highlight any specific technologies or tools you used and discuss the broader implications of your solution on team efficiency and company outcomes. Emphasize your role in the project, showcasing your initiative, technical skills, and ability to drive meaningful change in a data-driven environment.
Example: “At my previous job, we had a manual process for tracking and reporting sales leads that was incredibly time-consuming. We were using spreadsheets that required constant updating, and it was easy for errors to slip in. I saw an opportunity to streamline this by automating the process.
I proposed integrating a CRM tool with our existing systems, and worked closely with our IT team to ensure a smooth transition. We set up automated workflows for lead entry, follow-ups, and reporting. This reduced data entry errors and freed up significant time for the sales team to focus on actual selling rather than administrative tasks. Within a few months, we saw a 20% increase in our conversion rates because the team could respond to leads much more quickly and efficiently.”
18. How do you manage communication and collaboration within a cross-functional team?
Effective communication and collaboration within a cross-functional team is essential because it brings together diverse skill sets and perspectives, driving innovation and solving complex problems more efficiently. Companies like Upstart rely heavily on seamless interaction between departments to optimize their algorithms and improve their financial products. The ability to manage these interactions speaks volumes about your organizational skills, adaptability, and ability to integrate different viewpoints to achieve a common goal.
How to Answer: Emphasize your experience with tools and practices that facilitate clear communication and collaboration, such as regular stand-up meetings, project management software, and collaborative platforms. Highlight specific examples where you successfully navigated challenges and brought together team members from different backgrounds to deliver a successful project.
Example: “Strong communication and collaboration within a cross-functional team hinge on transparency and structured touchpoints. I make it a point to establish regular sync meetings where representatives from each function can update on their progress, share challenges, and align on priorities. These meetings are kept concise and focused to respect everyone’s time but ensure that critical information is exchanged.
I also leverage collaborative tools like Slack and Trello to keep everyone informed and on the same page outside of these meetings. For example, on a recent project, I set up a Trello board with columns for each stage of our workflow and assigned tasks to team members with clear deadlines. This way, everyone knew their responsibilities and could see how their work fit into the bigger picture. This combination of regular touchpoints and effective use of collaboration tools has consistently kept projects on track and fostered a sense of teamwork across different functions.”
19. Explain a challenging bug you encountered and how you resolved it.
Tackling a challenging bug is not just about technical prowess; it’s a testament to your problem-solving skills, persistence, and ability to handle stress. Upstart companies value innovative thinkers who can navigate through ambiguity and deliver effective solutions. They are looking for candidates who can demonstrate their ability to dissect complex problems, identify root causes, and implement fixes that prevent future issues. This question also reveals your approach to continuous learning and how you adapt when faced with unforeseen challenges.
How to Answer: Describe the bug, emphasizing its complexity and the impact it had on the project or system. Outline the steps you took to diagnose the issue, including any tools or methodologies you used. Highlight how you collaborated with team members or consulted documentation and resources to find a solution. Finally, discuss the resolution and any lessons learned or preventive measures you implemented to avoid similar issues in the future.
Example: “One of the trickiest bugs I encountered was during a time-sensitive release where the application was crashing intermittently under heavy load. Given the sporadic nature, it was tough to reproduce the issue on demand, making it harder to isolate the root cause.
I began by checking the logs meticulously, focusing on the time frames when the crashes occurred. After identifying a pattern, I set up more granular logging around the suspected areas. It turned out that under heavy load, our database connections were being exhausted, causing a cascade of failures. To resolve it, I optimized the connection pooling, added better error handling, and performed load testing to ensure stability. The fix not only resolved the immediate issue but also improved the overall performance of the application.”
20. Describe your approach to creating a positive customer experience in a high-stress environment.
Creating a positive customer experience in a high-stress environment requires a nuanced understanding of both emotional intelligence and operational efficiency. High-stress situations often test the limits of a customer’s patience and a company’s service protocols. For a company like Upstart, where the stakes are high and customer interactions can be intense, demonstrating the ability to stay calm under pressure, listen actively, and provide clear, empathetic communication is vital. This question assesses your ability to maintain composure, think on your feet, and turn potentially negative encounters into opportunities for building trust and loyalty. It digs into your capacity to navigate complex emotional landscapes while adhering to company policies and delivering consistent, high-quality service.
How to Answer: Highlight specific strategies you employ to manage stress and maintain a positive demeanor. Discuss techniques like active listening, clear communication, setting realistic expectations, and showing empathy. Use concrete examples from past experiences where you successfully diffused a high-stress situation and turned it into a positive outcome.
Example: “I focus on empathy and clear communication. In a high-stress environment, customers often feel frustrated and anxious, so it’s crucial to acknowledge their feelings and reassure them that I’m there to help. I start by actively listening to their concerns without interrupting, which helps build trust and shows that I genuinely care about resolving their issue.
Once I understand their problem, I break down the solution into simple, manageable steps, avoiding jargon and keeping my explanations clear. For example, at my previous job, a customer was upset about a loan application error and worried about missing a crucial deadline. I calmly explained the steps we’d take to rectify the issue, kept them updated throughout the process, and ensured a quick resolution. This approach not only alleviated their stress but also turned a potentially negative experience into a positive one, reinforcing their trust in our service.”
21. How do you ensure accuracy when processing high volumes of financial transactions?
Maintaining accuracy in processing high volumes of financial transactions is paramount in a company where precision directly impacts financial integrity and customer trust. This question delves into your methodical approach, attention to detail, and ability to handle repetitive tasks without compromising quality. It also reflects on your understanding of the systems and protocols in place to mitigate errors, which is crucial in a fast-paced, data-driven environment like Upstart. Demonstrating your capacity to uphold accuracy amid high transaction volumes can showcase your reliability and commitment to maintaining the company’s standards.
How to Answer: Emphasize specific strategies you employ to ensure accuracy, such as double-checking entries, using automated tools for verification, and adhering to strict procedural checklists. Mention any relevant experience with financial software or systems that facilitate error detection and correction. Highlight your proactive approach to identifying potential issues before they escalate and your ability to remain focused under pressure.
Example: “I prioritize a combination of meticulous attention to detail and leveraging technology to ensure accuracy. First, I double-check my work at each step of the process, whether it’s data entry or reviewing transaction reports. I also make use of automation tools wherever possible to minimize human error. In my previous role at a mid-sized financial firm, I implemented a two-step verification process where one colleague would process the transaction and another would review it before finalizing. This helped catch any discrepancies early on. Additionally, I regularly cross-reference our transaction logs with bank statements to ensure everything aligns perfectly.
I also keep updated with the latest compliance regulations and guidelines to ensure all transactions meet legal standards. Combining these methods has consistently helped me maintain a high level of accuracy even when handling large volumes.”
22. Explain how you would conduct a root cause analysis for discrepancies in financial reports.
Root cause analysis in financial reporting is essential to ensure accuracy and reliability in financial statements, which directly affect stakeholder trust and decision-making. This question tests your analytical skills, attention to detail, and ability to systematically identify underlying issues rather than just addressing surface-level symptoms. At a company like Upstart, understanding the root cause of discrepancies can prevent recurring errors and contribute to the overall financial integrity and operational efficiency.
How to Answer: Outline a systematic approach starting with data collection and verification, followed by identifying patterns or anomalies, and then drilling down to potential sources of error such as data entry mistakes, software issues, or procedural flaws. Emphasize the importance of collaboration with relevant departments to gather diverse insights and ensure a comprehensive analysis.
Example: “First, I would gather all relevant data and documentation to understand the scope of the discrepancy. I’d ensure I have access to the financial reports in question, previous periods’ reports, and any associated documentation or transactions.
Next, I’d analyze the reports to identify where and when the discrepancies occurred. This step involves looking at trends, comparing figures, and pinpointing anomalies. Once identified, I would drill down into the specific transactions or entries that led to the discrepancies.
Then, I would meet with the team members involved in the financial reporting process to gather insights and understand if there were any changes in procedures, software, or external factors that could have contributed to the issue.
Finally, I would summarize my findings and recommend corrective actions to prevent future discrepancies, such as updating procedures, providing additional training, or implementing new controls. In a previous role, I used this method to uncover a data entry error that had been overlooked due to a recent software update, which helped us resolve a significant variance promptly.”
23. Describe your experience with cloud-based platforms for software development.
A deep understanding and experience with cloud-based platforms for software development is essential for modern tech companies like Upstart. This question delves into your technical expertise and familiarity with cloud services, which are critical for deploying applications, managing infrastructure, and ensuring seamless integration across various platforms. It also assesses your ability to leverage these tools to optimize performance, reduce costs, and enhance the overall efficiency of software development processes.
How to Answer: Highlight specific cloud platforms you have worked with, such as AWS, Google Cloud, or Azure, and detail the projects you’ve completed using these services. Emphasize your experience with cloud-native development, including microservices, containerization with Docker, orchestration with Kubernetes, and continuous integration/continuous deployment (CI/CD) pipelines.
Example: “I’ve spent several years working with cloud-based platforms, primarily AWS and Azure, for various software development projects. One project that stands out involved migrating a legacy application to a serverless architecture using AWS Lambda, API Gateway, and DynamoDB. The goal was to improve scalability and reduce operational costs.
I collaborated closely with the development team to rewrite parts of the application to fit the serverless model and set up CI/CD pipelines using AWS CodePipeline and CodeDeploy to ensure smooth deployments. This transition not only enhanced the application’s performance but also cut down on our infrastructure costs by about 30%. It was a challenging project but incredibly rewarding to see the tangible benefits of leveraging cloud platforms to modernize our tech stack.”
24. How do you balance customer needs with company policies when resolving conflicts?
Balancing customer needs with company policies when resolving conflicts is a nuanced skill that goes beyond merely following a rulebook. This question delves into your ability to navigate the fine line between maintaining customer satisfaction and adhering to established guidelines, which is crucial for sustaining long-term relationships and protecting the company’s interests. Companies, especially those that operate in fast-evolving sectors like fintech, value employees who can exercise judgment and adapt solutions creatively while still upholding the integrity of company policies. This demonstrates not only your problem-solving skills but also your capacity to understand and respect the broader organizational framework.
How to Answer: Highlight specific instances where you successfully managed to satisfy a customer’s needs without compromising company policies. Discuss the strategies you used, such as active listening, empathy, and clear communication, to ensure that the customer felt heard and valued. Emphasize your ability to find middle ground, showing that you can think critically and act decisively in situations where there might be competing priorities.
Example: “I always start by listening to the customer’s concerns and validating their feelings, which helps to de-escalate any tension. Once I understand their needs, I look for a way to align those with the company’s policies. For example, at my previous job in a financial services company, a customer was upset about a late fee on their account. They had a history of on-time payments, so I checked our policy and found there was some leeway for waiving a first-time fee for loyal customers.
I explained to the customer that while we generally enforce late fees to encourage timely payments, we also value long-term relationships. I was able to waive the fee this time, while making sure they understood the importance of future on-time payments. This approach not only satisfied the customer but also adhered to our company’s policy framework. Balancing empathy with the guidelines ensures both the customer’s trust and the company’s integrity are maintained.”
25. Discuss your approach to mentoring junior team members in a technical role.
Mentoring junior team members in a technical role is crucial for fostering a collaborative and innovative environment. In a company like Upstart, the ability to effectively mentor less experienced colleagues ensures the continuous growth of the team and the seamless transfer of knowledge. This question delves into your leadership style, your ability to communicate complex technical concepts in an understandable way, and your commitment to the professional development of your peers. It also reflects how you contribute to a culture of continuous learning and improvement, which is vital in a fast-paced, tech-driven company.
How to Answer: Share specific examples of mentoring experiences, focusing on the methods and tools you used to help juniors grasp difficult concepts or improve their skills. Highlight any structured programs or informal strategies you employed, such as code reviews, pair programming, or regular feedback sessions.
Example: “I believe in a hands-on, personalized approach to mentoring. It’s crucial to first understand each junior team member’s strengths, weaknesses, and career aspirations. I like to start by setting clear, achievable goals and providing them with projects that challenge them but are also within their capability to complete successfully. By doing so, they can build confidence and gradually take on more complex tasks.
For example, with one junior developer, I noticed they were struggling with debugging. So, I paired them with a more experienced developer for a few sessions where they could learn different techniques and best practices. I also made it a point to have regular check-ins to discuss their progress, provide constructive feedback, and celebrate their wins. This not only helped them improve their technical skills but also fostered a sense of belonging and motivation within the team. Ultimately, I aim to create an environment where junior team members feel supported, encouraged, and excited to grow.”
26. How do you measure the success of a newly implemented operational strategy?
Measuring the success of a newly implemented operational strategy goes beyond simply tracking metrics; it involves understanding the broader impact on the organization’s goals and the ability to adapt based on feedback. Companies like Upstart place a premium on evaluating how strategies align with long-term objectives, customer satisfaction, and operational efficiency. They seek to understand if candidates can not only set measurable benchmarks but also interpret the results to refine and optimize processes continually.
How to Answer: Highlight specific metrics you use to gauge success, such as key performance indicators (KPIs), return on investment (ROI), or customer feedback. Describe how you analyze this data to make informed decisions and adjustments. Illustrate your answer with examples of past experiences where your measurement methods led to tangible improvements.
Example: “I start by defining clear, measurable objectives aligned with the overall business goals. For instance, if the new strategy aims to improve customer response times, I’d establish specific benchmarks like reducing response time from 24 hours to 12 hours within the first quarter.
I also closely monitor key performance indicators and gather feedback from both the team implementing the strategy and the end users affected by it. For example, in a past role, we rolled out a new customer service protocol and I tracked metrics like customer satisfaction scores, resolution times, and the volume of repeat inquiries. By combining quantitative data with qualitative insights, I could make data-driven adjustments and ensure the strategy was truly effective and sustainable over the long term.”
27. Explain how you would handle a sudden change in project requirements during development.
Navigating sudden changes in project requirements during development is a reflection of one’s adaptability and problem-solving skills. Companies like Upstart value employees who can remain composed and effective under pressure. The ability to pivot quickly, reassess priorities, and communicate effectively with team members and stakeholders is crucial. This question seeks to understand how you manage uncertainty and maintain productivity without compromising the project’s integrity or team morale.
How to Answer: Emphasize your methodical approach to managing change. Discuss specific strategies such as reassessing the project scope, re-evaluating resources, and maintaining open communication channels. Share a relevant example where you successfully navigated a similar situation, detailing the steps you took to ensure a smooth transition and the positive outcome that resulted.
Example: “First, I’d assess the impact of the new requirements on the existing project timeline and resources. I’d gather the team for a quick meeting to discuss the changes, ensuring everyone understands the new direction and how it affects their individual roles. Clear communication is key; I’d make sure all stakeholders are aligned and any potential risks or roadblocks are identified early.
In a previous role, we had a situation where mid-project, the client decided to change the core feature set of the app we were developing. I worked closely with the project manager to re-prioritize tasks and created a revised timeline. We also implemented more frequent check-ins with the client to ensure we were meeting their new expectations. This approach not only helped us adapt smoothly but also maintained team morale and client satisfaction.”
28. Describe your process for recruiting top talent in a competitive job market.
Recruiting top talent in a competitive job market requires a sophisticated approach that goes beyond simply posting job ads and reviewing resumes. It involves understanding the unique value propositions that attract high-caliber candidates, such as company culture, career growth opportunities, and innovative projects. Companies like Upstart seek individuals who can identify and engage with potential hires on multiple levels, including leveraging data analytics, networking, and employer branding. Demonstrating an ability to navigate these elements shows that you can bring in talent that not only fits the skill requirements but also the cultural and strategic vision of the company.
How to Answer: Focus on specific strategies you employ to attract and retain top talent. Discuss how you use data-driven insights to identify potential candidates and the importance of personalized outreach in building relationships. Highlight any innovative methods you’ve used, such as hosting industry events or utilizing social media platforms for talent scouting.
Example: “I believe it all starts with building genuine relationships. In a competitive job market, top talent often has multiple options, so it’s crucial to create a positive and memorable candidate experience. I start by really understanding the role and the team’s needs, which helps me to craft compelling job descriptions that highlight not only the responsibilities but also the unique opportunities and culture of the company.
When reaching out to potential candidates, I personalize my messages to show I’ve taken the time to learn about their background and how they could uniquely contribute to our team. I also make it a point to engage with passive candidates through networking events, industry conferences, and online platforms like LinkedIn. Once engaged, I ensure the interview process is smooth and transparent, always providing timely feedback and making myself available for any questions they might have. This dedication to building a relationship and ensuring a seamless experience often sets our opportunities apart from others they might be considering.”
29. How do you ensure data privacy and security when handling sensitive customer information?
Ensuring data privacy and security is central to maintaining customer trust and complying with legal standards. Companies need to safeguard sensitive information against breaches and misuse, which can have severe repercussions both legally and reputationally. This question delves into your understanding of data protection protocols and your commitment to upholding these standards. It’s about demonstrating that you recognize the gravity of handling sensitive information and have the technical and ethical acumen to protect it.
How to Answer: Highlight your familiarity with industry standards such as GDPR, CCPA, or other relevant regulations. Discuss specific measures you’ve implemented in past roles, like encryption, access controls, and regular audits. Mention any training or certifications that underscore your expertise in data security.
Example: “I prioritize data privacy and security by following strict protocols and best practices. This means always using encryption for data storage and transmission, keeping up with the latest security patches and software updates, and ensuring that access to sensitive information is restricted to only those who absolutely need it. I also make it a point to stay informed about the latest trends and threats in cybersecurity through continuous learning and training.
In my previous role at a fintech company, I led a project to implement two-factor authentication across our customer-facing platforms. This not only boosted security but also increased customer trust. By combining these technical measures with regular audits and compliance checks, I ensure that sensitive customer information is protected to the highest standard.”
30. Explain a time when you had to persuade stakeholders to support a new initiative.
Convincing stakeholders to back a new initiative goes beyond simply presenting data—it involves understanding their perspectives, addressing their concerns, and aligning the initiative with their interests and the company’s goals. In a company that values innovation and data-driven decision-making, such as Upstart, this skill is particularly valuable. Demonstrating your ability to navigate these conversations shows your proficiency in not just proposing ideas but also in effectively driving them forward, which is essential for fostering growth and implementing change.
How to Answer: Recount a specific instance where you had to persuade stakeholders, detailing the strategies you employed to understand their priorities and how you tailored your approach accordingly. Highlight the communication techniques you used, such as building rapport, presenting compelling evidence, and addressing potential objections.
Example: “In my previous role at a fintech startup, I proposed a new data analytics tool that I believed would significantly enhance our customer insights. The challenge was getting the buy-in from both the finance team, who were concerned about costs, and the marketing team, who were hesitant to adopt new technology.
I knew the key was showing them the value in a way that aligned with their goals. So, I prepared a detailed presentation that highlighted how the tool could streamline marketing efforts by providing more accurate targeting and therefore reduce ad spend in the long run. For the finance team, I put together a cost-benefit analysis that showed potential ROI within six months based on increased customer engagement and retention.
---
Solve enough SQL interview questions, and you’ll start to see the same patterns come up over-and-over again. As the author of Ace the Data Science Interview, and founder of SQL interview platform DataLemur, I’ve solved over 300 SQL interview questions myself and seen just about every type of SQL interview question that gets asked.
To save you time, I put together this guide on the top patterns you’ll find in SQL interview questions:
1 Give Me All X, BUT Filter It Down
2 Compute a Rolling Average
3 Analyze Pairs Of Things With Self-Joins
4 Find Top X Things (Rank Functions)
5 Optimizing Query Performance
Scroll down to dive into each pattern👇👇
SQL Interview Pattern 1: Give Me All X, BUT Filter It Down.
To answer the “gimme all X BUT make sure they’re Y and Z” questions, you’ll have to all records that match a certain condition. Sometimes, instead of simply ’ing records, you might have to give the count, average, or sum of all the records that match some condition but the logic is mostly the same.
Irregarldess, the crux of the question is about filtering down the dataset correctly based on one or several of these conditions: Filter based on data in another table (via join) Filter based on string matching (tests your use of regex and syntax) Filter based on timestamps (tests your ability to use date/time operators)
An example Meta interview question would be: “-For each month, find me the count of all facebook users who created their account that month-”
Your solution use the following snippet:
While date/time commands aren’t as common in SQL interviews (since the syntax varies so greatly between different SQL flavors), in my sql interview guide I shortlisted some of the most important PostgreSQL date/time functions for interviews.
SQL Interview Pattern 2: Rolling Average
You’ll be asked to find the rolling average, or trailing average for some set of data. While it’s common to have some type of orders, purchases, or transaction type dataset, for Product Data Science roles you might get a user analytics question like “What’s the 7-day rolling average Daily Active User count (DAU)”.
You could use a query like the following:
These questions are popular SQL interview questions because in-real world analytics, there is so much seasonality in day-to-day metrics. That’s why accounting for weekend effects, most of the time you're asked to create a rolling 7-day or 28-day metric.
Fun fact: using a 30-day rolling average (aka a monthly rolling average) isn’t a great idea at the biggest tech companies, because weekday vs. weekend behavior is super different. And unfortunately, in a 30-day period there might be 4 weekends, 5 weekends, or 4.5 weekends, which can throw off your numbers completely. Source: worked at FB’s growth team, and messed up an important analysis this way…..woops.
SQL Interview Pattern 3: Analyze Pairs Of Things With Self-Joins
Frequently you’ll be asked to analyze pairs of things – maybe pairs of social media users (to see how much they message each other), or pairs of products (to see how often they are bough together), etc.
For these SQL questions, always think of self-joins (where you join a table to itself).
For example, say you had website visitor data exported from a company's Google Analytics account, and you had to analyze pairs of pages for UX or navigational issues. As part of that analysis, you wanted to generate all pairs of URLs, but needed to avoid pairs where both the URLs were the same since that's not a valid pair.
The self-join query would look like the following:
This query returns the url of each page ( ) along with the url of the page that referred to it ( ). The self-join is performed using the field, which specifies the id of the page that referred the visitor to the current page, and avoids any pages that referred themself (aka data anomalies).
To try a self-join yourself, try this real SQL question from a Walmart interview about finding frequently purchased pairs of items!
SQL Interview Pattern 4: Find Top X Things
Another common SQL interview question pattern is being asked to find the top X things. For example:
1 find me the top 3 employee salaries in each department
2 find me the 2nd most sold product in each Amazon category
3 find me the 3rd transaction made for every Uber rider
For these questions, you’ll want to immediately think of window functions like and .
To test this pattern out yourself, checkout this Uber SQL Assessment Question about selecting a user's 3rd transaction made on the Uber platform.
SQL Interview Pattern 5: Optimizing Query Performance
Usually, for Data Analysts and Data Scientists, questions about improving SQL query performance aren’t asked by themself. Instead, after you write a query to solve an earlier problem, the interviewer will follow-up and ask you to optimize your SQL query.
Here they are testing to see how deep your knowledge of database internals goes – are you used to writing simple queries, or have you had to analyze performance bottlenecks in complex SQL queries, and truly understand the DBMS you’re working on top of?
While there’s a ton of info out there on SQL query optimization tips, for 90% of Data Analyst and Data Science interviews, you’ll pass by mentioning these three things: Don’t random fields - only get the columns you need Try to use database indexes Avoid joins if possible – try denormalizing your database

---
91 SQL Interview Questions
1 What is SQL and how does it differ from other programming languages?
2 Explain the difference between INNER JOIN and LEFT JOIN with examples.
3 Write a SQL query to find the second highest salary from a table named Employees.
4 What are primary keys and foreign keys? Provide examples.
5 Write a SQL query to retrieve all records from a table named Products where the price is greater than 100.
6 Explain the concept of normalization and its types.
7 Write a SQL query to count the number of employees in each department from a table named Employees.
8 What is a subquery? Provide an example of a subquery in a SELECT statement.
9 Write a SQL query to find all customers who have placed more than 5 orders from a table named Orders.
10 Explain the difference between UNION and UNION ALL.
11 Write a SQL query to update the email address of a customer in a table named Customers.
12 What are indexes in SQL? How do they improve query performance?
13 Write a SQL query to delete all records from a table named Logs where the created_at date is older than 1 year.
14 Explain the ACID properties in the context of database transactions.
15 Write a SQL query to retrieve the top 3 highest-paid employees from a table named Employees.
16 What is a view in SQL? How is it different from a table?
17 Write a SQL query to find the total sales amount from a table named Sales grouped by product.
18 Explain the concept of stored procedures and their advantages.
19 Write a SQL query to find all products that have not been sold from a table named Products and a table named Sales.
20 What is the purpose of the GROUP BY clause? Provide an example.
21 Write a SQL query to retrieve the names of employees who have the same job title as 'Manager'.
22 Explain the difference between a clustered index and a non-clustered index.
23 Write a SQL query to find the average salary of employees in each department from a table named Employees.
24 What is a trigger in SQL? Provide an example of when you might use one.
25 Write a SQL query to retrieve all distinct values from a column named Category in a table named Products.
26 What are window functions and how do they differ from aggregate functions?
27 Write a SQL query to find the nth highest salary using window functions.
28 Explain the difference between HAVING and WHERE clauses.
29 What is a Common Table Expression (CTE) and when would you use it?
30 Write a SQL query to find duplicate records in a table.
31 What is the difference between DELETE, TRUNCATE, and DROP?
32 How do you handle NULL values in SQL queries?
33 Write a SQL query to calculate a running total.
34 What are the different types of constraints in SQL?
35 Explain the concept of database transactions and isolation levels.
36 Write a SQL query to pivot data from rows to columns.
37 What is the difference between RANK(), DENSE_RANK(), and ROW_NUMBER()?
38 How do you optimize SQL query performance?
39 What are user-defined functions and how do they differ from stored procedures?
40 Write a SQL query to find the percentage of total sales for each product.
41 What is the difference between correlated and non-correlated subqueries?
42 How do you handle date and time operations in SQL?
43 Write a SQL query to find records that exist in one table but not in another.
44 What is database denormalization and when would you use it?
45 Explain the difference between a primary key and a unique key.
46 Write a SQL query to find the median value from a column.
47 What are the advantages and disadvantages of using views?
48 How do you implement full-text search in SQL?
49 Write a SQL query to find the first and last record for each group.
50 What is the purpose of the COALESCE function?
51 Explain the concept of recursive queries and provide an example.
52 What are materialized views and how do they differ from regular views?
53 Write a SQL query to calculate year-over-year growth percentage.
54 How do you handle large dataset queries efficiently?
55 What is the difference between CHAR and VARCHAR data types?
56 Write a SQL query to find gaps in sequential data.
57 Explain the concept of database partitioning and its types.
58 What are database locks and how do they work?
59 Write a SQL query to transpose rows to columns dynamically.
60 How do you implement audit trails in SQL databases?
61 What is the difference between OLTP and OLAP systems?
62 Write a SQL query to find the top N customers by sales in each region.
63 How do you handle database schema migrations?
64 What are the considerations for choosing appropriate data types?
65 Write a SQL query to calculate moving averages.
66 How do you implement row-level security in SQL databases?
67 What is the purpose of database connection pooling?
68 Write a SQL query to find customers who haven't placed orders in the last 6 months.
69 How do you handle database backup and recovery strategies?
70 What are the best practices for SQL query writing?
71 Write a SQL query to find overlapping date ranges.
72 How do you implement database replication and what are its types?
73 What is database sharding and when would you implement it?
74 Write a SQL query to calculate retention rates.
75 How do you monitor and troubleshoot database performance issues?
76 What are database constraints and how do they ensure data integrity?
77 Write a SQL query to generate a calendar table.
78 How do you implement database versioning and change management?
79 What are the security considerations for SQL databases?
80 Write a SQL query to find the longest consecutive sequence.
81 How do you handle time zone considerations in global applications?
82 What is the difference between database clustering and replication?
83 Write a SQL query to implement a recommendation system using collaborative filtering.
84 How do you implement data archiving strategies?
85 What are the considerations for database capacity planning?
86 Write a SQL query to detect anomalies in time-series data.
87 How do you implement database disaster recovery procedures?
88 What are the emerging trends in database technology?
89 Write a SQL query to implement a simple rating system with weighted averages.
90 How do you optimize database storage and reduce costs?
91 What are the key considerations for migrating from legacy database systems?
1. What is SQL and how does it differ from other programming languages?
Why you might get asked this: Understanding the fundamental differences between SQL and other programming languages is crucial for roles that involve database management and data manipulation, such as a Database Administrator or Data Analyst.
How to answer:
1 Define SQL as a domain-specific language used for managing and manipulating relational databases.
2 Highlight that SQL is declarative, focusing on what data to retrieve rather than how to retrieve it.
3 Contrast SQL with general-purpose programming languages, emphasizing its specialized use for database operations.
Example answer:
"SQL, or Structured Query Language, is a domain-specific language designed for managing and manipulating relational databases. Unlike general-purpose programming languages, SQL is declarative, meaning it focuses on what data to retrieve rather than how to retrieve it."
2. Explain the difference between INNER JOIN and LEFT JOIN with examples.
Why you might get asked this: Understanding the difference between INNER JOIN and LEFT JOIN is essential for roles that require complex data retrieval and manipulation, such as a Data Engineer or SQL Developer.
How to answer:
1 Define INNER JOIN as a join that returns only the matching rows from both tables.
2 Explain LEFT JOIN as a join that returns all rows from the left table and the matching rows from the right table, with NULLs for non-matching rows.
3 Provide a simple example query for each join to illustrate the differences.
Example answer:
"An INNER JOIN returns only the rows that have matching values in both tables, while a LEFT JOIN returns all rows from the left table and the matching rows from the right table, with NULL values for non-matching rows. For example, SELECT * FROM Orders INNER JOIN Customers ON Orders.CustomerID = Customers.CustomerID retrieves only the orders with matching customers, whereas SELECT * FROM Orders LEFT JOIN Customers ON Orders.CustomerID = Customers.CustomerID retrieves all orders, including those without matching customers."
3. Write a SQL query to find the second highest salary from a table named Employees.
Why you might get asked this: This question tests your ability to write complex SQL queries and demonstrates your problem-solving skills, which are crucial for roles like Data Analyst or Database Developer.
How to answer:
1 Explain the use of the LIMIT clause to restrict the number of rows returned.
2 Describe the use of a subquery to find the highest salary and exclude it from the results.
3 Provide a sample query using ORDER BY and LIMIT to retrieve the second highest salary.
Example answer:
"To find the second highest salary from a table named Employees, you can use a subquery to first identify the highest salary and then exclude it from the results. The query would look like this: SELECT MAX(Salary) FROM Employees WHERE Salary < (SELECT MAX(Salary) FROM Employees) ."
4. What are primary keys and foreign keys? Provide examples.
Why you might get asked this: Understanding primary keys and foreign keys is fundamental for ensuring data integrity and establishing relationships between tables, which is crucial for roles like Database Administrator or Data Architect.
How to answer:
1 Define a primary key as a unique identifier for each record in a table.
2 Explain a foreign key as a field in one table that uniquely identifies a row of another table.
3 Provide examples using simple table structures, such as an EmployeeID in an Employees table and a DepartmentID in a Departments table.
Example answer:
"A primary key is a unique identifier for each record in a table, ensuring that no two rows have the same key. A foreign key is a field in one table that uniquely identifies a row of another table, establishing a relationship between the two tables, such as EmployeeID in an Employees table and DepartmentID in a Departments table."
5. Write a SQL query to retrieve all records from a table named Products where the price is greater than 100.
Why you might get asked this: This question assesses your ability to write basic SQL queries for data retrieval, a fundamental skill for any role involving database management, such as a Data Analyst or SQL Developer.
How to answer:
1 Explain the use of the SELECT statement to retrieve data from the table.
2 Describe the WHERE clause to filter records based on the price condition.
3 Provide a sample query using SELECT * FROM Products WHERE price > 100 .
Example answer:
"To retrieve all records from a table named Products where the price is greater than 100, you can use the following SQL query: SELECT * FROM Products WHERE price > 100; This query selects all columns from the Products table where the price column has a value greater than 100."
6. Explain the concept of normalization and its types.
Why you might get asked this: Understanding the concept of normalization and its types is crucial for ensuring efficient database design and data integrity, which is essential for roles like Database Administrator or Data Architect, for example.
How to answer:
1 Define normalization as the process of organizing data to reduce redundancy and improve data integrity.
2 Briefly describe the different normal forms, such as 1NF, 2NF, and 3NF, and their purposes.
3 Provide a simple example to illustrate how normalization is applied in a database.
Example answer:
"Normalization is the process of organizing data in a database to reduce redundancy and improve data integrity. It involves dividing large tables into smaller, related tables and defining relationships between them to ensure data consistency."
7. Write a SQL query to count the number of employees in each department from a table named Employees.
Why you might get asked this: This question evaluates your ability to write aggregate queries, a fundamental skill for roles that involve data analysis and reporting, such as a Data Analyst or Business Intelligence Developer, for example.
How to answer:
1 Explain the use of the GROUP BY clause to group records by department.
2 Describe the use of the COUNT function to count the number of employees in each group.
3 Provide a sample query using SELECT Department, COUNT(*) FROM Employees GROUP BY Department .
Example answer:
"To count the number of employees in each department from a table named Employees, you can use the GROUP BY clause along with the COUNT function. The query would look like this: SELECT Department, COUNT(*) FROM Employees GROUP BY Department; "
8. What is a subquery? Provide an example of a subquery in a SELECT statement.
Why you might get asked this: Understanding subqueries and their application in SELECT statements is essential for roles that require complex data retrieval and manipulation, such as a Data Analyst or SQL Developer, for example.
How to answer:
1 Define a subquery as a query nested within another query.
2 Explain that subqueries can be used in SELECT, INSERT, UPDATE, or DELETE statements.
3 Provide a simple example, such as SELECT * FROM Employees WHERE Salary > (SELECT AVG(Salary) FROM Employees) .
Example answer:
"A subquery is a query nested within another query, often used to perform operations that require multiple steps. For example, SELECT * FROM Employees WHERE Salary > (SELECT AVG(Salary) FROM Employees) retrieves employees with salaries above the average."
9. Write a SQL query to find all customers who have placed more than 5 orders from a table named Orders.
Why you might get asked this: This question tests your ability to write complex SQL queries involving aggregate functions and conditional logic, which are crucial skills for roles that require data analysis and reporting, such as a Data Analyst or SQL Developer, for example.
How to answer:
1 Explain the use of the GROUP BY clause to group records by customer.
2 Describe the use of the HAVING clause to filter groups with more than 5 orders.
3 Provide a sample query using SELECT CustomerID FROM Orders GROUP BY CustomerID HAVING COUNT(*) > 5 .
Example answer:
"To find all customers who have placed more than 5 orders from a table named Orders, you can use the GROUP BY clause to group records by customer and the HAVING clause to filter groups with more than 5 orders. The query would look like this: SELECT CustomerID FROM Orders GROUP BY CustomerID HAVING COUNT(*) > 5; "
10. Explain the difference between UNION and UNION ALL.
Why you might get asked this: Understanding the difference between UNION and UNION ALL is crucial for roles that require data consolidation and query optimization, such as a Data Analyst or SQL Developer, for example.
How to answer:
1 Define UNION as a command that combines the results of two queries and removes duplicate rows.
2 Explain UNION ALL as a command that combines the results of two queries without removing duplicates.
3 Provide a simple example query for each to illustrate the differences.
Example answer:
"A UNION combines the results of two queries and removes duplicate rows, ensuring each row is unique in the final result set. In contrast, UNION ALL combines the results of two queries without removing duplicates, which can be more efficient when duplicates are not a concern."
11. Write a SQL query to update the email address of a customer in a table named Customers.
Why you might get asked this: This question assesses your ability to perform data updates, a fundamental skill for roles that involve database management and maintenance, such as a Database Administrator or SQL Developer, for example.
How to answer:
1 Explain the use of the UPDATE statement to modify existing records in the table.
2 Describe the SET clause to specify the new email address.
3 Include the WHERE clause to target the specific customer whose email address needs updating.
Example answer:
"To update the email address of a customer in a table named Customers, you can use the UPDATE statement along with the SET clause to specify the new email address. The query would look like this: UPDATE Customers SET email = 'newemail@example.com' WHERE customer_id = 1; "
12. What are indexes in SQL? How do they improve query performance?
Why you might get asked this: Understanding indexes and their impact on query performance is crucial for optimizing database operations, which is essential for roles that involve database management and optimization, such as a Database Administrator or SQL Developer, for example.
How to answer:
1 Define indexes as database objects that improve the speed of data retrieval operations.
2 Explain that indexes work by creating a data structure that allows for faster searches.
3 Highlight that while indexes improve read performance, they can slow down write operations due to the overhead of maintaining the index.
Example answer:
"Indexes in SQL are special data structures that improve the speed of data retrieval operations on a database table. They work by creating a quick lookup reference for the database, significantly reducing the time it takes to find specific rows."
13. Write a SQL query to delete all records from a table named Logs where the created_at date is older than 1 year.
Why you might get asked this: This question evaluates your ability to perform data maintenance tasks, which are crucial for roles that involve database management and optimization, such as a Database Administrator or SQL Developer, for example.
How to answer:
1 Explain the use of the DELETE statement to remove records from the table.
2 Describe the WHERE clause to filter records based on the created_at date.
3 Provide a sample query using DELETE FROM Logs WHERE created_at < NOW() - INTERVAL 1 YEAR .
Example answer:
"To delete all records from a table named Logs where the created_at date is older than 1 year, you can use the following SQL query: DELETE FROM Logs WHERE created_at < NOW() - INTERVAL 1 YEAR; This query ensures that only records older than one year are removed, keeping your table up-to-date."
14. Explain the ACID properties in the context of database transactions.
Why you might get asked this: Understanding the ACID properties is crucial for ensuring data integrity and reliability in database transactions, which is essential for roles that involve database management and optimization, such as a Database Administrator or SQL Developer, for example.
How to answer:
1 Define ACID as a set of properties that ensure reliable database transactions.
2 Briefly describe each property: Atomicity, Consistency, Isolation, and Durability.
3 Provide a simple example to illustrate how these properties maintain data integrity.
Example answer:
"ACID properties ensure reliable database transactions by maintaining data integrity. They stand for Atomicity, Consistency, Isolation, and Durability, which collectively guarantee that transactions are processed reliably."
15. Write a SQL query to retrieve the top 3 highest-paid employees from a table named Employees.
Why you might get asked this: This question tests your ability to write queries that involve sorting and limiting results, which is crucial for roles that require data analysis and reporting, such as a Data Analyst or SQL Developer, for example.
How to answer:
1 Explain the use of the ORDER BY clause to sort the salaries in descending order.
2 Describe the use of the LIMIT clause to restrict the number of rows returned to three.
3 Provide a sample query using SELECT * FROM Employees ORDER BY Salary DESC LIMIT 3 .
Example answer:
"To retrieve the top 3 highest-paid employees from a table named Employees, you can use the ORDER BY clause to sort the salaries in descending order and the LIMIT clause to restrict the number of rows returned to three. The query would look like this: SELECT * FROM Employees ORDER BY Salary DESC LIMIT 3; "
16. What is a view in SQL? How is it different from a table?
Why you might get asked this: Understanding the concept of views and their differences from tables is crucial for roles that involve database management and optimization, such as a Database Administrator or SQL Developer, for example.
How to answer:
1 Define a view as a virtual table based on the result set of a SQL query.
2 Explain that views do not store data physically but provide a way to simplify complex queries.
3 Highlight that unlike tables, views are dynamic and reflect changes in the underlying data in real-time.
Example answer:
"A view in SQL is a virtual table created based on the result set of a SQL query. Unlike a table, a view does not store data physically but provides a way to simplify complex queries."
17. Write a SQL query to find the total sales amount from a table named Sales grouped by product.
Why you might get asked this: This question evaluates your ability to perform aggregate calculations and group data, which are essential skills for roles that involve data analysis and reporting, such as a Data Analyst or Business Intelligence Developer, for example.
How to answer:
1 Explain the use of the SUM function to calculate the total sales amount.
2 Describe the GROUP BY clause to group the results by product.
3 Provide a sample query using SELECT product, SUM(amount) FROM Sales GROUP BY product .
Example answer:
"To find the total sales amount from a table named Sales grouped by product, you can use the SUM function along with the GROUP BY clause. The query would look like this: SELECT product, SUM(amount) FROM Sales GROUP BY product; "
18. Explain the concept of stored procedures and their advantages.
Why you might get asked this: Understanding stored procedures and their advantages is crucial for roles that involve database management and optimization, such as a Database Administrator or SQL Developer, for example.
How to answer:
1 Define a stored procedure as a precompiled collection of SQL statements.
2 Explain that stored procedures improve performance by reducing the need for repeated parsing and compilation.
3 Highlight that they enhance security by encapsulating business logic and restricting direct access to data.
Example answer:
"A stored procedure is a precompiled collection of SQL statements that can be executed as a single unit. They improve performance by reducing the need for repeated parsing and compilation, and enhance security by encapsulating business logic and restricting direct access to data."
19. Write a SQL query to find all products that have not been sold from a table named Products and a table named Sales.
Why you might get asked this: This question tests your ability to perform complex data retrieval operations involving multiple tables, which is crucial for roles that require advanced SQL skills, such as a Data Analyst or SQL Developer, for example.
How to answer:
1 Explain the use of a subquery to identify products that have been sold.
2 Describe the use of the NOT IN clause to filter out sold products from the Products table.
3 Provide a sample query using SELECT * FROM Products WHERE ProductID NOT IN (SELECT ProductID FROM Sales) .
Example answer:
"To find all products that have not been sold, you can use a subquery to identify sold products and then filter them out from the Products table. The query would look like this: SELECT * FROM Products WHERE ProductID NOT IN (SELECT ProductID FROM Sales); "
20. What is the purpose of the GROUP BY clause? Provide an example.
Why you might get asked this: Understanding the purpose of the GROUP BY clause is essential for roles that involve data aggregation and reporting, such as a Data Analyst or Business Intelligence Developer, for example.
How to answer:
1 Explain that the GROUP BY clause is used to group rows that have the same values in specified columns.
2 Highlight that it allows aggregate functions like SUM , COUNT , and AVG to be applied to each group.
3 Provide a simple example, such as SELECT department, COUNT(*) FROM Employees GROUP BY department .
Example answer:
"The GROUP BY clause is used to group rows that have the same values in specified columns, allowing aggregate functions to be applied to each group. For example, SELECT department, COUNT(*) FROM Employees GROUP BY department counts the number of employees in each department."
21. Write a SQL query to retrieve the names of employees who have the same job title as 'Manager'.
Why you might get asked this: This question tests your ability to write SQL queries that involve string matching and filtering, which is crucial for roles that require data retrieval and manipulation, such as a Data Analyst or SQL Developer, for example.
How to answer:
1 Explain the use of the SELECT statement to retrieve employee names.
2 Describe the WHERE clause to filter employees with the job title 'Manager'.
3 Provide a sample query using SELECT name FROM Employees WHERE job_title = 'Manager' .
Example answer:
"To retrieve the names of employees who have the same job title as 'Manager', you can use the following SQL query: SELECT name FROM Employees WHERE job_title = 'Manager'; This query selects the names of all employees whose job title is 'Manager' from the Employees table."
22. Explain the difference between a clustered index and a non-clustered index.
Why you might get asked this: Understanding the difference between a clustered index and a non-clustered index is crucial for optimizing database performance, which is essential for roles that involve database management and optimization, such as a Database Administrator or SQL Developer, for example.
How to answer:
1 Define a clustered index as a type of index that sorts and stores the data rows in the table based on the index key.
2 Explain that a non-clustered index creates a separate structure to store the index and includes a pointer to the data rows.
3 Highlight that a table can have only one clustered index but multiple non-clustered indexes.
Example answer:
"A clustered index sorts and stores the data rows in the table based on the index key, making data retrieval faster. In contrast, a non-clustered index creates a separate structure to store the index and includes a pointer to the data rows, allowing for multiple non-clustered indexes on a table."
23. Write a SQL query to find the average salary of employees in each department from a table named Employees.
Why you might get asked this: This question evaluates your ability to perform aggregate calculations and group data, which are essential skills for roles that involve data analysis and reporting, such as a Data Analyst or Business Intelligence Developer, for example.
How to answer:
1 Explain the use of the AVG function to calculate the average salary.
2 Describe the GROUP BY clause to group the results by department.
3 Provide a sample query using SELECT department, AVG(salary) FROM Employees GROUP BY department .
Example answer:
"To find the average salary of employees in each department from a table named Employees, you can use the AVG function along with the GROUP BY clause. The query would look like this: SELECT department, AVG(salary) FROM Employees GROUP BY department; "
24. What is a trigger in SQL? Provide an example of when you might use one.
Why you might get asked this: Understanding triggers and their applications is crucial for automating database tasks and ensuring data integrity, which is essential for roles that involve database management and optimization, such as a Database Administrator or SQL Developer, for example.
How to answer:
1 Define a trigger as a special type of stored procedure that automatically executes in response to certain events on a table or view.
2 Explain that triggers can be used to enforce business rules, maintain audit trails, or synchronize tables.
3 Provide an example, such as using a trigger to automatically update a stock quantity in an inventory table when a new order is inserted.
Example answer:
"A trigger in SQL is a special type of stored procedure that automatically executes in response to certain events on a table or view. For example, you might use a trigger to automatically update a stock quantity in an inventory table when a new order is inserted."
25. Write a SQL query to retrieve all distinct values from a column named Category in a table named Products.
Why you might get asked this: This question tests your ability to write queries that retrieve unique values, a fundamental skill for roles that involve data analysis and reporting, such as a Data Analyst or SQL Developer, for example.
How to answer:
1 Explain the use of the SELECT DISTINCT statement to retrieve unique values.
2 Describe the FROM clause to specify the table name.
3 Provide a sample query using SELECT DISTINCT Category FROM Products .
Example answer:
"To retrieve all distinct values from a column named Category in a table named Products, you can use the SELECT DISTINCT statement. The query would look like this: SELECT DISTINCT Category FROM Products; "
26. What are window functions and how do they differ from aggregate functions?
Why you might get asked this: Understanding window functions is crucial for advanced data analysis and reporting tasks, which is essential for roles like Data Analyst or Business Intelligence Developer.
How to answer:
1 Define window functions as functions that perform calculations across related rows within a result set.
2 Explain that unlike aggregate functions, window functions don't group rows into a single output row.
3 Mention that window functions use the OVER clause to define the window of rows.
Example answer: "Window functions perform calculations across a set of related rows without collapsing them into a single row like aggregate functions do. They use the OVER clause to define the window of rows, enabling calculations like running totals, rankings, and moving averages while preserving individual row details."
27. Write a SQL query to find the nth highest salary using window functions.
Why you might get asked this: This question tests your understanding of advanced SQL techniques for ranking and ordering data, which is crucial for roles involving complex data analysis.
How to answer:
1 Explain the use of the ROW_NUMBER() or DENSE_RANK() window function.
2 Describe how to order the data and filter for the specific rank.
3 Provide a sample query using window functions to find the nth highest salary.
Example answer: "To find the nth highest salary using window functions, you can use DENSE_RANK() to rank salaries and filter for the specific position. The query would look like: SELECT * FROM (SELECT *, DENSE_RANK() OVER (ORDER BY salary DESC) as rank FROM Employees) ranked WHERE rank = n;"
28. Explain the difference between HAVING and WHERE clauses.
Why you might get asked this: Understanding the distinction between HAVING and WHERE is fundamental for proper query construction, especially important for roles involving data filtering and aggregation.
How to answer:
1 Explain that WHERE filters rows before grouping occurs.
2 Describe that HAVING filters groups after GROUP BY has been applied.
3 Mention that HAVING can work with aggregate functions while WHERE cannot.
Example answer: "The WHERE clause filters individual rows before any grouping occurs, while the HAVING clause filters groups after the GROUP BY operation. WHERE cannot use aggregate functions, but HAVING can, making it essential for filtering grouped data based on aggregate conditions."
29. What is a Common Table Expression (CTE) and when would you use it?
Why you might get asked this: Understanding CTEs is important for writing readable and maintainable complex queries, which is valuable for roles involving advanced SQL development.
How to answer:
1 Define CTE as a named temporary result set that exists within the scope of a single statement.
2 Explain that CTEs improve query readability and can be referenced multiple times.
3 Mention use cases like recursive queries, complex joins, and breaking down complex logic.
Example answer: "A Common Table Expression (CTE) is a named temporary result set defined using the WITH clause that exists only for the duration of a query. CTEs improve readability, enable recursive operations, and can be referenced multiple times within the same query, making complex queries more maintainable."
30. Write a SQL query to find duplicate records in a table.
Why you might get asked this: Identifying duplicate data is a common data quality task essential for roles involving data cleaning and database maintenance.
How to answer:
1 Explain using GROUP BY with the columns to check for duplicates.
2 Describe using HAVING with COUNT to filter groups with more than one record.
3 Provide a sample query that identifies duplicate records.
Example answer: "To find duplicate records, you can group by the columns that should be unique and use HAVING to filter groups with more than one record: SELECT column1, column2, COUNT() FROM table_name GROUP BY column1, column2 HAVING COUNT() > 1;"
31. What is the difference between DELETE, TRUNCATE, and DROP?
Why you might get asked this: Understanding different methods of removing data is crucial for database management and maintenance roles.
How to answer:
1 Explain DELETE removes specific rows and can use WHERE clause.
2 Describe TRUNCATE removes all rows but keeps table structure.
3 Mention DROP removes the entire table including structure.
Example answer: "DELETE removes specific rows based on conditions and can be rolled back, TRUNCATE removes all rows quickly but cannot be rolled back in most databases, and DROP removes the entire table structure and data permanently. TRUNCATE is faster than DELETE for removing all data."
32. How do you handle NULL values in SQL queries?
Why you might get asked this: Proper NULL handling is essential for accurate data analysis and preventing unexpected query results.
How to answer:
1 Explain using IS NULL and IS NOT NULL for checking NULL values.
2 Mention functions like COALESCE, ISNULL, or IFNULL for handling NULLs.
3 Describe how NULLs behave in comparisons and aggregate functions.
Example answer: "NULL values require special handling using IS NULL or IS NOT NULL operators. Functions like COALESCE can replace NULLs with default values. NULLs in aggregate functions are typically ignored, and any comparison with NULL returns unknown, not true or false."
33. Write a SQL query to calculate a running total.
Why you might get asked this: Running totals are common in financial and analytical reports, making this skill valuable for data analysis roles.
How to answer:
1 Explain using window functions with the SUM function.
2 Describe the ORDER BY clause within the OVER clause.
3 Provide a sample query showing running total calculation.
Example answer: "To calculate a running total, use the SUM window function with an ORDER BY clause: SELECT date, amount, SUM(amount) OVER (ORDER BY date) as running_total FROM sales ORDER BY date;"
34. What are the different types of constraints in SQL?
Why you might get asked this: Understanding constraints is fundamental for maintaining data integrity, which is crucial for database design and administration roles.
How to answer:
1 List the main constraint types: PRIMARY KEY, FOREIGN KEY, UNIQUE, NOT NULL, CHECK.
2 Briefly explain the purpose of each constraint type.
3 Mention how constraints ensure data integrity and business rules.
Example answer: "SQL constraints include PRIMARY KEY (unique identifier), FOREIGN KEY (referential integrity), UNIQUE (no duplicates), NOT NULL (required values), and CHECK (custom validation rules). These constraints ensure data integrity and enforce business rules at the database level."
35. Explain the concept of database transactions and isolation levels.
Why you might get asked this: Understanding transactions is crucial for maintaining data consistency, especially important for roles involving financial or critical business data.
How to answer:
1 Define a transaction as a unit of work that either completes entirely or fails entirely.
2 Explain the four isolation levels: READ UNCOMMITTED, READ COMMITTED, REPEATABLE READ, SERIALIZABLE.
3 Mention the trade-offs between isolation levels and performance.
Example answer: "A database transaction is a logical unit of work that maintains data consistency. Isolation levels control how transaction changes are visible to other concurrent transactions, ranging from READ UNCOMMITTED (lowest isolation, highest concurrency) to SERIALIZABLE (highest isolation, lowest concurrency)."
36. Write a SQL query to pivot data from rows to columns.
Why you might get asked this: Data pivoting is essential for creating reports and transforming data for analysis, valuable for business intelligence roles.
How to answer:
1 Explain the concept of pivoting data using CASE statements or PIVOT function.
2 Describe how to transform row data into column format.
3 Provide a sample query showing data transformation.
Example answer: "To pivot data, you can use CASE statements with aggregate functions: SELECT product, SUM(CASE WHEN month = 'Jan' THEN sales ELSE 0 END) as Jan_Sales, SUM(CASE WHEN month = 'Feb' THEN sales ELSE 0 END) as Feb_Sales FROM sales_data GROUP BY product;"
37. What is the difference between RANK(), DENSE_RANK(), and ROW_NUMBER()?
Why you might get asked this: Understanding ranking functions is important for data analysis and reporting tasks that require ordering and positioning data.
How to answer:
1 Explain ROW_NUMBER() assigns unique sequential numbers.
2 Describe RANK() leaves gaps after ties.
3 Mention DENSE_RANK() doesn't leave gaps after ties.
Example answer: "ROW_NUMBER() assigns unique sequential numbers regardless of ties. RANK() assigns the same rank to tied values but leaves gaps in subsequent rankings. DENSE_RANK() assigns the same rank to tied values without leaving gaps in the sequence."
38. How do you optimize SQL query performance?
Why you might get asked this: Query optimization is crucial for maintaining application performance, especially important for senior database roles.
How to answer:
1 Mention proper indexing strategies and query structure.
2 Explain analyzing execution plans and identifying bottlenecks.
3 Describe techniques like avoiding SELECT *, using appropriate joins, and limiting result sets.
Example answer: "SQL query optimization involves proper indexing, analyzing execution plans, avoiding SELECT *, using appropriate join types, limiting result sets with WHERE clauses, and ensuring statistics are up to date. Regular monitoring and profiling help identify performance bottlenecks."
39. What are user-defined functions and how do they differ from stored procedures?
Why you might get asked this: Understanding different database objects is important for advanced database development and maintenance roles.
How to answer:
1 Define user-defined functions as reusable code blocks that return values.
2 Explain that functions return values while stored procedures may not.
3 Mention that functions can be used in SELECT statements while procedures cannot.
Example answer: "User-defined functions are reusable code blocks that return values and can be used within SQL statements like SELECT queries. Unlike stored procedures, functions must return a value and cannot perform operations like INSERT, UPDATE, or DELETE on the same database."
40. Write a SQL query to find the percentage of total sales for each product.
Why you might get asked this: Calculating percentages is common in business analysis and reporting, valuable for analytical roles.
How to answer:
1 Explain using window functions to calculate total sales.
2 Describe how to calculate individual product sales as a percentage of total.
3 Provide a sample query showing percentage calculation.
Example answer: "To find the percentage of total sales for each product: SELECT product, sales, (sales * 100.0 / SUM(sales) OVER()) as percentage_of_total FROM product_sales;"
41. What is the difference between correlated and non-correlated subqueries?
Why you might get asked this: Understanding subquery types is important for writing efficient complex queries, essential for advanced SQL development roles.
How to answer:
1 Define non-correlated subqueries as independent of the outer query.
2 Explain correlated subqueries reference columns from the outer query.
3 Mention performance implications of each type.
Example answer: "Non-correlated subqueries are independent and execute once, returning results used by the outer query. Correlated subqueries reference the outer query and execute once for each row of the outer query, often making them slower but more flexible for row-by-row comparisons."
42. How do you handle date and time operations in SQL?
Why you might get asked this: Date and time manipulation is common in business applications, important for roles involving temporal data analysis.
How to answer:
1 Mention common date functions like DATE_ADD, DATEDIFF, EXTRACT.
2 Explain formatting dates and handling time zones.
3 Describe best practices for date storage and querying.
Example answer: "Date and time operations use functions like DATE_ADD for arithmetic, DATEDIFF for calculating differences, and EXTRACT for getting specific parts. Always consider time zones, use appropriate data types (DATE, DATETIME, TIMESTAMP), and be careful with date formatting for consistent results."
43. Write a SQL query to find records that exist in one table but not in another.
Why you might get asked this: Finding data differences between tables is common in data validation and migration tasks.
How to answer:
1 Explain using LEFT JOIN with IS NULL condition.
2 Describe alternative approaches using NOT EXISTS or EXCEPT.
3 Provide sample queries showing different methods.
Example answer: "To find records in table A but not in table B, use: SELECT a.* FROM tableA a LEFT JOIN tableB b ON a.id = b.id WHERE b.id IS NULL; Alternatively, use NOT EXISTS: SELECT * FROM tableA WHERE NOT EXISTS (SELECT 1 FROM tableB WHERE tableB.id = tableA.id);"
44. What is database denormalization and when would you use it?
Why you might get asked this: Understanding when to denormalize is important for performance optimization in large-scale systems.
How to answer:
1 Define denormalization as intentionally introducing redundancy for performance.
2 Explain scenarios where denormalization is beneficial.
3 Mention trade-offs between performance and data integrity.
Example answer: "Denormalization intentionally introduces redundancy to improve query performance by reducing joins. It's useful for read-heavy systems, data warehouses, and reporting databases where query speed is more important than storage efficiency, but it requires careful maintenance to ensure data consistency."
45. Explain the difference between a primary key and a unique key.
Why you might get asked this: Understanding key constraints is fundamental for database design and data integrity.
How to answer:
1 Explain that primary keys cannot be NULL and there can be only one per table.
2 Describe that unique keys can be NULL and there can be multiple per table.
3 Mention their roles in indexing and referential integrity.
Example answer: "A primary key uniquely identifies rows, cannot contain NULL values, and there can be only one per table. A unique key also ensures uniqueness but can contain NULL values and there can be multiple unique keys per table. Both automatically create indexes."
46. Write a SQL query to find the median value from a column.
Why you might get asked this: Calculating statistical measures like median is important for data analysis roles.
How to answer:
1 Explain using window functions with PERCENTILE_CONT or similar functions.
2 Describe alternative approaches using ROW_NUMBER for databases without built-in median functions.
3 Provide sample queries for calculating median.
Example answer: "To find the median, use PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY column_name) or for databases without this function: SELECT AVG(salary) FROM (SELECT salary, ROW_NUMBER() OVER (ORDER BY salary) as rn, COUNT(*) OVER() as cnt FROM employees) WHERE rn IN ((cnt+1)/2, (cnt+2)/2);"
47. What are the advantages and disadvantages of using views?
Why you might get asked this: Understanding views is important for database design and security implementation.
How to answer:
1 List advantages: security, simplification, data abstraction.
2 Mention disadvantages: performance overhead, dependency issues.
3 Explain when views are most beneficial.
Example answer: "Views provide security by hiding sensitive columns, simplify complex queries, and offer data abstraction. However, they can have performance overhead, create dependencies, and complex views may not be updatable. They're best for frequently used complex queries and security requirements."
48. How do you implement full-text search in SQL?
Why you might get asked this: Full-text search capabilities are important for applications requiring advanced search functionality.
How to answer:
1 Explain full-text indexes and search functions like MATCH AGAINST.
2 Mention different search modes (natural language, Boolean).
3 Describe limitations and alternatives.
Example answer: "Full-text search uses specialized indexes and functions like MATCH() AGAINST() in MySQL or CONTAINS() in SQL Server. It supports natural language and Boolean search modes, ranking results by relevance. For complex requirements, dedicated search engines like Elasticsearch might be more appropriate."
49. Write a SQL query to find the first and last record for each group.
Why you might get asked this: Finding boundary records is common in time-series analysis and reporting.
How to answer:
1 Explain using window functions with FIRST_VALUE and LAST_VALUE.
2 Describe alternative approaches using subqueries and joins.
3 Provide sample queries showing different methods.
Example answer: "To find first and last records per group: SELECT *, FIRST_VALUE(value) OVER (PARTITION BY group_id ORDER BY date), LAST_VALUE(value) OVER (PARTITION BY group_id ORDER BY date ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) FROM table_name;"
50. What is the purpose of the COALESCE function?
Why you might get asked this: Understanding NULL handling functions is important for data quality and accurate reporting.
How to answer:
1 Define COALESCE as a function that returns the first non-NULL value.
2 Explain its use in handling NULL values and providing defaults.
3 Provide examples of practical applications.
Example answer: "COALESCE returns the first non-NULL value from a list of expressions, making it useful for handling NULL values and providing default values. For example, COALESCE(middle_name, '') returns an empty string if middle_name is NULL, ensuring consistent output formatting."
51. Explain the concept of recursive queries and provide an example.
Why you might get asked this: Recursive queries are important for hierarchical data processing, valuable for advanced database development roles.
How to answer:
1 Define recursive queries as queries that reference themselves.
2 Explain the structure with anchor and recursive members.
3 Provide an example using hierarchical data like organizational charts.
Example answer: "Recursive queries use Common Table Expressions to process hierarchical data by referencing themselves. They consist of an anchor member (base case) and recursive member. Example: WITH RECURSIVE emp_hierarchy AS (SELECT id, name, manager_id, 1 as level FROM employees WHERE manager_id IS NULL UNION ALL SELECT e.id, e.name, e.manager_id, eh.level+1 FROM employees e JOIN emp_hierarchy eh ON e.manager_id = eh.id) SELECT * FROM emp_hierarchy;"
52. What are materialized views and how do they differ from regular views?
Why you might get asked this: Understanding materialized views is important for performance optimization in data warehousing and analytics.
How to answer:
1 Define materialized views as physical storage of view results.
2 Explain performance benefits and refresh mechanisms.
3 Describe trade-offs with storage and data freshness.
Example answer: "Materialized views physically store query results, providing faster access than regular views that execute queries each time. They're ideal for complex aggregations and reporting but require refresh strategies to maintain data currency and consume additional storage space."
53. Write a SQL query to calculate year-over-year growth percentage.
Why you might get asked this: Year-over-year calculations are common in business analytics and financial reporting.
How to answer:
1 Explain using window functions with LAG to get previous year values.
2 Describe the formula for calculating growth percentage.
3 Provide a sample query showing the calculation.
Example answer: "To calculate year-over-year growth: SELECT year, revenue, LAG(revenue) OVER (ORDER BY year) as prev_year_revenue, ((revenue - LAG(revenue) OVER (ORDER BY year)) * 100.0 / LAG(revenue) OVER (ORDER BY year)) as yoy_growth_pct FROM annual_revenue ORDER BY year;"
54. How do you handle large dataset queries efficiently?
Why you might get asked this: Optimizing queries for large datasets is crucial for enterprise-level database performance.
How to answer:
1 Mention partitioning strategies and proper indexing.
2 Explain query optimization techniques and pagination.
3 Describe using appropriate hardware and configuration.
Example answer: "For large datasets, use table partitioning, proper indexing, query optimization with execution plan analysis, pagination for result sets, and consider parallel processing. Also implement appropriate WHERE clauses to limit data scanned and use summary tables for frequently accessed aggregations."
55. What is the difference between CHAR and VARCHAR data types?
Why you might get asked this: Understanding data types is fundamental for efficient database design and storage optimization.
How to answer:
1 Explain CHAR as fixed-length and VARCHAR as variable-length.
2 Mention storage implications and performance considerations.
3 Describe when to use each type.
Example answer: "CHAR is fixed-length and pads shorter values with spaces, while VARCHAR is variable-length and stores only the actual characters. CHAR is faster for fixed-size data but wastes space, while VARCHAR is more storage-efficient but has slight performance overhead for variable-length data."
56. Write a SQL query to find gaps in sequential data.
Why you might get asked this: Finding gaps in sequences is common in inventory management and audit scenarios.
How to answer:
1 Explain using window functions to identify missing sequences.
2 Describe comparing expected vs actual sequence values.
3 Provide a sample query for gap detection.
Example answer: "To find gaps in sequential data: SELECT (LAG(id) OVER (ORDER BY id) + 1) as gap_start, (id - 1) as gap_end FROM table_name WHERE id - LAG(id) OVER (ORDER BY id) > 1;"
57. Explain the concept of database partitioning and its types.
Why you might get asked this: Understanding partitioning is important for managing large databases and improving query performance.
How to answer:
1 Define partitioning as dividing large tables into smaller manageable pieces.
2 Explain different types: range, list, hash, and composite partitioning.
3 Mention benefits like improved performance and maintenance.
Example answer: "Database partitioning divides large tables into smaller, manageable segments based on partition keys. Types include range (date ranges), list (specific values), hash (even distribution), and composite (combination). Benefits include improved query performance, easier maintenance, and parallel processing capabilities."
58. What are database locks and how do they work?
Why you might get asked this: Understanding locking mechanisms is crucial for database concurrency and performance management.
How to answer:
1 Define locks as mechanisms to control concurrent access to data.
2 Explain different lock types: shared, exclusive, update locks.
3 Mention deadlocks and lock escalation.
Example answer: "Database locks control concurrent access to data, preventing conflicts between transactions. Types include shared locks (allow multiple reads), exclusive locks (prevent all other access), and update locks (prevent deadlocks during updates). Proper lock management prevents data corruption while maintaining concurrency."
59. Write a SQL query to transpose rows to columns dynamically.
Why you might get asked this: Dynamic transposition is useful for flexible reporting and data presentation.
How to answer:
1 Explain challenges of dynamic pivoting without knowing column names.
2 Describe using dynamic SQL or specific database features.
3 Provide an approach using conditional aggregation.
Example answer: "Dynamic transposition requires dynamic SQL since column names aren't known beforehand. Example approach: Build the SQL string dynamically based on distinct values in the pivot column, then execute it. Some databases offer PIVOT operators, but most require conditional aggregation with dynamically built CASE statements."
60. How do you implement audit trails in SQL databases?
Why you might get asked this: Audit trails are important for compliance and security requirements in enterprise applications.
How to answer:
1 Explain using triggers to capture data changes.
2 Mention audit table design with old/new values and metadata.
3 Describe alternative approaches like Change Data Capture.
Example answer: "Audit trails track data changes using triggers that insert records into audit tables containing old values, new values, operation type, user, and timestamp. Alternative approaches include Change Data Capture (CDC) features, database logs analysis, or application-level logging for better performance."
61. What is the difference between OLTP and OLAP systems?
Why you might get asked this: Understanding different database architectures is important for system design and data architecture roles.
How to answer:
1 Define OLTP as transaction-focused systems for daily operations.
2 Explain OLAP as analytics-focused systems for decision support.
3 Mention design differences and optimization strategies.
Example answer: "OLTP (Online Transaction Processing) systems handle high-volume transactions with normalized databases optimized for INSERT, UPDATE, DELETE operations. OLAP (Online Analytical Processing) systems are designed for complex queries and reporting with denormalized, dimensional models optimized for SELECT operations and aggregations."
62. Write a SQL query to find the top N customers by sales in each region.
Why you might get asked this: Ranking within groups is common in business analytics and competitive analysis.
How to answer:
1 Explain using window functions with partitioning.
2 Describe ranking and filtering for top N results.
3 Provide a sample query using ROW_NUMBER or RANK.
Example answer: "To find top N customers by sales in each region: SELECT * FROM (SELECT customer, region, sales, ROW_NUMBER() OVER (PARTITION BY region ORDER BY sales DESC) as rn FROM customer_sales) ranked WHERE rn <= N;"
63. How do you handle database schema migrations?
Why you might get asked this: Schema migrations are crucial for application deployment and database evolution management.
How to answer:
1 Explain version control for database schemas.
2 Mention migration tools and rollback strategies.
3 Describe testing and deployment best practices.
Example answer: "Database schema migrations use version-controlled scripts with tools like Flyway or Liquibase. Each migration is numbered sequentially, tested thoroughly, and includes rollback procedures. Best practices include backward compatibility, data migration validation, and coordination with application deployments."
64. What are the considerations for choosing appropriate data types?
Why you might get asked this: Proper data type selection affects storage efficiency, performance, and data integrity.
How to answer:
1 Mention storage requirements and performance implications.
2 Explain precision needs and range considerations.
3 Describe future scalability and standardization.
Example answer: "Data type selection considers storage efficiency, query performance, data range and precision requirements, and future scalability. Choose the smallest appropriate type, consider indexing implications, ensure proper precision for calculations, and maintain consistency across similar fields in the database."
65. Write a SQL query to calculate moving averages.
Why you might get asked this: Moving averages are important for trend analysis and financial calculations.
How to answer:
1 Explain using window functions with frame specifications.
2 Describe different frame options like ROWS and RANGE.
3 Provide sample queries for different moving average periods.
Example answer: "To calculate a moving average: SELECT date, value, AVG(value) OVER (ORDER BY date ROWS BETWEEN 2 PRECEDING AND CURRENT ROW) as moving_avg_3_days FROM sales_data ORDER BY date;"
66. How do you implement row-level security in SQL databases?
Why you might get asked this: Row-level security is important for multi-tenant applications and data privacy compliance.
How to answer:
1 Explain using security policies and predicates.
2 Mention role-based access control implementation.
3 Describe alternative approaches using views and functions.
Example answer: "Row-level security uses security policies with predicates to filter rows based on user context. Implementation involves creating security functions that return filter conditions, then applying them as policies. Alternative approaches include using views with user-based WHERE clauses or application-level filtering."
67. What is the purpose of database connection pooling?
Why you might get asked this: Connection pooling is crucial for application performance and resource management.
How to answer:
1 Explain reducing connection overhead and resource usage.
2 Mention improved application scalability and performance.
3 Describe configuration considerations and best practices.
Example answer: "Database connection pooling reuses existing connections instead of creating new ones for each request, reducing overhead and improving performance. It manages a pool of connections shared among application threads, preventing connection exhaustion and providing better resource utilization and scalability."
68. Write a SQL query to find customers who haven't placed orders in the last 6 months.
Why you might get asked this: Identifying inactive customers is important for customer retention and marketing analytics.
How to answer:
1 Explain using LEFT JOIN with date filtering.
2 Describe alternative approaches with NOT EXISTS.
3 Provide sample queries showing different methods.
Example answer: "To find inactive customers: SELECT c.* FROM customers c LEFT JOIN orders o ON c.customer_id = o.customer_id AND o.order_date >= DATE_SUB(NOW(), INTERVAL 6 MONTH) WHERE o.customer_id IS NULL;"
69. How do you handle database backup and recovery strategies?
Why you might get asked this: Backup and recovery planning is critical for database administration and business continuity.
How to answer:
1 Explain different backup types: full, incremental, differential.
2 Mention recovery models and point-in-time recovery.
3 Describe testing and automation strategies.
Example answer: "Database backup strategies include full backups (complete database), incremental (changes since last backup), and differential (changes since last full backup). Recovery planning involves defining RTO/RPO requirements, implementing automated backup schedules, regular restore testing, and maintaining both local and offsite copies."
70. What are the best practices for SQL query writing?
Why you might get asked this: Following best practices is important for maintainable, efficient, and secure database code.
How to answer:
1 Mention code formatting, naming conventions, and documentation.
2 Explain performance optimization and security considerations.
3 Describe testing and code review practices.
Example answer: "SQL best practices include consistent formatting and naming conventions, avoiding SELECT *, using proper indexing, parameterized queries for security, meaningful aliases, appropriate JOINs, query optimization with execution plans, comprehensive testing, and thorough documentation for complex logic."
71. Write a SQL query to find overlapping date ranges.
Why you might get asked this: Finding overlapping periods is common in scheduling, reservation, and temporal data analysis.
How to answer:
1 Explain the logic for detecting overlapping intervals.
2 Describe using self-joins or window functions.
3 Provide sample queries for overlap detection.
Example answer: "To find overlapping date ranges: SELECT a., b. FROM reservations a JOIN reservations b ON a.id < b.id AND a.start_date <= b.end_date AND a.end_date >= b.start_date;"
72. How do you implement database replication and what are its types?
Why you might get asked this: Database replication is important for high availability, scalability, and disaster recovery.
How to answer:
1 Explain master-slave and master-master replication.
2 Mention synchronous vs asynchronous replication.
3 Describe use cases and trade-offs.
Example answer: "Database replication types include master-slave (read replicas for scaling reads), master-master (bi-directional for high availability), and cluster replication. Synchronous replication ensures consistency but affects performance, while asynchronous replication provides better performance but potential data lag."
73. What is database sharding and when would you implement it?
Why you might get asked this: Sharding is important for scaling very large databases beyond single-server capabilities.
How to answer:
1 Define sharding as horizontal partitioning across servers.
2 Explain sharding strategies and key selection.
3 Mention complexity and trade-offs.
Example answer: "Database sharding horizontally partitions data across multiple servers using a shard key. It's implemented when vertical scaling limits are reached, enabling linear scaling. However, it adds complexity for cross-shard queries, transactions, and requires careful shard key selection to avoid hotspots."
74. Write a SQL query to calculate retention rates.
Why you might get asked this: Retention analysis is crucial for understanding customer behavior and business performance.
How to answer:
1 Explain defining cohorts and retention periods.
2 Describe calculating percentage of returning users.
3 Provide sample queries for retention calculation.
Example answer: "To calculate retention rates: SELECT cohort_month, period_number, COUNT(DISTINCT user_id) * 100.0 / first_month_users as retention_rate FROM user_activity_cohorts GROUP BY cohort_month, period_number ORDER BY cohort_month, period_number;"
75. How do you monitor and troubleshoot database performance issues?
Why you might get asked this: Performance monitoring is crucial for maintaining database health and application performance.
How to answer:
1 Mention monitoring tools and key metrics.
2 Explain query analysis and execution plans.
3 Describe proactive monitoring and alerting.
Example answer: "Database performance monitoring involves tracking metrics like CPU, memory, I/O, and query response times using tools like performance dashboards, slow query logs, and execution plan analysis. Implement alerts for threshold breaches, regular index analysis, and maintain baseline performance metrics for comparison."
76. What are database constraints and how do they ensure data integrity?
Why you might get asked this: Understanding constraints is fundamental for maintaining data quality and business rule enforcement.
How to answer:
1 List constraint types and their purposes.
2 Explain how constraints prevent invalid data.
3 Mention performance implications and best practices.
Example answer: "Database constraints enforce data integrity rules: PRIMARY KEY ensures unique identification, FOREIGN KEY maintains referential integrity, UNIQUE prevents duplicates, NOT NULL requires values, and CHECK validates business rules. They prevent invalid data entry and maintain consistency across the database."
77. Write a SQL query to generate a calendar table.
Why you might get asked this: Calendar tables are useful for date-based reporting and analytics applications.
How to answer:
1 Explain using recursive CTEs or number sequences.
2 Describe adding date attributes and business logic.
3 Provide sample queries for calendar generation.
Example answer: "To generate a calendar table: WITH RECURSIVE calendar AS (SELECT '2024-01-01' as date UNION ALL SELECT DATE_ADD(date, INTERVAL 1 DAY) FROM calendar WHERE date < '2024-12-31') SELECT date, YEAR(date) as year, MONTH(date) as month, DAY(date) as day, DAYNAME(date) as day_name FROM calendar;"
78. How do you implement database versioning and change management?
Why you might get asked this: Version control for databases is important for team collaboration and deployment management.
How to answer:
1 Explain database migration tools and version control.
2 Mention change scripts and rollback procedures.
3 Describe integration with application deployment.
Example answer: "Database versioning uses migration tools like Flyway or Liquibase with version-controlled SQL scripts. Each change is numbered sequentially, includes rollback procedures, and integrates with CI/CD pipelines. This ensures consistent schema evolution across environments and enables reliable deployments."
79. What are the security considerations for SQL databases?
Why you might get asked this: Database security is critical for protecting sensitive data and preventing breaches.
How to answer:
1 Mention access control, encryption, and SQL injection prevention.
2 Explain audit logging and network security.
3 Describe principle of least privilege and regular security updates.
Example answer: "SQL database security includes access control with role-based permissions, encryption at rest and in transit, SQL injection prevention through parameterized queries, comprehensive audit logging, network security with firewalls, regular security updates, and implementing the principle of least privilege for user access."
80. Write a SQL query to find the longest consecutive sequence.
Why you might get asked this: Finding consecutive sequences is useful in gaming, finance, and behavioral analysis.
How to answer:
1 Explain using window functions to identify sequence breaks.
2 Describe grouping consecutive elements.
3 Provide sample queries for sequence analysis.
Example answer: "To find longest consecutive sequence: SELECT MAX(consecutive_count) FROM (SELECT COUNT(*) as consecutive_count FROM (SELECT *, ROW_NUMBER() OVER (ORDER BY value) - ROW_NUMBER() OVER (PARTITION BY value ORDER BY value) as grp FROM sequences) grouped GROUP BY grp) counts;"
81. How do you handle time zone considerations in global applications?
Why you might get asked this: Time zone handling is important for international applications and accurate temporal data management.
How to answer:
1 Explain storing times in UTC and converting for display.
2 Mention time zone data types and functions.
3 Describe best practices for global applications.
Example answer: "Handle time zones by storing all timestamps in UTC and converting to local time zones for display. Use timezone-aware data types like TIMESTAMPTZ, implement proper conversion functions, maintain time zone reference data, and consider daylight saving time changes in business logic."
82. What is the difference between database clustering and replication?
Why you might get asked this: Understanding different high-availability architectures is important for system design decisions.
How to answer:
1 Define clustering as multiple servers acting as one system.
2 Explain replication as copying data to multiple servers.
3 Mention use cases and implementation differences.
Example answer: "Database clustering involves multiple servers working together as a single system with shared storage, providing high availability and load distribution. Replication copies data across separate database instances for read scaling and disaster recovery. Clustering offers automatic failover, while replication requires manual or automated failover management."
83. Write a SQL query to implement a recommendation system using collaborative filtering.
Why you might get asked this: Basic recommendation logic is useful for understanding how data-driven features work in applications.
How to answer:
1 Explain finding users with similar preferences.
2 Describe recommending items based on similar users' choices.
3 Provide sample queries for collaborative filtering.
Example answer: "Basic collaborative filtering: SELECT p2.product_id, COUNT(*) as similarity_score FROM purchases p1 JOIN purchases p2 ON p1.user_id = p2.user_id JOIN purchases p3 ON p3.product_id = p1.product_id WHERE p3.user_id = @target_user AND p2.product_id NOT IN (SELECT product_id FROM purchases WHERE user_id = @target_user) GROUP BY p2.product_id ORDER BY similarity_score DESC;"
84. How do you implement data archiving strategies?
Why you might get asked this: Data archiving is important for managing database size and maintaining performance while preserving historical data.
How to answer:
1 Explain identifying data for archiving based on age or usage.
2 Mention archival storage options and retrieval methods.
3 Describe automation and compliance considerations.
Example answer: "Data archiving strategies involve identifying old or infrequently accessed data, moving it to cheaper storage while maintaining accessibility. Implement automated archiving jobs based on date or usage patterns, use compressed storage formats, maintain indexes for archived data retrieval, and ensure compliance with data retention policies."
85. What are the considerations for database capacity planning?
Why you might get asked this: Capacity planning is crucial for maintaining database performance and avoiding resource constraints.
How to answer:
1 Mention monitoring current usage trends and growth patterns.
2 Explain projecting future requirements and resource needs.
3 Describe planning for peak loads and scalability.
Example answer: "Database capacity planning involves monitoring current storage, CPU, memory, and I/O usage, analyzing growth trends, projecting future requirements based on business growth, planning for peak loads, considering data retention policies, and implementing monitoring alerts for proactive scaling decisions."
86. Write a SQL query to detect anomalies in time-series data.
Why you might get asked this: Anomaly detection is important for monitoring systems and identifying unusual patterns in business data.
How to answer:
1 Explain using statistical functions to identify outliers.
2 Describe comparing current values to historical averages.
3 Provide sample queries for anomaly detection.
Example answer: "To detect anomalies using standard deviation: SELECT *, CASE WHEN ABS(value - avg_value) > 2 * stddev_value THEN 'Anomaly' ELSE 'Normal' END as status FROM (SELECT *, AVG(value) OVER (ORDER BY timestamp ROWS BETWEEN 30 PRECEDING AND 1 PRECEDING) as avg_value, STDDEV(value) OVER (ORDER BY timestamp ROWS BETWEEN 30 PRECEDING AND 1 PRECEDING) as stddev_value FROM time_series_data) analyzed;"
87. How do you implement database disaster recovery procedures?
Why you might get asked this: Disaster recovery planning is critical for business continuity and data protection.
How to answer:
1 Explain backup strategies and offsite storage.
2 Mention recovery time and point objectives.
3 Describe testing and documentation requirements.
Example answer: "Disaster recovery involves regular backups with offsite storage, defining RTO/RPO objectives, maintaining secondary sites or cloud infrastructure, implementing automated failover procedures, regular disaster recovery testing, comprehensive documentation, and staff training for emergency procedures."
88. What are the emerging trends in database technology?
Why you might get asked this: Understanding technology trends is important for making informed architectural decisions and career development.
How to answer:
1 Mention cloud-native databases and serverless architectures.
2 Explain NewSQL and distributed database systems.
3 Describe AI/ML integration and real-time analytics.
Example answer: "Emerging database trends include cloud-native and serverless databases for auto-scaling, NewSQL systems combining ACID guarantees with horizontal scaling, AI/ML integration for automated optimization, real-time analytics with streaming databases, multi-model databases supporting various data types, and edge computing databases for IoT applications."
89. Write a SQL query to implement a simple rating system with weighted averages.
Why you might get asked this: Rating systems are common in e-commerce and review applications.
How to answer:
1 Explain weighting ratings by factors like recency or reviewer credibility.
2 Describe calculating weighted averages and handling edge cases.
3 Provide sample queries for rating calculations.
Example answer: "To implement weighted ratings: SELECT product_id, SUM(rating * weight) / SUM(weight) as weighted_avg_rating FROM (SELECT product_id, rating, CASE WHEN reviewer_level = 'expert' THEN 2.0 WHEN review_date > DATE_SUB(NOW(), INTERVAL 30 DAY) THEN 1.5 ELSE 1.0 END as weight FROM reviews) weighted_reviews GROUP BY product_id;"
90. How do you optimize database storage and reduce costs?
Why you might get asked this: Storage optimization is important for managing database costs and improving performance.
How to answer:
1 Mention data compression and archiving strategies.
2 Explain proper data type selection and index optimization.
3 Describe partitioning and storage tiering approaches.
Example answer: "Database storage optimization involves implementing data compression, choosing appropriate data types, archiving old data, optimizing indexes by removing unused ones, implementing table partitioning, using storage tiering for different data access patterns, and regularly analyzing storage usage patterns to identify optimization opportunities."
91. What are the key considerations for migrating from legacy database systems?
Why you might get asked this: Database migration is common in modernization projects and requires careful planning and execution.
How to answer:
1 Explain assessing current system and defining migration strategy.
2 Mention data mapping, testing, and rollback procedures.
3 Describe minimizing downtime and ensuring data integrity.
Example answer: "Legacy database migration requires thorough assessment of current system, data mapping and transformation planning, choosing appropriate migration tools, extensive testing with production-like data, implementing rollback procedures, planning for minimal downtime, training staff on new systems, and post-migration monitoring to ensure performance and data integrity."

---
Advanced SQL Interview Questions and Answers
Sanjay Kumar PhD6 min read · Mar 26, 2025--
Press enter or click to view image in full size
1. What is the difference between RANK(), DENSE_RANK(), and ROW_NUMBER()?
ROW_NUMBER() assigns a unique sequential number to each row.
RANK() gives the same rank to ties but leaves gaps.
DENSE_RANK() gives the same rank to ties without leaving gaps.
SELECT name, salary,
RANK() OVER (ORDER BY salary DESC) AS rank,
DENSE_RANK() OVER (ORDER BY salary DESC) AS dense_rank,
ROW_NUMBER() OVER (ORDER BY salary DESC) AS row_num
FROM employees;
2. How do you find the second highest salary from an Employee table?
SELECT MAX(salary) AS SecondHighest
FROM employees
WHERE salary < (SELECT MAX(salary) FROM employees);
Or using LIMIT
— — — — — — — — — — — — — — — — — — — — — — — — -
SELECT DISTINCT salary
FROM employees
ORDER BY salary DESC
LIMIT 1 OFFSET 1;
3. Explain Common Table Expressions (CTEs) and their use.
CTEs are temporary result sets used to simplify complex joins and subqueries.
WITH DeptTotal AS (
SELECT department_id, SUM(salary) AS total_salary
FROM employees
GROUP BY department_id
)
SELECT e.name, e.salary, d.total_salary
FROM employees e
JOIN DeptTotal d ON e.department_id = d.department_id;
4. How do you detect and remove duplicate records from a table?
To find duplicates:
SELECT name, COUNT(*)
FROM employees
GROUP BY name
HAVING COUNT(*) > 1;
To delete duplicates (keeping the lowest ID):
DELETE FROM employees
WHERE id NOT IN (
SELECT MIN(id)
FROM employees
GROUP BY name, department_id, salary
);
5. What is a window function? Give an example.
A window function performs a calculation across a set of table rows related to the current row.
Example: Running Total
SELECT name, salary,
SUM(salary) OVER (PARTITION BY department_id ORDER BY salary) AS running_total
FROM employees;
6. Write a query to pivot data in SQL.
Using CASE WHEN:
SELECT department_id,
SUM(CASE WHEN gender = ‘M’ THEN 1 ELSE 0 END) AS male_count,
SUM(CASE WHEN gender = ‘F’ THEN 1 ELSE 0 END) AS female_count
FROM employees
GROUP BY department_id;
7. Explain the difference between INNER JOIN, LEFT JOIN, RIGHT JOIN, and FULL JOIN.
INNER JOIN: Returns matching rows.
LEFT JOIN: Returns all from the left table, and matched rows from the right.
RIGHT JOIN: All from the right table, and matched from the left.
FULL JOIN: All rows when there’s a match in one of the tables.
8. What is the use of EXISTS vs IN vs JOIN?
IN works on a list of values.
EXISTS returns true if subquery returns rows.
JOIN merges rows from multiple tables.
EXISTS is faster than IN in correlated subqueries with large data sets.
SELECT name
FROM employees e
WHERE EXISTS (
SELECT 1
FROM departments d
WHERE d.manager_id = e.id
);
9. What is a recursive CTE?
Used to query hierarchical data like org charts.
WITH RECURSIVE EmployeeHierarchy AS (
SELECT id, name, manager_id
FROM employees
WHERE manager_id IS NULL
UNION ALL
SELECT e.id, e.name, e.manager_id
FROM employees e
INNER JOIN EmployeeHierarchy eh ON e.manager_id = eh.id
)
SELECT * FROM EmployeeHierarchy;
10. How would you optimize a slow SQL query?
Use EXPLAIN to analyze.
Add indexes on filtered/joined columns.
Avoid **SELECT ***; select only required columns.
Use CTEs or temp tables for complex subqueries.
Minimize use of functions in WHERE clause.
11. What is the difference between CROSS JOIN and INNER JOIN?
1 CROSS JOIN: Returns the Cartesian product of two tables. No condition is used.
2 INNER JOIN: Returns only matching rows based on a join condition.
— CROSS JOIN
SELECT * FROM employees CROSS JOIN departments;
— INNER JOIN
SELECT * FROM employees INNER JOIN departments
ON employees.department_id = departments.id;
12. How do you calculate a rolling average using SQL?
SELECT name, salary,
AVG(salary) OVER (ORDER BY hire_date ROWS BETWEEN 2 PRECEDING AND CURRENT ROW) AS rolling_avg
FROM employees;
This gives a 3-row moving average (current row + 2 previous rows).
13. Explain MERGE (aka UPSERT) statement.
Used to insert, update, or delete records based on conditions.
MERGE INTO target_table AS target
USING source_table AS source
ON target.id = source.id
WHEN MATCHED THEN
UPDATE SET target.name = source.name
WHEN NOT MATCHED THEN
INSERT (id, name) VALUES (source.id, source.name);
14. How do you find gaps in a sequence?
Suppose you have employee IDs and want to find missing ones:
SELECT (t1.id + 1) AS start_gap
FROM employees t1
LEFT JOIN employees t2 ON t1.id + 1 = t2.id
WHERE t2.id IS NULL;
15. How do you rank items within groups in SQL (e.g., top 3 per department)?
SELECT *
FROM (
SELECT name, department_id, salary,
RANK() OVER (PARTITION BY department_id ORDER BY salary DESC) AS dept_rank
FROM employees
) ranked
WHERE dept_rank <= 3;
16. How do you handle NULLs in aggregations?
Use COALESCE or default values:
SELECT department_id, SUM(COALESCE(salary, 0)) AS total_salary
FROM employees
GROUP BY department_id;
17. Difference between DELETE, TRUNCATE, and DROP?
Press enter or click to view image in full size
18. How do you perform full outer join in MySQL (which doesn’t support it directly)?
SELECT *
FROM table1
LEFT JOIN table2 ON table1.id = table2.id
UNION
SELECT *
FROM table1
RIGHT JOIN table2 ON table1.id = table2.id;
19. What is the use of LAG() and LEAD()?
1 LAG(): Get value from a previous row.
2 LEAD(): Get value from a following row.
SELECT name, salary,
LAG(salary, 1) OVER (ORDER BY hire_date) AS prev_salary,
LEAD(salary, 1) OVER (ORDER BY hire_date) AS next_salary
FROM employees;
20. What is a correlated subquery?
A subquery that depends on the outer query for its value.
SELECT name, salary
FROM employees e
WHERE salary > (
SELECT AVG(salary)
FROM employees
WHERE department_id = e.department_id
);
21. What is the difference between HAVING and WHERE clauses?
1 WHERE filters rows before grouping.
2 HAVING filters rows after grouping (used with aggregates).
— Example
SELECT department_id, COUNT(*) AS emp_count
FROM employees
WHERE status = ‘Active’
GROUP BY department_id
HAVING COUNT(*) > 5;
22. What are indexes? What are their types and trade-offs?
1 Indexes speed up query lookups by creating a data structure (usually B-tree).
2 Types:
3 Single-column index
4 Composite index
5 Unique index
6 Full-text index
7 Bitmap index (for low-cardinality columns)
Trade-offs:
1 Faster reads.
2 Slower inserts/updates/deletes.
3 Consumes storage.
23. How do you detect slow queries in a SQL database?
1 Use EXPLAIN or EXPLAIN ANALYZE.
2 Use Query Execution Plan to see:
3 Full Table Scans
4 Missing Indexes
5 High Cost Steps
EXPLAIN SELECT * FROM employees WHERE salary > 100000;
24. What’s the difference between normalization and denormalization?
Press enter or click to view image in full size
25. What is the difference between UNION, UNION ALL, INTERSECT, and EXCEPT?
Press enter or click to view image in full size
26. How do you implement pagination in SQL?
— For PostgreSQL/MySQL
SELECT * FROM employees
ORDER BY name
LIMIT 10 OFFSET 20;
— For SQL Server
SELECT *
FROM (
SELECT *, ROW_NUMBER() OVER (ORDER BY name) AS rn
FROM employees
) AS sub
WHERE rn BETWEEN 21 AND 30;
27. How can you pivot and unpivot data in SQL Server?
— PIVOT example
SELECT *
FROM (
SELECT department, gender
FROM employees
) AS source
PIVOT (
COUNT(gender)
FOR gender IN ([M], [F])
) AS pivoted;
28. How do you remove duplicate rows but keep the most recent based on a timestamp?
DELETE FROM employees
WHERE id NOT IN (
SELECT MAX(id)
FROM employees
GROUP BY email
);
29. How do you perform case-insensitive searches in SQL?
SELECT * FROM employees
WHERE LOWER(name) = ‘john doe’;
Or using ILIKE in PostgreSQL:
SELECT * FROM employees
WHERE name ILIKE ‘john%’;
30. What are materialized views and how are they different from regular views?
Feature

---

SQL Practice Questions for Airbnb Interview
Question 1: Basic Filtering and Aggregation
Scenario: Find top performing listings in a specific city
-- Given tables: listings (listing_id, host_id, city, price_per_night, property_type, created_date)
-- Write a query to find all listings in Paris with price > $100 per night,
-- showing the average price by property_type, ordered by average price descending
Question 2: Simple JOIN
Scenario: Match hosts with their booking performance
-- Given tables:
-- hosts (host_id, host_name, join_date, host_status)
-- bookings (booking_id, listing_id, host_id, guest_id, check_in_date, check_out_date, total_price)
-- Find all hosts who joined in 2024 and their total booking revenue
Question 3: Multiple JOINs with Aggregation
Scenario: Analyze guest booking patterns and satisfaction
-- Given tables:
-- bookings (booking_id, guest_id, listing_id, check_in_date, nights_stayed, total_paid)
-- reviews (review_id, booking_id, rating, review_date)
-- guests (guest_id, signup_date, country)
-- Find the average rating by guest country for bookings in the last 90 days
Question 4: Subquery - Identifying High-Value Guests
Scenario: Find guests who spend above average
-- Using the bookings table, identify guests whose average booking value
-- is higher than the overall average booking value
-- Show guest_id and their average booking value
Question 5: Self-JOIN - Finding Repeat Guests
Scenario: Identify guests who rebooked within 30 days
-- Given bookings table, find all instances where the same guest
-- made another booking within 30 days of their previous checkout
-- Show both booking IDs and the gap in days
Question 6: Window Functions - Ranking
Scenario: Rank hosts by revenue within each city
-- Given bookings and listings tables, rank hosts by their total revenue
-- within each city using ROW_NUMBER() or RANK()
-- Show only the top 3 hosts per city
Question 7: Window Functions - Running Totals
Scenario: Calculate cumulative bookings for marketing campaign analysis
-- Given tables:
-- bookings (booking_id, booking_date, channel_source)
-- marketing_campaigns (campaign_id, channel_source, start_date, end_date, budget)
-- Calculate daily bookings and cumulative bookings by channel_source
-- for the last 30 days using window functions
Question 8: Complex Subquery with EXISTS
Scenario: Find hosts who've never received a bad review
-- Find all hosts who have at least 10 bookings but have never
-- received a review with rating < 4
-- Use EXISTS or NOT EXISTS in your solution
Question 9: Advanced Window Function - Lead/Lag
Scenario: Analyze pricing changes and occupancy
-- Given tables:
-- listing_calendar (listing_id, date, price, available)
-- Calculate the price change from previous day for each listing
-- and identify dates where price increased by more than 20%
-- Also show the occupancy rate (7-day moving average) using window functions
Question 10: Comprehensive Business Question
Scenario: Marketing Attribution and Customer Lifetime Value
-- Given tables:
-- users (user_id, signup_date, acquisition_channel, first_booking_date)
-- bookings (booking_id, user_id, booking_date, total_value, booking_status)
-- marketing_spend (date, channel, daily_spend)
-- Calculate:
-- 1. Customer acquisition cost (CAC) by channel for Q1 2024
-- 2. Average customer lifetime value (CLV) by acquisition channel
-- 3. The ratio of CLV to CAC by channel
-- 4. Identify channels where CLV/CAC > 3
-- Use CTEs or subqueries to build this analysis step by step
Key SQL Concepts to Review:
Essential Functions:
• GROUP BY with HAVING
• CASE WHEN statements
• Date functions (DATE_DIFF, DATE_ADD, DATE_TRUNC)
• String functions (CONCAT, SUBSTRING, REGEXP)
Window Functions:
• ROW_NUMBER(), RANK(), DENSE_RANK()
• LAG(), LEAD()
• SUM() OVER(), AVG() OVER()
• Partition and ordering clauses
Performance Considerations:
• When to use CTEs vs subqueries
• Index usage implications
• Query optimization basics
Tips for the Interview:
1. Think out loud - Explain your approach before writing SQL 2. Start simple - Build complexity incrementally 3. Consider edge cases - NULLs, duplicates, date boundaries 4. Validate assumptions - Ask about data quality, table relationships 5. Business context - Connect your queries to business impact
I'll provide detailed solutions for all 10 questions with explanations and alternative approaches where relevant.
SQL Solutions for Airbnb Interview Practice
Question 1: Basic Filtering and Aggregation
-- Solution:
SELECT
property_type,
AVG(price_per_night) as avg_price,
COUNT(*) as listing_count
FROM listings
WHERE city = 'Paris'
AND price_per_night > 100
GROUP BY property_type
ORDER BY avg_price DESC;
-- Alternative with ROUND for cleaner output:
SELECT
property_type,
ROUND(AVG(price_per_night), 2) as avg_price,
COUNT(*) as listing_count,
MIN(price_per_night) as min_price,
MAX(price_per_night) as max_price
FROM listings
WHERE city = 'Paris'
AND price_per_night > 100
GROUP BY property_type
HAVING COUNT(*) >= 5 -- Only show property types with at least 5 listings
ORDER BY avg_price DESC;
Question 2: Simple JOIN
-- Solution:
SELECT
h.host_id,
h.host_name,
h.join_date,
COUNT(DISTINCT b.booking_id) as total_bookings,
SUM(b.total_price) as total_revenue
FROM hosts h
LEFT JOIN bookings b
ON h.host_id = b.host_id
WHERE YEAR(h.join_date) = 2024
GROUP BY h.host_id, h.host_name, h.join_date
ORDER BY total_revenue DESC;
-- Note: Using LEFT JOIN to include hosts with no bookings yet
-- Could use COALESCE to handle NULL values:
SELECT
h.host_id,
h.host_name,
h.join_date,
COALESCE(COUNT(DISTINCT b.booking_id), 0) as total_bookings,
COALESCE(SUM(b.total_price), 0) as total_revenue
FROM hosts h
LEFT JOIN bookings b
ON h.host_id = b.host_id
WHERE YEAR(h.join_date) = 2024
GROUP BY h.host_id, h.host_name, h.join_date
ORDER BY total_revenue DESC;
Question 3: Multiple JOINs with Aggregation
-- Solution:
SELECT
g.country,
COUNT(DISTINCT b.booking_id) as total_bookings,
AVG(r.rating) as avg_rating,
COUNT(DISTINCT r.review_id) as total_reviews
FROM bookings b
INNER JOIN reviews r
ON b.booking_id = r.booking_id
INNER JOIN guests g
ON b.guest_id = g.guest_id
WHERE b.check_in_date >= CURRENT_DATE - INTERVAL '90 days'
-- Or: WHERE b.check_in_date >= DATE_SUB(CURRENT_DATE, INTERVAL 90 DAY)
GROUP BY g.country
HAVING COUNT(DISTINCT b.booking_id) >= 10 -- Countries with meaningful sample size
ORDER BY avg_rating DESC;
-- Alternative with review completion rate:
SELECT
g.country,
COUNT(DISTINCT b.booking_id) as total_bookings,
ROUND(AVG(r.rating), 2) as avg_rating,
COUNT(DISTINCT r.review_id) as total_reviews,
ROUND(100.0 * COUNT(DISTINCT r.review_id) / COUNT(DISTINCT b.booking_id), 1) as review_rate
FROM bookings b
LEFT JOIN reviews r
ON b.booking_id = r.booking_id
INNER JOIN guests g
ON b.guest_id = g.guest_id
WHERE b.check_in_date >= CURRENT_DATE - INTERVAL '90 days'
GROUP BY g.country
ORDER BY avg_rating DESC;
Question 4: Subquery - Identifying High-Value Guests
-- Solution using subquery in WHERE:
SELECT
guest_id,
AVG(total_paid) as avg_booking_value,
COUNT(*) as booking_count
FROM bookings
GROUP BY guest_id
HAVING AVG(total_paid) > (
SELECT AVG(total_paid)
FROM bookings
)
ORDER BY avg_booking_value DESC;
-- Alternative using CTE (often clearer):
WITH overall_metrics AS (
SELECT AVG(total_paid) as overall_avg
FROM bookings
)
SELECT
b.guest_id,
AVG(b.total_paid) as avg_booking_value,
COUNT(*) as booking_count,
om.overall_avg,
ROUND(AVG(b.total_paid) - om.overall_avg, 2) as diff_from_avg
FROM bookings b
CROSS JOIN overall_metrics om
GROUP BY b.guest_id, om.overall_avg
HAVING AVG(b.total_paid) > om.overall_avg
ORDER BY avg_booking_value DESC;
Question 5: Self-JOIN - Finding Repeat Guests
-- Solution:
SELECT
b1.guest_id,
b1.booking_id as first_booking,
b2.booking_id as repeat_booking,
b1.check_in_date as first_checkin,
b2.check_in_date as repeat_checkin,
DATEDIFF(b2.check_in_date, DATE_ADD(b1.check_in_date, INTERVAL b1.nights_stayed DAY)) as days_between
FROM bookings b1
INNER JOIN bookings b2
ON b1.guest_id = b2.guest_id
AND b1.booking_id < b2.booking_id -- Ensure we don't duplicate pairs
AND b2.check_in_date <= DATE_ADD(b1.check_in_date, INTERVAL b1.nights_stayed + 30 DAY)
AND b2.check_in_date > DATE_ADD(b1.check_in_date, INTERVAL b1.nights_stayed DAY)
ORDER BY b1.guest_id, b1.check_in_date;
-- Alternative with more details:
SELECT
b1.guest_id,
b1.booking_id as first_booking,
b2.booking_id as repeat_booking,
b1.check_in_date as first_checkin,
DATE_ADD(b1.check_in_date, INTERVAL b1.nights_stayed DAY) as first_checkout,
b2.check_in_date as repeat_checkin,
DATEDIFF(b2.check_in_date, DATE_ADD(b1.check_in_date, INTERVAL b1.nights_stayed DAY)) as gap_days
FROM bookings b1
INNER JOIN bookings b2
ON b1.guest_id = b2.guest_id
WHERE b2.check_in_date BETWEEN
DATE_ADD(b1.check_in_date, INTERVAL b1.nights_stayed DAY)
AND DATE_ADD(b1.check_in_date, INTERVAL b1.nights_stayed + 30 DAY)
AND b1.booking_id < b2.booking_id
ORDER BY gap_days ASC;
Question 6: Window Functions - Ranking
-- Solution:
WITH host_revenue AS (
SELECT
l.city,
b.host_id,
SUM(b.total_paid) as total_revenue,
COUNT(DISTINCT b.booking_id) as total_bookings
FROM bookings b
INNER JOIN listings l
ON b.listing_id = l.listing_id
GROUP BY l.city, b.host_id
),
ranked_hosts AS (
SELECT
city,
host_id,
total_revenue,
total_bookings,
ROW_NUMBER() OVER (PARTITION BY city ORDER BY total_revenue DESC) as revenue_rank,
RANK() OVER (PARTITION BY city ORDER BY total_revenue DESC) as revenue_rank_with_ties,
DENSE_RANK() OVER (PARTITION BY city ORDER BY total_revenue DESC) as dense_revenue_rank
FROM host_revenue
)
SELECT
city,
host_id,
total_revenue,
total_bookings,
revenue_rank
FROM ranked_hosts
WHERE revenue_rank <= 3
ORDER BY city, revenue_rank;
-- Simpler version without CTE:
SELECT * FROM (
SELECT
l.city,
b.host_id,
SUM(b.total_paid) as total_revenue,
COUNT(*) as booking_count,
ROW_NUMBER() OVER (PARTITION BY l.city ORDER BY SUM(b.total_paid) DESC) as rank
FROM bookings b
INNER JOIN listings l ON b.listing_id = l.listing_id
GROUP BY l.city, b.host_id
) ranked
WHERE rank <= 3
ORDER BY city, rank;
Question 7: Window Functions - Running Totals
-- Solution:
WITH daily_bookings AS (
SELECT
DATE(booking_date) as booking_day,
channel_source,
COUNT(*) as daily_count
FROM bookings
WHERE booking_date >= CURRENT_DATE - INTERVAL '30 days'
GROUP BY DATE(booking_date), channel_source
)
SELECT
booking_day,
channel_source,
daily_count,
SUM(daily_count) OVER (
PARTITION BY channel_source
ORDER BY booking_day
ROWS UNBOUNDED PRECEDING
) as cumulative_bookings,
AVG(daily_count) OVER (
PARTITION BY channel_source
ORDER BY booking_day
ROWS BETWEEN 6 PRECEDING AND CURRENT ROW
) as moving_avg_7day
FROM daily_bookings
ORDER BY channel_source, booking_day;
-- Alternative with campaign budget utilization:
WITH daily_metrics AS (
SELECT
DATE(b.booking_date) as booking_day,
b.channel_source,
COUNT(*) as daily_bookings,
SUM(b.total_paid) as daily_revenue,
MAX(mc.budget) as campaign_budget
FROM bookings b
LEFT JOIN marketing_campaigns mc
ON b.channel_source = mc.channel_source
AND DATE(b.booking_date) BETWEEN mc.start_date AND mc.end_date
WHERE b.booking_date >= CURRENT_DATE - INTERVAL '30 days'
GROUP BY DATE(b.booking_date), b.channel_source
)
SELECT
booking_day,
channel_source,
daily_bookings,
daily_revenue,
SUM(daily_bookings) OVER (PARTITION BY channel_source ORDER BY booking_day) as cumulative_bookings,
SUM(daily_revenue) OVER (PARTITION BY channel_source ORDER BY booking_day) as cumulative_revenue,
campaign_budget,
ROUND(100.0 * SUM(daily_revenue) OVER (PARTITION BY channel_source ORDER BY booking_day) / campaign_budget, 2) as budget_utilization_pct
FROM daily_metrics
ORDER BY channel_source, booking_day;
Question 8: Complex Subquery with EXISTS
-- Solution using NOT EXISTS:
SELECT
h.host_id,
COUNT(DISTINCT b.booking_id) as total_bookings,
AVG(r.rating) as avg_rating
FROM hosts h
INNER JOIN bookings b ON h.host_id = b.host_id
LEFT JOIN reviews r ON b.booking_id = r.booking_id
WHERE NOT EXISTS (
SELECT 1
FROM bookings b2
INNER JOIN reviews r2 ON b2.booking_id = r2.booking_id
WHERE b2.host_id = h.host_id
AND r2.rating < 4
)
GROUP BY h.host_id
HAVING COUNT(DISTINCT b.booking_id) >= 10
ORDER BY avg_rating DESC, total_bookings DESC;
-- Alternative using subquery in WHERE:
SELECT
host_id,
total_bookings,
avg_rating,
min_rating
FROM (
SELECT
b.host_id,
COUNT(DISTINCT b.booking_id) as total_bookings,
AVG(r.rating) as avg_rating,
MIN(r.rating) as min_rating
FROM bookings b
LEFT JOIN reviews r ON b.booking_id = r.booking_id
GROUP BY b.host_id
HAVING COUNT(DISTINCT b.booking_id) >= 10
) host_stats
WHERE min_rating >= 4 OR min_rating IS NULL -- Include hosts with no reviews yet
ORDER BY avg_rating DESC, total_bookings DESC;
Question 9: Advanced Window Function - Lead/Lag
-- Solution:
WITH pricing_analysis AS (
SELECT
listing_id,
date,
price,
available,
LAG(price, 1) OVER (PARTITION BY listing_id ORDER BY date) as prev_price,
LEAD(price, 1) OVER (PARTITION BY listing_id ORDER BY date) as next_price,
-- Calculate 7-day occupancy rate
AVG(CASE WHEN available = FALSE THEN 1 ELSE 0 END) OVER (
PARTITION BY listing_id
ORDER BY date
ROWS BETWEEN 3 PRECEDING AND 3 FOLLOWING
) as occupancy_rate_7day
FROM listing_calendar
)
SELECT
listing_id,
date,
price,
prev_price,
ROUND((price - prev_price) * 100.0 / prev_price, 2) as price_change_pct,
ROUND(occupancy_rate_7day * 100, 2) as occupancy_pct,
CASE
WHEN (price - prev_price) * 100.0 / prev_price > 20 THEN 'Surge Pricing'
WHEN (price - prev_price) * 100.0 / prev_price < -20 THEN 'Deep Discount'
ELSE 'Normal'
END as pricing_strategy
FROM pricing_analysis
WHERE prev_price IS NOT NULL
AND (price - prev_price) * 100.0 / prev_price > 20 -- Filter for >20% increases
ORDER BY listing_id, date;
-- Alternative with more insights:
WITH calendar_metrics AS (
SELECT
listing_id,
date,
price,
available,
LAG(price, 1) OVER (PARTITION BY listing_id ORDER BY date) as prev_day_price,
LAG(price, 7) OVER (PARTITION BY listing_id ORDER BY date) as prev_week_price,
AVG(price) OVER (
PARTITION BY listing_id
ORDER BY date
ROWS BETWEEN 6 PRECEDING AND CURRENT ROW
) as avg_price_7day,
SUM(CASE WHEN available = FALSE THEN 1 ELSE 0 END) OVER (
PARTITION BY listing_id
ORDER BY date
ROWS BETWEEN 6 PRECEDING AND CURRENT ROW
) * 100.0 / 7 as occupancy_rate_7day
FROM listing_calendar
)
SELECT
listing_id,
date,
price,
prev_day_price,
ROUND((price - prev_day_price) * 100.0 / NULLIF(prev_day_price, 0), 2) as daily_price_change_pct,
ROUND((price - prev_week_price) * 100.0 / NULLIF(prev_week_price, 0), 2) as weekly_price_change_pct,
ROUND(price - avg_price_7day, 2) as diff_from_7day_avg,
ROUND(occupancy_rate_7day, 1) as occupancy_pct
FROM calendar_metrics
WHERE date >= CURRENT_DATE - INTERVAL '30 days'
AND prev_day_price IS NOT NULL
ORDER BY listing_id, date DESC;
Question 10: Comprehensive Business Question
-- Solution:
WITH user_cohorts AS (
-- Get users who signed up in Q1 2024
SELECT
user_id,
acquisition_channel,
signup_date,
first_booking_date
FROM users
WHERE signup_date BETWEEN '2024-01-01' AND '2024-03-31'
),
channel_acquisitions AS (
-- Count acquisitions by channel
SELECT
acquisition_channel,
COUNT(DISTINCT user_id) as users_acquired,
COUNT(DISTINCT CASE WHEN first_booking_date IS NOT NULL THEN user_id END) as users_with_bookings
FROM user_cohorts
GROUP BY acquisition_channel
),
channel_spending AS (
-- Calculate total spend by channel for Q1 2024
SELECT
channel,
SUM(daily_spend) as total_spend
FROM marketing_spend
WHERE date BETWEEN '2024-01-01' AND '2024-03-31'
GROUP BY channel
),
user_ltv AS (
-- Calculate LTV for each user
SELECT
u.user_id,
u.acquisition_channel,
COUNT(b.booking_id) as total_bookings,
SUM(b.total_value) as lifetime_value
FROM user_cohorts u
LEFT JOIN bookings b
ON u.user_id = b.user_id
AND b.booking_status = 'completed'
GROUP BY u.user_id, u.acquisition_channel
),
channel_ltv AS (
-- Average LTV by channel
SELECT
acquisition_channel,
AVG(lifetime_value) as avg_ltv,
PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY lifetime_value) as median_ltv,
AVG(total_bookings) as avg_bookings_per_user
FROM user_ltv
GROUP BY acquisition_channel
),
final_metrics AS (
-- Combine all metrics
SELECT
ca.acquisition_channel,
ca.users_acquired,
ca.users_with_bookings,
ROUND(100.0 * ca.users_with_bookings / ca.users_acquired, 2) as conversion_rate,
cs.total_spend,
ROUND(cs.total_spend / ca.users_acquired, 2) as cac,
ROUND(cl.avg_ltv, 2) as avg_ltv,
ROUND(cl.median_ltv, 2) as median_ltv,
ROUND(cl.avg_ltv / NULLIF(cs.total_spend / ca.users_acquired, 0), 2) as ltv_cac_ratio,
cl.avg_bookings_per_user
FROM channel_acquisitions ca
LEFT JOIN channel_spending cs ON ca.acquisition_channel = cs.channel
LEFT JOIN channel_ltv cl ON ca.acquisition_channel = cl.acquisition_channel
)
-- Final output with channels where LTV/CAC > 3
SELECT
acquisition_channel,
users_acquired,
conversion_rate,
cac,
avg_ltv,
median_ltv,
ltv_cac_ratio,
CASE
WHEN ltv_cac_ratio > 3 THEN 'High Performance'
WHEN ltv_cac_ratio > 2 THEN 'Good Performance'
WHEN ltv_cac_ratio > 1 THEN 'Breaking Even'
ELSE 'Underperforming'
END as channel_status
FROM final_metrics
WHERE ltv_cac_ratio IS NOT NULL
ORDER BY ltv_cac_ratio DESC;
-- Simpler version focusing on key metrics:
WITH q1_metrics AS (
SELECT
u.acquisition_channel,
COUNT(DISTINCT u.user_id) as users_acquired,
SUM(ms.daily_spend) as total_spend,
SUM(b.total_value) as total_revenue
FROM users u
LEFT JOIN bookings b
ON u.user_id = b.user_id
AND b.booking_status = 'completed'
LEFT JOIN marketing_spend ms
ON u.acquisition_channel = ms.channel
AND ms.date BETWEEN '2024-01-01' AND '2024-03-31'
WHERE u.signup_date BETWEEN '2024-01-01' AND '2024-03-31'
GROUP BY u.acquisition_channel
)
SELECT
acquisition_channel,
users_acquired,
ROUND(total_spend / NULLIF(users_acquired, 0), 2) as cac,
ROUND(total_revenue / NULLIF(users_acquired, 0), 2) as avg_ltv,
ROUND(total_revenue / NULLIF(total_spend, 0), 2) as roi
FROM q1_metrics
WHERE total_revenue / NULLIF(total_spend, 0) > 3
ORDER BY roi DESC;
Key Takeaways and Tips:
1. Always handle NULLs - Use COALESCE, NULLIF, or explicit NULL checks 2. Consider performance - CTEs are often more readable than nested subqueries 3. Window functions - Know when to use ROWS vs RANGE in window frames 4. Business context - Always think about what the metric means for Airbnb's business 5. Edge cases - Consider scenarios like new users with no bookings, hosts with no
reviews, etc.
Remember during the interview:
• Start with a simple version, then add complexity
• Explain your assumptions about the data
• Mention alternative approaches when relevant
• Think about query performance for large datasets
Comprehensive SQL Interview Questions Guide
Table of Contents
1. Sample Database Schema 2. Basic SELECT Queries 3. Filtering and Conditional Logic 4. JOIN Operations 5. Aggregations and GROUP BY 6. Subqueries 7. Window Functions 8. Common Table Expressions (CTEs) 9. Data Modification 10. Advanced Patterns
Sample Database Schema
Let's work with a company database containing employees, departments, projects, and sales data:
-- Employees table
CREATE TABLE employees (
employee_id INT PRIMARY KEY,
first_name VARCHAR(50),
last_name VARCHAR(50),
email VARCHAR(100),
phone_number VARCHAR(20),
hire_date DATE,
job_title VARCHAR(50),
salary DECIMAL(10, 2),
manager_id INT,
department_id INT
);
-- Sample data
INSERT INTO employees VALUES
(1, 'John', 'Smith', 'john.smith@company.com', '555-0101', '2020-01-15', 'Software Engineer', 75000, 5, 1),
(2, 'Sarah', 'Johnson', 'sarah.j@company.com', '555-0102', '2019-03-22', 'Senior Software Engineer', 95000, 5, 1),
(3, 'Michael', 'Brown', 'michael.b@company.com', '555-0103', '2021-06-01', 'Data Analyst', 65000, 6, 2),
(4, 'Emily', 'Davis', 'emily.d@company.com', '555-0104', '2020-09-15', 'Marketing Manager', 80000, 7, 3),
(5, 'David', 'Wilson', 'david.w@company.com', '555-0105', '2018-02-01', 'Engineering Manager', 110000, NULL, 1),
(6, 'Lisa', 'Anderson', 'lisa.a@company.com', '555-0106', '2019-07-10', 'Analytics Manager', 95000, NULL, 2),
(7, 'James', 'Taylor', 'james.t@company.com', '555-0107', '2017-05-15', 'Marketing Director', 120000, NULL, 3),
(8, 'Jennifer', 'Martinez', 'jennifer.m@company.com', '555-0108', '2022-01-10', 'Junior Developer', 60000, 2, 1),
(9, 'Robert', 'Lee', 'robert.l@company.com', '555-0109', '2021-11-01', 'Sales Representative', 55000, 10, 4),
(10, 'Maria', 'Garcia', 'maria.g@company.com', '555-0110', '2018-08-20', 'Sales Manager', 90000, NULL, 4);
-- Departments table
CREATE TABLE departments (
department_id INT PRIMARY KEY,
department_name VARCHAR(50),
location VARCHAR(100)
);
INSERT INTO departments VALUES
(1, 'Engineering', 'San Francisco'),
(2, 'Analytics', 'New York'),
(3, 'Marketing', 'Los Angeles'),
(4, 'Sales', 'Chicago'),
(5, 'HR', 'Boston');
-- Projects table
CREATE TABLE projects (
project_id INT PRIMARY KEY,
project_name VARCHAR(100),
start_date DATE,
end_date DATE,
budget DECIMAL(12, 2)
);
INSERT INTO projects VALUES
(1, 'Mobile App Development', '2023-01-01', '2023-06-30', 500000),
(2, 'Data Warehouse Migration', '2023-03-15', '2023-09-15', 750000),
(3, 'Marketing Campaign Q2', '2023-04-01', '2023-06-30', 200000),
(4, 'Customer Portal', '2023-02-01', '2023-08-31', 600000);
-- Employee_Projects table (many-to-many relationship)
CREATE TABLE employee_projects (
employee_id INT,
project_id INT,
role VARCHAR(50),
hours_allocated INT,
PRIMARY KEY (employee_id, project_id)
);
INSERT INTO employee_projects VALUES
(1, 1, 'Developer', 500),
(1, 4, 'Developer', 300),
(2, 1, 'Lead Developer', 600),
(2, 2, 'Consultant', 200),
(3, 2, 'Data Analyst', 800),
(4, 3, 'Project Manager', 400),
(5, 1, 'Technical Lead', 300),
(8, 4, 'Developer', 700);
-- Sales table
CREATE TABLE sales (
sale_id INT PRIMARY KEY,
employee_id INT,
sale_date DATE,
amount DECIMAL(10, 2),
product_category VARCHAR(50)
);
INSERT INTO sales VALUES
(1, 9, '2023-01-15', 15000, 'Software'),
(2, 9, '2023-01-20', 8000, 'Services'),
(3, 10, '2023-01-22', 25000, 'Software'),
(4, 9, '2023-02-10', 12000, 'Hardware'),
(5, 10, '2023-02-15', 30000, 'Software'),
(6, 9, '2023-03-01', 18000, 'Services'),
(7, 10, '2023-03-10', 22000, 'Hardware'),
(8, 9, '2023-03-15', 9000, 'Software');
Basic SELECT Queries
Question 1: Select all employees
Question: Retrieve all employee information from the employees table.
Answer:
SELECT * FROM employees;
Question 2: Select specific columns
Question: Get the first name, last name, and salary of all employees.
Answer:
SELECT first_name, last_name, salary
FROM employees;
Question 3: Using aliases
Question: Display employee names with their annual salary (use meaningful column aliases).
Answer:
SELECT
first_name + ' ' + last_name AS full_name,
salary AS monthly_salary,
salary * 12 AS annual_salary
FROM employees;
-- For MySQL/PostgreSQL use CONCAT:
SELECT
CONCAT(first_name, ' ', last_name) AS full_name,
salary AS monthly_salary,
salary * 12 AS annual_salary
FROM employees;
Question 4: DISTINCT values
Question: Find all unique job titles in the company.
Answer:
SELECT DISTINCT job_title
FROM employees
ORDER BY job_title;
Filtering and Conditional Logic
Question 5: WHERE clause
Question: Find all employees earning more than $80,000.
Answer:
SELECT first_name, last_name, salary
FROM employees
WHERE salary > 80000
ORDER BY salary DESC;
Question 6: Multiple conditions
Question: Find employees in the Engineering department (id=1) earning between $70,000 and $100,000.
Answer:
SELECT first_name, last_name, salary, department_id
FROM employees
WHERE department_id = 1
AND salary BETWEEN 70000 AND 100000
ORDER BY salary;
Question 7: Pattern matching with LIKE
Question: Find all employees whose email starts with 'j'.
Answer:
SELECT first_name, last_name, email
FROM employees
WHERE email LIKE 'j%';
Question 8: IN operator
Question: Find employees who are either managers or directors.
Answer:
SELECT first_name, last_name, job_title
FROM employees
WHERE job_title IN ('Engineering Manager', 'Marketing Director', 'Sales Manager', 'Analytics Manager')
ORDER BY job_title;
Question 9: NULL handling
Question: Find all employees who don't have a manager (top-level employees).
Answer:
SELECT first_name, last_name, job_title
FROM employees
WHERE manager_id IS NULL;
Question 10: CASE statements
Question: Categorize employees by salary ranges.
Answer:
SELECT
first_name,
last_name,
salary,
CASE
WHEN salary < 60000 THEN 'Entry Level'
WHEN salary BETWEEN 60000 AND 80000 THEN 'Mid Level'
WHEN salary BETWEEN 80001 AND 100000 THEN 'Senior Level'
ELSE 'Executive Level'
END AS salary_category
FROM employees
ORDER BY salary;
JOIN Operations
Question 11: INNER JOIN
Question: List all employees with their department names.
Answer:
SELECT
e.first_name,
e.last_name,
e.job_title,
d.department_name,
d.location
FROM employees e
INNER JOIN departments d ON e.department_id = d.department_id
ORDER BY d.department_name, e.last_name;
Question 12: LEFT JOIN
Question: Show all departments and their employees (including departments with no employees).
Answer:
SELECT
d.department_name,
d.location,
e.first_name,
e.last_name
FROM departments d
LEFT JOIN employees e ON d.department_id = e.department_id
ORDER BY d.department_name, e.last_name;
Question 13: Self JOIN
Question: List all employees with their manager's name.
Answer:
SELECT
e.first_name + ' ' + e.last_name AS employee_name,
e.job_title,
m.first_name + ' ' + m.last_name AS manager_name
FROM employees e
LEFT JOIN employees m ON e.manager_id = m.employee_id
ORDER BY manager_name, employee_name;
Question 14: Multiple JOINs
Question: Show employees with their department and project information.
Answer:
SELECT
e.first_name,
e.last_name,
d.department_name,
p.project_name,
ep.role,
ep.hours_allocated
FROM employees e
INNER JOIN departments d ON e.department_id = d.department_id
INNER JOIN employee_projects ep ON e.employee_id = ep.employee_id
INNER JOIN projects p ON ep.project_id = p.project_id
ORDER BY e.last_name, p.project_name;
Question 15: FULL OUTER JOIN (or simulation)
Question: Find all employees and all departments, showing matches where they exist.
Answer:
-- For databases supporting FULL OUTER JOIN:
SELECT
e.first_name,
e.last_name,
d.department_name
FROM employees e
FULL OUTER JOIN departments d ON e.department_id = d.department_id;
-- For MySQL (doesn't support FULL OUTER JOIN):
SELECT
e.first_name,
e.last_name,
d.department_name
FROM employees e
LEFT JOIN departments d ON e.department_id = d.department_id
UNION
SELECT
e.first_name,
e.last_name,
d.department_name
FROM employees e
RIGHT JOIN departments d ON e.department_id = d.department_id;
Aggregations and GROUP BY
Question 16: Basic aggregation
Question: Find the average, minimum, and maximum salary in the company.
Answer:
SELECT
AVG(salary) AS avg_salary,
MIN(salary) AS min_salary,
MAX(salary) AS max_salary,
COUNT(*) AS total_employees
FROM employees;
Question 17: GROUP BY with COUNT
Question: Count the number of employees in each department.
Answer:
SELECT
d.department_name,
COUNT(e.employee_id) AS employee_count
FROM departments d
LEFT JOIN employees e ON d.department_id = e.department_id
GROUP BY d.department_id, d.department_name
ORDER BY employee_count DESC;
Question 18: GROUP BY with HAVING
Question: Find departments with average salary above $80,000.
Answer:
SELECT
d.department_name,
AVG(e.salary) AS avg_salary,
COUNT(e.employee_id) AS employee_count
FROM departments d
INNER JOIN employees e ON d.department_id = e.department_id
GROUP BY d.department_id, d.department_name
HAVING AVG(e.salary) > 80000
ORDER BY avg_salary DESC;
Question 19: Multiple aggregations
Question: For each department, show total salary expense and average salary.
Answer:
SELECT
d.department_name,
COUNT(e.employee_id) AS num_employees,
SUM(e.salary) AS total_salary_expense,
AVG(e.salary) AS average_salary,
MIN(e.salary) AS min_salary,
MAX(e.salary) AS max_salary
FROM departments d
LEFT JOIN employees e ON d.department_id = e.department_id
GROUP BY d.department_id, d.department_name
ORDER BY total_salary_expense DESC;
Question 20: GROUP BY with multiple columns
Question: Find the number of employees by department and job title.
Answer:
SELECT
d.department_name,
e.job_title,
COUNT(*) AS employee_count,
AVG(e.salary) AS avg_salary
FROM employees e
INNER JOIN departments d ON e.department_id = d.department_id
GROUP BY d.department_name, e.job_title
ORDER BY d.department_name, employee_count DESC;
Subqueries
Question 21: Subquery in WHERE clause
Question: Find employees who earn more than the average salary.
Answer:
SELECT
first_name,
last_name,
salary
FROM employees
WHERE salary > (SELECT AVG(salary) FROM employees)
ORDER BY salary DESC;
Question 22: Subquery with IN
Question: Find employees who work on projects with budget over $600,000.
Answer:
SELECT DISTINCT
e.first_name,
e.last_name
FROM employees e
WHERE e.employee_id IN (
SELECT ep.employee_id
FROM employee_projects ep
INNER JOIN projects p ON ep.project_id = p.project_id
WHERE p.budget > 600000
)
ORDER BY e.last_name;
Question 23: Correlated subquery
Question: Find employees who earn more than the average salary in their department.
Answer:
SELECT
e1.first_name,
e1.last_name,
e1.salary,
e1.department_id
FROM employees e1
WHERE e1.salary > (
SELECT AVG(e2.salary)
FROM employees e2
WHERE e2.department_id = e1.department_id
)
ORDER BY e1.department_id, e1.salary DESC;
Question 24: EXISTS clause
Question: Find departments that have at least one employee earning over $100,000.
Answer:
SELECT
d.department_name,
d.location
FROM departments d
WHERE EXISTS (
SELECT 1
FROM employees e
WHERE e.department_id = d.department_id
AND e.salary > 100000
);
Question 25: Subquery in SELECT clause
Question: Show each employee with their department's average salary.
Answer:
SELECT
e.first_name,
e.last_name,
e.salary,
(SELECT AVG(salary)
FROM employees e2
WHERE e2.department_id = e.department_id) AS dept_avg_salary,
e.salary - (SELECT AVG(salary)
FROM employees e2
WHERE e2.department_id = e.department_id) AS salary_diff
FROM employees e
ORDER BY e.department_id, salary_diff DESC;
Window Functions
Question 26: ROW_NUMBER()
Question: Rank employees by salary within each department.
Answer:
SELECT
first_name,
last_name,
department_id,
salary,
ROW_NUMBER() OVER (PARTITION BY department_id ORDER BY salary DESC) AS salary_rank
FROM employees
ORDER BY department_id, salary_rank;
Question 27: RANK() vs DENSE_RANK()
Question: Show the difference between RANK and DENSE_RANK for employee salaries.
Answer:
SELECT
first_name,
last_name,
salary,
RANK() OVER (ORDER BY salary DESC) AS rank,
DENSE_RANK() OVER (ORDER BY salary DESC) AS dense_rank,
ROW_NUMBER() OVER (ORDER BY salary DESC) AS row_num
FROM employees
ORDER BY salary DESC;
Question 28: Running totals
Question: Calculate running total of sales by date.
Answer:
SELECT
sale_date,
employee_id,
amount,
SUM(amount) OVER (ORDER BY sale_date) AS running_total,
SUM(amount) OVER (PARTITION BY employee_id ORDER BY sale_date) AS running_total_by_employee
FROM sales
ORDER BY sale_date;
Question 29: LAG and LEAD
Question: Compare each sale with the previous and next sale.
Answer:
SELECT
sale_id,
sale_date,
amount,
LAG(amount, 1) OVER (ORDER BY sale_date) AS previous_sale,
LEAD(amount, 1) OVER (ORDER BY sale_date) AS next_sale,
amount - LAG(amount, 1) OVER (ORDER BY sale_date) AS diff_from_previous
FROM sales
ORDER BY sale_date;
Question 30: Moving averages
Question: Calculate 3-month moving average of sales.
Answer:
SELECT
sale_date,
amount,
AVG(amount) OVER (
ORDER BY sale_date
ROWS BETWEEN 2 PRECEDING AND CURRENT ROW
) AS moving_avg_3
FROM sales
ORDER BY sale_date;
Question 31: NTILE
Question: Divide employees into salary quartiles.
Answer:
SELECT
first_name,
last_name,
salary,
NTILE(4) OVER (ORDER BY salary) AS salary_quartile
FROM employees
ORDER BY salary;
Common Table Expressions (CTEs)
Question 32: Basic CTE
Question: Use a CTE to find high-earning employees (top 25%).
Answer:
WITH salary_stats AS (
SELECT
PERCENTILE_CONT(0.75) WITHIN GROUP (ORDER BY salary) AS percentile_75
FROM employees
)
SELECT
e.first_name,
e.last_name,
e.salary
FROM employees e, salary_stats s
WHERE e.salary >= s.percentile_75
ORDER BY e.salary DESC;
-- Alternative using subquery for databases without PERCENTILE_CONT:
WITH salary_ranked AS (
SELECT
first_name,
last_name,
salary,
NTILE(4) OVER (ORDER BY salary DESC) AS quartile
FROM employees
)
SELECT
first_name,
last_name,
salary
FROM salary_ranked
WHERE quartile = 1
ORDER BY salary DESC;
Question 33: Multiple CTEs
Question: Calculate department statistics and identify departments above average.
Answer:
WITH dept_stats AS (
SELECT
department_id,
COUNT(*) AS employee_count,
AVG(salary) AS avg_salary,
SUM(salary) AS total_salary
FROM employees
GROUP BY department_id
),
company_avg AS (
SELECT AVG(avg_salary) AS overall_avg_salary
FROM dept_stats
)
SELECT
d.department_name,
ds.employee_count,
ds.avg_salary,
ca.overall_avg_salary,
ds.avg_salary - ca.overall_avg_salary AS diff_from_company_avg
FROM dept_stats ds
CROSS JOIN company_avg ca
INNER JOIN departments d ON ds.department_id = d.department_id
WHERE ds.avg_salary > ca.overall_avg_salary
ORDER BY diff_from_company_avg DESC;
Question 34: Recursive CTE
Question: Build an employee hierarchy tree showing reporting structure.
Answer:
WITH RECURSIVE emp_hierarchy AS (
-- Anchor: top-level employees
SELECT
employee_id,
first_name,
last_name,
manager_id,
job_title,
0 AS level,
CAST(employee_id AS VARCHAR(200)) AS path
FROM employees
WHERE manager_id IS NULL
UNION ALL
-- Recursive part
SELECT
e.employee_id,
e.first_name,
e.last_name,
e.manager_id,
e.job_title,
h.level + 1,
CONCAT(h.path, '->', e.employee_id)
FROM employees e
INNER JOIN emp_hierarchy h ON e.manager_id = h.employee_id
)
SELECT
level,
REPEAT(' ', level) || first_name || ' ' || last_name AS employee_name,
job_title,
path
FROM emp_hierarchy
ORDER BY path;
Data Modification
Question 35: INSERT with SELECT
Question: Create a backup of high-salary employees.
Answer:
-- First create the backup table
CREATE TABLE high_earners_backup AS
SELECT * FROM employees WHERE 1=0; -- Structure only
-- Insert high earners
INSERT INTO high_earners_backup
SELECT *
FROM employees
WHERE salary > 90000;
Question 36: UPDATE with JOIN
Question: Give a 10% raise to all employees in the Engineering department.
Answer:
UPDATE employees e
SET e.salary = e.salary * 1.10
WHERE e.department_id IN (
SELECT department_id
FROM departments
WHERE department_name = 'Engineering'
);
-- Alternative with JOIN (MySQL syntax):
UPDATE employees e
INNER JOIN departments d ON e.department_id = d.department_id
SET e.salary = e.salary * 1.10
WHERE d.department_name = 'Engineering';
Question 37: DELETE with subquery
Question: Delete employees who haven't been assigned to any projects.
Answer:
DELETE FROM employees
WHERE employee_id NOT IN (
SELECT DISTINCT employee_id
FROM employee_projects
);
-- Alternative using NOT EXISTS:
DELETE FROM employees e
WHERE NOT EXISTS (
SELECT 1
FROM employee_projects ep
WHERE ep.employee_id = e.employee_id
);
Question 38: MERGE/UPSERT
Question: Update or insert employee records based on a staging table.
Answer:
-- SQL Server / Oracle MERGE syntax:
MERGE employees AS target
USING staging_employees AS source
ON target.employee_id = source.employee_id
WHEN MATCHED THEN
UPDATE SET
target.salary = source.salary,
target.job_title = source.job_title
WHEN NOT MATCHED THEN
INSERT (employee_id, first_name, last_name, salary, job_title)
VALUES (source.employee_id, source.first_name, source.last_name,
source.salary, source.job_title);
-- MySQL UPSERT:
INSERT INTO employees (employee_id, first_name, last_name, salary)
VALUES (1, 'John', 'Smith', 80000)
ON DUPLICATE KEY UPDATE
salary = VALUES(salary);
-- PostgreSQL UPSERT:
INSERT INTO employees (employee_id, first_name, last_name, salary)
VALUES (1, 'John', 'Smith', 80000)
ON CONFLICT (employee_id) DO UPDATE
SET salary = EXCLUDED.salary;
Advanced Patterns
Question 39: Finding duplicates
Question: Find duplicate email addresses in the employees table.
Answer:
-- Method 1: Using GROUP BY and HAVING
SELECT
email,
COUNT(*) AS duplicate_count
FROM employees
GROUP BY email
HAVING COUNT(*) > 1;
-- Method 2: Using Window Functions
WITH email_counts AS (
SELECT
*,
COUNT(*) OVER (PARTITION BY email) AS email_count
FROM employees
)
SELECT *
FROM email_counts
WHERE email_count > 1
ORDER BY email, employee_id;
Question 40: Gaps and Islands
Question: Find gaps in employee IDs.
Answer:
WITH id_gaps AS (
SELECT
employee_id,
LEAD(employee_id) OVER (ORDER BY employee_id) AS next_id
FROM employees
)
SELECT
employee_id + 1 AS gap_start,
next_id - 1 AS gap_end
FROM id_gaps
WHERE next_id - employee_id > 1;
Question 41: Pivot table
Question: Create a pivot table showing employee count by department and job title.
Answer:
-- Using CASE statements (works in all databases):
SELECT
job_title,
SUM(CASE WHEN department_id = 1 THEN 1 ELSE 0 END) AS Engineering,
SUM(CASE WHEN department_id = 2 THEN 1 ELSE 0 END) AS Analytics,
SUM(CASE WHEN department_id = 3 THEN 1 ELSE 0 END) AS Marketing,
SUM(CASE WHEN department_id = 4 THEN 1 ELSE 0 END) AS Sales
FROM employees
GROUP BY job_title;
-- SQL Server PIVOT syntax:
SELECT *
FROM (
SELECT job_title, department_id
FROM employees
) AS source_table
PIVOT (
COUNT(department_id)
FOR department_id IN ([1], [2], [3], [4])
) AS pivot_table;
Question 42: Cumulative percentage
Question: Calculate cumulative percentage of total salary by employee.
Answer:
WITH salary_ordered AS (
SELECT
first_name,
last_name,
salary,
SUM(salary) OVER (ORDER BY salary DESC) AS cumulative_salary,
SUM(salary) OVER () AS total_salary
FROM employees
)
SELECT
first_name,
last_name,
salary,
cumulative_salary,
ROUND(100.0 * cumulative_salary / total_salary, 2) AS cumulative_percentage
FROM salary_ordered
ORDER BY salary DESC;
Question 43: Top N per group
Question: Find the top 2 highest-paid employees in each department.
Answer:
-- Using ROW_NUMBER()
WITH ranked_employees AS (
SELECT
e.*,
d.department_name,
ROW_NUMBER() OVER (PARTITION BY e.department_id ORDER BY e.salary DESC) AS rank
FROM employees e
INNER JOIN departments d ON e.department_id = d.department_id
)
SELECT
department_name,
first_name,
last_name,
salary,
rank
FROM ranked_employees
WHERE rank <= 2
ORDER BY department_name, rank;
Question 44: Year-over-Year comparison
Question: Compare monthly sales with the same month from the previous year.
Answer:
WITH monthly_sales AS (
SELECT
DATE_FORMAT(sale_date, '%Y-%m') AS month,
SUM(amount) AS total_sales
FROM sales
GROUP BY DATE_FORMAT(sale_date, '%Y-%m')
),
yoy_comparison AS (
SELECT
month,
total_sales,
LAG(total_sales, 12) OVER (ORDER BY month) AS prev_year_sales
FROM monthly_sales
)
SELECT
month,
total_sales,
prev_year_sales,
ROUND(100.0 * (total_sales - prev_year_sales) / prev_year_sales, 2) AS yoy_growth_percent
FROM yoy_comparison
WHERE prev_year_sales IS NOT NULL;
Question 45: Finding consecutive records
Question: Find employees hired on consecutive days.
Answer:
WITH consecutive_hires AS (
SELECT
e1.first_name AS emp1_first,
e1.last_name AS emp1_last,
e1.hire_date AS emp1_hire_date,
e2.first_name AS emp2_first,
e2.last_name AS emp2_last,
e2.hire_date AS emp2_hire_date
FROM employees e1
INNER JOIN employees e2
ON e2.hire_date = DATE_ADD(e1.hire_date, INTERVAL 1 DAY)
)
SELECT * FROM consecutive_hires
ORDER BY emp1_hire_date;
Question 46: Hierarchical queries
Question: Show the complete reporting chain for each employee.
Answer:
WITH RECURSIVE reporting_chain AS (
SELECT
e.employee_id,
e.first_name,
e.last_name,
e.manager_id,
e.first_name || ' ' || e.last_name AS reporting_chain,
1 AS chain_level
FROM employees e
UNION ALL
SELECT
rc.employee_id,
rc.first_name,
rc.last_name,
e.manager_id,
e.first_name || ' ' || e.last_name || ' -> ' || rc.reporting_chain AS reporting_chain,
rc.chain_level + 1
FROM reporting_chain rc
INNER JOIN employees e ON rc.manager_id = e.employee_id
WHERE e.manager_id IS NOT NULL
)
SELECT
employee_id,
first_name || ' ' || last_name AS employee_name,
reporting_chain,
chain_level
FROM reporting_chain
WHERE manager_id IS NULL
ORDER BY employee_id;
Question 47: Data validation queries
Question: Find data quality issues in the employees table.
Answer:
-- Check for various data quality issues
WITH data_issues AS (
-- Check for null values in required fields
SELECT 'NULL in required field' AS issue_type, COUNT(*) AS issue_count
FROM employees
WHERE first_name IS NULL OR last_name IS NULL OR email IS NULL
UNION ALL
-- Check for duplicate emails
SELECT 'Duplicate email' AS issue_type, COUNT(*) - COUNT(DISTINCT email) AS issue_count
FROM employees
UNION ALL
-- Check for invalid salary values
SELECT 'Invalid salary' AS issue_type, COUNT(*) AS issue_count
FROM employees
WHERE salary <= 0 OR salary > 1000000
UNION ALL
-- Check for future hire dates
SELECT 'Future hire date' AS issue_type, COUNT(*) AS issue_count
FROM employees
WHERE hire_date > CURRENT_DATE
UNION ALL
-- Check for employees reporting to themselves
SELECT 'Self-reporting' AS issue_type, COUNT(*) AS issue_count
FROM employees
WHERE employee_id = manager_id
)
SELECT * FROM data_issues
WHERE issue_count > 0;
Question 48: Complex date calculations
Question: Calculate employee tenure and categorize by experience level.
Answer:
SELECT
first_name,
last_name,
hire_date,
DATEDIFF(CURRENT_DATE, hire_date) AS days_employed,
FLOOR(DATEDIFF(CURRENT_DATE, hire_date) / 365.25) AS years_employed,
FLOOR((DATEDIFF(CURRENT_DATE, hire_date) % 365.25) / 30) AS months_employed,
CASE
WHEN DATEDIFF(CURRENT_DATE, hire_date) < 365 THEN 'New Employee (<1 year)'
WHEN DATEDIFF(CURRENT_DATE, hire_date) < 365 * 3 THEN 'Junior (1-3 years)'
WHEN DATEDIFF(CURRENT_DATE, hire_date) < 365 * 5 THEN 'Mid-level (3-5 years)'
ELSE 'Senior (5+ years)'
END AS experience_level
FROM employees
ORDER BY days_employed DESC;
Question 49: String manipulation
Question: Parse and clean employee email addresses.
Answer:
SELECT
email,
SUBSTRING_INDEX(email, '@', 1) AS username,
SUBSTRING_INDEX(email, '@', -1) AS domain,
UPPER(LEFT(first_name, 1)) || LOWER(SUBSTRING(first_name, 2)) AS proper_first_name,
LENGTH(email) - LENGTH(REPLACE(email, '.', '')) AS dot_count,
CASE
WHEN email REGEXP '^[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Za-z]{2,}$'
THEN 'Valid'
ELSE 'Invalid'
END AS email_validation
FROM employees;
Question 50: Performance optimization patterns
Question: Rewrite this inefficient query to find employees not assigned to projects.
Inefficient Query:
SELECT * FROM employees
WHERE employee_id NOT IN (SELECT employee_id FROM employee_projects);
Optimized Answer:
-- Method 1: Using NOT EXISTS (typically fastest)
SELECT e.*
FROM employees e
WHERE NOT EXISTS (
SELECT 1
FROM employee_projects ep
WHERE ep.employee_id = e.employee_id
);
-- Method 2: Using LEFT JOIN
SELECT e.*
FROM employees e
LEFT JOIN employee_projects ep ON e.employee_id = ep.employee_id
WHERE ep.employee_id IS NULL;
-- Method 3: Using EXCEPT (PostgreSQL/SQL Server)
SELECT employee_id, first_name, last_name
FROM employees
EXCEPT
SELECT e.employee_id, e.first_name, e.last_name
FROM employees e
INNER JOIN employee_projects ep ON e.employee_id = ep.employee_id;
Common Interview Patterns to Remember
1. Ranking Patterns
• ROW_NUMBER(): Unique ranking
• RANK(): Allows ties, skips numbers
• DENSE_RANK(): Allows ties, no gaps
• NTILE(): Divides into buckets
2. Comparison Patterns
• Self-joins for comparing rows
• LAG/LEAD for sequential comparisons
• Correlated subqueries for group comparisons
3. Aggregation Patterns
• GROUP BY for summaries
• Window functions for running totals
• CTEs for multi-level aggregations
4. Data Quality Patterns
• Finding duplicates
• Identifying NULL values
• Data validation checks
• Consistency checks across tables
5. Performance Patterns
• EXISTS vs IN for better performance
• Proper indexing considerations
• Avoiding SELECT *
• Using appropriate JOIN types
6. Date/Time Patterns
• Date arithmetic
• Extracting date parts
• Period-over-period comparisons
• Finding gaps in date sequences
---
Below is a compact Airbnb-style schema we’ll use for all questions, followed by 30 SQL prompts that mirror what’s typically tested ( joins, aggregations, filters, subqueries, self-joins, window functions, pivoting/conditional agg, date ops, CTEs, semi/anti joins, etc.). I’ve kept each question self-contained and tagged the target concepts.
Working schema (columns you can assume exist)
• listings(listing_id, host_id, city, country, room_type, n_bedrooms, created_at, is_active)
• hosts(host_id, host_since, superhost_flag, country)
• guests(guest_id, signup_dt, country)
• bookings(booking_id, listing_id, guest_id, checkin_dt, checkout_dt, booked_dt, status, nights, guests_count, price_usd)
• reviews(review_id, booking_id, listing_id, guest_id, review_dt, rating, review_text)
• calendar(listing_id, dt, available_flag, nightly_price_usd)
• search_sessions(session_id, guest_id, search_dt, device, country, origin_channel)
• search_results(session_id, listing_id, position, clicked_flag, booked_flag)
• marketing_campaigns(campaign_id, channel, campaign_name, start_dt, end_dt, country)
• ad_impressions(imp_id, campaign_id, guest_id, imp_dt)
• ad_clicks(click_id, campaign_id, guest_id, click_dt)
• product_events(event_dt, guest_id, listing_id, event_name, session_id)
• countries(country, region)
30 SQL interview-style prompts (Airbnb context)
# Scenario / Prompt Tables Concepts Tested
1 Monthly bookings and GMV. For each YYYY-MM, compute total confirmed bookings and GMV (sum(price_usd)) for active listings only.
bookings, listings Filter, join, group by month
2 Host-level performance. Top 10 hosts by GMV last 90 days; include bookings count, distinct guests, and share of bookings with nights >= 7.
bookings, listings, hosts Join, filters, agg w/ conditional sums
3 Occupancy rate. For the last full calendar month, compute occupancy rate per city: booked_nights / total_nights using calendar.available_flag and bookings (confirmed only).
calendar, bookings, listings
Date math, joins, aggregation
4 Search-to-book funnel. By channel, last 30 days: sessions, clicks, bookings, and CTR/BTB rates.
search_sessions, search_results
Semi-joins, conditional agg
5 Repeat guests. % of bookings in last 180 days from guests with ≥2 prior confirmed bookings (any time before the booking’s booked_dt).
bookings Correlated subquery or window lag/count
6 Review coverage. For each city: share of bookings that received a review within 30 days of checkout.
bookings, reviews, listings
Left join, date diff, conditional agg
7 Superhost uplift. Compare average rating and GMV for superhosts vs non-superhosts in the last quarter; return group, bookings, avg_rating, avg_price, GMV.
hosts, bookings, reviews, listings
Joins, grouping, window alt: none
8 Cancellations. Weekly cancellation rate (status = 'canceled') by room_type; include 3-week moving average.
bookings, listings Window (moving avg), date binning
9 Search ranking effect. Compute conversion to booking by position bucket (1–3, 4–10, >10) controlling for device and country.
search_results, search_sessions
Case/pivot, multi-dim group
10 Longest stays. For each country, list the single longest nights stay in last year with listing & host details (ties keep highest price_usd).
bookings, listings, hosts Row_number partition, top-1 per group
11 Guest lifecycle. For each guest, report first_booking_dt, days_to_second_booking, and whether they churned (no bookings last 365d).
bookings, guests Window (min/lead), derived flags
12 Activation cohort. Monthly host cohorts by host_since and their month-2 GMV (GMV in cohort_month+1).
hosts, bookings, listings Cohorts via date trunc + joins
13 Price dispersion. For active listings in a city, compute median nightly price over last 60 days and IQR from calendar.
calendar, listings Percentiles (approx or window), filter
14 Calendar integrity check. Find listings that were unavailable (available_flag=0) on any date where a booking overlapped but nights were priced at 0 or NULL.
calendar, bookings Interval overlap, data QA
15 Attribution (last touch). Within a 7-day attribution window ending at booked_dt, assign each booking to the most recent ad click per guest; output channel-level GMV.
bookings, ad_clicks, marketing_campaigns
Window over time, last-touch join
16 Marketing reach & frequency. For a date range, compute unique guests reached and average frequency by campaign.
ad_impressions Distinct count, ratio
17 Cross-sell signal. Among guests who booked ≥2 cities in the last year, top 10 city pairs by count (unordered pairs).
bookings, listings Self-join or set ops, combinatorics
18 Supply growth. 3-month rolling new listings by region (countries), and % change vs prior 3-month window.
listings, countries Rolling sums, joins, pct change
19 Time to first review. For each listing created in the last year, days from created_at to first review_dt; return P50 and P90 by city.
listings, reviews First-value via min, percentiles
20 Click quality. For sessions with ≥1 click, compute probability a clicked listing gets booked within 48h; return by device.
search_sessions, search_results
Filters, time window, conditional agg
21 Room-type mix. For each city, share of active supply by room_type and Herfindahl index (HHI) of room types.
listings Pivot via conditional sums, HHI calc
22 Price vs rating elasticity proxy. Correlate listing’s median price (last 90d) with average rating (last 365d) by city; return Pearson r.
calendar, reviews, listings
Aggregation to features, corr formula
23 Host multi-listing behavior. Flag hosts with ≥N active listings and compute their share of city GMV last quarter.
listings, bookings Threshold filtering, share of total
24 Cancellation window abuse. Identify guests with ≥2 cancellations within 24h of booked_dt across different listings in past 180d.
bookings Self-join on guest, time diff
25 Underpriced blackout. Find top 50 listings where calendar shows available_flag=1 but nightly_price_usd is below the city’s 10th percentile in the next 14 days.
calendar, listings Percentiles by group, date filter
26 Session to search gap. Compute median minutes from first product_events.event_name='sessio n_start' to first search per session.
product_events Window by session, time diff
27 Geo expansion. For new countries launched in last 18 months (first listing), report month-over-month bookings for first 6 months post-launch.
listings, bookings Launch detection, aligned windows
28 Review bias. Compare average rating for stays ≤2 nights vs ≥7 nights, controlling for city (return city-level deltas).
bookings, reviews, listings
Join, group, diff calculations
29 Seasonality index. For each city, compute monthly seasonality index = month_GMV / avg_month_GMV using 24 months of history.
bookings, listings Window avg over partition
30 Booking lead time distribution. For last quarter, output deciles of lead time (booked_dt→checkin_dt) by room_type.
bookings, listings Date diff, percentiles by group
Notes on expected approaches (best practices)
• Prefer date_trunc('month', …) or DATE_TRUNC('month', …) depending on Presto/Hive.
• Use CTEs for readability, especially when deriving features (e.g., lead time, cohorts).
• For percentiles in Presto: approx_percentile(x, 0.5); in Hive: percentile_approx.
• For rolling windows, use window frames: avg(x) over (partition by k order by d rows between 2 preceding and current row).
• For interval overlaps: GREATEST(start1, start2) < LEAST(end1, end2).
• For attribution: row_number() over (partition by booking_id order by click_dt desc) after joining clicks ≤ booked_dt and within window.
• For distinct counts at scale, approximate functions (HLL/approx_distinct) may be acceptable if the engine supports them.
If you’d like, I can provide answer keys for any subset (e.g., 1–10) tailored to Presto/Hive syntax and optimized for readability and performance.
Below is a compact, interview-style set of 30 Airbnb-context SQL questions spanning the usual segments ( joins, aggregation, filtering, subqueries, self-joins, windowing, date/time, arrays/maps, set ops, funnels/retention, and experiment/marketing measurement). They are framed for Presto/Hive-style SQL (Airbnb commonly uses SQL engines in that family) and align with the role’s emphasis on experimentation/causal methods, product & growth metrics, and building robust reporting/metrics. (Careers at Airbnb)
Starter schema (assumed)
Table Key columns (subset)
users user_id (PK), created_at (ts), country, device_type
listings listing_id (PK), host_id, city, room_type, created_at
bookings booking_id (PK), user_id, listing_id, checkin_date (date), checkout_date (date), status (‘confirmed’, ‘cancelled’), booked_at (ts), total_price (decimal), coupon_code
reviews review_id, booking_id, user_id, listing_id, rating (int), reviewed_at (ts)
calendar listing_id, dt (date), is_available (bool), price (decimal)
search_events event_ts (ts), user_id, query, city, result_count, device_type
sessions session_id, user_id, session_start (ts), session_end (ts), source (utm_source), medium (utm_medium)
marketing_impre ssions
imp_id, user_id, campaign_id, imp_ts (ts), channel, cost
marketing_click s
click_id, user_id, campaign_id, click_ts (ts), channel
payments payment_id, booking_id, paid_at (ts), amount, method
refunds refund_id, booking_id, refunded_at (ts), amount
experiments exp_name, user_id, variant (‘control’, ‘treatment’), assigned_ts
hosts host_id (PK), joined_at (ts), country
messages message_id, booking_id, from_user_id, to_user_id, sent_at (ts)
Use these columns as needed; not every column will be used per question.
30 interview-style SQL questions (Airbnb themed)
# Segment Scenario Prompt (what to return) Key concepts tested
1 Filtering + Agg
New guest conversion
Among users created in last 90 days, % with ≥1 confirmed booking within 30 days of signup.
Date filters, conditional agg
2 Joins Search→booking By device, weekly search→booking conversion: users with a search this week who booked within 7 days after first search.
Join on user & time window
3 Window fn Repeat bookers Top 10 cities by share of repeat guests in last 12 months (repeat = ≥2 confirmed bookings).
Grouping, window percent
4 Self-join Re-engagement Users inactive ≥180 days who returned to book; compute days since prior booking at return time.
Self-join by user, LAG
5 Subquery Listing quality For listings with ≥10 reviews, output avg rating and count; keep top decile by avg rating per city.
Aggregation, percentile filter
6 Window fn Host activation funnel
For new hosts ( joined last quarter), compute time from join to first booking; report P50/P90 by host country.
DATEDIFF, percentiles
7 Anti-join Unreviewed stays Bookings completed 14+ days ago with no review yet; list booking_id, user_id, listing_id.
LEFT/ANTI join
8 String/date Peak pricing For calendar, find month/day-of-week combos with highest median price per city (last year).
DATE_TRUNC, extract DOW
9 Joins + CASE Cancellation rate By room_type and month, cancellation rate = cancelled / (cancelled+confirmed).
Conditional sums
10 Window fn Greatest-N per group
For each city, top 3 listings by nights booked in the last 90 days. Break ties by total revenue.
ROW_NUMBER, ORDER BY multi
11 CTE + Window
Sessionization sanity
Compute avg sessions/user/week and median session duration by source for past 8 weeks.
TIMESTAMP math, pctls
12 Arrays/Maps Query analysis From search_events.query, count searches containing any of ['pool','pet','wifi'] by device, week.
Simple substring/regex
13 Dedup Identity stitching Clicks table may have dup events. Dedup by (user_id, campaign_id, minute(click_ts)). Count unique clicks/day.
QUALIFY ROW_NUMBER
14 Funnel (semi-joins)
Booking funnel Build 3-step funnel: search → listing view (assume sessions page events) → booking; output step rates by city.
Semi-join, distinct users
15 Retention 8-week retention Weekly cohort retention for guests by signup week; return CohortWeek, WeekK, RetainedUsers.
Cohort labeling, joins
16 Marketing join
CPA by channel Join marketing_clicks to first booking within 7 days post-click; compute CPA and CVR by channel.
Attribution, first touch
17 MMM prep (SQL)
Feature table Create weekly panel with bookings, revenue, and ad spend by channel (sum(marketing_impression s.cost)).
Time bucketing, panel
18 CUPED inputs
Pre-period metric For an experiment on new search UI, compute each user’s pre-period mean daily searches (baseline covariate).
Pre window agg
19 Guardrails Abuse spike In experiment, report week-over-week % change in cancellation rate and refund rate by variant.
Joins, ratios, WoW
20 Percentiles Price dispersion For each city, P10, P50, P90 of nightly price (from calendar) restricted to available nights.
approx_percentile
21 Multi-table join
Host response For completed bookings, compute median host response time (first message after guest message).
Self join by time, MIN
22 Set ops Supply vs demand
Cities with demand growth (bookings +20% YoY) but supply growth (active listings) <5% YoY.
Intersections, YoY
23 Rollups KPI cube Produce a cube of bookings/revenue by (city, room_type, device_type), including subtotals and grand total.
GROUPING SETS/ROLLUP
24 QA outliers Price sanity Identify listings whose median price is >5× city median in a month (potential data issues).
Median by group, compare
25 Window fn RFM proxy For each user, compute recency (days since last booking), frequency (bookings last 365d), and monetary (sum revenue).
MAX over user, sums
26 Time zones Booking hour Distribution of booked_at by local listing time zone hour; assume listing city→TZ mapping table city_tz.
TZ conversion
27 Joins + CASE Net revenue Net revenue = payments − refunds; report by month and room_type for past 12 months.
Joins, arithmetic
28 Self-join Cannibalization For users in experiment with new pricing, compute if first booking post-assignment was cheaper than their prior booking (same city).
Pairwise compare
29 Window fn SLA breaches For support messages during a booking, % of first host replies > 12 hours; breakdown by city.
FIRST_VALUE/MIN, CASE
30 UDF/Regex Coupon lift For bookings with coupon_code, estimate incremental lift proxy: avg revenue with coupon vs similar listings without (same city, room_type, month).
Matched grouping

---
SQL Live Interview Playbook — Sr. Manager, Advanced Analytics (Marketing)
Use this as a rapid reference during live SQL rounds. The idioms are BigQuery‑flavored but portable. All snippets fit within ~80 chars/line.
0) How to use this
• Skim Section 1 to structure your approach under time pressure.
• Use Section 2 to map the prompt → pattern quickly.
• Paste skeletons from Section 3–4 and adapt with table/column names.
• Run the Section 5 checklist before executing.
• Drill with Section 6. Start with P1–P8 solutions, then the rest.
1) 8‑step approach under time pressure
Step What to do Key checks & phrases
1 Restate goal Output grain, columns, sort, tie‑breaks, NULL policy
2 Identify data Table names, keys, types, time zone, partitions
3 Define grain e.g., user‑day, user‑session, order, cohort‑week
4 Edge cases Dups, late events, backfills, bots, test traffic
5 Plan query CTE stages: filter → join → enrich → agg → present
6 Write Use WITH CTEs, small test filters, determinism
7 Verify Row counts, sample rows, invariants (e.g., totals)
8 Finalize Remove test filters, add order/limit, comments
Ask explicitly: time window (inclusive/exclusive), time zone, attribution rule (first/last, click>view), dedupe rule (latest by ts, max version), and whether to count unique users or events.
2) Prompt → Pattern decision matrix
Clues in prompt Pattern to use Core idiom
"latest record", "one per user"
Dedupe by ranking QUALIFY ROW_NUMBER()
"top N per group" Greatest‑N per group QUALIFY ROW_NUMBER<=N
"first/last touch" Attribution window JOIN ... ON ts BETWEEN ... + rank
"within 30 mins" Sessionization LAG + DIF > 30m run‑sum groups
"consecutive days" Gaps & islands LAG + (date!=lag+1) group labels
"retention by cohort" Cohort cube First date per user → DATEDIFF bucket
"rolling 7 days" Moving window SUM(...) OVER win ROWS/RANGE
"funnel step 1→n" Step gating Semi‑joins or MAX(step_k) method
"percentile/median" Quantiles APPROX_QUANTILES(x,100)[OFFS ET(p)]
"users not in ..." Anti‑join LEFT JOIN ... WHERE right IS NULL
"second highest" Rank/Distinct DENSE_RANK or LIMIT/OFFSET
"missing days" Date spine GENERATE_DATE_ARRAY left join
3) Reusable pattern library (BigQuery‑friendly)
3.1 Dedupe: latest record per key (deterministic)
WITH ranked AS (
SELECT t.*, ROW_NUMBER() OVER (
PARTITION BY user_id ORDER BY event_ts DESC, event_id DESC
) AS rn
FROM `proj.ds.events` t
)
SELECT * FROM ranked QUALIFY rn = 1;
3.2 Top‑N per group (ties broken deterministically)
WITH ranked AS (
SELECT p.product_id, p.user_id, p.revenue,
ROW_NUMBER() OVER (
PARTITION BY user_id ORDER BY revenue DESC, product_id
) AS rk
FROM `proj.ds.purchases` p
)
SELECT * FROM ranked QUALIFY rk <= 3;
3.3 Gaps & islands: consecutive‑day streaks per user
WITH d AS (
SELECT user_id, DATE(event_ts) AS d
FROM `proj.ds.events`
GROUP BY user_id, d
), x AS (
SELECT *, DATE_DIFF(d, LAG(d) OVER (PARTITION BY user_id ORDER BY d),
DAY) AS diff
FROM d
), y AS (
SELECT *, SUM(CASE WHEN diff = 1 THEN 0 ELSE 1 END) OVER (
PARTITION BY user_id ORDER BY d) AS grp
FROM x
)
SELECT user_id, MIN(d) AS start_d, MAX(d) AS end_d, COUNT(*) AS days
FROM y
GROUP BY user_id, grp;
3.4 Date spine (fill missing days)
WITH spine AS (
SELECT d AS dt
FROM UNNEST(GENERATE_DATE_ARRAY('2025-01-01','2025-01-31')) AS d
), agg AS (
SELECT DATE(event_ts) AS dt, COUNT(*) AS cnt
FROM `proj.ds.events`
WHERE event_ts >= '2025-01-01' AND event_ts < '2025-02-01'
GROUP BY dt
)
SELECT s.dt, COALESCE(a.cnt, 0) AS cnt
FROM spine s
LEFT JOIN agg a USING (dt)
ORDER BY s.dt;
3.5 Rolling 7‑day sum per user
SELECT user_id, DATE(event_ts) AS d,
SUM(value) OVER (
PARTITION BY user_id ORDER BY d
RANGE BETWEEN INTERVAL 6 DAY PRECEDING AND CURRENT ROW
) AS val_7d
FROM `proj.ds.events`;
3.6 Funnel (step gating; 1→N in order)
WITH s AS (
SELECT user_id,
MIN(CASE WHEN step='view' THEN event_ts END) AS ts1,
MIN(CASE WHEN step='add' THEN event_ts END) AS ts2,
MIN(CASE WHEN step='pay' THEN event_ts END) AS ts3
FROM `proj.ds.funnel_events`
GROUP BY user_id
)
SELECT
COUNTIF(ts1 IS NOT NULL) AS view_users,
COUNTIF(ts2 IS NOT NULL AND ts2 >= ts1) AS add_users,
COUNTIF(ts3 IS NOT NULL AND ts3 >= ts2 AND ts2 >= ts1) AS pay_users;
3.7 Sessionization (30‑minute inactivity)
WITH e AS (
SELECT user_id, event_ts,
IF(TIMESTAMP_DIFF(event_ts,
LAG(event_ts) OVER (
PARTITION BY user_id ORDER BY event_ts),
MINUTE) > 30, 1, 0) AS new_sess
FROM `proj.ds.events`
), s AS (
SELECT *, SUM(new_sess) OVER (
PARTITION BY user_id ORDER BY event_ts) AS sess_id
FROM e
)
SELECT user_id, sess_id,
MIN(event_ts) AS session_start,
MAX(event_ts) AS session_end,
COUNT(*) AS events
FROM s
GROUP BY user_id, sess_id;
3.8 Cohort retention (weekly, user first‑purchase cohort)
WITH firsts AS (
SELECT user_id,
DATE_TRUNC(MIN(order_date), WEEK(MONDAY)) AS cohort_w
FROM `proj.ds.orders`
GROUP BY user_id
), weeks AS (
SELECT user_id, DATE_TRUNC(order_date, WEEK(MONDAY)) AS active_w
FROM `proj.ds.orders`
GROUP BY user_id, active_w
)
SELECT f.cohort_w,
DATE_DIFF(w.active_w, f.cohort_w, WEEK) AS wk,
COUNT(DISTINCT w.user_id) AS active_users
FROM firsts f
JOIN weeks w USING (user_id)
GROUP BY cohort_w, wk
ORDER BY cohort_w, wk;
3.9 Ads attribution: click over view within 7 days
-- clicks c(user_id, click_ts, campaign_id)
-- views v(user_id, view_ts, campaign_id)
-- conv k(user_id, conv_ts)
WITH cand AS (
SELECT k.user_id, k.conv_ts,
c.campaign_id AS camp,
c.click_ts AS at_ts,
1 AS pri
FROM k
JOIN c
ON c.user_id = k.user_id
AND c.click_ts BETWEEN TIMESTAMP_SUB(k.conv_ts, INTERVAL 7 DAY)
AND k.conv_ts
UNION ALL
SELECT k.user_id, k.conv_ts,
v.campaign_id, v.view_ts, 2
FROM k
JOIN v
ON v.user_id = k.user_id
AND v.view_ts BETWEEN TIMESTAMP_SUB(k.conv_ts, INTERVAL 7 DAY)
AND k.conv_ts
), picked AS (
SELECT *, ROW_NUMBER() OVER (
PARTITION BY user_id, conv_ts ORDER BY pri, at_ts DESC
) AS rn
FROM cand
)
SELECT user_id, conv_ts, camp
FROM picked
QUALIFY rn = 1;
3.10 Percentiles / median (approximate is fine live)
SELECT APPROX_QUANTILES(metric, 100)[OFFSET(50)] AS p50
FROM `proj.ds.table`;
3.11 Anti‑join / semi‑join
-- Users who purchased but never installed app
SELECT DISTINCT p.user_id
FROM `proj.ds.purchases` p
LEFT JOIN `proj.ds.installs` i USING (user_id)
WHERE i.user_id IS NULL;
3.12 UTM parsing (lowercase, safe)
SELECT user_id,
LOWER(REGEXP_EXTRACT(url, r"[?&]utm_source=([^&#]+)")) AS src,
LOWER(REGEXP_EXTRACT(url, r"[?&]utm_medium=([^&#]+)")) AS med,
LOWER(REGEXP_EXTRACT(url, r"[?&]utm_campaign=([^&#]+)")) AS cmp
FROM `proj.ds.pageviews`;
4) Marketing analytics staples (ready‑to‑run)
4.1 DAU / WAU / MAU and stickiness (DAU/MAU)
WITH u AS (
SELECT DATE(event_ts) AS d, user_id
FROM `proj.ds.events`
GROUP BY d, user_id
)
SELECT
(SELECT COUNT(DISTINCT user_id)
FROM u WHERE d = '2025-08-28') AS dau,
(SELECT COUNT(DISTINCT user_id)
FROM u WHERE d BETWEEN '2025-08-22' AND '2025-08-28') AS wau,
(SELECT COUNT(DISTINCT user_id)
FROM u WHERE d BETWEEN '2025-07-30' AND '2025-08-28') AS mau;
-- stickiness = dau / mau
4.2 CAC by channel per week
-- spend(channel, dt, cost)
-- signups(user_id, dt, channel)
WITH s AS (
SELECT DATE_TRUNC(dt, WEEK(MONDAY)) AS wk, channel,
SUM(cost) AS spend
FROM `proj.ds.spend`
GROUP BY wk, channel
), u AS (
SELECT DATE_TRUNC(dt, WEEK(MONDAY)) AS wk, channel,
COUNT(DISTINCT user_id) AS users
FROM `proj.ds.signups`
GROUP BY wk, channel
)
SELECT s.wk, s.channel, spend, users, SAFE_DIVIDE(spend, users) AS cac
FROM s JOIN u USING (wk, channel)
ORDER BY wk, channel;
4.3 LTV within 90 days of signup
-- orders(user_id, order_ts, net_rev)
-- signups(user_id, signup_ts)
WITH joined AS (
SELECT s.user_id, s.signup_ts, o.order_ts, o.net_rev
FROM `proj.ds.signups` s
LEFT JOIN `proj.ds.orders` o
ON o.user_id = s.user_id
AND o.order_ts BETWEEN s.signup_ts AND
TIMESTAMP_ADD(s.signup_ts, INTERVAL 90 DAY)
)
SELECT DATE(s.signup_ts) AS signup_d,
AVG(net_rev) AS ltv_90d
FROM joined s
GROUP BY signup_d
ORDER BY signup_d;
4.4 Reactivation rate (churned ≥30d then active)
WITH act AS (
SELECT user_id, DATE(event_ts) AS d
FROM `proj.ds.events`
GROUP BY user_id, d
), gap AS (
SELECT user_id, d,
LAG(d) OVER (PARTITION BY user_id ORDER BY d) AS prev_d
FROM act
), reac AS (
SELECT user_id, d
FROM gap
WHERE prev_d IS NOT NULL AND DATE_DIFF(d, prev_d, DAY) >= 30
)
SELECT d AS reactivation_d, COUNT(*) AS users
FROM reac
GROUP BY reactivation_d
ORDER BY reactivation_d;
4.5 A/B test metric (mean revenue/user; guardrails)
-- events contain (user_id, variant, revenue)
WITH by_u AS (
SELECT variant, user_id, SUM(revenue) AS r
FROM `proj.ds.exp_events`
GROUP BY variant, user_id
)
SELECT variant,
COUNT(*) AS users,
AVG(r) AS arpu,
STDDEV_SAMP(r) AS sd
FROM by_u
GROUP BY variant;
-- Compute diff, CI in Python or use APPROX_QUANTILES for quick sanity.
5) BigQuery correctness & performance checklist
Area Rule
Filters Always filter by _PARTITIONTIME/date on large tables
Select Avoid SELECT *. Project only needed columns
Joins Pre‑aggregate before many‑to‑many joins; add join keys list
Dups Make ranking/tie‑breakers deterministic (secondary ORDER BY)
Nulls Use COALESCE, SAFE_ casts, and decide inclusion early
Time Fix time zone; define window boundaries precisely
Windows Prefer QUALIFY to prune after window ranking
Arrays UNNEST deliberately; pre‑filter to avoid explosion
Approx APPROX_* for speed in interviews if exactness not needed
Costs LIMIT in dev; remove before final, or justify keeping it
6) Practice problems (20)
ID Topic Prompt
P1 Dedupe Latest paid order per user in last 60 days
P2 Top‑N Top 3 cities per country by bookings (ties deterministic)
P3 Funnel view→wishlist→book conversion counts & rates
P4 Rolling 7‑day rolling cancellations per market
P5 Attribution Assign each booking to click>view within 7 days
P6 Cohort Weekly retention for hosts after first listing
P7 Gaps Longest consecutive active streak per guest
P8 Anti‑join Guests who booked but never messaged a host
P9 Date spine Fill missing days of bookings per market
P10 Percentile Median nightly price per city per month
P11 Session Sessionize guest web events (30‑min gap)
P12 UTM CAC by utm_source
P13 WAU/MAU Compute DAU/WAU/MAU + stickiness
P14 Reactivation Guests inactive 60d who returned
P15 AB ARPU by variant with sanity guardrails
P16 String Extract room type from title via regex
P17 Join Hosts with ≥3 listings and ≥2 bookings each
P18 Window 2nd highest booking value per host
P19 Quality Flag suspected test traffic by rules
P20 Cleanup Remove duplicates by (id, ts) keep max version
Selected solutions & skeletons
P1 Latest paid order per user (60 days)
WITH f AS (
SELECT *
FROM `proj.ds.orders`
WHERE status = 'paid'
AND order_ts >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 60 DAY)
), r AS (
SELECT *, ROW_NUMBER() OVER (
PARTITION BY user_id ORDER BY order_ts DESC, order_id DESC
) AS rn
FROM f
)
SELECT * FROM r WHERE rn = 1;
P2 Top 3 cities per country by bookings
WITH agg AS (
SELECT country, city, COUNT(*) AS b
FROM `proj.ds.bookings`
GROUP BY country, city
), r AS (
SELECT *, ROW_NUMBER() OVER (
PARTITION BY country ORDER BY b DESC, city
) AS rk
FROM agg
)
SELECT country, city, b FROM r WHERE rk <= 3 ORDER BY country, rk;
P3 Funnel view→wishlist→book
WITH s AS (
SELECT user_id,
MIN(CASE WHEN ev='view' THEN ts END) AS t1,
MIN(CASE WHEN ev='wishlist' THEN ts END) AS t2,
MIN(CASE WHEN ev='book' THEN ts END) AS t3
FROM `proj.ds.site_events`
GROUP BY user_id
)
SELECT
COUNTIF(t1 IS NOT NULL) AS v,
COUNTIF(t2 IS NOT NULL AND t2 >= t1) AS w,
COUNTIF(t3 IS NOT NULL AND t3 >= t2 AND t2 >= t1) AS b,
SAFE_DIVIDE(COUNTIF(t2 IS NOT NULL AND t2 >= t1),
COUNTIF(t1 IS NOT NULL)) AS v2w,
SAFE_DIVIDE(COUNTIF(t3 IS NOT NULL AND t3 >= t2 AND t2 >= t1),
COUNTIF(t1 IS NOT NULL)) AS v2b;
P4 7‑day rolling cancellations per market
WITH d AS (
SELECT market, DATE(cancel_ts) AS d, COUNT(*) AS c
FROM `proj.ds.cancels`
GROUP BY market, d
)
SELECT market, d,
SUM(c) OVER (
PARTITION BY market ORDER BY d
RANGE BETWEEN INTERVAL 6 DAY PRECEDING AND CURRENT ROW
) AS c_7d
FROM d;
P5 Booking attribution click>view
-- Use 3.9; replace tables with bookings(conv) and ads clicks/views.
P6 Weekly retention for hosts after first listing
WITH firsts AS (
SELECT host_id,
DATE_TRUNC(MIN(list_ts), WEEK(MONDAY)) AS cohort_w
FROM `proj.ds.listings`
GROUP BY host_id
), act AS (
SELECT host_id, DATE_TRUNC(event_ts, WEEK(MONDAY)) AS w
FROM `proj.ds.host_events`
GROUP BY host_id, w
)
SELECT cohort_w, DATE_DIFF(w, cohort_w, WEEK) AS wk,
COUNT(DISTINCT host_id) AS hosts
FROM firsts JOIN act USING (host_id)
GROUP BY cohort_w, wk
ORDER BY cohort_w, wk;
P7 Longest active streak per guest
-- Use 3.3 and then take MAX(days) per user.
P8 Guests booked but never messaged
SELECT DISTINCT b.user_id
FROM `proj.ds.bookings` b
LEFT JOIN `proj.ds.messages` m USING (user_id)
WHERE m.user_id IS NULL;
(For P9–P20, use the matching patterns from Section 3.)
7) Speed tactics for live rounds
• Write CTEs in vertical slices. Name CTEs by purpose: filt, agg, rnk.
• Enforce determinism in every ORDER BY used for ranking.
• Develop with a narrow date filter, then widen. Keep the filter commented.
• Print sample rows for sanity: ORDER BY ... LIMIT 10.
• Prefer QUALIFY over re‑wrapping ranked CTEs when simple.
8) Debugging checklist
• Row count growth across CTEs as expected? Any blowups after joins?
• Keys unique at the correct grain before joining? If not, pre‑agg.
• Off‑by‑one dates? Inclusive/exclusive windows consistent?
• NULL‑safety: SAFE_ casts, COALESCE defaults.
• Time zone alignment: pick one and convert at inputs.
9) Minimal BigQuery function glossary
• QUALIFY, GENERATE_DATE_ARRAY, APPROX_QUANTILES, SAFE_CAST
• DATE_TRUNC, TIMESTAMP_ADD/SUB, DATE_DIFF, IFNULL/COALESCE
• LAG/LEAD, ROW_NUMBER, DENSE_RANK, COUNTIF
• RANGE‑based window frames for true time windows

---
An Exhaustive Guide to Mastering SQL for Technical Interviews
…
Section 1.2: The Anatomy of a Query
The SELECT statement is the cornerstone of data retrieval in SQL. Understanding its components and the logical order in which they are processed is fundamental to writing correct and efficient queries.
Core Syntax and Logical Order of Operations
While a query is written in a specific order (SELECT, FROM, WHERE, etc.), the database engine processes it in a different logical order. A solid mental model of this processing order helps in understanding how clauses interact. 1. FROM / JOIN: Identifies the source tables and combines them. 2. WHERE: Filters individual rows based on conditions. 3. GROUP BY: Aggregates the filtered rows into groups. 4. HAVING: Filters the aggregated groups. 5. SELECT: Selects the final columns and performs calculations. 6. DISTINCT: Removes duplicate rows from the result. 7. ORDER BY: Sorts the final result set. 8. LIMIT / OFFSET: Restricts the number of rows returned.
Clauses & Operators
• SELECT: Specifies the columns to be returned. Using * selects all columns, but it is a best
practice in production code to explicitly name the required columns to improve clarity and performance.8
• DISTINCT: Used within the SELECT clause to return only unique values, eliminating duplicate rows from the result set.13
• WHERE: Filters the result set based on one or more conditions applied to individual rows before any grouping occurs.12
• ORDER BY: Sorts the final result set in ascending (ASC, default) or descending (DESC) order based on one or more columns.11
• LIMIT / OFFSET: LIMIT restricts the number of rows returned, while OFFSET skips a specified number of rows before starting to return results. These are crucial for implementing pagination.13
• Operators:
• Comparison: =, <>, !=, >, <, >=, <= are used for comparing values.13
• Logical: AND, OR, NOT are used to combine multiple conditions in a WHERE clause.13
• Pattern Matching & Lists: ■ LIKE: Used with wildcards (% for zero or more characters, _ for a single
character) for pattern matching in strings.11
■ IN: Checks if a value exists within a specified list of values.12
■ BETWEEN: Selects values within a given range (inclusive).13
■ IS NULL / IS NOT NULL: Specifically used to check for the presence or absence of a value, as NULL cannot be compared using standard operators like = or <>.14
Practice Questions
• Question: Write a query to find employees whose names start with ‘Int’.
• Answer: SELECT * FROM employees WHERE employee_name LIKE 'Int%';.13
• Question: Write a query to find orders where the order amount exists between 1000 and 5000.
• Answer: SELECT * FROM orders WHERE order_amount BETWEEN 1000 AND 5000;.12
• Question: Explain the difference between BETWEEN and IN.
• Answer: BETWEEN is used to select values within a continuous range (e.g., numbers,
dates), and it is inclusive of the start and end values. IN is used to check if a value matches any value in a discrete list of specified values.12
• Question: Are NULL values equal to zero or a blank space? Explain.
• Answer: No. A NULL value represents the absence of data or an unknown value. It is
distinct from zero, which is a specific number, and a blank space or empty string, which is a character value of length zero. NULL cannot be compared with any other
value using standard comparison operators; one must use IS NULL or IS NOT NULL.12
Section 1.3: Data Definition and Manipulation (DDL, DML, DCL, TCL)
SQL commands are categorized into functional subsets. A clear understanding of these categories is often tested in foundational interview questions.
• Data Definition Language (DDL): Defines and manages the structure of database
objects.
• Commands: CREATE, ALTER, DROP, TRUNCATE.1
• Data Manipulation Language (DML): Used to manage the data within the schema objects.
• Commands: INSERT, UPDATE, DELETE, SELECT.1
• Data Control Language (DCL): Manages access rights and permissions to the database.
• Commands: GRANT, REVOKE.1
• Transaction Control Language (TCL): Manages transactions within the database.
• Commands: COMMIT, ROLLBACK, SAVEPOINT.13
• Data Query Language (DQL): Primarily consists of the SELECT statement for retrieving data.1
The DELETE vs. TRUNCATE vs. DROP Debate
This is a classic interview question that tests a candidate's understanding of how the database handles data removal at different levels.
• DELETE: A DML command that removes rows from a table one by one. It can be used with a WHERE clause to remove specific rows. Because it is a logged operation, it is generally slower and can be rolled back within a transaction. It also fires any DELETE triggers associated with the table.11
• TRUNCATE: A DDL command that removes all rows from a table by deallocating the data pages. It is much faster than DELETE for large tables as it is minimally logged. It cannot be used with a WHERE clause and typically cannot be rolled back (though behavior can vary by RDBMS within an explicit transaction). It does not fire DELETE triggers.11
• DROP: A DDL command that completely removes the table itself, including its structure, data, indexes, constraints, and permissions. The action is irreversible without a backup.11
Practice Questions
• Question: What is the difference between DELETE and TRUNCATE? Can you roll back a TRUNCATE statement?
• Answer: DELETE is a DML command that removes rows individually and is logged,
making it slower but allowing for rollback and firing triggers. TRUNCATE is a DDL command that deallocates all data pages at once, making it faster but generally not rollbackable (with some exceptions like SQL Server within a BEGIN TRAN...ROLLBACK block) and it does not fire triggers.11
• Question: How would you add a column to an existing table?
• Answer: Using the ALTER TABLE command with the ADD COLUMN clause. For
example: ALTER TABLE employees ADD email VARCHAR(100);.14
• Question: How can you copy data from one table to another?
• Answer: Using the INSERT INTO... SELECT... statement. For example, to copy all
records: INSERT INTO new_table SELECT * FROM old_table;.13 To create a new table with the same structure and data, one can use SELECT * INTO new_table FROM old_table; in some SQL dialects.11
Section 1.4: Database Normalization
Normalization is a systematic approach to designing a database schema to minimize data redundancy and improve data integrity. It involves dividing larger tables into smaller, well-structured tables and defining relationships between them.11
Normal Forms (1NF, 2NF, 3NF, BCNF)
• First Normal Form (1NF): A table is in 1NF if all its columns contain atomic (indivisible) values, and each row is unique. This means no repeating groups or multi-valued columns.11
• Second Normal Form (2NF): A table must be in 1NF and all of its non-key attributes must be fully functionally dependent on the entire primary key. This rule applies to tables with composite primary keys and aims to eliminate partial dependencies, where a non-key
attribute depends on only part of the composite primary key.11
• Third Normal Form (3NF): A table must be in 2NF and all its attributes must be dependent only on the primary key. This eliminates transitive dependencies, where a non-key attribute is dependent on another non-key attribute.11
• Boyce-Codd Normal Form (BCNF): A stricter version of 3NF. A table is in BCNF if for every non-trivial functional dependency X→Y, X is a superkey. In simpler terms, every determinant must be a candidate key.13
Denormalization
Denormalization is the process of intentionally introducing redundancy into a normalized database design to improve query performance. By combining tables and reducing the number of required JOINs, read operations can be made significantly faster. This is a common strategy in data warehousing and reporting databases (OLAP systems) where read performance is more critical than write efficiency.11
Practice Questions
• Question: Explain the different normal forms (1NF, 2NF, 3NF).
• Answer: 1NF ensures atomic values in columns. 2NF builds on 1NF and removes
partial dependencies on composite keys. 3NF builds on 2NF and removes transitive dependencies, where non-key attributes depend on other non-key attributes.13
• Question: What is denormalization and when would you use it?
• Answer: Denormalization is the strategic introduction of redundancy to a database
to improve read performance by reducing the need for complex joins. It is often used in analytical systems (OLAP) or high-traffic applications where query speed is a primary concern, and the trade-off of increased storage and more complex updates is acceptable.11
Part II: Intermediate SQL - Aggregating and Combining Data
This section transitions from single-table operations to the core analytical tasks of summarizing and merging data from multiple sources. The progression from simple JOINs to subqueries and then to Common Table Expressions (CTEs) reflects a growing sophistication in a developer's approach to problem-solving. Interviews often test this maturity curve. A candidate who can solve a problem with a JOIN is competent; one who recognizes when a CTE offers superior clarity and efficiency demonstrates a higher level of expertise.
Section 2.1: Mastering Aggregation
Aggregation is the process of transforming detailed, row-level data into summarized, meaningful information.
Aggregate Functions
These functions operate on a set of values to return a single, summary value. They are essential for calculating metrics.
• COUNT(): Returns the number of rows. COUNT(*) counts all rows, while
COUNT(column_name) counts non-NULL values in that column.13
• SUM(): Calculates the total sum of a numeric column.13
• AVG(): Calculates the average value of a numeric column.13
• MIN(): Returns the minimum value in a column.13
• MAX(): Returns the maximum value in a column.13
Grouping Data with GROUP BY
The GROUP BY clause is used with aggregate functions to group rows that have the same values in specified columns into summary rows. For each group, the aggregate function calculates a summary value.13
Filtering Groups with WHERE vs. HAVING
This distinction is a fundamental and frequently asked interview question.
• WHERE clause filters individual rows before they are passed to the GROUP BY clause and aggregate functions. It operates on row-level data.12
• HAVING clause filters groups after the GROUP BY clause has been applied and the aggregations have been calculated. It operates on the aggregated results.12
Practice Questions
• Question: Find the average salary for each department.
• Answer: SELECT department, AVG(salary) AS avg_salary FROM employees GROUP
BY department;.13
• Question: List departments with more than 10 employees.
• Answer: SELECT department, COUNT(*) FROM employees GROUP BY department
HAVING COUNT(*) > 10;.14
• Question: Explain, with an example, the difference between the WHERE and HAVING clauses.
• Answer: WHERE filters rows before aggregation, while HAVING filters groups after
aggregation. For example, to find the total sales for products with a price over $10, but only for categories with total sales exceeding $1000, you would use WHERE price > 10 to filter products first, then GROUP BY category, and finally HAVING SUM(sales) > 1000 to filter the resulting categories.12
Section 2.2: The Art of the JOIN
JOIN clauses are the primary mechanism for combining data from two or more tables based on a related column.
Core JOIN Types
• INNER JOIN: Returns only the rows that have matching values in both tables. It is the most common type of join.11
• LEFT JOIN (or LEFT OUTER JOIN): Returns all rows from the left table and the matched rows from the right table. If there is no match, NULL is returned for the columns from the right table. This is useful for finding entities that may not have a corresponding entry in another table (e.g., customers who have never placed an order).11
• RIGHT JOIN (or RIGHT OUTER JOIN): The inverse of a LEFT JOIN. It returns all rows from the right table and the matched rows from the left table. NULL is returned for left table columns where there is no match.11
• FULL OUTER JOIN: Returns all rows when there is a match in either the left or the right table. It effectively combines the results of both LEFT and RIGHT joins. If there is no match for a given row, the columns from the other table will contain NULL.11
Advanced JOIN Types
• CROSS JOIN: Returns the Cartesian product of the two tables, meaning every row from the first table is combined with every row from the second table. It is used less frequently but can be useful for generating all possible combinations of data.11
• SELF JOIN: This is a regular join, but the table is joined with itself. It is used to query hierarchical data or to compare rows within the same table. Table aliases are required to distinguish between the two instances of the table in the query.11
Practice Questions
• Question: Write a query to join 3 tables.
• Answer: SELECT * FROM table1 t1 JOIN table2 t2 ON t1.id = t2.t1_id JOIN table3 t3
ON t2.id = t3.t2_id;.13
• Question: Get all employees and their project names, showing NULL if an employee is not assigned a project.
• Answer: This requires a LEFT JOIN from the employees table to the projects table.
SELECT e.name, p.project_name FROM employees e LEFT JOIN projects p ON e.id = p.employee_id;.13
• Question: Write a query to find pairs of employees who have the same salary.
• Answer: This is a classic SELF JOIN problem. SELECT e1.name, e2.name, e1.salary
FROM employees e1 JOIN employees e2 ON e1.salary = e2.salary AND e1.id < e2.id;
The e1.id < e2.id condition is crucial to avoid listing the same pair twice (e.g., A-B and B-A) and an employee with themselves.13
• Question: Find all salespeople and customers who live in the same city.
• Answer: This requires an INNER JOIN between the two tables on the city column.
SELECT s.name, c.name, s.city FROM salespeople s INNER JOIN customers c ON s.city = c.city;.12
Section 2.3: Advanced Data Combination
Beyond JOINs, SQL provides other powerful tools for combining and filtering datasets.
Set Operators
Set operators combine the results of two or more SELECT statements.
• UNION vs. UNION ALL: This is a very common interview topic.
• UNION: Combines the result sets and removes duplicate rows. This deduplication
step requires extra processing, making it slower.11
• UNION ALL: Combines the result sets but includes all rows, including duplicates. It is significantly more performant because it does not need to check for duplicates.11
• INTERSECT: Returns only the rows that appear in both result sets.12
• EXCEPT (or MINUS in Oracle): Returns the rows from the first result set that do not appear in the second result set.13
Subqueries (Nested Queries)
A subquery is a SELECT statement nested inside another statement. They allow for complex, multi-step logic where the result of an inner query is used to guide the outer query.
• Placement: Subqueries can be used in the SELECT list, the FROM clause (where they are
often called derived tables), and most commonly, the WHERE clause.11
• Correlated vs. Non-Correlated Subqueries:
• Non-Correlated: The inner query can be run independently of the outer query. Its result is calculated once and then used by the outer query.11
• Correlated: The inner query depends on the outer query for its values. It is evaluated once for each row processed by the outer query. Correlated subqueries can be less efficient than other methods like JOINs or CTEs.11
Practice Questions
• Question: What is the difference between UNION and UNION ALL? Which is more performant and why?
• Answer: UNION combines results and removes duplicates, while UNION ALL
combines results and keeps duplicates. UNION ALL is more performant because it avoids the computationally expensive sort or hash operation required to identify and remove duplicates.11
• Question: Write a query to fetch employees who earn more than the average salary in the entire company.
• Answer: This is a classic use case for a subquery in the WHERE clause. SELECT
name, salary FROM employees WHERE salary > (SELECT AVG(salary) FROM employees);.13
• Question: Explain the difference between a correlated and a non-correlated subquery.
• Answer: A non-correlated subquery is self-contained and executes once, passing its
result to the outer query. A correlated subquery references columns from the outer query and thus must be re-executed for each row processed by the outer query, which can lead to performance issues.11
Part III: Advanced SQL for Modern Data Analysis
This section delves into the modern SQL features that are now standard expectations in data-centric interviews. Mastery of window functions and Common Table Expressions (CTEs) is what separates top-tier candidates from the rest. These features are not merely individual topics to learn; they form a synergistic toolset. The combination of CTEs and window functions is a modern SQL "power combo" capable of elegantly solving most complex analytical questions.
CTEs provide the framework for readable, modular logic, while window functions provide the engine for sophisticated calculations.
Section 3.1: Unleashing the Power of Window Functions
Window functions perform a calculation across a set of table rows that are related to the current row. Unlike GROUP BY aggregations, they do not collapse the result set; they return a value for each row, preserving the original row's identity.9 This capability is essential for tasks like ranking, calculating running totals, and comparing values between rows. The core of a window function is the OVER() clause, which defines the "window" or set of rows the function operates on. It has two main components:
• PARTITION BY: Divides the rows into groups (partitions). The window function is applied
independently to each partition. This is conceptually similar to GROUP BY but does not collapse the rows.
• ORDER BY: Orders the rows within each partition. This is crucial for functions that depend on sequence, such as RANK() or LAG().
Key Categories of Window Functions
1. Ranking Functions: Used to assign a rank to each row within a partition based on a specified order. The differences in how they handle tied values are a critical interview topic.
• ROW_NUMBER(): Assigns a unique, sequential integer to each row, regardless of
ties.15
• RANK(): Assigns the same rank to tied values but leaves gaps in the subsequent ranks (e.g., 1, 2, 2, 4).14
• DENSE_RANK(): Assigns the same rank to tied values but does not leave gaps (e.g., 1, 2, 2, 3). This is often the most useful ranking function in interviews.12
• NTILE(n): Distributes the rows in an ordered partition into a specified number of ranked groups (buckets).23
2. Navigation/Offset Functions: Used to access data from a different row relative to the current row within the same result set.
• LAG(column, offset, default): Accesses data from a previous row in the partition.9
• LEAD(column, offset, default): Accesses data from a subsequent row in the partition.9
3. Aggregate Window Functions: Apply standard aggregate functions (SUM, AVG, COUNT, MIN, MAX) over a defined window.
• Frame Clause (ROWS BETWEEN...): This clause provides fine-grained control over
the window frame. For example, ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW defines a window that includes all rows from the start of the partition up to the current row, which is essential for calculating running totals.13
The following table provides a quick reference for these essential functions.
Function Syntax Example Purpose Common Interview Use Case
ROW_NUMBER() ROW_NUMBER() OVER (ORDER BY salary DESC)
Assigns a unique sequential integer to each row.
Arbitrarily breaking ties; assigning a unique ID to a result set.
RANK() RANK() OVER (PARTITION BY dept ORDER BY salary DESC)
Ranks rows, with gaps after ties.
General ranking scenarios where gaps are acceptable.
DENSE_RANK() DENSE_RANK() OVER (PARTITION BY dept ORDER BY salary DESC)
Ranks rows, with no gaps after ties.
Finding "Top N per group" (e.g., top 3 salaries in each department).
NTILE(n) NTILE(4) OVER (ORDER BY sales DESC)
Divides rows into n ranked groups (e.g., quartiles).
Segmenting customers into sales quartiles or performance buckets.
LAG() LAG(sales, 1) OVER (PARTITION BY product ORDER BY month)
Accesses a value from a previous row.
Calculating period-over-period growth (e.g., month-over-month sales change).
LEAD() LEAD(event_time, 1) OVER (PARTITION BY user_id ORDER BY event_time)
Accesses a value from a subsequent row.
Calculating the duration until the next event for sessionization.
SUM() OVER() SUM(sales) OVER (PARTITION BY year ORDER BY month ROWS UNBOUNDED PRECEDING)
Calculates a sum over a window.
Calculating a running total or cumulative sum (e.g., year-to-date sales).
AVG() OVER() AVG(price) OVER (ORDER BY date ROWS BETWEEN 6 PRECEDING AND CURRENT ROW)
Calculates an average over a window.
Calculating a moving average (e.g., 7-day rolling average price).
Section 3.2: Simplifying Complexity with Common Table Expressions (CTEs)
A Common Table Expression, defined using the WITH clause, creates a temporary, named result set that can be referenced within a single SELECT, INSERT, UPDATE, or DELETE statement. CTEs are indispensable for breaking down complex queries into logical, readable steps, making them far superior to deeply nested subqueries.1
Recursive CTEs
A powerful feature of CTEs is their ability to be recursive, meaning a CTE can reference itself. This is the standard SQL method for querying hierarchical data, such as organizational charts, bill of materials, or network graphs.13 A recursive CTE has two parts: an anchor member that returns the base result, and a recursive member that references the CTE itself, joined with the anchor. The recursion stops when the recursive member returns no more rows.
Practice Questions
• Question: Rewrite a complex query containing multiple nested subqueries to use CTEs for improved readability.
• Answer: This involves identifying each subquery's purpose, converting it into a
named CTE using the WITH clause, and then joining these CTEs in the final SELECT statement.
• Question: Given an employees table with employee_id and manager_id, write a query to find the entire reporting hierarchy for a specific employee.
• Answer: This is a classic recursive CTE problem. The anchor member selects the
starting employee. The recursive member repeatedly joins the employees table to the CTE to find the next level of direct reports, continuing until all levels of the hierarchy are traversed.13
• Question: Use a CTE to rank customers by total purchase amount and return the top 10.
• Answer: A first CTE would calculate the total purchase amount for each customer
using SUM() and GROUP BY. A second CTE would then use DENSE_RANK() on the result of the first CTE. The final query would select from the second CTE where the rank is less than or equal to 10.13
Section 3.3: Data Transformation Techniques
Beyond selecting and joining data, SQL provides a rich set of functions for transforming data within a query.
• Conditional Logic with CASE: The CASE statement provides if-then-else logic, allowing
for the creation of new columns or conditional aggregations based on specified rules. It is incredibly versatile for tasks like bucketing data, creating flags, or pivoting.6
• Handling NULLs:
• COALESCE(val1, val2,...): Returns the first non-NULL value from a list of expressions.
It is commonly used to substitute a default value for NULLs.12
• NULLIF(expr1, expr2): Returns NULL if the two expressions are equal; otherwise, it returns the first expression. Useful for preventing division-by-zero errors by converting a zero denominator to NULL.25
• Date/Time Manipulation: Nearly every analytical query involves dates. Common functions include EXTRACT() or DATE_PART() to get components like year or month, DATEDIFF() to find the interval between two dates, and various formatting functions.9
• String Manipulation: Functions like CONCAT() (or ||), SUBSTRING(), UPPER(), LOWER(), and TRIM() are essential for cleaning and formatting text data.12
• Pivoting Data: This involves transforming data from a row-based format to a column-based format. While some databases have a native PIVOT function, the universal method involves using an aggregate function with a CASE statement for each desired new column.5
Practice Questions
• Question: Write a query to swap gender values ('M' to 'F' and 'F' to 'M') in a table.
• Answer: UPDATE employees SET gender = CASE WHEN gender = 'M' THEN 'F' ELSE
'M' END;.13
• Question: Calculate the number of days an employee has been with the company.
• Answer: SELECT name, DATEDIFF(CURDATE(), joining_date) AS days_with_company
FROM employees; (Syntax may vary by SQL dialect).13
• Question: Pivot a table of sales data to show total sales for each product category ('Electronics', 'Clothing', 'Home Goods') as separate columns for each year.
• Answer: SELECT year, SUM(CASE WHEN category = 'Electronics' THEN sales ELSE 0
END) AS electronics_sales, SUM(CASE WHEN category = 'Clothing' THEN sales ELSE 0 END) AS clothing_sales, SUM(CASE WHEN category = 'Home Goods' THEN sales ELSE 0 END) AS home_goods_sales FROM sales_table GROUP BY year;
Part IV: Cracking the Code - SQL Interview Patterns and Puzzles
Moving from syntax to strategy is the most critical step in interview preparation. Technical interviews are not just about knowing commands; they are about recognizing a problem's underlying structure and applying the most efficient solution pattern. Candidates who struggle often do so because they treat each problem as unique, attempting to invent a solution from scratch under pressure. Successful candidates, however, have developed a mental library of these patterns, allowing them to quickly classify a problem and deploy a proven technique. The most challenging problems on platforms like LeetCode are frequently combinations of two or more fundamental patterns. For instance, a "Hard" problem might first require sessionizing user activity (a LAG-based pattern) and then finding the top N longest sessions (a DENSE_RANK pattern). This section deconstructs these complex problems into their core building blocks, teaching not just the patterns themselves but also how to combine them. The following matrix provides a high-level map, linking common question types to their primary solution techniques. This framework should guide the approach to any new problem encountered.
Problem Pattern / Question Type
Primary SQL Technique(s)
Key Functions Example LeetCode/DataLem ur Problem
Nth Highest Salary Window Function or Subquery/LIMIT
DENSE_RANK(), LIMIT, OFFSET
LeetCode 176, 177 24
Top N per Group Window Function with Partitioning
DENSE_RANK(), PARTITION BY
LeetCode 185 24
Finding Duplicates Aggregation or Window Function
GROUP BY, HAVING COUNT(*) > 1, ROW_NUMBER()
LeetCode 182, 196 24
Rolling Average/Sum
Aggregate Window Function with Frame
AVG(), SUM(), ROWS BETWEEN
Tweets' Rolling Averages 29
Period-over-Period Navigation Window Function
LAG() Y-on-Y Growth Rate 29
Consecutive Events / Streaks
Window Functions to create groups
ROW_NUMBER(), Date Arithmetic
LeetCode 180 24
Gaps and Islands Window Functions to identify groups
ROW_NUMBER(), LAG()
Human Traffic of Stadium 24
Analyzing Pairs Self-Join JOIN table AS a JOIN table AS b
Analyze Pairs of Things 7
Hierarchy Traversal Recursive Common Table Expression
WITH RECURSIVE Tree Node 24
Sessionization Navigation Window Function
LAG(), LEAD() Calculating User Activity 23
Section 4.1: Foundational Patterns
These patterns represent the most common tasks and are frequently asked in interviews for all data roles.
Top-N / Nth-Highest
This pattern involves finding a specific rank or the top N records, either across an entire table or within distinct groups.
• Technique: The most robust and modern solution is to use window functions like DENSE_RANK() or RANK() partitioned by the grouping column. An outer query or CTE is then used to filter on the calculated rank. For simpler cases (Nth highest overall), a subquery or LIMIT with OFFSET can also be used.7
• Example Problem: Find the top 3 salaries in each department.
• Logic:
1. Use DENSE_RANK() to assign a salary rank for each employee, restarting the rank for each department.
2. The OVER clause will be (PARTITION BY department_id ORDER BY salary DESC). 3. Use a CTE to store this ranked result. 4. Select from the CTE where the rank is less than or equal to 3.24
Finding Duplicates
This involves identifying rows that share the same values across one or more specified columns.
• Technique 1 (GROUP BY): Group the data by the columns that define a duplicate and use HAVING COUNT(*) > 1 to filter for the groups with more than one entry.9
• Technique 2 (Window Function): Use ROW_NUMBER() OVER (PARTITION BY col1, col2... ORDER BY some_column) to assign a sequence number to each row within a group of duplicates. Any row with a ROW_NUMBER > 1 is a duplicate that can be filtered or deleted.11 This method is particularly useful for deleting duplicates while keeping one instance.
• Example Problem: Find all duplicate emails in the Person table.
• Logic (GROUP BY):
1. GROUP BY Email. 2. Filter the groups with HAVING COUNT(Email) > 1.24
Existence Checks
This pattern addresses questions about finding records in one table that do or do not have a corresponding record in another table.
• Technique 1 (LEFT JOIN): LEFT JOIN from the primary table to the secondary table. Where a match is not found, the columns from the secondary table will be NULL. Filtering WHERE secondary_table.key IS NULL will find all records from the primary table that have no match.13
• Technique 2 (NOT IN): Use a subquery to select all keys from the secondary table and filter the primary table WHERE primary_table.key NOT IN (...). Caution: This method can
produce unexpected empty results if the subquery's result set contains any NULL values.20
• Technique 3 (NOT EXISTS): Use a correlated subquery with NOT EXISTS. This is often more performant and safer with NULLs than NOT IN.9
• Example Problem: Find all customers who have never placed an order.
• Logic (LEFT JOIN):
1. LEFT JOIN the Customers table to the Orders table on customer_id. 2. Filter the results WHERE Orders.order_id IS NULL.24
Section 4.2: Analytical Patterns
These patterns are central to business intelligence and data science, focusing on time-series analysis and trend calculation.
Rolling Metrics (Moving Averages)
This pattern involves calculating an aggregate (like an average or sum) over a moving window of time (e.g., a 7-day rolling average).
• Technique: Use an aggregate window function (AVG(), SUM()) with an ORDER BY clause to define the sequence and a frame clause (ROWS BETWEEN N PRECEDING AND CURRENT ROW) to define the size of the window.6
• Example Problem: Calculate the 7-day rolling average of daily sales.
• Logic:
1. First, ensure you have a table with daily total sales. 2. If not, create one using a CTE with SUM() and GROUP BY date. 3. On this daily sales table, apply the window function AVG(daily_sales) OVER (ORDER
BY date ROWS BETWEEN 6 PRECEDING AND CURRENT ROW).
Cumulative Metrics (Running Totals)
This pattern involves calculating the cumulative sum or count of a metric over time.
• Technique: Use an aggregate window function (SUM(), COUNT()) with an ORDER BY clause and the frame clause ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW.13
• Example Problem: For each player, report the cumulative games played so far by date.
• Logic:
1. Use the window function SUM(games_played) OVER (PARTITION BY player_id ORDER BY event_date).
2. The default frame for ORDER BY is RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW, which achieves the running total.24
Period-over-Period Analysis
This involves comparing a metric from the current period to the same metric from a previous period (e.g., month-over-month, year-over-year).
• Technique: The LAG() window function is the perfect tool. It allows you to pull a value from a previous row into the current row, making direct comparison and calculation possible.23
• Example Problem: Calculate the month-over-month percentage growth in sales.
• Logic:
1. Create a CTE that aggregates sales by month. 2. In a second query on the CTE, use LAG(monthly_sales, 1) OVER (ORDER BY month) to
get the previous month's sales. 3. Calculate the growth using the formula:
(current_sales−previous_sales)/previous_sales.
Section 4.3: Advanced Patterns & Puzzles
These patterns often appear in interviews for senior roles or at companies with complex data challenges. They test a candidate's ability to solve non-obvious problems with creative applications of SQL features.
Consecutive Events / Streaks (Gaps and Islands)
This pattern involves identifying continuous sequences of events (islands) separated by breaks (gaps). A common variant is finding N consecutive events.
• Technique: The core trick is to create a grouping identifier for each "island." This is done
by subtracting a sequence number generated by ROW_NUMBER() from the actual date or ID. For consecutive items, this difference will be constant.
• Example Problem: Find all numbers that appear at least three times consecutively in a Logs table.
• Logic: 1. A simple solution uses self-joins: SELECT l1.Num FROM Logs l1 JOIN Logs l2 ON l1.Id
= l2.Id - 1 AND l1.Num = l2.Num JOIN Logs l3 ON l1.Id = l3.Id - 2 AND l1.Num = l3.Num.24
2. A more robust "Gaps and Islands" solution: ■ Use a CTE to calculate ROW_NUMBER() OVER (ORDER BY id) and
ROW_NUMBER() OVER (PARTITION BY Num ORDER BY id). ■ The difference between these two row numbers will be constant for any
consecutive block of the same number. ■ Group by this difference and the number itself, and use HAVING COUNT(*) >= 3.
Sessionization
This involves grouping a stream of user events into distinct sessions, typically defined by a period of inactivity.
• Technique: Use the LAG() function to get the timestamp of the previous event for a user. Calculate the time difference between the current event and the previous one. A CASE statement can then create a flag (e.g., set to 1) whenever this difference exceeds the inactivity threshold (e.g., 30 minutes). A cumulative sum of this flag then serves as a unique session_id for each group of events.23
• Example Problem: Group user clicks into sessions, where a new session starts after 30 minutes of inactivity.
• Logic: 1. Use a CTE to calculate LAG(event_timestamp, 1) OVER (PARTITION BY user_id ORDER
BY event_timestamp) to get the previous_event_time. 2. Use a second CTE to calculate the time difference and set a new_session_flag using
a CASE statement.
3. Use a third CTE to create a session_id by calculating SUM(new_session_flag) OVER (PARTITION BY user_id ORDER BY event_timestamp).
4. Finally, group by user_id and session_id to analyze each session.
Section 4.4: A Curated Gauntlet of Practice Problems
This section provides a comprehensive set of practice problems, categorized by pattern and difficulty, to solidify the concepts learned. Each problem includes a detailed breakdown of the logic and the final query.
(Note: Due to the exhaustive nature of a full problem set, a representative selection is provided below. A complete interview preparation would involve working through dozens of such problems from the provided resources.)
Easy Difficulty
• Problem: LeetCode 182: Duplicate Emails 24
• Pattern: Finding Duplicates
• Statement: Write a SQL query to find all duplicate emails in a table named Person.
• Schema: Person (Id INT, Email VARCHAR)
• Logic: This is a direct application of the GROUP BY and HAVING pattern. Group the
rows by the Email column and then use the HAVING clause to filter for groups where the count of rows is greater than 1.
• Query: SQL SELECT Email FROM Person GROUP BY Email HAVING COUNT(Email) > 1;
• Problem: LeetCode 181: Employees Earning More Than Their Managers 24
• Pattern: Analyzing Pairs (Self-Join)
• Statement: Given an Employee table with Id, Name, Salary, and ManagerId, find
employees who earn more than their managers.
• Schema: Employee (Id INT, Name VARCHAR, Salary INT, ManagerId INT)
• Logic: This requires comparing rows within the same table. A SELF JOIN is the ideal pattern. Join the Employee table to itself, aliasing one instance as E (for employee) and the other as M (for manager). The join condition will be E.ManagerId = M.Id. The WHERE clause then filters for rows where E.Salary > M.Salary.
• Query: SQL SELECT E.Name AS Employee FROM Employee E JOIN Employee M ON E.ManagerId = M.Id WHERE E.Salary > M.Salary;
Medium Difficulty
• Problem: LeetCode 176: Second Highest Salary 24
• Pattern: Nth-Highest
• Statement: Write a SQL query to get the second highest salary from the Employee
table. If there is no second highest salary, the query should return null.
• Schema: Employee (Id INT, Salary INT)
• Logic (Subquery): Find the maximum salary in the table. Then, find the maximum
salary that is less than the overall maximum salary. Using MAX() on a filtered set handles cases with ties for the highest salary correctly.
• Query (Subquery): SQL SELECT MAX(Salary) AS SecondHighestSalary FROM Employee WHERE Salary < (SELECT MAX(Salary) FROM Employee);
• Logic (Window Function): Use DENSE_RANK() to rank salaries in descending order. Then select the salary where the rank is 2. This approach is more generalizable for finding the Nth highest salary.
• Query (Window Function): SQL WITH RankedSalaries AS ( SELECT Salary, DENSE_RANK() OVER (ORDER BY Salary DESC) as rnk FROM Employee ) SELECT Salary
FROM RankedSalaries WHERE rnk = 2;
• Problem: LeetCode 184: Department Highest Salary 24
• Pattern: Top-N per Group (where N=1)
• Statement: Find employees who have the highest salary in each department.
• Schema: Employee (Id, Name, Salary, DepartmentId), Department (Id, Name)
• Logic: This requires finding the maximum salary for each department and then
finding the employee(s) who match that salary in that department. A window function is the cleanest approach.
• Query (Window Function): SQL WITH RankedEmployees AS ( SELECT D.Name AS Department, E.Name AS Employee, E.Salary, RANK() OVER (PARTITION BY E.DepartmentId ORDER BY E.Salary DESC) as rnk FROM Employee E JOIN Department D ON E.DepartmentId = D.Id ) SELECT Department, Employee, Salary FROM RankedEmployees WHERE rnk = 1;
Hard Difficulty
• Problem: LeetCode 185: Department Top Three Salaries 24
• Pattern: Top-N per Group
• Statement: Find employees who earn the top three salaries in each department.
• Schema: Employee (Id, Name, Salary, DepartmentId), Department (Id, Name)
• Logic: This is a direct extension of the previous problem. The key is to use
DENSE_RANK() instead of RANK() to handle ties correctly (e.g., if two employees share the 2nd highest salary, the next salary is still the 3rd highest).
• Query: SQL WITH RankedSalaries AS (
SELECT E.*, D.Name as DepartmentName, DENSE_RANK() OVER (PARTITION BY E.DepartmentId ORDER BY E.Salary DESC) as rnk FROM Employee E JOIN Department D ON E.DepartmentId = D.Id ) SELECT DepartmentName AS Department, Name AS Employee, Salary FROM RankedSalaries WHERE rnk <= 3;
• Problem: LeetCode 262: Trips and Users 24
• Pattern: Conditional Aggregation
• Statement: Find the cancellation rate of requests with unbanned users for each day
between '2013-10-01' and '2013-10-03'.
• Schema: Trips (Id, Client_Id, Driver_Id, Status, Request_at), Users (Users_Id, Banned,
Role)
• Logic: This complex problem requires several steps:
1. Filter the Trips table for the specified date range. 2. Filter out trips involving banned clients or banned drivers. This can be done with
a WHERE clause and subqueries on the Users table. 3. Group the results by Request_at date. 4. For each day, calculate the total number of valid requests (COUNT(*)). 5. For each day, calculate the number of canceled requests using conditional
aggregation: SUM(CASE WHEN Status LIKE 'cancelled%' THEN 1 ELSE 0 END). 6. Divide the canceled count by the total count and round to two decimal places.
• Query: SQL SELECT T.Request_at AS Day, ROUND( SUM(CASE WHEN T.Status IN ('cancelled_by_driver', 'cancelled_by_client') THEN 1.0 ELSE 0.0 END) / COUNT(T.Id), 2 ) AS "Cancellation Rate" FROM Trips T JOIN Users C ON T.Client_Id = C.Users_Id JOIN Users D ON T.Driver_Id = D.Users_Id
WHERE C.Banned = 'No' AND D.Banned = 'No' AND T.Request_at BETWEEN '2013-10-01' AND '2013-10-03' GROUP BY T.Request_at;
Part V: Beyond the Query - Performance, Optimization, and Architecture
In interviews for senior roles, writing a correct query is merely the first step. The follow-up questions often transition into performance, scalability, and system architecture. These questions are designed to gauge a candidate's real-world experience and engineering mindset. A junior candidate can write a query that works; a senior candidate understands why it works, how it will perform at scale, and its impact on the broader system. Demonstrating this deeper level of thinking can significantly influence the outcome of an interview and the level of the subsequent offer.8
Section 5.1: Writing Performant SQL
Optimizing a slow-running query is a common task and a frequent interview scenario. A structured approach to diagnosis is key.
Indexes: The Key to Fast Lookups
An index is a database object that provides a fast lookup path to data in a table, much like an index in a book. Without an index, the database must perform a full table scan, reading every row to find the ones that match the query's conditions. With an index, it can directly seek the relevant data pages.12
• Clustered vs. Non-Clustered Index:
• Clustered Index: Determines the physical order of data in a table. Because the data
can only be physically sorted in one way, a table can have only one clustered index. It is highly efficient for range queries.11
• Non-Clustered Index: Has a structure separate from the data rows. It contains the indexed values and pointers to the actual data rows. A table can have multiple non-clustered indexes. They are efficient for specific lookups.11
• Trade-offs: While indexes dramatically speed up SELECT queries, they slow down data modification operations (INSERT, UPDATE, DELETE) because the index itself must also be updated.14
Execution Plans
An execution plan (or query plan) is the sequence of steps the database query optimizer generates to execute a SQL statement. Analyzing the execution plan using a command like EXPLAIN is the primary method for diagnosing performance bottlenecks. It reveals whether indexes are being used effectively, identifies costly operations like full table scans or inefficient join methods, and provides cost estimates for each step of the query.13
Common Optimization Techniques
• SELECT Specific Columns: Avoid using SELECT *. Specifying only the necessary columns reduces the amount of data that needs to be processed and transferred.7
• Filter Early: Apply WHERE clauses as early as possible to reduce the size of the dataset that subsequent operations (like joins and aggregations) have to work with.7
• Use JOINs Efficiently: Ensure that join conditions are on indexed columns. Understand the different join algorithms (e.g., Nested Loop, Hash Join, Merge Join) and how the optimizer chooses them.
• Prefer UNION ALL: If duplicate rows are acceptable or known not to exist, UNION ALL is always more performant than UNION because it skips the deduplication step.11
• Avoid Unnecessary Subqueries: While useful, subqueries (especially correlated ones) can sometimes be rewritten as more efficient JOINs or CTEs.7
Practice Questions
• Question: Your query is running slow. What are the first steps you would take to diagnose the problem?
• Answer: 1) Analyze the query execution plan (EXPLAIN) to identify bottlenecks like
full table scans or expensive joins. 2) Verify that appropriate indexes exist on columns used in WHERE clauses and JOIN conditions. 3) Check table statistics to ensure the query optimizer has accurate information. 4) Evaluate the query logic for potential improvements, such as rewriting subqueries or simplifying complex conditions.8
• Question: Explain the difference between a clustered and a non-clustered index.
• Answer: A clustered index dictates the physical storage order of rows in a table;
there can be only one. A non-clustered index is a separate structure with pointers to the data rows; a table can have many. Clustered indexes are generally faster for range scans, while non-clustered indexes are better for point lookups.11
Section 5.2: Database Architecture and System Design Concepts
For senior roles, questions may touch upon higher-level database architecture and design principles.
• OLTP vs. OLAP:
• Online Transaction Processing (OLTP): These systems are designed to handle a
large number of short, atomic transactions (e.g., e-commerce order entry, banking transactions). They are optimized for fast writes and updates, with highly normalized schemas to ensure data integrity.11
• Online Analytical Processing (OLAP): These systems are designed for complex queries and analysis on large volumes of data (e.g., data warehouses). They are optimized for fast reads, often using denormalized schemas (like star or snowflake schemas) to minimize joins and speed up aggregations.11
• ACID Properties: These are a set of four properties that guarantee the reliability of database transactions.
• Atomicity: Ensures that a transaction is an "all or nothing" operation. Either all of its
operations complete successfully, or none of them do.11
• Consistency: Ensures that a transaction brings the database from one valid state to another, upholding all integrity constraints.11
• Isolation: Ensures that concurrent transactions do not interfere with each other, producing the same result as if they were executed sequentially.11
• Durability: Guarantees that once a transaction has been committed, it will remain so,
even in the event of a system failure.11
• Database Partitioning: The process of dividing a very large table into smaller, more manageable pieces (partitions) while still treating it as a single table logically. This can dramatically improve query performance and manageability.
• Horizontal Partitioning: Divides a table by rows (e.g., partitioning sales data by
month or year).13
• Vertical Partitioning: Divides a table by columns (e.g., separating frequently accessed columns from rarely accessed large text columns).13
Practice Questions
• Question: Explain the ACID properties.
• Answer: ACID stands for Atomicity (all or nothing), Consistency (database remains in
a valid state), Isolation (concurrent transactions don't interfere), and Durability (committed changes are permanent). Together, they ensure transactions are processed reliably.11
• Question: What is the difference between OLTP and OLAP databases?
• Answer: OLTP systems are for managing day-to-day transactions, optimized for
writes, and highly normalized (e.g., an e-commerce site's live database). OLAP systems are for analysis and business intelligence, optimized for reads, and often denormalized (e.g., a data warehouse for reporting).11
Conclusion and Final Recommendations
This guide has synthesized a vast array of resources into a structured, comprehensive curriculum for SQL interview preparation. The journey from foundational syntax to advanced analytical patterns and architectural considerations reflects the evolving demands of the modern data industry. Success in a SQL interview is not merely a function of memorizing answers but of developing a deep, intuitive understanding of how to model problems and apply the most effective tools to solve them. The key takeaways are twofold:
1. Master the Fundamentals, but Differentiate with the Advanced: A flawless command of basic JOINs, aggregations, and filtering is the price of entry. The ability to fluently
deploy window functions and CTEs to solve complex analytical puzzles is what distinguishes a top-tier candidate. A significant portion of preparation time should be dedicated to mastering the concepts in Part III and recognizing the patterns in Part IV.
2. Think in Patterns, Not Just Problems: The most effective preparation strategy is to move beyond solving individual, disconnected problems and instead focus on identifying and mastering the underlying patterns. Recognizing a question as a "Top-N-per-Group" problem or a "Gaps-and-Islands" puzzle immediately narrows the solution space and provides a clear path forward, which is a critical advantage under the pressure of an interview.
Actionable Recommendations for the Candidate:
• Structured Practice: Work through the practice questions in this guide sequentially, ensuring a solid grasp of one section before moving to the next. Supplement this with active problem-solving on platforms like LeetCode and DataLemur, but always with the goal of categorizing each new problem into one of the patterns discussed.
• Verbalize Your Logic: For every practice problem, articulate the step-by-step logic before writing the code. Explain the chosen pattern, the function of each clause, and any assumptions made. This practice is invaluable for the live interview setting, where communicating the thought process is as important as the final query.
• Focus on Performance: After solving a problem correctly, always ask the follow-up question: "How can I make this more efficient?" Consider indexing strategies, alternative query structures (e.g., JOIN vs. EXISTS), and the potential impact of large data volumes. Being prepared to discuss optimization demonstrates seniority and real-world experience.
By adopting this structured, pattern-oriented approach, the candidate will be well-equipped to not only answer the questions asked but also to demonstrate the analytical rigor and deep technical understanding that top companies seek in their data professionals.

---


---


---


---


---


---


---


---

