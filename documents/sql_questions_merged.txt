WITH CTE AS 
(
    SELECT user_id, 
           SUM(
                CASE WHEN DATE_PART('MONTH', event_date) = 6 AND DATE_PART('YEAR', event_date) = 2022 THEN 1
                ELSE 0 END
              ) AS jun,
           SUM(
                CASE WHEN DATE_PART('MONTH', event_date) = 7 AND DATE_PART('YEAR', event_date) = 2022 THEN 1
                ELSE 0 END
              ) AS jul
              
    FROM user_actions
    GROUP BY user_id
)

SELECT '7' AS month,
       COUNT(user_id) AS monthly_active_users
FROM CTE
WHERE jun > 0 AND jul > 0;
SELECT advertiser_id, ROUND((SUM(revenue) / SUM(spend))::decimal, 2) AS ROAS
FROM ad_campaigns
GROUP BY advertiser_id
ORDER BY advertiser_id;
SELECT  COALESCE(advertiser.user_id, daily_pay.user_id) AS user_id,
        CASE 
            WHEN paid IS NULL THEN 'CHURN'
            WHEN (paid IS NOT NULL AND status = 'CHURN') THEN 'RESURRECT'
            WHEN (paid IS NOT NULL AND status IN ('EXISTING', 'NEW', 'RESURRECT')) THEN 'EXISTING'
            WHEN (paid IS NOT NULL AND status IS NULL) THEN 'NEW'
        END AS new_status
FROM advertiser
FULL OUTER JOIN daily_pay
USING(user_id)
ORDER BY user_id;
SELECT app_id, 
ROUND(100.0 * SUM(CASE WHEN event_type = 'click' THEN 1 ELSE 0 END) / SUM(CASE WHEN event_type = 'impression' THEN 1 ELSE 0 END), 2) AS ctr
FROM events
WHERE DATE_PART('YEAR', timestamp) = 2022
GROUP BY app_id;
SELECT merchant_id, 
SUM(CASE WHEN LOWER(payment_method) = 'apple pay' THEN transaction_amount ELSE 0 END) total_transaction
FROM transactions
GROUP BY merchant_id
ORDER BY total_transaction DESC;
SELECT ROUND(SUM(yearly_seat_cost * num_seats) / COUNT(*)::DECIMAL, 2)
FROM contracts;
SELECT user_id, DATE_PART('DAY', MAX(post_date) - MIN(post_date)) AS days_between
FROM posts
WHERE DATE_PART('YEAR', post_date) = 2021
GROUP BY user_id
HAVING COUNT(user_id) > 1;
SELECT DATE_PART('MONTH', submit_date) mth, product_id, ROUND(AVG(stars), 2)
FROM reviews
GROUP BY DATE_PART('MONTH', submit_date), product_id
ORDER BY DATE_PART('MONTH', submit_date), product_id;
WITH Oredered_Product_Selling AS
(
  SELECT category_name,
         product_name,
         DENSE_RANK() 
         OVER(PARTITION BY category_name ORDER BY sales_quantity DESC, rating DESC)AS Rank
  FROM products
  INNER JOIN product_sales
  USING(product_id)
)


SELECT category_name,
       product_name
FROM Oredered_Product_Selling
WHERE Rank = 1
ORDER BY category_name, product_name;


SELECT card_name,
       issued_amount
FROM
(
    SELECT card_name,
          issued_amount,
          DENSE_RANK() OVER(PARTITION BY card_name ORDER BY issue_year, issue_month) AS rank
    FROM monthly_cards_issued
) AS TP
WHERE rank = 1
ORDER BY issued_amount DESC;
SELECT card_name, MAX(issued_amount) - MIN(issued_amount) AS difference
FROM monthly_cards_issued
GROUP BY card_name
ORDER BY difference DESC;
SELECT u.city, COUNT(t.user_id) AS total_orders 
FROM trades t
INNER JOIN users u ON t.user_id = u.user_id
WHERE t.status = 'Completed'
GROUP BY u.city
ORDER BY total_orders DESC
LIMIT 3;


WITH CT AS
(
  SELECT title, AVG(salary) av
  FROM employee_pay
  GROUP BY title
)

SELECT DISTINCT ep.employee_id, salary,
      CASE WHEN salary >= 2 * CT.av THEN 'Overpaid' ELSE'Underpaid' END AS status
      
FROM employee_pay ep, CT
WHERE salary >= (2 * CT.av) OR salary * 2 <  CT.av AND ep.title = CT.title
ORDER BY employee_id;
SELECT ROUND(CAST(SUM(item_count * order_occurrences) / SUM(order_occurrences) AS NUMERIC), 1) AS mean
FROM items_per_order;
SELECT item_count AS mode
FROM items_per_order
WHERE order_occurrences = 
(
  SELECT MODE() WITHIN GROUP(ORDER BY order_occurrences DESC)
  FROM items_per_order
)
ORDER BY mode;
SELECT candidate_id
FROM candidates
WHERE skill IN('Python', 'Tableau', 'PostgreSQL')
GROUP BY candidate_id
HAVING COUNT(candidate_id) = 3
ORDER BY candidate_id;
SELECT COUNT(DISTINCT t1.company_id) AS co_w_duplicate_jobs
FROM  job_listings t1
JOIN job_listings t2 on t1.company_id = t2.company_id
WHERE t1.title = t2.title AND t1.description = t2.description AND t1.job_id !=t2.job_id
SELECT account_id, 
SUM(CASE WHEN transaction_type = 'Deposit' THEN amount ELSE -amount END) AS final_balance
FROM transactions
GROUP BY account_id;
WITH CT AS
(
SELECT *,
ROW_NUMBER() OVER(PARTITION BY user_id ORDER BY transaction_date) AS RN
FROM user_transactions
)

SELECT COUNT(user_id) AS users
FROM CT
WHERE RN = 1 AND spend >= 50;

SELECT user_id, COUNT(product_id) AS product_num
FROM user_transactions
GROUP BY user_id
HAVING SUM(spend) >= 1000
ORDER BY COUNT(product_id) DESC, SUM(spend) DESC 
LIMIT 3;
WITH CT AS
(
   SELECT category, product, SUM(spend) AS total_spend,
   ROW_NUMBER() OVER(PARTITION BY category ORDER BY SUM(spend) DESC) AS RN
   FROM product_spend
   WHERE DATE_PART('year', transaction_date) = 2022
   GROUP BY category, product
)

SELECT category, product, total_spend 
FROM CT
WHERE RN IN(1, 2)

WITH CT AS
(
SELECT user_id, COUNT(tweet_id)  tweet_bucket
FROM tweets
WHERE DATE_PART('YEAR', tweet_date) = 2022
GROUP BY user_id
)

SELECT tweet_bucket, COUNT(user_id) users_num
FROM CT
GROUP BY tweet_bucket;
WITH CTE AS
(
    SELECT user_id, 
           transaction_date,
           product_id,
           DENSE_RANK() OVER(PARTITION BY user_id ORDER BY transaction_date DESC) AS rank
           
    FROM user_transactions
)

SELECT transaction_date,
       user_id,
       COUNT(product_id) AS purchase_count
FROM CTE
WHERE rank = 1
GROUP BY transaction_date, user_id
ORDER BY transaction_date
# A/B Testing Examples for Upstart FinTech Interview Preparation

## Example 1: Determining Sample Size for a Loan Application Page Test

**Scenario:** Upstart wants to test two versions of their personal loan application page to see which leads to higher completed applications.

**Current metrics:**
- Current completion rate: 12%
- Minimum important effect to detect: 1.5 percentage point increase (to 13.5%)
- Desired confidence level: 95% (α = 0.05)
- Desired statistical power: 80%

**Problem:** Calculate the required sample size per variant.

**Solution:**
For a two-sided test comparing proportions:

```
n = 2 × (Z₁₋ₐ/₂ + Z₁₋ᵦ)² × p × (1-p) / (p₁ - p₂)²

Where:
- Z₁₋ₐ/₂ = 1.96 (for 95% confidence)
- Z₁₋ᵦ = 0.84 (for 80% power)
- p = (p₁ + p₂)/2 = (0.12 + 0.135)/2 = 0.1275
- (p₁ - p₂)² = (0.135 - 0.12)² = 0.015²
```

Plugging in the values:
```
n = 2 × (1.96 + 0.84)² × 0.1275 × (1-0.1275) / (0.015)²
n = 2 × (2.8)² × 0.1275 × 0.8725 / 0.000225
n = 2 × 7.84 × 0.1275 × 0.8725 / 0.000225
n = 2 × 7.84 × 0.11124375 / 0.000225
n = 2 × 7.84 × 494.4172222
n = 2 × 3876.231
n = 7,752.46
```

Therefore, Upstart would need approximately 7,753 visitors per variant, or a total of 15,506 visitors for the entire test.

## Example 2: Determining Test Duration

**Scenario:** Upstart is testing a new underwriting model against the current model to see if it improves loan approval rates while maintaining acceptable default rates.

**Given information:**
- Average daily loan applications: 1,200
- Required sample size (from prior calculation): 24,000 applications per variant
- Total required sample: 48,000 applications

**Problem:** How long should Upstart run this test?

**Solution:**
```
Test duration = Total required sample / Daily applications
Test duration = 48,000 / 1,200
Test duration = 40 days
```

However, this simple calculation doesn't account for:

1. **Seasonal effects**: Loan application volumes fluctuate throughout the month (higher near paydays) and year (holiday seasons). A 40-day test spanning different parts of months might introduce bias.

2. **External factors**: Major economic news, marketing campaigns, or competitor actions can affect application volume.

**Adjusted recommendation:**
Run the test for a full 6 weeks (42 days) to ensure:
- Complete weekly cycles are captured
- Adequate sample size is achieved
- Beginning and end dates fall on the same day of the week
- Test encompasses a full monthly cycle of applicant behavior

## Example 3: Dealing with Traffic Imbalance Issues

**Scenario:** Upstart is running an A/B test on their landing page for personal loans, but they notice significant traffic differences between weekdays and weekends.

**Problem:** How should Upstart account for this traffic imbalance to ensure valid test results?

**Solution:**
1. **Stratified Sampling**: Ensure each variant receives the same proportion of weekday and weekend traffic.

2. **Time-based Analysis**: Analyze test results separately for different time periods:
```
# Pseudocode for time-based segmentation
for each variant (A and B):
    weekday_conversion_rate = weekday_conversions / weekday_visitors
    weekend_conversion_rate = weekend_conversions / weekend_visitors
    
    # Weighted average based on typical traffic distribution
    overall_rate = (weekday_conversion_rate * 0.75) + (weekend_conversion_rate * 0.25)
```

3. **CUPED Implementation**: Use pre-experiment data to reduce variance:
```
# Simplified CUPED implementation
adjusted_metric = current_conversion_rate - θ * (pre_experiment_metric - average_pre_experiment_metric)

Where θ is the correlation coefficient between pre-experiment and experiment metrics
```

## Example 4: Handling Common A/B Testing Issues in FinTech

**Scenario:** Upstart is testing two different interest rate display methods on their loan offer page. They've encountered several issues during the test.

**Problem 1: Sample Ratio Mismatch**

The test shows 52% of users in variant A and 48% in variant B, despite a planned 50/50 split.

**Solution:**
1. Calculate the Sample Ratio Mismatch (SRM) p-value:
```
# Assuming 10,000 total visitors
# Expected: 5,000 in each variant
# Actual: 5,200 in A, 4,800 in B

# Chi-square test
chi_sq = ((5200-5000)² / 5000) + ((4800-5000)² / 5000)
chi_sq = (40000 / 5000) + (40000 / 5000)
chi_sq = 8 + 8
chi_sq = 16

# With 1 degree of freedom, this gives p < 0.001
```

2. Investigate tracking issues or technical problems causing the imbalance
3. If systemic, restart the test after fixing the issue

**Problem 2: Different Conversion Paths**

Upstart notices some users take multiple sessions to complete a loan application, complicating attribution.

**Solution:**
1. Implement cookie-based tracking to maintain variant assignment across sessions
2. Use a "time to event" analysis (survival analysis) instead of simple conversion rates:
```
# Kaplan-Meier Estimator for survival analysis
S(t) = ∏(nᵢ - dᵢ) / nᵢ

Where:
- S(t) is the survival function
- nᵢ is the number at risk at time i
- dᵢ is the number of events at time i
```

**Problem 3: Simpson's Paradox**

Overall, variant B shows better approval rates, but when segmented by credit score bands, variant A performs better in each segment.

**Solution:**
1. Perform stratified analysis across important segments:
```
# Example credit score bands
segments = ['300-579', '580-669', '670-739', '740-799', '800-850']

for segment in segments:
    variant_A_rate = A_approvals_in_segment / A_applications_in_segment
    variant_B_rate = B_approvals_in_segment / B_applications_in_segment
    print(f"Segment {segment}: A = {variant_A_rate}, B = {variant_B_rate}")
```

2. Investigate if variant assignment correlates with user characteristics
3. Use segmented results for decision-making if a Simpson's Paradox is confirmed

## Example 5: Calculating Expected Value of an A/B Test

**Scenario:** Upstart is testing a new risk model that might increase loan approval rates.

**Given information:**
- Current approval rate: 25%
- Variant B approval rate: 27% (+2 percentage points)
- Average loan amount: $12,000
- Average profit per loan: $800
- Average daily applications: 1,200

**Problem:** Calculate the expected annual value of implementing variant B.

**Solution:**
```
Additional approvals = Total applications × Approval rate increase
Additional approvals = 1,200 × 365 × 0.02
Additional approvals = 8,760 loans per year

Additional profit = Additional approvals × Profit per loan
Additional profit = 8,760 × $800
Additional profit = $7,008,000 per year
```

However, we must consider the increased risk:
```
# If the new model increases default rate by 0.3 percentage points
Additional defaults = Additional approvals × Default rate increase
Additional defaults = 8,760 × 0.003
Additional defaults = 26.28 additional defaults

Cost of defaults = Additional defaults × Average loan amount
Cost of defaults = 26.28 × $12,000
Cost of defaults = $315,360

Net annual value = Additional profit - Cost of defaults
Net annual value = $7,008,000 - $315,360
Net annual value = $6,692,640
```

The expected value of implementing variant B is approximately $6.7 million annually, assuming the test results scale proportionally to the full user base.

## Conclusion

These examples cover fundamental A/B testing concepts specifically related to Upstart's personal loan business. Understanding these principles will help you demonstrate not just statistical knowledge, but how to apply that knowledge to make business decisions in a FinTech context. Remember to emphasize both the statistical rigor and business implications in your interview answers.

# More A/B Testing Scenarios for Upstart

Let me walk you through additional A/B testing examples that would be relevant for Upstart's personal loan business. I'll include detailed scenarios with specific numbers to demonstrate how these tests might be analyzed and interpreted.

## Example 1: Testing Loan Term Presentation

**Scenario:** Upstart wants to test whether showing monthly payment amounts more prominently than total interest impacts loan term selection.

**Test Setup:**
- Control (A): Standard display with equal prominence to loan term, APR, and monthly payment
- Variant (B): Monthly payment displayed in larger font, with a comparison table showing payment differences between terms

**Current metrics:**
- 60% of approved customers choose 3-year terms (36 months)
- 40% of approved customers choose 5-year terms (60 months)
- Average loan amount: $15,000
- Average APR: 12% for 3-year terms, 15% for 5-year terms

**Test Results (after 4 weeks):**
- Control (A): 8,500 customers
  - 5,100 chose 3-year terms (60%)
  - 3,400 chose 5-year terms (40%)
- Variant (B): 8,450 customers
  - 4,395 chose 3-year terms (52%)
  - 4,055 chose 5-year terms (48%)

**Analysis:**
```
# Testing if the difference in proportions is significant
p₁ = 0.60 (proportion choosing 3-year in Control)
p₂ = 0.52 (proportion choosing 3-year in Variant)
p = (5100 + 4395) / (8500 + 8450) = 9495/16950 = 0.56
n₁ = 8500
n₂ = 8450

z = (p₁ - p₂) / √[p(1-p)(1/n₁ + 1/n₂)]
z = (0.60 - 0.52) / √[0.56(0.44)(1/8500 + 1/8450)]
z = 0.08 / √[0.2464(0.000118 + 0.000118)]
z = 0.08 / √[0.2464 × 0.000236]
z = 0.08 / √0.0000582
z = 0.08 / 0.00762
z = 10.5

# z = 10.5 corresponds to p < 0.0001
```

**Business Impact Analysis:**
```
# Current revenue calculation
A_revenue = (5100 × $15,000 × 0.12 × 3) + (3400 × $15,000 × 0.15 × 5)
A_revenue = $27,540,000 + $38,250,000 = $65,790,000

# Variant revenue calculation
B_revenue = (4395 × $15,000 × 0.12 × 3) + (4055 × $15,000 × 0.15 × 5)
B_revenue = $23,733,000 + $45,618,750 = $69,351,750

# Difference
Increased revenue = $69,351,750 - $65,790,000 = $3,561,750
```

**Recommendation:** Implement variant B as it leads to an 8% shift toward 5-year terms, potentially increasing overall revenue by approximately $3.5 million while giving customers lower monthly payments.

## Example 2: Testing Risk-Based Pricing Algorithm

**Scenario:** Upstart is testing a new AI-driven pricing algorithm against their current model.

**Test Setup:**
- Control (A): Current underwriting model
- Variant (B): New AI model incorporating additional behavioral features

**Metrics to track:**
- Approval rate
- Average APR
- Take-rate (% of approved applicants who accept the loan)
- Expected default rate (based on historical data)

**Test Results (after 5,000 applicants per variant):**

| Metric | Control (A) | Variant (B) | Difference |
|--------|-------------|-------------|------------|
| Approval Rate | 32.0% | 35.5% | +3.5% |
| Average APR | 14.2% | 13.8% | -0.4% |
| Take-rate | 70.5% | 74.8% | +4.3% |
| Est. Default Rate | 5.4% | 5.5% | +0.1% |

**Statistical Analysis:**
```
# Approval rate comparison
p₁ = 0.32 (Control)
p₂ = 0.355 (Variant)
p = (1600 + 1775) / 10000 = 0.3375
n₁ = n₂ = 5000

z = (0.355 - 0.32) / √[0.3375(0.6625)(1/5000 + 1/5000)]
z = 0.035 / √[0.2236(0.0004)]
z = 0.035 / √0.0000894
z = 0.035 / 0.00946
z = 3.70 (p < 0.001)

# Similar calculations show statistical significance for take-rate difference
```

**Expected Value Calculation:**
```
# For Control (A):
Applications = 100,000
Approvals = 100,000 × 0.32 = 32,000
Accepted loans = 32,000 × 0.705 = 22,560
Average loan amount = $14,000
Total loan volume = 22,560 × $14,000 = $315,840,000
Revenue (APR) = $315,840,000 × 0.142 = $44,849,280
Defaults = 22,560 × 0.054 = 1,218 loans
Default cost = 1,218 × $14,000 = $17,052,000
Net revenue = $44,849,280 - $17,052,000 = $27,797,280

# For Variant (B):
Applications = 100,000
Approvals = 100,000 × 0.355 = 35,500
Accepted loans = 35,500 × 0.748 = 26,554
Total loan volume = 26,554 × $14,000 = $371,756,000
Revenue (APR) = $371,756,000 × 0.138 = $51,302,328
Defaults = 26,554 × 0.055 = 1,460 loans
Default cost = 1,460 × $14,000 = $20,440,000
Net revenue = $51,302,328 - $20,440,000 = $30,862,328

# Difference in net revenue
Improvement = $30,862,328 - $27,797,280 = $3,065,048
```

**Recommendation:** The new AI model shows a statistically significant improvement in approval rate and take-rate, with only a slight increase in default rate. The expected annual net revenue improvement is over $3 million, justifying implementation of the new model.

## Example 3: Testing Income Verification Methods

**Scenario:** Upstart is testing a simplified income verification process against their traditional method.

**Test Setup:**
- Control (A): Traditional method requiring paystubs or W-2 forms
- Variant (B): Bank transaction analysis with optional document upload

**Current metrics:**
- Income verification completion rate: 78%
- Average verification time: 36 hours
- Average application-to-funding time: 5.2 days
- Verification error rate (discrepancy >10% from stated income): 8.5%

**Test Results (4,000 applications per variant):**

| Metric | Control (A) | Variant (B) | Difference |
|--------|-------------|-------------|------------|
| Completion Rate | 77.8% | 89.5% | +11.7% |
| Avg. Verification Time | 35.2 hrs | 5.8 hrs | -29.4 hrs |
| App-to-Funding Time | 5.1 days | 3.8 days | -1.3 days |
| Verification Error Rate | 8.3% | 9.1% | +0.8% |

**Statistical Analysis for Completion Rate:**
```
p₁ = 0.778 (Control)
p₂ = 0.895 (Variant)
p = (3112 + 3580) / 8000 = 0.8365
n₁ = n₂ = 4000

z = (0.895 - 0.778) / √[0.8365(0.1635)(1/4000 + 1/4000)]
z = 0.117 / √[0.1368(0.0005)]
z = 0.117 / √0.0000684
z = 0.117 / 0.00827
z = 14.15 (p < 0.0001)
```

**Impact Analysis:**
```
# Assuming 200,000 annual applications
Additional completed verifications = 200,000 × 0.117 = 23,400
Average loan amount = $16,000
Conversion rate (verified to funded) = 60%
Additional funded loans = 23,400 × 0.6 = 14,040
Additional loan volume = 14,040 × $16,000 = $224,640,000
Average profit margin = 6%
Additional profit = $224,640,000 × 0.06 = $13,478,400

# Operational savings
Time saved per verification = 29.4 hours
Cost per hour of verification = $25
Savings per verification = 29.4 × $25 = $735
Total verifications with new system = 200,000 × 0.895 = 179,000
Total operational savings = 179,000 × $735 = $13,156,500

# Potential cost of increased verification errors
Additional errors = 200,000 × 0.895 × (0.091 - 0.083) = 1,432
Average cost per error = $500
Total error cost = 1,432 × $500 = $716,000

# Net benefit
Total benefit = $13,478,400 (additional profit) + $13,156,500 (operational savings) - $716,000 (error cost) = $25,918,900
```

**Recommendation:** Implement the new verification system as it significantly reduces friction in the application process, leading to higher completion rates and faster funding times. The estimated annual benefit of over $25 million far outweighs the slight increase in verification errors.

## Example 4: Testing Application Form Length

**Scenario:** Upstart is testing a shortened application form against their standard form.

**Test Setup:**
- Control (A): Standard 18-field application form
- Variant (B): Streamlined 12-field form, with some fields moved to post-approval

**Current metrics:**
- Application start-to-submission rate: 62%
- Average time to complete: 8.5 minutes
- Application bounce rate: 28%

**Test Results (10,000 visitors per variant):**

| Metric | Control (A) | Variant (B) | Difference |
|--------|-------------|-------------|------------|
| Start-to-submission | 61.5% | 73.2% | +11.7% |
| Avg. Completion Time | 8.3 mins | 5.6 mins | -2.7 mins |
| Bounce Rate | 29.1% | 21.8% | -7.3% |
| Approval Rate | 34.2% | 33.8% | -0.4% |

**Statistical Analysis:**
```
# Start-to-submission comparison
p₁ = 0.615 (Control)
p₂ = 0.732 (Variant)
p = (6150 + 7320) / 20000 = 0.6735
n₁ = n₂ = 10000

z = (0.732 - 0.615) / √[0.6735(0.3265)(1/10000 + 1/10000)]
z = 0.117 / √[0.2199(0.0002)]
z = 0.117 / √0.000044
z = 0.117 / 0.00663
z = 17.65 (p < 0.0001)
```

**Funnel Analysis:**
```
# For Control (A):
Visitors: 10,000
Started applications: 7,090 (100% - 29.1% bounce)
Completed applications: 6,150 (61.5% of total)
Approved loans: 2,103 (34.2% of completed)

# For Variant (B):
Visitors: 10,000
Started applications: 7,820 (100% - 21.8% bounce)
Completed applications: 7,320 (73.2% of total)
Approved loans: 2,474 (33.8% of completed)

# Conversion improvement
Increase in approved loans = 2,474 - 2,103 = 371 (+17.6%)
```

**ROI Calculation:**
```
# Assuming 500,000 annual visitors
Additional approved loans = 500,000 × (2,474 - 2,103) / 10,000 = 18,550
Average loan amount = $18,000
Average profit per loan = $1,080 (6% of loan amount)
Additional annual profit = 18,550 × $1,080 = $20,034,000

# Implementation cost
Development and testing: $150,000
Risk analysis: $50,000
Total cost: $200,000

# ROI
First year ROI = ($20,034,000 - $200,000) / $200,000 = 99.17x
```

**Recommendation:** Implement the streamlined application form immediately. The shorter form substantially increases completion rates with only a minimal decrease in approval rate, leading to a significant net increase in approved loans. The projected ROI is extremely high, and the improved user experience aligns with Upstart's goal of making credit more accessible.

## Example 5: Testing Credit Score Display and Education

**Scenario:** Upstart is testing whether showing applicants their credit score with educational content affects loan performance.

**Test Setup:**
- Control (A): Standard process with no credit score disclosure
- Variant (B): Show applicants their credit score with personalized tips

**Test Duration:** 6 months with 3-month performance tracking

**Test Results (7,500 loans per variant):**

| Metric | Control (A) | Variant (B) | Difference |
|--------|-------------|-------------|------------|
| 30-day Delinquency | 2.8% | 2.4% | -0.4% |
| 90-day Delinquency | 1.5% | 1.2% | -0.3% |
| Customer Support Contacts | 0.8/loan | 0.6/loan | -0.2/loan |
| Repeat Application Rate | 15.2% | 18.7% | +3.5% |

**Statistical Analysis for 30-day Delinquency:**
```
p₁ = 0.028 (Control)
p₂ = 0.024 (Variant)
p = (210 + 180) / 15000 = 0.026
n₁ = n₂ = 7500

z = (0.028 - 0.024) / √[0.026(0.974)(1/7500 + 1/7500)]
z = 0.004 / √[0.02532(0.000267)]
z = 0.004 / √0.00000677
z = 0.004 / 0.0026
z = 1.54 (p = 0.12)
```

This doesn't reach statistical significance at α = 0.05.

**For 90-day Delinquency:**
```
p₁ = 0.015 (Control)
p₂ = 0.012 (Variant)
p = (112.5 + 90) / 15000 = 0.0135
n₁ = n₂ = 7500

z = (0.015 - 0.012) / √[0.0135(0.9865)(1/7500 + 1/7500)]
z = 0.003 / √[0.01332(0.000267)]
z = 0.003 / √0.00000356
z = 0.003 / 0.00189
z = 1.59 (p = 0.11)
```

Also not statistically significant at α = 0.05.

**Impact Analysis:**
```
# Assuming the observed differences would become significant with larger sample
Average loan amount = $20,000
Portfolio size = 100,000 loans
Annual value of 30-day delinquency reduction:
- Reduction in delinquent loans = 100,000 × 0.004 = 400 loans
- Average collections cost per delinquent loan = $120
- Total savings = 400 × $120 = $48,000

Annual value of 90-day delinquency reduction:
- Reduction in seriously delinquent loans = 100,000 × 0.003 = 300 loans
- Average loss given default = $8,000 per loan
- Total savings = 300 × $8,000 = $2,400,000

Annual value of increased repeat applications:
- Additional repeat applications = 100,000 × 0.035 = 3,500
- Conversion rate for repeat applications = 50%
- Average profit per repeat loan = $1,200
- Total additional profit = 3,500 × 0.5 × $1,200 = $2,100,000

Customer support savings:
- Reduction in contacts = 100,000 × 0.2 = 20,000 contacts
- Average cost per contact = $15
- Total savings = 20,000 × $15 = $300,000

Total annual benefit = $48,000 + $2,400,000 + $2,100,000 + $300,000 = $4,848,000
```

**Recommendation:** While the delinquency reductions haven't reached statistical significance yet, the combined benefits of increased repeat business and reduced customer support contacts make this a promising feature. Continue monitoring delinquency metrics while implementing the credit score display and education feature, as the potential upside ($4.8 million annually) far outweighs the implementation costs.

These examples demonstrate various A/B testing scenarios specifically relevant to Upstart's personal loan business. They cover key aspects like user experience, underwriting models, operational efficiency, and customer education—all critical components for a successful FinTech company.

# SQL Query Patterns: A Framework for Data Extraction

I'll walk you through the major SQL query patterns and when to use each one based on the type of question you're trying to answer.

## 1. Basic SELECT Queries

**When to use:** When you need simple filtering and sorting of data from a single table.

```sql
SELECT first_name, last_name, hire_date
FROM employees
WHERE department_id = 10
ORDER BY hire_date DESC;
```

This is your foundation - use when questions are focused on a single entity ("Show me all sales from last month").

## 2. JOIN Operations

**When to use:** When your question involves relationships between multiple entities.

```sql
SELECT c.customer_name, o.order_date, p.product_name, oi.quantity
FROM customers c
JOIN orders o ON c.customer_id = o.customer_id
JOIN order_items oi ON o.order_id = oi.order_id
JOIN products p ON oi.product_id = p.product_id
WHERE o.order_date > '2023-01-01';
```

Use when questions involve relationships: "Which customers purchased which products?" The type of join depends on the inclusivity needed:
- INNER JOIN: Only matching records
- LEFT/RIGHT JOIN: All records from one table, matching from another
- FULL JOIN: All records from both tables

## 3. Aggregation Queries

**When to use:** When you need to summarize data or answer "how many," "what's the average," etc.

```sql
SELECT 
    department_id,
    COUNT(*) AS employee_count,
    AVG(salary) AS avg_salary,
    MIN(hire_date) AS earliest_hire
FROM employees
GROUP BY department_id
HAVING COUNT(*) > 5
ORDER BY avg_salary DESC;
```

Useful for questions about totals, averages, minimums, maximums, or counts within groups.

## 4. Subqueries

**When to use:** When one query depends on the results of another query.

```sql
SELECT employee_id, first_name, last_name
FROM employees
WHERE salary > (
    SELECT AVG(salary) 
    FROM employees
);
```

Good for questions like "Find employees who earn above the average salary" or "Which products have sold more than the company average?"

## 5. Common Table Expressions (CTEs)

**When to use:** When you need to break a complex query into logical steps or when a subquery would be used multiple times.

```sql
WITH monthly_sales AS (
    SELECT 
        DATE_TRUNC('month', order_date) AS month,
        SUM(amount) AS total_sales
    FROM orders
    GROUP BY DATE_TRUNC('month', order_date)
),
avg_monthly AS (
    SELECT AVG(total_sales) AS avg_monthly_sales
    FROM monthly_sales
)
SELECT 
    month,
    total_sales,
    total_sales - avg_monthly_sales AS sales_difference
FROM monthly_sales, avg_monthly
ORDER BY month;
```

Excellent for multi-step analysis: "Compare each month's sales to the overall monthly average." CTEs make queries more readable than nested subqueries.

## 6. Window Functions

**When to use:** When you need to perform calculations across rows while still maintaining row identity.

```sql
SELECT 
    employee_id,
    department_id,
    salary,
    AVG(salary) OVER(PARTITION BY department_id) AS dept_avg,
    salary - AVG(salary) OVER(PARTITION BY department_id) AS diff_from_avg,
    RANK() OVER(PARTITION BY department_id ORDER BY salary DESC) AS salary_rank
FROM employees;
```

Perfect for questions involving:
- Rankings: "What's each product's sales rank within its category?"
- Running totals: "What are the cumulative sales by day?"
- Comparisons: "How does each employee's salary compare to their department average?"

## 7. Set Operations

**When to use:** When you need to combine or compare results from multiple queries.

```sql
-- Customers who purchased Product A but not Product B
SELECT customer_id
FROM orders
JOIN order_items ON orders.order_id = order_items.order_id
WHERE product_id = 'A'
EXCEPT
SELECT customer_id
FROM orders
JOIN order_items ON orders.order_id = order_items.order_id
WHERE product_id = 'B';
```

Useful for:
- UNION: Combining results (with UNION ALL for duplicates)
- INTERSECT: Finding common results
- EXCEPT: Finding differences between results

## 8. Conditional Logic

**When to use:** When you need to apply different logic based on conditions.

```sql
SELECT 
    order_id,
    amount,
    CASE 
        WHEN amount < 100 THEN 'Small'
        WHEN amount BETWEEN 100 AND 1000 THEN 'Medium'
        ELSE 'Large'
    END AS order_size
FROM orders;
```

Great for:
- Categorization: "Label orders as small, medium, or large"
- Conditional aggregation: "Count orders by size category"

## 9. Recursive CTEs

**When to use:** When dealing with hierarchical data or graph relationships.

```sql
WITH RECURSIVE org_hierarchy AS (
    -- Base case: CEO (no manager)
    SELECT employee_id, name, manager_id, 1 AS level
    FROM employees
    WHERE manager_id IS NULL
    
    UNION ALL
    
    -- Recursive case: get all direct reports
    SELECT e.employee_id, e.name, e.manager_id, h.level + 1
    FROM employees e
    JOIN org_hierarchy h ON e.manager_id = h.employee_id
)
SELECT * FROM org_hierarchy ORDER BY level, employee_id;
```

Essential for:
- Organizational hierarchies: "Show me all reports up to 3 levels deep"
- Bill of materials: "List all components needed for a product"
- Path finding: "Find all possible routes between cities"

## Decision Framework

When approaching a data question, consider this sequence:

1. **Identify the entities involved**
   - Single entity → Basic SELECT
   - Multiple related entities → JOINs

2. **Determine the calculation type**
   - Summarization across groups → GROUP BY aggregation
   - Row-by-row calculations with access to other rows → Window functions
   - Conditional results → CASE statements

3. **Consider the query complexity**
   - Query needs multiple logical steps → CTEs
   - Query requires a result from another query → Subquery
   - Hierarchical relationships → Recursive CTE
   - Set-based operations → UNION/INTERSECT/EXCEPT

4. **Optimize for readability**
   - Repeated subqueries → CTEs
   - Deeply nested logic → Break into multiple CTEs

## Practical Examples by Question Type

### Question: "What is the sales trend by month?"
```sql
SELECT 
    DATE_TRUNC('month', order_date) AS month,
    SUM(amount) AS monthly_sales
FROM orders
GROUP BY DATE_TRUNC('month', order_date)
ORDER BY month;
```

### Question: "Which customers increased their spending each month?"
```sql
WITH monthly_spend AS (
    SELECT 
        customer_id,
        DATE_TRUNC('month', order_date) AS month,
        SUM(amount) AS total_spend
    FROM orders
    GROUP BY customer_id, DATE_TRUNC('month', order_date)
)
SELECT 
    customer_id,
    month,
    total_spend,
    LAG(total_spend) OVER(PARTITION BY customer_id ORDER BY month) AS prev_month_spend,
    CASE 
        WHEN total_spend > LAG(total_spend) OVER(PARTITION BY customer_id ORDER BY month) THEN 'Increased'
        ELSE 'Decreased or Same'
    END AS spend_trend
FROM monthly_spend
ORDER BY customer_id, month;
```

### Question: "What's the market share of each product category by quarter?"
```sql
WITH quarterly_sales AS (
    SELECT 
        DATE_TRUNC('quarter', o.order_date) AS quarter,
        p.category,
        SUM(oi.quantity * oi.unit_price) AS category_sales
    FROM orders o
    JOIN order_items oi ON o.order_id = oi.order_id
    JOIN products p ON oi.product_id = p.product_id
    GROUP BY DATE_TRUNC('quarter', o.order_date), p.category
),
total_quarterly AS (
    SELECT 
        quarter,
        SUM(category_sales) AS total_sales
    FROM quarterly_sales
    GROUP BY quarter
)
SELECT 
    qs.quarter,
    qs.category,
    qs.category_sales,
    tq.total_sales,
    ROUND((qs.category_sales / tq.total_sales) * 100, 2) AS market_share
FROM quarterly_sales qs
JOIN total_quarterly tq ON qs.quarter = tq.quarter
ORDER BY qs.quarter, market_share DESC;
```

By understanding these patterns and when to apply them, you'll be able to tackle most data extraction challenges efficiently and effectively.
# The Best Medium-Hard Data Analyst SQL Interview Questions  

By Zachary Thomas ([zthomas.nc@gmail.com](mailto:zthomas.nc@gmail.com), [Twitter](https://twitter.com/zach_i_thomas), [LinkedIn](https://www.linkedin.com/in/thomaszi/)) 

**Tip: **See the Table of Contents (document outline) by hovering over the vertical line on the right side of the page 

## Background & Motivation

> The first 70% of SQL is pretty straightforward but the remaining 30% can be pretty tricky.


Between the fall of 2015 and the summer of 2019 I interviewed for data analyst and data scientists positions four separate times, getting to onsite interviews at over a dozen companies. After an interview in 2017 went poorly — mostly due to me floundering at the more difficult SQL questions they asked me — I started putting together a study guide of medium and hard SQL questions to better prepare and found it particularly useful during my 2019 interview cycle. Over the past year I have shared that guide with a couple of friends, and with the extra time on my hands due to the coronavirus pandemic, I have polished it up into this doc. 

There are plenty of great beginner SQL guides out there. My favorites are Codecademy’s [interactive SQL courses](https://www.codecademy.com/learn/learn-sql) and Zi Chong Kao’s [Select Star SQL](https://selectstarsql.com/). However, like I told a friend, while the first 70% of SQL is pretty straightforward, the remaining 30% can be pretty tricky. Data analyst and data scientist interview questions at technology companies often pull from that 30%.  

Strangely, I have never really found a comprehensive source online for those medium-hard SQL questions, which is why I put together this guide. 

Working through this guide should improve your performance on data analyst interviews. It should also make you better at your current and future job positions. Personally, I find some of the SQL patterns found in this doc useful for ETLs powering reporting tools featuring trends over time. 

To be clear, data analyst and data scientist interviews consist of more than SQL questions. Other common topics include explaining past projects, A/B testing, metric development and open-ended analytical problems. This [Quora answer](https://qr.ae/pNrdGV) has Facebook’s product analyst interview guide circa 2017, which discusses this topic in more depth. That said, if improving your SQL skills can make your interviews less stressful than they already are, it could very well be worth your time. 

In the future, I may transition this doc to a website like [Select Star SQL](https://selectstarsql.com/) with an embedded SQL editor so that readers can write SQL statements to questions and get real-time feedback on their code. Another option could be adding these questions as problems on Leetcode. For the time being though I just wanted to publish this doc so that people could find it useful now.  

**I would love to get your feedback on this doc. Please drop a note if you find this useful, have improvements/corrections, or encounter other good resources for medium/hard difficulty SQL questions. **

## Assumptions & How to use this guide 

**Assumptions about SQL proficiency: **This guide assumes you have a working knowledge of SQL. You probably use it frequently at work already but want to sharpen your skills on topics like self-joins and window functions. 

**How to use this guide:** Since interviews usually utilize a whiteboard or a virtual (non-compiling) notepad, my recommendation is to get out a pencil and paper and write out your solutions to each part of the problem, and once complete compare your answers to the answer key. Or, complete these with a friend who can act as the interviewer!

* Small SQL syntax errors aren’t a big deal during whiteboard/notepad interviews. However, they can distracting to the interviewer, so ideally practice reducing these so your logic shines through in the interview. 
* The answers I provide may not be the only way to successfully solve the question. Feel free to message with additional solutions and I can add them to this guide! 

## Tips on solving difficult SQL interview questions 

This advice mirrors typical code interview advice ... 

1. Listen carefully to problem description, repeat back the crux of the problem to the interviewer
2. Spell out an edge case to demonstrate you actually understand problem (i.e. a row that *wouldn’t* be included in the output of the SQL you are about to sketch out) 
3. (If the problem involves a self-join) For your own benefit sketch out what the self-join will look like — this will typically be at least three columns: a column of interest from the main table, the column to join from the main table, and the column to join from the secondary table 
    1. Or, as you get more used to self-join problems, you can explain this step verbally 
4. Start writing SQL — err towards writing SQL versus trying to perfectly understand the problem. Verbalize your assumptions as you go so your interviewer can correct you if you go astray. 

## Acknowledgments and Additional Resources 

Some of the problems listed here are adapted from old Periscope blog posts (mostly written around 2014 by [Sean Cook](https://www.linkedin.com/in/seangcook/), although his authorship seems to have been removed from the posts following SiSense's [merger with](https://www.sisense.com/blog/sisense-and-periscope-data-merge-2/) Periscope) or discussions from Stack Overflow; I've noted them at the start of questions as appropriate. 

[Select Star SQL](https://selectstarsql.com/) has good[challenge questions](https://selectstarsql.com/questions.html#challenge_questions) that are complementary to the questions in this doc. 

Please note that these questions are not literal copies of SQL interview questions I have encountered while interviewing nor were they interview questions used at a company I have worked at or work at. 
* * *

# Self-Join Practice Problems 

## #1: MoM Percent Change 

**Context:** Oftentimes it's useful to know how much a key metric, such as monthly active users, changes between months. Say we have a table `logins` in the form: 

```
| user_id | date       |
|---------|------------|
| 1       | 2018-07-01 |
| 234     | 2018-07-02 |
| 3       | 2018-07-02 |
| 1       | 2018-07-02 |
| ...     | ...        |
| 234     | 2018-10-04 |
```

**Task**: Find the month-over-month percentage change for monthly active users (MAU). 
* * *
***Solution:***

*(This solution, like other solution code blocks you will see in this doc, contains comments about SQL syntax that may differ between flavors of SQL or other comments about the solutions as listed) *

```
WITH mau AS 
(
  SELECT 
   /* 
    * Typically, interviewers allow you to write psuedocode for date functions 
    * i.e. will NOT be checking if you have memorized date functions. 
    * Just explain what your function does as you whiteboard 
    *
    * DATE_TRUNC() is available in Postgres, but other SQL date functions or 
    * combinations of date functions can give you a identical results   
    * See https://www.postgresql.org/docs/9.0/functions-datetime.html#FUNCTIONS-DATETIME-TRUNC
    */ 
    DATE_TRUNC('month', date) month_timestamp,
    COUNT(DISTINCT user_id) mau
  FROM 
    logins 
  GROUP BY 
    DATE_TRUNC('month', date)
  )
 
 SELECT 
    /*
    * You don't literally need to include the previous month in this SELECT statement. 
    * 
    * However, as mentioned in the "Tips" section of this guide, it can be helpful 
    * to at least sketch out self-joins to avoid getting confused which table 
    * represents the prior month vs current month, etc. 
    */ 
    a.month_timestamp previous_month, 
    a.mau previous_mau, 
    b.month_timestamp current_month, 
    b.mau current_mau, 
    ROUND(100.0*(b.mau - a.mau)/a.mau,2) AS percent_change 
 FROM
    mau a 
 JOIN 
    /*
    * Could also have done `ON b.month_timestamp = a.month_timestamp + interval '1 month'` 
    */
    mau b ON a.month_timestamp = b.month_timestamp - interval '1 month' 
  
```



## #2: Tree Structure Labeling   

**Context:** Say you have a table `tree` with a column of nodes and a column corresponding parent nodes 

```
node   parent
1       2
2       5
3       5
4       3
5       NULL 
```

**Task:** Write SQL such that we label each node as a “leaf”, “inner” or “Root” node, such that for the nodes above we get: 

```
node    label  
1       Leaf
2       Inner
3       Inner
4       Leaf
5       Root
```

(Side note: [this link](http://ceadserv1.nku.edu/longa//classes/mat385_resources/docs/trees.html) has more details on Tree data structure terminology. Not needed to solve the problem though!)
* * *
***Solution:***

**Acknowledgement:** this more generalizable solution was contributed by Fabian Hofmann on 5/2/20. Thank, FH! 

```
WITH join_table AS
(
    SELECT 
        cur.node, 
        cur.parent, 
        COUNT(next.node) AS num_children
    FROM 
        tree cur
    LEFT JOIN 
        tree next ON (next.parent = cur.node)
    GROUP BY 
        cur.node, 
        cur.parent
)

SELECT
    node,
    CASE
        WHEN parent IS NULL THEN "Root"
        WHEN num_children = 0 THEN "Leaf"
        ELSE "Inner"
    END AS label
FROM 
    join_table 
```

An alternate solution, without explicit joins: 

```
SELECT 
    node,
    CASE 
        WHEN parent IS NULL THEN 'Root'
        WHEN node NOT IN (SELECT parent FROM tree) THEN 'Leaf'
        WHEN node IN (SELECT parent FROM tree) AND parent IS NOT NULL THEN 'Inner'
    END AS label 
 from 
    tree
```



## #3: Retained Users Per Month (multi-part)

**Acknowledgement: **this problem is adapted from SiSense’s [“Using Self Joins to Calculate Your Retention, Churn, and Reactivation Metrics”](https://www.sisense.com/blog/use-self-joins-to-calculate-your-retention-churn-and-reactivation-metrics/) blog post

### Part 1: 

**Context:** Say we have login data in the table `logins`: 

```
| user_id | date       |
|---------|------------|
| 1       | 2018-07-01 |
| 234     | 2018-07-02 |
| 3       | 2018-07-02 |
| 1       | 2018-07-02 |
| ...     | ...        |
| 234     | 2018-10-04 |
```

**Task:** Write a query that gets the number of retained users per month. In this case, retention for a given month is defined as the number of users who logged in that month who also logged in the immediately previous month. 
* * *
***Solution:***

```
SELECT 
    DATE_TRUNC('month', a.date) month_timestamp, 
    COUNT(DISTINCT a.user_id) retained_users 
 FROM 
    logins a 
 JOIN 
    logins b ON a.user_id = b.user_id 
        AND DATE_TRUNC('month', a.date) = DATE_TRUNC('month', b.date) + 
                                             interval '1 month'
 GROUP BY 
    date_trunc('month', a.date)
```



### Part 2: 

**Task:** Now we’ll take retention and turn it on its head: Write a query to find many users last month *did not* come back this month. i.e. the number of churned users.  
* * *
***Solution:***

```
SELECT 
    DATE_TRUNC('month', a.date) month_timestamp, 
    COUNT(DISTINCT b.user_id) churned_users 
FROM 
    logins a 
FULL OUTER JOIN 
    logins b ON a.user_id = b.user_id 
        AND DATE_TRUNC('month', a.date) = DATE_TRUNC('month', b.date) + 
                                         interval '1 month'
WHERE 
    a.user_id IS NULL 
GROUP BY 
    DATE_TRUNC('month', a.date)
```

Note that there are solutions to this problem that can use `LEFT` or `RIGHT` joins. 


### Part 3: 

**Note:** this question is probably more complex than the kind you would encounter in an interview. Consider it a challenge problem, or feel free to skip it!

**Context**: Good work on the previous two problems! Data engineering has decided to give you a helping hand by creating a table of churned users per month, `user_churns`. If a user is active last month but then not active this month, then that user gets an entry for this month. `user_churns` has the form: 

```
| user_id | month_date |
|---------|------------|
| 1       | 2018-05-01 |
| 234     | 2018-05-01 |
| 3       | 2018-05-01 |
| 12      | 2018-05-01 |
| ...     | ...        |
| 234     | 2018-10-01 |
```


**Task**: You now want to do a cohort analysis of active users this month *who have been reactivated users in the past*. Create a table that contains these users. You may use the tables `user_churns` as well as `logins` to create this cohort. In Postgres, the current timestamp is available through `current_timestamp`.
* * *
***Solution:***

```
WITH user_login_data AS 
(
    SELECT 
        DATE_TRUNC('month', a.date) month_timestamp,
        a.user_id,
        /* 
        * At least in the flavors of SQL I have used, you don't need to 
        * include the columns used in HAVING in the SELECT statement.
        * I have written them out for clarity here.  
        */ 
        MAX(b.month_date) as most_recent_churn, 
        MAX(DATE_TRUNC('month', c.date)) as most_recent_active 
     FROM 
        logins a
     JOIN 
        user_churns b 
            ON a.user_id = b.user_id AND DATE_TRUNC('month', a.date) > b.month_date 
     JOIN
        logins c 
            ON a.user_id = c.user_id 
            AND 
            DATE_TRUNC('month', a.date) > DATE_TRUNC('month', c.date)
     WHERE 
        DATE_TRUNC('month', a.date) = DATE_TRUNC('month', current_timestamp)
     GROUP BY 
        DATE_TRUNC('month', a.date),
        a.user_id
     HAVING 
        most_recent_churn > most_recent_active
```



## #4: Cumulative Sums 

**Acknowledgement:** This problem was inspired by Sisense’s[“Cash Flow modeling in SQL”](https://www.sisense.com/blog/cash-flow-modeling-in-sql/) blog post 

**Context:** Say we have a table `transactions` in the form:

```
| date       | cash_flow |
|------------|-----------|
| 2018-01-01 | -1000     |
| 2018-01-02 | -100      |
| 2018-01-03 | 50        |
| ...        | ...       |
```

Where `cash_flow` is the revenues minus costs for each day. 

**Task: **Write a query to get *cumulative* cash flow for each day such that we end up with a table in the form below: 

```
| date       | cumulative_cf |
|------------|---------------|
| 2018-01-01 | -1000         |
| 2018-01-02 | -1100         |
| 2018-01-03 | -1050         |
| ...        | ...           |
```

* * *
***Solution:***

```
SELECT 
    a.date date, 
    SUM(b.cash_flow) as cumulative_cf 
FROM
    transactions a
JOIN b 
    transactions b ON a.date >= b.date 
GROUP BY 
    a.date 
ORDER BY 
    date ASC
```

Alternate solution using a window function (more efficient!):  

```
SELECT 
    date, 
    SUM(cash_flow) OVER (ORDER BY date ASC) as cumulative_cf 
FROM
    transactions 
ORDER BY 
    date ASC
```

## #5: Rolling Averages 

**Acknowledgement:** This problem is adapted from Sisense’s [“Rolling Averages in MySQL and SQL Server”](https://www.sisense.com/blog/rolling-average/) blog post 

**Note:** there are different ways to compute rolling/moving averages. Here we'll use a preceding average which means that the metric for the 7th day of the month would be the average of the preceding 6 days and that day itself. 

**Context**: Say we have table `signups` in the form: 

```
| date       | sign_ups |
|------------|----------|
| 2018-01-01 | 10       |
| 2018-01-02 | 20       |
| 2018-01-03 | 50       |
| ...        | ...      |
| 2018-10-01 | 35       |
```

**Task**: Write a query to get 7-day rolling (preceding) average of daily sign ups. 
* * *
***Solution:***

```
SELECT 
  a.date, 
  AVG(b.sign_ups) average_sign_ups 
FROM 
  signups a 
JOIN 
  signups b ON a.date <= b.date + interval '6 days' AND a.date >= b.date
GROUP BY 
  a.date
```



## #6: Multiple Join Conditions 

**Acknowledgement:** This problem was inspired by Sisense’s [“Analyzing Your Email with SQL”](https://www.sisense.com/blog/analyzing-your-email-with-sql/) blog post 

**Context:** Say we have a table `emails` that includes emails sent to and from [`zach@g.com`](mailto:zach@g.com):

```
| id | subject  | from         | to           | timestamp           |
|----|----------|--------------|--------------|---------------------|
| 1  | Yosemite | zach@g.com   | thomas@g.com | 2018-01-02 12:45:03 |
| 2  | Big Sur  | sarah@g.com  | thomas@g.com | 2018-01-02 16:30:01 |
| 3  | Yosemite | thomas@g.com | zach@g.com   | 2018-01-02 16:35:04 |
| 4  | Running  | jill@g.com   | zach@g.com   | 2018-01-03 08:12:45 |
| 5  | Yosemite | zach@g.com   | thomas@g.com | 2018-01-03 14:02:01 |
| 6  | Yosemite | thomas@g.com | zach@g.com   | 2018-01-03 15:01:05 |
| .. | ..       | ..           | ..           | ..                  |
```

**Task: **Write a query to get the response time per email (`id`) sent to `zach@g.com` . Do not include `id`s that did not receive a response from [zach@g.com](mailto:zach@g.com). Assume each email thread has a unique subject. Keep in mind a thread may have multiple responses back-and-forth between [zach@g.com](mailto:zach@g.com) and another email address. 
* * *
***Solution:***

```
SELECT 
    a.id, 
    MIN(b.timestamp) - a.timestamp as time_to_respond 
FROM 
    emails a 
JOIN
    emails b 
        ON 
            b.subject = a.subject 
        AND 
            a.to = b.from
        AND 
            a.from = b.to 
        AND 
            a.timestamp < b.timestamp 
 WHERE 
    a.to = 'zach@g.com' 
 GROUP BY 
    a.id 
```



# Window Function Practice Problems 

## #1: Get the ID with the highest value 

**Context:** Say we have a table `salaries` with data on employee salary and department in the following format: 

```
  depname  | empno | salary |     
-----------+-------+--------+
 develop   |    11 |   5200 | 
 develop   |     7 |   4200 | 
 develop   |     9 |   4500 | 
 develop   |     8 |   6000 | 
 develop   |    10 |   5200 | 
 personnel |     5 |   3500 | 
 personnel |     2 |   3900 | 
 sales     |     3 |   4800 | 
 sales     |     1 |   5000 | 
 sales     |     4 |   4800 | 
```

**Task**: Write a query to get the `empno` with the highest salary. Make sure your solution can handle ties!
* * *
***Solution:***

```
WITH max_salary AS (
    SELECT 
        MAX(salary) max_salary
    FROM 
        `salaries
    )
SELECT 
    s.empno
FROM 
    `salaries s
JOIN 
    max_salary ms ON s.salary = ms.max_salary ``
```

Alternate solution using `RANK()`:

```
WITH sal_rank AS 
  (SELECT 
    empno, 
    RANK() OVER(ORDER BY salary DESC) rnk
  FROM 
    salaries)
SELECT 
  empno
FROM
  sal_rank
WHERE 
  rnk = 1;
```



## #2: Average and rank with a window function (multi-part)

### Part 1: 

**Context**: Say we have a table `salaries` in the format:

```
  depname  | empno | salary |     
-----------+-------+--------+
 develop   |    11 |   5200 | 
 develop   |     7 |   4200 | 
 develop   |     9 |   4500 | 
 develop   |     8 |   6000 | 
 develop   |    10 |   5200 | 
 personnel |     5 |   3500 | 
 personnel |     2 |   3900 | 
 sales     |     3 |   4800 | 
 sales     |     1 |   5000 | 
 sales     |     4 |   4800 | 
```

**Task:** Write a query that returns the same table, but with a new column that has average salary per `depname`. We would expect a table in the form: 

```
  depname  | empno | salary | avg_salary |     
-----------+-------+--------+------------+
 develop   |    11 |   5200 |       5020 |
 develop   |     7 |   4200 |       5020 | 
 develop   |     9 |   4500 |       5020 |
 develop   |     8 |   6000 |       5020 | 
 develop   |    10 |   5200 |       5020 | 
 personnel |     5 |   3500 |       3700 |
 personnel |     2 |   3900 |       3700 |
 sales     |     3 |   4800 |       4867 | 
 sales     |     1 |   5000 |       4867 | 
 sales     |     4 |   4800 |       4867 |
```

* * *
***Solution:***

```
SELECT 
    *, 
    /*
    * AVG() is a Postgres command, but other SQL flavors like BigQuery use 
    * AVERAGE()
    */ 
    ROUND(AVG(salary),0) OVER (PARTITION BY depname) avg_salary
FROM
    salaries
```

### Part 2:

**Task:** Write a query that adds a column with the rank of each employee based on their salary within their department, where the employee with the highest salary gets the rank of `1`. We would expect a table in the form: 

```
  depname  | empno | salary | salary_rank |     
-----------+-------+--------+-------------+
 develop   |    11 |   5200 |           2 |
 develop   |     7 |   4200 |           5 | 
 develop   |     9 |   4500 |           4 |
 develop   |     8 |   6000 |           1 | 
 develop   |    10 |   5200 |           2 | 
 personnel |     5 |   3500 |           2 |
 personnel |     2 |   3900 |           1 |
 sales     |     3 |   4800 |           2 | 
 sales     |     1 |   5000 |           1 | 
 sales     |     4 |   4800 |           2 | 
```

* * *
***Solution:***

```
SELECT 
    *, 
    RANK() OVER(PARTITION BY depname ORDER BY salary DESC) salary_rank
 FROM  
    salaries 
```



# Other Medium/Hard SQL Practice Problems 

## #1: Histograms 

**Context:** Say we have a table `sessions` where each row is a video streaming session with length in seconds: 

```
| session_id | length_seconds |
|------------|----------------|
| 1          | 23             |
| 2          | 453            |
| 3          | 27             |
| ..         | ..             |
```

**Task:** Write a query to count the number of sessions that fall into bands of size 5, i.e. for the above snippet, produce something akin to: 

```
| bucket  | count |
|---------|-------|
| 20-25   | 2     |
| 450-455 | 1     |
```

Get complete credit for the proper string labels (“5-10”, etc.) but near complete credit for something that is communicable as the bin. 
* * *
***Solution:***

```
WITH bin_label AS 
(SELECT 
    session_id, 
    FLOOR(length_seconds/5) as bin_label 
 FROM
    sessions 
 )
 SELECT 
    `CONCATENTATE(STR(bin_label*5), '-', STR(`bin_label*5+5)) bucket, 
    COUNT(DISTINCT session_id) count ``
 GROUP BY 
    bin_label
 ORDER BY 
    `bin_label ASC `
```



## #2: CROSS JOIN (multi-part)

### Part 1: 

**Context:** Say we have a table `state_streams` where each row is a state and the total number of hours of streaming from a video hosting service: 

```
| state | total_streams |
|-------|---------------|
| NC    | 34569         |
| SC    | 33999         |
| CA    | 98324         |
| MA    | 19345         |
| ..    | ..            |
```

(In reality these kinds of aggregate tables would normally have a date column, but we’ll exclude that component in this problem) 

**Task:** Write a query to get the pairs of states with total streaming amounts within 1000 of each other. For the snippet above, we would want to see something like:

```
| state_a | state_b |
|---------|---------|
| NC      | SC      |
| SC      | NC      |
```

* * *
***Solution:***

```
SELECT
    a.state as state_a, 
    b.state as state_b 
 FROM   
    state_streams a
 CROSS JOIN 
    state_streams b 
 WHERE 
    ABS(a.total_streams - b.total_streams) < 1000
    AND 
    a.state <> b.state 
```

FYI, `CROSS JOIN` s can also be written without explicitly specifying a join: 

```
SELECT
    a.state as state_a, 
    b.state as state_b 
 FROM   
    state_streams a, state_streams b 
 WHERE 
    ABS(a.total_streams - b.total_streams) < 1000
    AND 
    a.state <> b.state 
```



### Part 2: 

**Note:** This question is considered more of a bonus problem than an actual SQL pattern. Feel free to skip it!

**Task:** How could you modify the SQL from the solution to Part 1 of this question so that duplicates are removed? For example, if we used the sample table from Part 1, the pair `NC` and `SC` should only appear in one row instead of two. 
* * *
***Solution: ***

```
SELECT
    a.state as state_a, 
    b.state as state_b 
 FROM   
    state_streams a, state_streams b 
 WHERE 
    ABS(a.total_streams - b.total_streams) < 1000
    AND 
    a.state > b.state 
```



## #3: Advancing Counting 

**Acknowledgement:** This question is adapted from [this Stack Overflow question](https://stackoverflow.com/questions/54488894/using-case-to-properly-count-items-with-if-else-logic-in-sql) by me (zthomas.nc) 

**Note:** this question is probably more complex than the kind you would encounter in an interview. Consider it a challenge problem, or feel free to skip it! 

**Context: **Say I have a table `table` in the following form, where a `user` can be mapped to multiple values of `class`:

```
| user | class |
|------|-------|
| 1    | a     |
| 1    | b     |
| 1    | b     |
| 2    | b     |
| 3    | a     |
```

**Task:** Assume there are only two possible values for `class`. Write a query to count the number of users in each class such that any user who has label `a` and `b` gets sorted into `b`, any user with just `a` gets sorted into `a` and any user with just `b` gets into `b`. 

For `table` that would result in the following table: 

```
| class | count |
|-------|-------|
| a     | 1     |
 | b     | 2     |
```

* * *
***Solution: ***

```
WITH usr_b_sum AS 
(
    SELECT 
        user, 
        SUM(CASE WHEN class = 'b' THEN 1 ELSE 0 END) num_b
    FROM 
        table
    GROUP BY 
        user
), 

usr_class_label AS 
(
    SELECT 
        user, 
        CASE WHEN num_b > 0 THEN 'b' ELSE 'a' END class 
    FROM 
        usr_b_sum
)

SELECT 
    class, 
    COUNT(DISTINCT user) count 
FROM
    usr_class_label
GROUP BY 
    class 
ORDER BY 
    class ASC

    
```

Alternate solution: Using `SELECT`s in the `SELECT` statement and `UNION`: 

```
SELECT 
    "a" class,
    COUNT(DISTINCT user_id) - 
        (SELECT COUNT(DISTINCT user_id) FROM table WHERE class = 'b') count 
UNION
SELECT 
    "b" class,
    (SELECT COUNT(DISTINCT user_id) FROM table WHERE class = 'b') count 
```
### Understanding the Role 
Based on the job description, this Senior Manager position in Advanced Analytics for Marketing 
at Airbnb involves leading a team of analysts to drive data-informed decisions in growth 
marketing and product strategy. You'll focus on advanced analysis, experimentation (including 
causal inference), building metrics and tools, and collaborating cross-functionally with teams 
like Data Science and Finance. It's remote-eligible in the US (with some restrictions on states), 
requires 12+ years in analytics/data science and 8+ in management, and prefers experience in 
marketplaces, statistical modeling, and tools like LLMs. Strong communication and stakeholder 
management are emphasized. 
To prepare effectively, I'll break this down into targeted sections: SQL technical prep, case 
study prep for product metrics, general interview tips, and additional resources. Since the 
interview simulates a real work environment (e.g., allowing syntax lookups), focus on 
conceptual understanding, problem-solving logic, and applying concepts to Airbnb-like 
scenarios (e.g., bookings, hosts, guests, marketing campaigns). 
### SQL Preparation 
The technical portion will cover basic to intermediate SQL, using dialects like Hive or Presto 
(which are similar to standard SQL but optimized for big data). Key concepts include: - **Joins**: INNER, LEFT, RIGHT, FULL, CROSS; handling multiple tables. - **Aggregating**: GROUP BY, COUNT, SUM, AVG, MAX/MIN; with HAVING for 
post-aggregation filters. - **Filtering**: WHERE clauses, conditional logic (CASE, IF). - **Subqueries**: Nested queries for filtering or aggregation. - **Self-joins**: Joining a table to itself, e.g., for hierarchical or time-based comparisons. - **Window functions**: ROW_NUMBER, RANK, LAG/LEAD, PARTITION BY for analytics over 
partitions. 
Practice by writing queries on hypothetical Airbnb datasets. Assume tables like: - `users` (user_id, signup_date, country) - `listings` (listing_id, host_id, city, price) - `bookings` (booking_id, guest_id, listing_id, booking_date, checkin_date, nights, revenue) - `campaigns` (campaign_id, user_id, exposure_date, type)  // For marketing exposure 
#### Practice Questions 
I'll provide 10 questions progressing from basic to intermediate, with Airbnb-themed contexts. 
Try solving them first, then check the solutions. Use online SQL sandboxes (e.g., DB-Fiddle) or 
tools like ChatGPT for syntax checks during practice. 
1. **Basic Filtering and Aggregation**: Find the total revenue from bookings in San Francisco in 
2024. - Solution: `SELECT SUM(revenue) AS total_revenue FROM bookings JOIN listings ON 
bookings.listing_id = listings.listing_id WHERE listings.city = 'San Francisco' AND 
YEAR(booking_date) = 2024;` 
2. **Joins**: List all users who have hosted at least one listing but never made a booking as a 
guest. - Solution: `SELECT u.user_id, u.signup_date FROM users u JOIN listings l ON u.user_id = 
l.host_id LEFT JOIN bookings b ON u.user_id = b.guest_id WHERE b.booking_id IS NULL 
GROUP BY u.user_id, u.signup_date HAVING COUNT(l.listing_id) >= 1;` 
3. **Subqueries**: Find listings with prices above the average price in their city. - Solution: `SELECT listing_id, city, price FROM listings WHERE price > (SELECT AVG(price) 
FROM listings AS sub WHERE sub.city = listings.city);` 
4. **Self-Joins**: Identify pairs of bookings by the same guest that occurred within 7 days of 
each other (e.g., for repeat travel patterns). - Solution: `SELECT b1.booking_id, b1.guest_id, b1.booking_date, b2.booking_id, 
b2.booking_date FROM bookings b1 JOIN bookings b2 ON b1.guest_id = b2.guest_id AND 
b1.booking_id < b2.booking_id WHERE DATEDIFF(b2.booking_date, b1.booking_date) <= 7;` 
5. **Window Functions (Basic)**: Rank listings by revenue within each city, showing the top 3 
per city. - Solution: `SELECT listing_id, city, revenue, RANK() OVER (PARTITION BY city ORDER BY 
revenue DESC) AS rank FROM (SELECT l.listing_id, l.city, SUM(b.revenue) AS revenue FROM 
listings l JOIN bookings b ON l.listing_id = b.listing_id GROUP BY l.listing_id, l.city) AS sub 
WHERE rank <= 3;` 
6. **Aggregating with Conditions**: Calculate the average nights stayed per booking, but only 
for users exposed to a 'discount' marketing campaign. - Solution: `SELECT AVG(nights) AS avg_nights FROM bookings b JOIN campaigns c ON 
b.guest_id = c.user_id WHERE c.type = 'discount';` 
7. **Subqueries in Joins**: Find the percentage of users who booked within 30 days of signup. - Solution: `SELECT (COUNT(DISTINCT b.guest_id) * 100.0 / COUNT(DISTINCT u.user_id)) 
AS conversion_rate FROM users u LEFT JOIN bookings b ON u.user_id = b.guest_id AND 
DATEDIFF(b.booking_date, u.signup_date) <= 30;` 
8. **Window Functions (Advanced)**: For each booking, calculate the running total revenue 
per host over time. - Solution: `SELECT host_id, booking_date, revenue, SUM(revenue) OVER (PARTITION BY 
host_id ORDER BY booking_date) AS running_total FROM bookings b JOIN listings l ON 
b.listing_id = l.listing_id ORDER BY host_id, booking_date;` 
9. **Self-Joins with Aggregation**: Compare monthly booking growth year-over-year for each 
city. - Solution: `SELECT curr.city, curr.month, curr.bookings, prev.bookings AS prev_bookings, 
(curr.bookings - prev.bookings) * 100.0 / prev.bookings AS growth_pct FROM (SELECT l.city, 
DATE_TRUNC('month', b.booking_date) AS month, COUNT(b.booking_id) AS bookings FROM 
bookings b JOIN listings l ON b.listing_id = l.listing_id GROUP BY l.city, month) curr LEFT JOIN 
(SELECT l.city, DATE_TRUNC('month', b.booking_date) AS month, COUNT(b.booking_id) AS 
bookings FROM bookings b JOIN listings l ON b.listing_id = l.listing_id GROUP BY l.city, month) 
prev ON curr.city = prev.city AND DATE_ADD(curr.month, INTERVAL -1 YEAR) = prev.month;` 
10. **Combined Concepts**: Using window functions and subqueries, find the top 5% of hosts 
by revenue contribution in the last year, with their share of total revenue. - Solution: `WITH host_revenue AS (SELECT host_id, SUM(revenue) AS total_revenue 
FROM bookings b JOIN listings l ON b.listing_id = l.listing_id WHERE YEAR(b.booking_date) = 
2024 GROUP BY host_id), ranked_hosts AS (SELECT host_id, total_revenue, NTILE(20) OVER 
(ORDER BY total_revenue DESC) AS percentile FROM host_revenue) SELECT host_id, 
total_revenue, (total_revenue / (SELECT SUM(total_revenue) FROM host_revenue)) * 100 AS 
revenue_share FROM ranked_hosts WHERE percentile = 1;` 
Tips for SQL Portion: - Practice explaining your query logic aloud—interviewers may ask why you chose a subquery 
over a join. - Handle edge cases: NULLs (use COALESCE), date functions (DATEADD, DATEDIFF), string 
manipulation (CONCAT, SUBSTRING). - If stuck, verbalize: "I'd look up the Presto syntax for window functions here." - Resources: LeetCode SQL problems (filter by medium), HackerRank, or Airbnb-specific mocks 
on Glassdoor. 
### Case Study Preparation 
The case study will assess your high-level approach to measuring product development and 
solving metrics problems. Expect scenarios like evaluating a new feature's impact or optimizing 
marketing spend. Structure your responses using a framework like: 
1. **Clarify the Problem**: Ask questions to define scope (e.g., "What are the success 
criteria?"). 
2. **Define Key Metrics**: North Star (e.g., bookings), proxies (e.g., engagement), guards (e.g., 
churn). 
3. **Analytical Approach**: Data sources, methods (A/B tests, causal inference), tools (SQL for 
querying, Python/R for modeling). 
4. **Dig Deeper**: Segment analysis, potential biases, next steps. 
5. **Recommendations**: Actionable insights, risks. 
#### Sample Case Studies 
Practice these with timed responses (20-30 mins), then discuss trade-offs. 
1. **Measuring a New Feature**: "Airbnb launches a 'personalized recommendations' feature 
for guests. How would you measure its success?" - Approach: North Star: Increase in booking conversion rate. Proxies: Click-through rate on 
recs, session time. Guards: No drop in host satisfaction. Method: A/B test (randomize users), 
SQL to query pre/post data, causal inference if rollout isn't clean (e.g., 
difference-in-differences). Dig deeper: Segment by user type (new vs. repeat), check for 
novelty effects. Recommendation: If uplift >5%, scale; else, iterate on algo. 
2. **Marketing Campaign Analysis**: "Evaluate the ROI of a email campaign targeting inactive 
hosts to relist properties." - Approach: Metrics: Incremental listings/reactivations, revenue lift, cost per acquisition. 
Approach: Control group (non-exposed hosts), propensity score matching for causal impact. 
SQL: Join campaigns and listings tables, aggregate reactivations post-exposure. Dig deeper: 
Cohort analysis by host tenure, attribution (first-touch vs. multi-touch). Recommendation: 
Calculate ROI as (incremental revenue - campaign cost) / cost; optimize by segmenting 
high-value hosts. 
3. **Product Metrics Problem**: "Bookings are down 10% YoY. How do you diagnose and 
propose fixes?" - Approach: Funnel breakdown: Traffic > Searches > Views > Bookings. Metrics: Drop-off 
rates at each stage. Approach: SQL cohort analysis, window functions for trends. Dig deeper: 
External factors (seasonality, competition via web data), internal (feature changes). 
Recommendation: If issue is in search-to-view, A/B test UI improvements; prioritize based on 
impact size. 
Tips for Case Study: - Tie to Airbnb's marketplace: Balance guest/host sides, consider network effects. - Mention advanced methods from JD: Causal inference (e.g., for non-random experiments), 
statistical modeling. - Practice with real data: Use public datasets like Inside Airbnb for mock analysis. 
### General Interview Tips - **Behavioral/Management**: Prepare stories on leading teams (e.g., developing analysts, 
handling conflicts), stakeholder influence, and driving impact (STAR method: Situation, Task, 
Action, Result). - **Airbnb-Specific**: Research recent news (e.g., AI integrations, post-pandemic recovery). 
Know core values: Champion the mission, be a host. - **Mock Interviews**: Use Pramp or Interviewing.io for practice. Record yourself for clarity. - **Tools/Resources**:  - Books: "Lean Analytics" for metrics, "SQL for Data Analysis" by Cathy Tanimura. 
- Online: Coursera's Google Data Analytics, Airbnb's Data University blog posts. - Search for experiences: Look up "Airbnb analytics interview" on Reddit/Glassdoor for 
patterns. - **Mindset**: Stay calm, think aloud, ask clarifying questions. Since it's senior, emphasize 
strategy over tactical execution. 
This should give you a strong foundation—focus on 2-3 hours daily on SQL practice and case 
walkthroughs. If you share more details (e.g., your background), I can refine this further! 
#1 Reframe the question and ask clarifying questions
Always start by confirming your understanding of the question. I do this by reframing the question in my own words. This shows my thought process and gives the interviewer a chance to correct me before I dive in. (Just in case I misunderstood or misinterpreted the question)

If the question involves some ambiguity, this is also your chance to clarify and align with your interviewer.

For example, if the interviewer asked you a question like: "What is the growth rate of X metric?"

There are so many ways to define growth:

→ Do we want absolute change or relative change?
→ Are we looking at long-term growth or short-term growth?

Use this first step as a chance to solidify how you will approach the question. This discussion shows off your product sense and great communication skills.

#2 State assumptions you have about the data
Typically, a SQL question would come with one or more tables. Before starting to write any queries, I find it helpful to state my assumptions on the table(s).

This step is so underrated, but it helps avoid misunderstandings and shows that you’re familiar with relational database concepts.

Some assumptions you can state include

Which column is the primary key?

Which columns are the foreign keys?

Can an event take place multiple times?

Are there any columns with unique values?

#3 Outline your approach
Again, you want to demonstrate your ability to communicate clearly and solve problems systematically.

I feel like a broken record saying the same thing over and over again. But the point I want to drive home is: you want to use your SQL interviews to demonstrate strong communication skills, and structured problem-solving approach.

There are a couple of good ways to outline your approach:

Bullet points → Outline the key steps you'll take (join tables, filter, aggregate, etc.)

Pseudocode → Quickly sketch out what the query will look like (especially useful if you’ll use multiple CTEs)

But if it’s a simple question, you don’t need to overdo it. Just talk through your approach out loud.

#4 Fill in your SQL query (talking aloud as you go)
Alright — now it’s time to actually write the query.

You’l be surprised how much easier this becomes because of all the steps you’ve completed before this. By now, you’ve already:

→ Clarified the question
→ Aligned on assumptions
→ Outlined your approach

At this point, your brain can go on auto-pilot as you fill in the blanks. As you code, talk through what you’re doing out loud, focusing on explaining your logic.

Side note, I personally also like doing this because it fills in the awkward silence.

#5 Review your query — catch any errors
In my opinion, this is the most important step.

I’ll say it again, this is the most important step.

Before you lock in your answer, take a minute to do a thorough review of your query.

Please… do not skip this step.

Why?

✅ Because if you catch the errors → that shows attention to detail.
❌ If your interviewer catches the errors → that shows a lack of skill.

Slow down here. It's worth it.

#6 Provide suggestions for improving the query
THIS is how you demonstrate that you’re a thought leader, and not a SQL monkey.

Finishing the query isn’t enough. That’s the baseline expectation.

You want to stand out from the crowd.

At the end of each query, answer at least one of these questions:

What are edge cases that the query did not catch?

What are some ways to optimize your query?

What are some drawbacks to this approach?
1. Introduction

1.1: About the instructor
1.2: What you will learn
2. Single Table

2.1: SELECT, WHERE, AND, OR, ORDER BY, LIMIT 
2.1.1: select where and or order by limit exercises 
2.1.2: select where and or order by limit solutions 
2.2: COUNT, SUM, AVERAGE, MIN, MAX, GROUP BY, HAVING 
2.2.1: count sum average min max group by having exercises 
2.2.2: count sum average min max group by having solutions 
2.3: IN, BETWEEN, LIKE, CASE WHEN 
2.3.1: in between like case when exercises 
2.3.2: in between like case when solutions 
2.4: More exercises 
2.5: Chapter 2 summary 
3. Multi-Table

3.1: INNER JOIN 
3.1.1: inner join exercises 
3.1.2: inner join solutions 
3.2: OUTER JOIN 
3.2.1: outer join exercises 
3.2.2: outer join solutions 
3.3: UNION, UNION ALL 
3.3.1: union, union all exercises 
3.4: multi table more exercises 
3.5: Chapter 3 summary 
4. Window Functions

4.0: Introduction 
4.1: AVG, MIN/MAX, SUM 
4.1.1: avg, min/max, sum exercises 
4.1.2: avg, min/max, sum solutions 
4.2: ROW_NUMBER, RANK, DENSE_RANK 
4.2.1: row_number rank dense_rank exercises 
4.2.2: row_number rank dense_rank solutions 
4.3: NTILE 
4.3.1: ntile exercises 
4.4: LAG, LEAD 
4.4.1: lag, lead exercises 
4.4.2: lag, lead solutions 
4.5: More exercises 
4.6: Chapter 4 summary 
5. Wrapup


1. Understanding the Role: Advanced Analytics, Sr. Manager, Marketing 
From the job description you provided, this role is not just about technical skills; it's a 
leadership position that requires a blend of technical expertise, business acumen, and strong 
communication skills. Here's a breakdown of what Airbnb is looking for: 
● Leadership & Management: You'll be leading a team of analysts, so be prepared to 
talk about your management style, how you mentor and develop team members, and 
your experience in leading projects. 
● Technical Expertise: 
○ SQL is a must: Hive and Presto are the flavors of SQL they use. 
○ Advanced Analytics: This includes statistical modeling, experimentation (A/B 
testing), and causal inference. 
○ Data Storytelling: You need to be able to translate complex data into 
actionable insights for stakeholders. 
● Business Acumen & Strategy: 
○ Marketing Focus: Understand the nuances of marketing analytics, including 
campaign measurement, customer segmentation, and marketing channel 
optimization. 
○ Product Sense: The case study will test your ability to measure product 
development and define product metrics. 
○ Strategic Thinking: You'll be working with senior stakeholders, so you need to 
demonstrate that you can think strategically and align your team's work with 
the company's goals. 
● Collaboration: You'll be working with various teams (Finance, Data Engineering, Data 
Science), so be ready to discuss your collaboration and stakeholder management 
skills. 
2. Mastering the SQL Technical Interview 
The technical screen will focus on "basic and intermediate SQL." While they say you can look 
up syntax, you should still have a strong command of the concepts. Here's how to prepare: 
Key SQL Concepts to Master: 
● Joining Tables: 
○ INNER JOIN, LEFT JOIN, RIGHT JOIN, FULL OUTER JOIN. 
○ Be prepared to explain the differences and when to use each. 
○ Self-Joins: These are explicitly mentioned, so practice problems that involve 
joining a table to itself. 
● Aggregating and Filtering: 
○ GROUP BY and aggregate functions (COUNT, SUM, AVG, MAX, MIN). 
○ WHERE vs. HAVING: Understand the order of operations and when to use each. 
● Subqueries: 
○ Practice using subqueries in SELECT, FROM, and WHERE clauses. 
○ Understand the difference between correlated and non-correlated subqueries. 
● Window Functions: This is an intermediate/advanced topic that they've called out. 
Master these: 
○ ROW_NUMBER(), RANK(), DENSE_RANK(): For ranking items. 
○ LEAD(), LAG(): For accessing data in subsequent or preceding rows. 
○ SUM(), AVG(), COUNT() as window functions with OVER(PARTITION BY ...): For 
running totals and moving averages. 
● Common Table Expressions (CTEs): Using the WITH clause to break down complex 
queries and improve readability. 
Practice Platforms: 
● DataLemur: Created by the author of "Ace the Data Science Interview," this site has a 
great collection of SQL interview questions from top tech companies. 
● StrataScratch: Offers real interview questions from companies like Airbnb, with a 
built-in SQL editor to practice. 
● LeetCode: Known for software engineering questions, but its database section has 
excellent SQL problems. 
Hive and Presto Specifics: 
● Hive and Presto are SQL query engines designed for big data. While the syntax is very 
similar to standard SQL, it's good to be aware of some differences, especially in 
performance and available functions. A quick search for "Hive vs. Presto syntax" will 
give you the key distinctions. For the interview, a strong grasp of standard SQL will be 
sufficient. 
3. Nailing the Case Study Interview 
The case study will assess your product sense and analytical thinking. They want to see how 
you structure your thoughts and approach a problem. Here's a framework to guide you: 
The CIRCLES Method™ for Product Case Studies: 
This is a popular framework for product-related case studies. It's a great way to structure your 
response: 
1. C - Clarify: Ask clarifying questions to understand the problem fully. What's the goal? 
Who are the users? What are the constraints? 
2. I - Identify Users: Who are the key user segments for this product or feature? (For 
Airbnb, this is often Hosts and Guests). 
3. R - Report on User Needs: What are the pain points and needs of these users? 
4. C - Cut Through and Prioritize: Brainstorm potential solutions or metrics, then 
prioritize them based on impact and effort. 
5. L - List Solutions: Detail your proposed solutions or analytical approaches. 
6. E - Evaluate Trade-offs: Discuss the pros and cons of your proposed solutions. 
7. S - Summarize: Conclude with a clear summary of your recommendation and next 
steps. 
Example Case Study Prompt: 
"We've just launched a new feature that allows guests to add ‘experiences’ to their bookings. 
How would you measure the success of this feature?" 
Applying a Framework to the Example: 
1. Clarify: 
○ "What is the primary goal of this feature? Is it to increase revenue, improve user 
engagement, or drive more bookings?" 
○ "Is this feature available on all platforms (web, iOS, Android)?" 
○ "What is the timeline for measuring success (e.g., first week, first month, first 
quarter)?" 
2. Define Success Metrics: Structure your metrics into categories. For Airbnb, a 
two-sided marketplace, consider both sides. 
○ Guest-Side Metrics (The User): 
■ Adoption Rate: % of users who see the feature vs. % who interact with 
it. 
■ Conversion Rate: % of users who book an experience after seeing the 
feature. 
■ Average Order Value (AOV): How much does the feature add to the 
average booking value? 
■ User Satisfaction: Net Promoter Score (NPS) or user surveys on the 
new feature. 
■ Retention: Do users who book experiences have a higher retention rate 
than those who don't? 
○ Host-Side Metrics (The Provider): 
■ Host Engagement: # of hosts creating experiences. 
■ Host Revenue: Increase in earnings for hosts offering experiences. 
■ Host Satisfaction: NPS from hosts on the new feature. 
○ Ecosystem Metrics (The Business): 
■ Overall Revenue: Incremental revenue generated by the feature. 
■ Impact on Core Business: Does this feature lead to more or fewer 
accommodation bookings? (This is a "guardrail" metric to ensure the 
new feature isn't hurting the core business). 
3. Propose an Analytical Approach: 
○ A/B Testing: This is the gold standard. "I would propose an A/B test where a 
control group of users does not see the new feature, and a treatment group 
does. We can then compare the key metrics between the two groups to 
measure the causal impact of the feature." 
○ Pre-Post Analysis (if A/B testing isn't possible): "If an A/B test isn't feasible, 
we could do a pre-post analysis, comparing user behavior before and after the 
feature launch. We would need to be mindful of seasonality and other external 
factors." 
4. Data and Dashboarding: 
○ "To track these metrics, I would work with the data engineering team to ensure 
the necessary data is being logged. Then, I would create a dashboard in a tool 
like Tableau or Superset to monitor the performance of the feature in 
real-time." 
4. Airbnb-Specific Preparation 
Show them you've done your homework. Spend some time researching: 
● Airbnb's Business Model: Understand how they make money, their key markets, and 
their competitive landscape. 
● Recent Product Launches: What new features have they launched recently? Think 
about how you would measure their success. 
● Marketing Campaigns: Look at their recent marketing campaigns. What channels are 
they using? Who are they targeting? 
● The Travel Industry: Be aware of the current trends in the travel and hospitality 
industry. 
5. General Interview Tips 
● Think Out Loud: During both the technical and case study portions, communicate 
your thought process. They want to see how you think, not just the final answer. 
● Prepare Your Questions: Have thoughtful questions ready to ask your interviewers. 
This shows your interest and engagement. Examples: 
○ "What is the biggest analytical challenge the marketing team is currently 
facing?" 
○ "How does the data and analytics team collaborate with the marketing and 
product teams?" 
○ "What does success look like for someone in this role in the first 6-12 months?" 
● Be a Storyteller: When answering behavioral questions ("Tell me about a time 
when..."), use the STAR method (Situation, Task, Action, Result) to structure your 
answers and make them compelling. 
1. What does the GROUP BY clause do in SQL?
It groups rows that have the same values in specified columns into summary rows (e.g., COUNT, SUM).

SELECT department, COUNT(*) 
FROM employees 
GROUP BY department;
Input Table: employees

id | name  | department
---+-------+-----------
1  | John  | HR
2  | Alice | IT
3  | Bob   | HR
Output

department | COUNT(*)
-----------+----------
HR         | 2
IT         | 1
2. What is the purpose of the HAVING clause?
It filters groups based on aggregate results (e.g., groups with COUNT > 1).

SELECT department, COUNT(*) 
FROM employees 
GROUP BY department 
HAVING COUNT(*) > 1;
Output

department | COUNT(*)
-----------+----------
HR         | 2
3. What’s the difference between WHERE and HAVING?
WHERE filters rows before grouping.
HAVING filters groups after aggregation.
SELECT department, COUNT(*) 
FROM employees 
WHERE name != 'Bob'
GROUP BY department 
HAVING COUNT(*) > 0;
4. How to count employees per department and show only departments with more than 2 employees?
SELECT department, COUNT(*) 
FROM employees 
GROUP BY department 
HAVING COUNT(*) > 2;
5. Can you use GROUP BY without aggregation?
Yes, but it’s rare and mostly used with SELECT DISTINCT-like behavior.

SELECT department 
FROM employees 
GROUP BY department;
6. Can you use aggregate functions without GROUP BY?
Yes, aggregate functions can be used on the entire result set.

SELECT COUNT(*) FROM employees;
7. How do you get the total salary per department?
SELECT department, SUM(salary) 
FROM employees 
GROUP BY department;
8. How to find the average salary of each department?
SELECT department, AVG(salary) 
FROM employees 
GROUP BY department;
9. Show departments where the average salary is above 50,000.
SELECT department, AVG(salary) AS avg_salary 
FROM employees 
GROUP BY department 
HAVING AVG(salary) > 50000;
10. Can you group by multiple columns?
Yes.

SELECT department, job_title, COUNT(*) 
FROM employees 
GROUP BY department, job_title;
11. Show the department with the highest average salary.
SELECT department, AVG(salary) AS avg_salary 
FROM employees 
GROUP BY department 
ORDER BY avg_salary DESC 
LIMIT 1;
12. How to find how many employees joined each year?
Assuming a join_date column:

SELECT YEAR(join_date) AS year_joined, COUNT(*) 
FROM employees 
GROUP BY YEAR(join_date);
13. How to find duplicate employee names?
SELECT name, COUNT(*) 
FROM employees 
GROUP BY name 
HAVING COUNT(*) > 1;
14. How to filter rows where total salary exceeds 100,000 per department?
SELECT department, SUM(salary) AS total_salary 
FROM employees 
GROUP BY department 
HAVING SUM(salary) > 100000;
15. Can we use column aliases in the HAVING clause?
Most databases do not support using aliases in HAVING. Use the function directly.

Works:

SELECT department, SUM(salary) AS total_salary 
FROM employees 
GROUP BY department 
HAVING SUM(salary) > 50000;
Avoid:

HAVING total_salary > 50000;
16. How do NULLs affect GROUP BY?
Rows with NULL values in GROUP BY columns are grouped together.

SELECT manager_id, COUNT(*) 
FROM employees 
GROUP BY manager_id;
17. How to get a count of different job titles per department?
SELECT department, COUNT(DISTINCT job_title) 
FROM employees 
GROUP BY department;
18. How to show departments with at least one employee earning more than 90,000?
SELECT department 
FROM employees 
WHERE salary > 90000 
GROUP BY department;
19. How to get the max salary in each department, but only show departments where the max is over 100,000?
SELECT department, MAX(salary) AS max_salary 
FROM employees 
GROUP BY department 
HAVING MAX(salary) > 100000;
20. How to include aggregated results along with non-aggregated data?
Use GROUP BY for all non-aggregated columns.

SELECT department, job_title, COUNT(*) 
FROM employees 
GROUP BY department, job_title;
1. What is Index in SQL?
With the help of Indexes, information retrieval from the database happens faster and with greater efficiency. Thus, indexes improve performance. There are three types of indexes:

Clustered: Used for reordering tables and searching information with key values.
Non-clustered: Used for maintaining the order of the tables.
Unique: They ban fields from having duplicate values.
There can be many non-clustered indexes in a table, however, there can be only one clustered index.

2. What is a Synonym in SQL?
As the name suggests, a synonym is used to give different names to the same object in the database. In the case of object-renaming or object schema-change, existing applications can continue to use older names because of synonyms. A synonym must only reference an object and not another synonym. Additionally, synonyms can also be used to reference objects in different databases or servers, by using 3 or 4 part object names. There can be many names for a single database object as long as all the names directly refer to the same database object.

You must ensure that you know answers to such SQL interview questions as answering them correctly will give you the much-needed confidence for the more difficult ones.

3. Mention some advantages of Synonyms.
Below are some advantages of using Synonyms:

Synonyms create a layer of abstraction for the specific object
For objects, with complex 3 or 4 part names, residing on the same server, Synonyms can give a simpler alias
Offers the flexibility to change object location without having to change the existing code
When the name of an object is changed or dropped, Synonym offers backward compatibility for older applications
Synonyms are also useful in front-end query tools such as Access linked tables and spreadsheets if there is a direct link to these tables
4. Are there any disadvantages to using Synonyms?
Yes, there are some disadvantages.

Synonyms are only loosely linked to the referenced object and thus, can be deleted without warning when being used to reference a different database object
Inside chaining cannot take place, meaning that the synonym of a synonym cannot be created
One cannot create a table with the same Synonym name
The checking for the object for which the Synonym is created happens at runtime and not at the time of creation. This means if there is an error, such as a spelling error, it will only show up at runtime creating a problem in accessing the object
Synonyms cannot be referenced in DDL statements
For SQL interview questions that ask you to talk about the advantages or disadvantages of a certain component or tool, ensure that you list as many as you can. Also, you can make your answer to such an SQL interview question meaty by adding personal anecdotes about some of the advantages or disadvantages.

5. Are NULL values equal to zero?
No. NULL values show an absence of characters, whereas zero is a numerical value. NULL values occur when a character is unavailable or not known. NULL values should also not be confused with blank space because a blank space is not supposed to have any data attached to it, whereas a NULL value shows a data record without any value assigned to it.

6. What are Scalar subqueries and Correlated subqueries?
A Scalar subquery is when a query returns just one row and one column of data. A Correlated subquery occurs when a query cannot process without information from an outer query. In such cases, table aliases define the scope of the argument and the subquery is parameterized by an outer query. Thus, there is a correlation between the inner and outer queries. As a result, back and forth execution takes place where a single row of results from the outer query passes parameters to the inner query.

SQL interview questions like the one above try to ascertain the depth of your knowledge of SQL.

7. What is the difference between NVL and NVL2 functions?
The function NVL (exp1, exp2) is a conversion function that changes exp1 into the target exp2 under the condition that exp1 is NULL. The data type of exp1 is the same as that of a return value. The function NVL2 (exp1, exp2, exp3), on the other hand, is a checking function, which determines whether exp1 is null or not. When exp1 is not null, exp2 is returned as the result. When exp1 is null, exp3 is returned as the result.

8. What do you mean by ‘auto increment’?
With the auto-increment command, one can generate unique numbers when new records are added to a table. This function is especially useful when one wants to automatically generate the primary key field values upon inserting new records. This command comes in handy on several platforms. The auto-increment command for the SQL servers is “identity”.

9. What is the main use of ‘recursive stored procedure’?
The main use of the recursive stored procedure is to make the code calls till the time certain boundary conditions are reached. This helps programmers enhance productivity by using the same code multiple times.

An SQL interview question like this one shows that even though some of the advanced concepts may be easy to understand, they may be difficult to recount when suddenly faced with the question. Thus, when you prepare for SQL interview questions, ensure to revise all types of concepts.

10. Describe ‘datawarehouse’ in SQL.
A ‘datawarehouse’ is a system used for analyzing and reporting data. It is very similar to a physical warehouse where inventory is stored and assessed before being sent to a customer. Here, data is stored, analyzed, and reported. A datawarehouse functions as a central repository of data integrated from different areas and sources and makes this data available for use.

11. What is DBMS?
DBMS is an abbreviation for Database Management System for creating and managing databases. There are two types of databases:

Relational Database Management Systems (RDBMS) - Data is stored in tables.
Non-Relational Database Management Systems - Mostly referred to as NoSQL, stores data in non-tabular format.
12. What is the difference between SQL and MySQL?
Structured Query Language is utilized for handling and modifying data in relational databases. With SQL, you can generate and alter databases, tables, and other related objects, alongside executing various data operations, including record insertion, updates, and deletions.

MySQL, on the other hand, is a specific relational database management system (RDBMS) that uses SQL as its primary language for managing data. MySQL is an open-source RDBMS that is widely used for web applications,

13. List the type of SQL statements or subsets.
Below are the popular subsets used in SQL:

DDL (Data Definition Language) - It is used to define and structure tables. Users can CREATE, ALTER, and DELETE the database tables.
DCL (Data Control Language) - Administrators use it to give users privileges to GRANT or REVOKE permissions to the database.
DML (Data Manipulation Language) - It allows users to either UPDATE, INSERT, RETRIEVE, or DELETE information from the database.
14. Define what joins are in SQL.
Joins is a statement used to join two or more rows based on their relationship. There are four types of Join statements:

Left Join
Right Join
Inner Join
Full Join
15. What is a Primary Key?
A primary key is used to identify unique rows or tables in a database. Primary keys must always contain unique values. Null or duplicate values are not considered primary keys.

16. What is a Foreign Key?
A foreign key is used to link two or more tables together. Its values match with a primary key from a different table. Foreign keys are like references between tables.

17. What is a unique key?
A unique key ensures a table has a unique value not found or contained in other rows or columns. Unlike the primary key, the unique key may have multiple columns. You can create a unique key using the keyword "UNIQUE" when defining the table.

18. Create an employee table example.
Below is how to create an employee table:

Image 15-05-23 at 10.29 PM_11zon.webp

19. What is a SELECT statement used for?
SELECT is a DML command used for fetching one or more tables. It queries for information which usually returns a set of results.

20. Name the clauses used in the SELECT statement.
WHERE - filters the rows according to their criteria

ORDER BY - Sorts the tables/rows according to the ASC clause (ascending order) or DESC clause (descending order)

GROUP BY - groups data from different tables that have similar rows in the database

21. What are CHAR and VARCHAR?
CHAR is a fixed-length string character, whereas VARCHAR is a variable-length string data structure. VARCHAR is preferred over CHAR because it is more space-efficient when storing strings with variable lengths.

22. List the types of relationships found in SQL.
One-to-one relationship - This relationship exists between two tables when a single row in one table corresponds to a single row in another table. This relationship is usually established using a foreign key constraint.

One-to-Many/Many-to-One - This relationship exists between two tables when a single row in one table corresponds to multiple rows in another table. This relationship is also established using a foreign key constraint.

Many-to-Many - This relationship exists between two tables when multiple rows in one table correspond to multiple rows in another table. This relationship is usually implemented using an intermediate table that contains foreign keys to the two tables being related.

23. What is the difference between TRUNCATE and DELETE?
The truncate command is used when you want to delete all rows and values from a table. It is a DDL type of command which is faster. While the DELETE command is used when you want to delete a specific row in a table. It is a DML command type and less efficient than the truncate statement.

24. What is a cursor?
A cursor is a temporary memory allocated by the server when performing any DML queries. They are used to store Database Tables. Basically a cursor in sql is an object in database code that allows processes to process rows one by one. While in other programming languages sets of data is processed individually through a loop, in SQL, data is processed in a set through a cursor.

Two types of cursors are Implicit cursors and Explicit cursors.

Implicit Cursors:
They are Default Cursors of SQL SERVER. Allocated when the user performs DML operations.

Explicit Cursors:
They are created by users in need. They are used for Fetching data from Tables in Row-By-Row Manner.

25. Define normalization.
Normalization is a method of breaking down larger, complex data into smaller tables. It helps in filtering unnecessary, redundant data and leaves only unique values.

26. What is ETL?
ETL is an acronym for Extract, Transform, and Load. It is a process where you extract data from different sources, transform the data quality, and finally load it into the database.

27. What is the difference between Local and Global variables?
Local variables are used inside a function and can’t be reused by other functions, whereas global variables can be accessed and used throughout the program.

28. What is a subquery?
A subquery is a query that is found in another query. Usually referred to as an inner query, its output is typically used by another query.

29. What is ACID?
ACID in SQL refers to a set of properties that guarantee the reliable and consistent processing of database transactions. It is an acronym where each letter stands for one of the properties:

Atomicity: Ensures that a transaction is either fully completed or not executed at all. If any part of a transaction fails, the entire transaction is rolled back, and the database remains unchanged.

Consistency: Guarantees that the database transitions from one consistent state to another upon the completion of a transaction. All data must adhere to predefined rules and constraints.

Isolation: Provides a degree of separation between concurrent transactions, ensuring that they do not interfere with one other. It helps maintain data integrity by controlling the visibility of changes made by one transaction to another.

Durability: Guarantees that after a transaction has been committed, the modifications made to the database become permanent, even if a system failure or crash occurs.

ACID properties are vital in maintaining data integrity and consistency in relational database management systems (RDBMS) and ensuring the robustness of transactions.

30. Define stored procedure.
A stored procedure is a function that contains a group of query statements that can be reused. They are stored inside a named object in the database and can be executed anytime they are required.

31. What are triggers in SQL?
Triggers are special stored procedures that run when there's an event in the database server, such as changing data in a table. A trigger is different from a regular stored procedure as it cannot be directly called like a regular stored procedure.

32. Define an ER.
An Entity Relationship (ER) diagram is a visual representation of the relationship tables found in the database. It displays the table structures and primary and foreign keys.

33. When are Triggers used?
Triggers in SQL are used to automatically enforce business rules or maintain data integrity by executing predefined actions in response to specific database events, such as INSERT, UPDATE, or DELETE. Common use cases include data validation, data auditing, and maintaining referential integrity or complex relationships between tables.

34. What are Sparse Columns?
Sparse columns are columns that provide optimized storage for null values. They reduce space that is usually taken up by null values and can be defined by using CREATE or ALTER statements.

35. Define Check Constraints.
Check constraints are used for checking and ensuring that values in a table follow domain integrity. Users can apply Check constraints to single and multiple columns.

36. What is Collation?
In SQL, collation refers to a set of rules that govern the proper ordering, comparison, and representation of characters in a particular character set or encoding. Collation influences how text data in a database is sorted, searched, and compared. It typically accounts for various linguistic considerations such as case sensitivity, accent sensitivity, and specific language-based conventions.

37. Write a SQL query for the salespeople and customers who live in the same city.
To write a SQL query that shows salespeople and customers who live in the same city, you need to have information about both salespeople and customers. Here's an example SQL query assuming you have two tables: salespeople and customers.

Image 15-05-23 at 10.30 PM_11zon.webp

In this query, we're selecting the salesperson name, customer name, and city from two tables (salespeople and customers) using an INNER JOIN to connect them based on the condition that the city in the salespeople table equals the city in the customers table.

38. Write a SQL query to find orders where the order amount exists between 1000 and 5000.
To find orders with an order amount between 1000 and 5000, you can use the following SQL query:

Image 15-05-23 at 10.30 PM (1)_11zon.webp

In this query, replace "orders" with the actual name of your orders table, and "order_amount" with the appropriate column name representing the order amount in your table. This query will return all rows where the order amount falls between 1000 and 5000, inclusive.

39. Write a SQL query to find those employees whose salaries are less than 7000.
Image 15-05-23 at 10.30 PM (2)_11zon.webp

40. What is a Filtered Index?
A filtered index is a non-clustered index that comes with optimized disk restore. It is created when a column has few values for queries. The purpose of a filtered index is to optimize query performance by reducing the size of the index and the number of index pages that need to be read. It helps in improving performance, storage reduction, and index maintenance.

41. What is a Clause?
A clause is one of the SQL query statements that filters or customizes data for a query. It allows users to limit the results by providing a conditional statement to the query. It is typically used when a large amount of data is in the database.

42. Write a SQL query that removes duplicates from the table.
The following SQL query removes the duplicate values

Image 15-05-23 at 10.30 PM (3)_11zon.webp

43. What is a Case Function?
A case function is a SQL logic that uses the if-then-else statements. It evaluates the conditions of a table and returns multiple result expressions.

44. Define a VIEW.
A view is a virtual table containing values in one or multiple tables. Views restrict data by selecting only required values to make queries easy.

45. What is a SCHEMA?
A schema in SQL is a collection of database objects, including tables, indexes, sequences, and other schema objects. It defines how data is organized in a relational database system. It is used to manage database objects and control access to them by different users.

46. Differentiate between HAVING and WHERE clauses.
These conditions are used for searching values except that the HAVING clause is used with the SELECT statement accompanied by the GROUP BY clause. The HAVING clause is used in combination with the GROUP BY clause to filter the data based on aggregate values, while the WHERE clause is used to filter the data based on individual values.

47. Define what is meant by CTE.
In SQL, a CTE (Common Table Expression) is a temporary result set, often used to simplify complex queries by breaking them into smaller, more manageable pieces. A CTE is created using the WITH clause and is available only within the context of the query that follows it.

48. What are SQL operators?
Operators are special characters or words that perform specific operations. They are used with the WHERE clause to filter data in most cases.

49. Write a SQL query to find the second-highest salary.
Image 15-05-23 at 10.31 PM_11zon.webp

50. What is CDC?
CDC means change data capture. It records the recent activities made by the INSERT, DELETE, and UPDATE statements made to the tables. It is basically a process of identifying and capturing changes made to data in the database and returning those changes in real time. This capture of changes from transactions in a source database and transferring them to the target, all in real-time, keeps the system in sync. This allows for reliable data copying and zero-downtime cloud migrations.

51. Define Auto Increment.
“AUTO INCREMENT” is a clause used to generate unique values whenever a new record is created and inserted into a table. It means that every time a new row is inserted into the table, the database system automatically generates a new value for that column.

52. What is a COALESCE?
COALESCE is a function that takes a set of inputs and returns the first non-null values. It is used to handle null values in a query's result set.
1. What is the difference between RANK(), DENSE_RANK(), and ROW_NUMBER()?
ROW_NUMBER() assigns a unique sequential number to each row.

RANK() gives the same rank to ties but leaves gaps.

DENSE_RANK() gives the same rank to ties without leaving gaps.

SELECT name, salary,

RANK() OVER (ORDER BY salary DESC) AS rank,

DENSE_RANK() OVER (ORDER BY salary DESC) AS dense_rank,

ROW_NUMBER() OVER (ORDER BY salary DESC) AS row_num

FROM employees;

2. How do you find the second highest salary from an Employee table?
SELECT MAX(salary) AS SecondHighest

FROM employees

WHERE salary < (SELECT MAX(salary) FROM employees);

Or using LIMIT

— — — — — — — — — — — — — — — — — — — — — — — — -

SELECT DISTINCT salary

FROM employees

ORDER BY salary DESC

LIMIT 1 OFFSET 1;

3. Explain Common Table Expressions (CTEs) and their use.
CTEs are temporary result sets used to simplify complex joins and subqueries.

WITH DeptTotal AS (

SELECT department_id, SUM(salary) AS total_salary

FROM employees

GROUP BY department_id

)

SELECT e.name, e.salary, d.total_salary

FROM employees e

JOIN DeptTotal d ON e.department_id = d.department_id;

4. How do you detect and remove duplicate records from a table?
To find duplicates:

SELECT name, COUNT(*)

FROM employees

GROUP BY name

HAVING COUNT(*) > 1;

To delete duplicates (keeping the lowest ID):

DELETE FROM employees

WHERE id NOT IN (

SELECT MIN(id)

FROM employees

GROUP BY name, department_id, salary

);

5. What is a window function? Give an example.
A window function performs a calculation across a set of table rows related to the current row.

Example: Running Total

SELECT name, salary,

SUM(salary) OVER (PARTITION BY department_id ORDER BY salary) AS running_total

FROM employees;

6. Write a query to pivot data in SQL.
Using CASE WHEN:

SELECT department_id,

SUM(CASE WHEN gender = ‘M’ THEN 1 ELSE 0 END) AS male_count,

SUM(CASE WHEN gender = ‘F’ THEN 1 ELSE 0 END) AS female_count

FROM employees

GROUP BY department_id;

7. Explain the difference between INNER JOIN, LEFT JOIN, RIGHT JOIN, and FULL JOIN.
INNER JOIN: Returns matching rows.

LEFT JOIN: Returns all from the left table, and matched rows from the right.

RIGHT JOIN: All from the right table, and matched from the left.

FULL JOIN: All rows when there’s a match in one of the tables.

8. What is the use of EXISTS vs IN vs JOIN?
IN works on a list of values.

EXISTS returns true if subquery returns rows.

JOIN merges rows from multiple tables.

EXISTS is faster than IN in correlated subqueries with large data sets.

SELECT name

FROM employees e

WHERE EXISTS (

SELECT 1

FROM departments d

WHERE d.manager_id = e.id

);

9. What is a recursive CTE?
Used to query hierarchical data like org charts.

WITH RECURSIVE EmployeeHierarchy AS (

SELECT id, name, manager_id

FROM employees

WHERE manager_id IS NULL

UNION ALL

SELECT e.id, e.name, e.manager_id

FROM employees e

INNER JOIN EmployeeHierarchy eh ON e.manager_id = eh.id

)

SELECT * FROM EmployeeHierarchy;

10. How would you optimize a slow SQL query?
Use EXPLAIN to analyze.

Add indexes on filtered/joined columns.

Avoid **SELECT ***; select only required columns.

Use CTEs or temp tables for complex subqueries.

Minimize use of functions in WHERE clause.

11. What is the difference between CROSS JOIN and INNER JOIN?
CROSS JOIN: Returns the Cartesian product of two tables. No condition is used.
INNER JOIN: Returns only matching rows based on a join condition.
— CROSS JOIN

SELECT * FROM employees CROSS JOIN departments;

— INNER JOIN

SELECT * FROM employees INNER JOIN departments

ON employees.department_id = departments.id;

12. How do you calculate a rolling average using SQL?
SELECT name, salary,

AVG(salary) OVER (ORDER BY hire_date ROWS BETWEEN 2 PRECEDING AND CURRENT ROW) AS rolling_avg

FROM employees;

This gives a 3-row moving average (current row + 2 previous rows).

13. Explain MERGE (aka UPSERT) statement.
Used to insert, update, or delete records based on conditions.

MERGE INTO target_table AS target

USING source_table AS source

ON target.id = source.id

WHEN MATCHED THEN

UPDATE SET target.name = source.name

WHEN NOT MATCHED THEN

INSERT (id, name) VALUES (source.id, source.name);

14. How do you find gaps in a sequence?
Suppose you have employee IDs and want to find missing ones:

SELECT (t1.id + 1) AS start_gap

FROM employees t1

LEFT JOIN employees t2 ON t1.id + 1 = t2.id

WHERE t2.id IS NULL;

15. How do you rank items within groups in SQL (e.g., top 3 per department)?
SELECT *

FROM (

SELECT name, department_id, salary,

RANK() OVER (PARTITION BY department_id ORDER BY salary DESC) AS dept_rank

FROM employees

) ranked

WHERE dept_rank <= 3;

16. How do you handle NULLs in aggregations?
Use COALESCE or default values:

SELECT department_id, SUM(COALESCE(salary, 0)) AS total_salary

FROM employees

GROUP BY department_id;

17. Difference between DELETE, TRUNCATE, and DROP?

Press enter or click to view image in full size

18. How do you perform full outer join in MySQL (which doesn’t support it directly)?

SELECT *

FROM table1

LEFT JOIN table2 ON table1.id = table2.id

UNION

SELECT *

FROM table1

RIGHT JOIN table2 ON table1.id = table2.id;

19. What is the use of LAG() and LEAD()?
LAG(): Get value from a previous row.
LEAD(): Get value from a following row.
SELECT name, salary,

LAG(salary, 1) OVER (ORDER BY hire_date) AS prev_salary,

LEAD(salary, 1) OVER (ORDER BY hire_date) AS next_salary

FROM employees;

20. What is a correlated subquery?
A subquery that depends on the outer query for its value.

SELECT name, salary

FROM employees e

WHERE salary > (

SELECT AVG(salary)

FROM employees

WHERE department_id = e.department_id

);

21. What is the difference between HAVING and WHERE clauses?
WHERE filters rows before grouping.
HAVING filters rows after grouping (used with aggregates).
— Example

SELECT department_id, COUNT(*) AS emp_count

FROM employees

WHERE status = ‘Active’

GROUP BY department_id

HAVING COUNT(*) > 5;

22. What are indexes? What are their types and trade-offs?
Indexes speed up query lookups by creating a data structure (usually B-tree).
Types:
Single-column index
Composite index
Unique index
Full-text index
Bitmap index (for low-cardinality columns)
Trade-offs:

Faster reads.
Slower inserts/updates/deletes.
Consumes storage.
23. How do you detect slow queries in a SQL database?
Use EXPLAIN or EXPLAIN ANALYZE.
Use Query Execution Plan to see:
Full Table Scans
Missing Indexes
High Cost Steps
EXPLAIN SELECT * FROM employees WHERE salary > 100000;

24. What’s the difference between normalization and denormalization?
Press enter or click to view image in full size

25. What is the difference between UNION, UNION ALL, INTERSECT, and EXCEPT?
Press enter or click to view image in full size

26. How do you implement pagination in SQL?
— For PostgreSQL/MySQL

SELECT * FROM employees

ORDER BY name

LIMIT 10 OFFSET 20;

— For SQL Server

SELECT *

FROM (

SELECT *, ROW_NUMBER() OVER (ORDER BY name) AS rn

FROM employees

) AS sub

WHERE rn BETWEEN 21 AND 30;

27. How can you pivot and unpivot data in SQL Server?
— PIVOT example

SELECT *

FROM (

SELECT department, gender

FROM employees

) AS source

PIVOT (

COUNT(gender)

FOR gender IN ([M], [F])

) AS pivoted;

28. How do you remove duplicate rows but keep the most recent based on a timestamp?
DELETE FROM employees

WHERE id NOT IN (

SELECT MAX(id)

FROM employees

GROUP BY email

);

29. How do you perform case-insensitive searches in SQL?
SELECT * FROM employees

WHERE LOWER(name) = ‘john doe’;

Or using ILIKE in PostgreSQL:

SELECT * FROM employees

WHERE name ILIKE ‘john%’;

30. What are materialized views and how are they different from regular views?
Feature
8 Upstart SQL Interview Questions (Updated 2025)

By
Nick Singh

(Ex-Facebook & Best-Selling Data Science Author)




Updated on

January 10, 2025

At Upstart, SQL is often used for analyzing loan databases for risk assessment, and for customer analytics. That's the reason behind why Upstart almost always asks SQL query questions during interviews for Data Analytics, Data Science, and Data Engineering jobs.

As such, to help you prepare for the Upstart SQL interview, this blog covers 8 Upstart SQL interview questions – able to solve them?

8 Upstart SQL Interview Questions
SQL Question 1: Calculate Monthly Average Loan Amount
Upstart, being an online lending platform, wants to understand how their average loan amount changes over time. This could help them identify any trends or seasonality in their loans business to make better business decisions. They specifically want to understand the monthly average loan amount for each unique loan purpose over the last year.

You're provided with a loans table with the following schema:

loans Example Input:
loan_id	loan_purpose	loan_amount	loan_date
2345	Car	18000	2019-03-23
3782	Investment	15000	2019-03-23
8542	Education	25000	2019-04-14
9529	Investment	20000	2019-04-14
1654	Car	12000	2019-04-14
Write a SQL query to get the monthly average loan amount for each unique loan purpose over the last year. The output should include the year, the month, the loan purpose, and the relevant monthly average loan amount.

The output of the SQL query should look like this:

Example Output:
yr	mth	purpose	avg_loan_amount
2019	03	Car	18000.00
2019	03	Investment	15000.00
2019	04	Car	12000.00
2019	04	Investment	20000.00
2019	04	Education	25000.00
Answer:
SELECT 
    EXTRACT(YEAR FROM loan_date) AS yr,
    EXTRACT(MONTH FROM loan_date) AS mth,
    loan_purpose AS purpose,
    AVG(loan_amount) AS avg_loan_amount
FROM 
    loans 
WHERE 
    loan_date >= NOW() - INTERVAL '1 year'
GROUP BY 
    yr,
    mth,
    purpose
ORDER BY 
    yr,
    mth,
    purpose;
This SQL query uses the EXTRACT function to break down the loan_date into yr and mth. The AVG is then used as our window function to compute the average loan_amount for each combination of yr, mth, and purpose. The WHERE clause limits our data to the last year. Grouping and ordering is done by yr, mth, and purpose to break down the average loan amounts as per the requirements.

For more window function practice, try this Uber SQL problem on DataLemur's online SQL code editor:

Uber Data Science SQL Interview Question

SQL Question 2: Loan Distribution Analysis
Upstart is a leading AI lending platform that partners with banks to improve access to affordable credit. A part of their business is to understand how their loans are distributed among different customers in terms of loan size, occupation. They have two tables borrowers and loans.

borrowers table:
This table contains information about each borrower who took a loan with upstart.

borrower_id	age	occupation
101	25	Engineer
102	32	Teacher
103	28	Doctor
104	35	Engineer
105	30	Teacher
loans table:
This table contains information about the loans taken by the borrowers.

loan_id	borrower_id	loan_size
201	101	10000
202	102	15000
203	103	8000
204	104	20000
205	105	5000
The business problem: Upstart wants to know the average loan size by occupation of borrowers and their age. They want to check if occupation and age are factors that determine the size of the loan a customer takes.

Answer:
We can solve this problem by joining the two tables on 'borrower_id' and then grouping by 'occupation' and 'age' to calculate the average loan size.

SELECT b.occupation, b.age, AVG(l.loan_size) AS avg_loan_size
FROM borrowers AS b
JOIN loans AS l
ON b.borrower_id = l.borrower_id
GROUP BY b.occupation, b.age
This query will return a table with each row representing a unique combination of occupation and age, and a column representing the average loan size for that combination. This will allow Upstart to analyze whether certain occupations or age groups tend to take out larger loans.

SQL Question 3: What sets a cross join apart from a natural join?
Imagine you are organizing a party and have two database tables: one table of people you want to invite and another list of food items you want to serve.

A cross join would be like inviting every person on your list to the party and serving them every food item on the menu, regardless of whether they like the food or not. So, if you had 10 people on your invite list and 5 food items on the menu, you would generate all 50 different combinations of people and food (10 x 5 = 50).

On the other hand, a natural join would be like inviting only the people who like the food items on the menu (based on doing a inner/left/right/outer JOIN on a common key like food_id).

Upstart SQL Interview Questions

SQL Question 4: Calculating Product View to Cart Conversion Rate
Upstart wants to understand the click-through-rate from viewing a product to adding a product to the cart. They have two tables:

product_views table, which records every time a user views a product.
cart_adds table, which records every time a user adds a product to their cart.
Calculate the conversion rate from viewing a product to adding a product to cart, for each product.

product_views Example Input:
view_id	user_id	product_id	time_stamp
8171	123	10001	06/08/2022 00:00:00
5802	265	20002	06/10/2022 00:00:00
8293	362	10001	06/18/2022 00:00:00
9352	192	30003	07/26/2022 00:00:00
9517	981	20002	07/05/2022 00:00:00
cart_adds Example Input:
add_id	user_id	product_id	time_stamp
4171	123	10001	06/08/2022 00:01:00
2802	265	20002	06/10/2022 00:01:00
6293	362	10001	06/18/2022 00:01:00
8352	981	20002	07/05/2022 00:01:00
Answer:
SELECT 
  pv.product_id,
  COUNT(DISTINCT pv.user_id) as view_count,
  COUNT(DISTINCT ca.user_id) as add_to_cart_count,
  COUNT(DISTINCT ca.user_id)::float / GREATEST(COUNT(DISTINCT pv.user_id)::float, 1) as conversion_rate
FROM
  product_views pv
LEFT JOIN 
  cart_adds ca
ON 
  pv.user_id = ca.user_id AND pv.product_id = ca.product_id
GROUP BY 
  pv.product_id;
This query counts the distinct users who viewed and added each product to their cart. We make a LEFT JOIN from product_views to cart_adds on user_id and product_id to see which product views led to adds. The conversion rate is then calculated by dividing add_to_cart_count by view_count. We use GREATEST function to avoid division by zero.

To solve a similar SQL problem on DataLemur's free interactive SQL code editor, try this SQL interview question asked by Facebook:SQL interview question asked by Facebook

SQL Question 5: In database normalization, what's the distinction between 1NF, 2NF, and 3NF?
Normal forms are guidelines that are used to help design a relational database in a way that minimizes redundancy and ensures the integrity of the data. The 3 most commonly use normal forms are the 1st, 2nd, and 3rd normal forms. Here's a brief explanation of each:

1st Normal Form (1NF) is all about keeping it simple - each column should only have one value and there should be no repeating groups of data.

2nd Normal Form (2NF) is about organization - your database should already be in 1NF and all the non-key columns should depend on the primary key. This means that each non-key column should be completely dependent on the entire primary key, not just part of it.

3rd Normal Form (3NF) is about independence - if your database is already in 2NF, then all the non-key columns should not depend on each other. They should be self-sufficient and not rely on other non-key columns.

SQL Question 6: Calculate Average Loan Amount by State
Upstart provides personal loans to customers. The loans table contains information about each loan issued by Upstart, including the loan ID, the customer ID, the loan amount, and the date the loan was issued. The customers table contains data about each customer, including their state of residence.

Given the loans and customers tables, write a SQL query to calculate the average loan amount by state for the past year.

Here are the sample tables:

loans Example Input:
loan_id	customer_id	loan_amount	loan_date
6171	123	7000	09/05/2021
7802	265	5000	11/20/2021
5293	362	9000	12/10/2021
6352	192	8000	04/25/2021
4517	981	6000	07/15/2021
customers Example Input:
customer_id	state
123	NY
265	CA
362	TX
192	NY
981	CA
Answer:
SELECT c.state, AVG(l.loan_amount) as average_loan_amount 
FROM loans l 
JOIN customers c ON c.customer_id = l.customer_id
WHERE l.loan_date >= '2021-01-01' AND l.loan_date <= '2021-12-31'
GROUP BY c.state;
This query joins the loans and customers tables on customer_id, and then selects the state and average_loan_amount after grouping the data by state. The WHERE clause filters the data to include only loans issued in the past year.

Because join questions come up routinely during SQL interviews, take a stab at this Spotify JOIN SQL question:Spotify JOIN SQL question

SQL Question 7: What is denormalization, and in what situations might it be a useful?
Denormalization is the practice of altering a database schema in a way that breaks the normalization rules (1st, 2nd, 3rd normal forms).

For example, in a database that stores Upstart sales analytics data, you might have separate tables for "customers," "orders," and "products," with foreign key constraints linking the tables together. This helps to ensure the integrity of the data and reduces redundancy, but it can also make queries that involve multiple tables more complex and slower to execute.

By de-normalizing the database, you can combine some of the data from separate tables into a single table, which can reduce the number of joins that are required to retrieve the data you need. This can speed up queries and improve the performance of your database.

However, it's important to carefully consider the trade-offs of de-normalization before making any changes to your database. De-normalization can make it more difficult to maintain the integrity and reliability of your data, and can also increase the risk of data redundancy. It's generally best to use de-normalization as a performance optimization technique only when necessary, and to carefully evaluate the benefits and drawbacks in the context of your specific database and workload.

SQL Question 8: Average Loan Amount by Month
As an analyst at Upstart, a company providing personal loans online, you have been tasked to monitor the average loan amount provided per month. Given a table 'loans', create a SQL query to find the average loan amount for each month.

loans Example Input:
loan_id	user_id	loan_date	loan_amount
6789	345	01/22/2022 00:00:00	15000
7980	256	03/10/2022 00:00:00	22000
3284	563	03/25/2022 00:00:00	18000
5382	785	07/19/2022 00:00:00	30000
1478	143	10/09/2022 00:00:00	24000
Example Output:
month	average_loan_amount
1	15000.00
3	20000.00
7	30000.00
10	24000.00
Answer:
SELECT 
  EXTRACT(MONTH FROM loan_date) AS month, 
  AVG(loan_amount) AS average_loan_amount
FROM 
  loans
GROUP BY 
  EXTRACT(MONTH FROM loan_date)
ORDER BY 
  month
In this query, we use the PostgreSQL specific EXTRACT function to get the month from loan_date. The AVG() function is used to calculate average loan amount for each month. We then GROUP BY month to find average loan amount for each distinct month. Finally, we ORDER BY month to provide a chronologically ordered output.
11 Airbnb SQL Interview Questions - Can You Solve Them?

By
Nick Singh

(Ex-Facebook & Best-Selling Data Science Author)




Updated on

April 30, 2025

At Airbnb, SQL is used day-to-day for analyzing customer behavior to improve property recommendations and monitoring system performance for seamless booking experiences. Unsurprisingly this is why Airbnb LOVES to ask SQL problems in interviews for Data Analyst, Data Science, and BI jobs.

In case you want to ace the SQL Assessment, we've curated 11 Airbnb SQL interview questions to practice, which are similar to recently asked questions at Airbnb – able to answer them all?

Airbnb SQL Interview Questions

11 Airbnb SQL Interview Questions
SQL Question 1: Booking Referral Source
The strategy team in Airbnb is trying to analyze the impact of Covid-19 during 2021. To do so, they need you to write a query that outputs the average vacant days across the AirBnbs in 2021. Some properties have gone out of business, so you should only analyze rentals that are currently active. Round the results to a whole number.

Assumptions:

is_active field equals to 1 when the property is active, and 0 otherwise.
In cases where the check-in or check-out date is in another year other than 2021, limit the calculation to the beginning or end of the year 2021 respectively.
Listing can be active even if there are no bookings throughout the year.
bookings Table:
Column Name	Type
listing_id	integer
checkin_date	date
checkout_date	date
bookings Example Input:
listing_id	checkin_date	checkout_date
1	08/17/2021 00:00:00	08/19/2021 00:00:00
1	08/19/2021 00:00:00	08/25/2021 00:00:00
2	08/19/2021 00:00:00	09/22/2021 00:00:00
3	12/23/2021 00:00:00	01/05/2022 00:00:00
listings Table:
Column Name	Type
listing_id	integer
is_active	integer
listings Example Input:
listing_id	is_active
1	1
2	0
3	1
Example Output:
avg_vacant_days
357
Explanation:
Property 1 was rented for 8 days, thus the property has 365 - 8 = 357 vacant days.
Property 2 is excluded as it is not active.
Property 3 was rented out for 12 days, thus the property as 365 - 12 = 353 vacant days.
Average vacant days are 355 days. (357 + 353 / 2).

The dataset you are querying against may have different input & output - this is just an example!

Answer:
WITH listing_vacancies AS (
SELECT 
  listings.listing_id,
  365 - COALESCE(
    SUM(
      CASE WHEN checkout_date>'12/31/2021' THEN '12/31/2021' ELSE checkout_date END -
      CASE WHEN checkin_date<'01/01/2021' THEN '01/01/2021' ELSE checkin_date END 
  ),0) AS vacant_days
FROM listings 
LEFT JOIN bookings
  ON listings.listing_id = bookings.listing_id 
WHERE listings.is_active = 1
GROUP BY listings.listing_id)

SELECT ROUND(AVG(vacant_days)) 
FROM listing_vacancies;
To solve this question join DataLemur Premium to try this Airbnb SQL interview question:Airbnb SQL Interview Question

SQL Question 2: Analyzing Monthly Average Ratings of Airbnb Property Listings
Given the reviews table with columns: review_id, user_id, submit_date, listing_id, stars, write a SQL query to get the average rating of each Airbnb property listing per month. The submit_date column represents when the review was submitted. The listing_id column represents the unique ID of the Airbnb property, and stars represents the rating given by the user where 1 is the lowest and 5 is the highest rating.

reviews Example Input:
review_id	user_id	submit_date	listing_id	stars
6171	123	01/02/2022 00:00:00	50001	4
7802	265	01/15/2022 00:00:00	69852	4
5293	362	01/22/2022 00:00:00	50001	3
6352	192	02/05/2022 00:00:00	69852	3
4517	981	02/10/2022 00:00:00	69852	2
Answer:
SELECT 
EXTRACT(MONTH from submit_date) as mth, 
listing_id, 
AVG(stars) as avg_stars
FROM 
reviews
GROUP BY 
mth, 
listing_id
ORDER BY 
listing_id, 
mth;
This SQL query will first group the data by month and listing_id. For each group, it calculates the average stars rating. The EXTRACT(MONTH from submit_date) function is used to get the month from submit_date. The AVG(stars) function is used to calculate the average stars rating. In the end, it orders the results by listing_id and mth.

Example Output:
mth	listing_id	avg_stars
1	50001	3.50
1	69852	4.00
2	69852	2.50
p.s. Window functions show up super frequently during SQL interviews, so practice the 27+ window function questions on DataLemur

DataLemur SQL Questions

SQL Question 3: Average Vacant Days
The strategy team in Airbnb is trying to analyze the impact of Covid-19 during 2021. To do so, they need you to write a query that outputs the average vacant days across the AirBnbs in 2021. Some properties have gone out of business, so you should only analyze rentals that are currently active. Round the results to a whole number.

Assumptions:

is_active field equals to 1 when the property is active, and 0 otherwise.
In cases where the check-in or check-out date is in another year other than 2021, limit the calculation to the beginning or end of the year 2021 respectively.
Listing can be active even if there are no bookings throughout the year.
bookings Table:
Column Name	Type
listing_id	integer
checkin_date	date
checkout_date	date
bookings Example Input:
listing_id	checkin_date	checkout_date
1	08/17/2021 00:00:00	08/19/2021 00:00:00
1	08/19/2021 00:00:00	08/25/2021 00:00:00
2	08/19/2021 00:00:00	09/22/2021 00:00:00
3	12/23/2021 00:00:00	01/05/2022 00:00:00
listings Table:
Column Name	Type
listing_id	integer
is_active	integer
listings Example Input:
listing_id	is_active
1	1
2	0
3	1
Example Output:
avg_vacant_days
357
Solution:
WITH listing_vacancies AS (
SELECT 
  listings.listing_id,
  365 - COALESCE(
    SUM(
      CASE WHEN checkout_date>'12/31/2021' THEN '12/31/2021' ELSE checkout_date END -
      CASE WHEN checkin_date<'01/01/2021' THEN '01/01/2021' ELSE checkin_date END 
  ),0) AS vacant_days
FROM listings 
LEFT JOIN bookings
  ON listings.listing_id = bookings.listing_id 
WHERE listings.is_active = 1
GROUP BY listings.listing_id)

SELECT ROUND(AVG(vacant_days)) 
FROM listing_vacancies;
To solve this question on DataLemur's free interactive site, try this Airbnb SQL interview question:Airbnb SQL Interview Question

SQL Question 4: Retrieve Housing Data from Specific Cities
You're a data analyst at Airbnb and you've been tasked with retrieving housing data from specific cities. You want to find all Airbnb listings in San Francisco and New York that have at least 10 reviews and an average rating equal to or above 4.5.

Assume you have two tables: a listings table with the ID of the housing, its name, city, and the total number of reviews; and a reviews table with the ID of the listing, the review ID, the rating, and the date submitted.

listings Example Input:
listing_id	name	city	reviews_count
10001	"Central Loft"	"San Francisco"	15
10002	"Cozy Apartment"	"New York"	20
10003	"Sunny Studio"	"San Francisco"	8
10004	"Stylish Suite"	"Las Vegas"	13
10005	"Dreamy Duplex"	"New York"	5
reviews Example Input:
listing_id	review_id	stars	submit_date
10001	15001	4.5	2022-06-08
10001	15002	5.0	2022-06-10
10002	15003	4.0	2022-06-18
10002	15004	5.0	2022-07-26
10003	15005	3.5	2022-07-05
10004	15006	4.5	2022-06-08
10005	15007	3.0	2022-06-10
Answer:
SELECT l.listing_id, l.name, l.city, avg(r.stars) as average_rating
FROM listings l
JOIN reviews r ON l.listing_id = r.listing_id
WHERE l.city in ('San Francisco', 'New York') 
AND l.reviews_count >= 10
GROUP BY l.listing_id
HAVING avg(r.stars) >= 4.5;
The above query works as follows:

We join the listings table (alias 'l') with the reviews table (alias 'r') using the listing_id as the common column between the two tables.
In the WHERE clause, we filter the cities to "San Francisco" and "New York", and the reviews_count to be at least 10.
In the GROUP BY clause, we group by listing_id. This allows us to calculate the average rating for each listing.
In the HAVING clause, we filter the average rating to be at least 4.5.
By joining the tables, filtering, grouping, and running the average function, we're able to get the desired listings in those cities with the specific review conditions.

SQL Question 5: What is the purpose of the SQL constraint UNIQUE?
The UNIQUE constraint is used to ensure the uniqueness of the data in a column or set of columns in a table. It prevents the insertion of duplicate values in the specified column or columns and helps to ensure the integrity and reliability of the data in the database.

For example, say you were on the Marketing Analytics team at Airbnb and were doing some automated keyword research:

Your keyword database might store SEO data like this:

CREATE TABLE keywords (
    keyword_id INTEGER PRIMARY KEY,
    keyword VARCHAR(255) NOT NULL UNIQUE,
    search_volume INTEGER NOT NULL,
    competition FLOAT NOT NULL
);
In this example, the UNIQUE constraint is applied to the "keyword" field to ensure that each keyword is unique. This helps to ensure the integrity of the data in the database and prevents errors that could occur if two rows had the same keyword.

SQL Question 6: Find the Average Number of Guests per Booking in Each City for Airbnb
As an analyst at Airbnb, one of the most useful insights you could provide would be to understand the average number of guests per booking across locations. For this question, we would like you to write a SQL query that will find the average number of guests per booking in each city.

bookings Example Input:
booking_id	property_id	guests	booking_date
101	4523	3	01/01/2022
102	9871	2	01/05/2022
103	4523	4	02/10/2022
104	7452	1	02/20/2022
105	9871	3	03/01/2022
properties Example Input:
property_id	city
4523	New York
9871	Los Angeles
7452	Chicago
Example Output:
city	average_guests
New York	3.5
Los Angeles	2.5
Chicago	1.0
Answer:
SELECT p.city, AVG(b.guests) AS average_guests
FROM bookings b
JOIN properties p ON b.property_id = p.property_id
GROUP BY p.city;
In this query, we first join the bookings and properties tables on property_id, allowing us to access both the city and guests columns in the same query. We then group by city, so we get a separate row for each city in our output. For each group, we calculate the average number of guests.

To practice a very similar question try this interactive Robinhood Cities With Completed Trades Question which is similar for requiring SQL analysis grouped by city or this Amazon Average Review Ratings Question which is similar for needing an average calculation.

SQL Question 7: What's the difference between relational and non-relational databases?
While both types of databases are used to store data (no duh!), relational databases and non-relational (also known as NoSQL databases) differ in a few important ways, most importantly on the way data is stored. Relational databases use a data model consisting of tables and rows, while NoSQL databases use a variety of data models, including document, key-value, columnar, and graph storage formats.

This added flexibilty makes NoSQL databases great for non-tabular data (like hierarchal data or JSON data), or data where the type/format is constantly evolving. With this added flexibility, comes one big weakness – you won't get ACID-compliance. That means, unlike relational databases which are typically adhere to the ACID properties (atomic, consistent, isolated, and durable), you don't get as strong guarentees with most non-relational databases.

SQL Question 8: Analyzing click-through rates for Airbnb Listing Views and Bookings
The scenario is that Airbnb wants to analyze the click-through conversion rates (CTRs) of their listings. The CTR is calculated by dividing the number of bookings by the number of listing views, giving a proportion of views that resulted in a booking.

Consider you have two tables: one showing all the views for a listing (listing_views) and another one showing all bookings (bookings).

listing_views Example Input:
view_id	user_id	visit_date	listing_id
101	10	7/08/2022	1001
102	12	7/08/2022	1002
103	14	7/09/2022	1001
104	10	7/10/2022	1003
105	13	7/11/2022	1002
bookings Example Input:
booking_id	user_id	booking_date	listing_id
201	10	7/09/2022	1001
202	12	7/10/2022	1002
203	15	7/12/2022	1003
204	13	7/13/2022	1002
205	12	7/14/2022	1001
The question is to write a SQL query to find the CTR for every unique listing in July 2022.

Answer:
Here is a PostgreSQL query answer:

WITH total_views AS (
    SELECT 
        listing_id, 
        COUNT(*) AS view_count
    FROM
        listing_views
    WHERE
        DATE_TRUNC('month', visit_date) = '2022-07-01'
    GROUP BY 
        listing_id
),
total_bookings AS (
    SELECT 
        listing_id, 
        COUNT(*) AS booking_count
    FROM
        bookings
    WHERE
        DATE_TRUNC('month', booking_date) = '2022-07-01'
    GROUP BY 
        listing_id
)
SELECT 
    V.listing_id, 
    B.booking_count::decimal / NULLIF(V.view_count, 0) AS CTR
FROM 
    total_views V
LEFT JOIN 
    total_bookings B ON V.listing_id = B.listing_id;
The query utilises two subqueries (total_views and total_bookings) to calculate the counts of views and bookings respectively for each listing for the month of July. The final query joins these subqueries via a LEFT JOIN (to include those listings which were viewed but never booked), and calculates the CTR as booking_count / view_count. The NULLIF function is used to avoid division by zero error.

To practice a similar SQL problem on DataLemur's free online SQL coding environment, solve this Meta SQL interview question:Facebook App CTR SQL Interview question

SQL Question 9: The Most Popular City for Airbnb Stays
As a data analyst for Airbnb, you've been asked to determine the city that has had the most bookings (reservations) in the past year. You are given two tables - a 'bookings' table with booking IDs, user IDs, listing IDs, and booking dates, and a 'listings' table with listing IDs, city locations, and host IDs.

Provide a SQL query that returns the city with the maximum number of bookings, along with the number of bookings.

bookings Example Input:
booking_id	user_id	listing_id	booking_date
101	123	50001	06/08/2022
102	265	69852	06/10/2022
103	362	50001	06/18/2022
104	192	69852	07/26/2022
105	981	69852	07/05/2022
listings Example Input:
listing_id	city	host_id
50001	Amsterdam	876
69852	Barcelona	974
Answer:
SELECT l.city, COUNT(b.booking_id) AS num_bookings
FROM bookings b
JOIN listings l ON b.listing_id = l.listing_id
GROUP BY l.city
ORDER BY num_bookings DESC
LIMIT 1;
This query works by joining the 'bookings' table with the 'listings' table on the shared 'listing_id'. After joining, the 'city' in the 'listings' table is grouped by, and for each city, the number of bookings is counted (by counting the 'booking_id' in the 'bookings' table). The results are then ordered by the number of bookings in descending order, and finally, the top result (the city with the most bookings) is selected. undefined

SQL Question 10: What's the SQL command INTERSECT do, and when would you use it?
The SQL command INTERSECT merges the results of multiple SELECT statements and keeps only those rows that are present in all sets.

For example, say you were doing an HR Analytics project for Airbnb, and had access to Airbnb's employees and contractors data. Assume that some employees were previously contractors, and vice versa, and thus would show up in both tables. You could use INTERSECT operator to find all contractors who also show up in the employees table:

SELECT first_name, last_name
FROM airbnb_contractors

INTERSECT

SELECT first_name, last_name
FROM airbnb_employees
SQL Question 11: Analyze Host Listings and Booking Transactions
As an Airbnb data analyst, you have been asked to analyze the performance of hosts' listings in the past year. Your task is to identify the top 10 listings with the most bookings.

There are two tables involved. One is the 'hosts' table that provides details about each host and its respective listing. The other one is 'bookings' table that records each booking transaction.

The 'hosts' table:

host_id	listing_id	listing_name	city
101	201	Penthouse	New York
102	202	Ocean View	San Francisco
103	203	Country House	Austin
The 'bookings' table:

booking_id	user_id	listing_id	booking_date
301	401	201	06/08/2022
302	402	202	06/10/2022
303	403	202	07/10/2022
304	404	203	08/08/2022
305	405	201	09/08/2022
Remarks:
Both tables can be joined on the 'listing_id' field.

Answer:
SELECT H.listing_id, H.listing_name, COUNT(B.booking_id) AS number_of_bookings 
FROM hosts H
LEFT JOIN bookings B ON H.listing_id = B.listing_id 
GROUP BY H.listing_id, H.listing_name 
ORDER BY number_of_bookings DESC 
LIMIT 10;
This query first joins the 'hosts' and 'bookings' tables based on the 'listing_id' field. It then groups the result by 'listing_id' and 'listing_name' from the 'hosts' table. For each group, it calculates the total number of bookings, and finally sorts the groups based on this count in descending order. It returns the top 10 listings with the most bookings.
13 Product-Sense Interview Questions & Tips for Data Scientists

By
Nick Singh

(Ex-Facebook & Best-Selling Data Science Author)




Updated on

May 3, 2025

Product sense interviews are no walk in the park but don't worry, I've got your back. In this article, we'll explore the ins and outs of product sense interview questions, equipping you with the know-how to tackle them confidently. So, grab a seat, and let's navigate the world of product management hiring together.

Product Sense Interview & Guide

What Does it Mean to Have Product Sense?
Having product sense means possessing a deep understanding of user needs, market dynamics, and strategic thinking essential for successful product management. It involves intuitively grasping the nuances of product development and effectively translating them into innovative solutions.

How Do I Improve my Product Sense?
Improving your product sense involves a combination of research, practice, and continuous learning. Start by immersing yourself in your industry, understanding user needs, and staying updated on market trends.

Practice critical thinking by analyzing products and identifying areas for improvement, and seek feedback from mentors or peers to refine your skills over time. Additionally, actively engage in real-world product projects and leverage resources such as books, courses, and workshops to deepen your understanding of product management principles and strategies.

What is the Product Sense Interview?
The product sense interview evaluates candidates' capacity to comprehend and strategize product development and management effectively. It assesses their critical thinking, innovation, understanding of user needs, and market dynamics—all crucial for product management roles.

Employers use this interview to gauge candidates' product intuition, analytical skills, and communication abilities, essential for driving product strategy and growth within an organization. Through hypothetical scenarios and case studies, candidates demonstrate their product acumen and potential contribution to the company's success.

Do Data Scientists Need to Prepare for the Product Sense Interview?
While data scientists primarily focus on analyzing data, preparing for the product sense interview can still be beneficial. Product sense involves understanding user needs and market dynamics, and valuable skills for data science roles involving product development or collaboration with product teams.

Product Sense Interview

By familiarizing themselves with common product-sense interview questions, data scientists can showcase their versatility and readiness to contribute to broader business objectives, enhancing their candidacy and career opportunities.

Four most common types of Product Interview Questions
Before we dive into the specifics of the product sense interview, it's important to get an understanding of the four most common types of product-focused data science interview questions.

Defining a Product Metric
In product sense interviews, product metric questions gauge your ability to identify and articulate the key performance indicators crucial for assessing a product's success. These questions often require candidates to demonstrate their understanding of various metrics relevant to the specific product or industry and their capacity to define them accurately.

Successful responses to product metric questions showcase the candidate's analytical mindset, strategic thinking, and alignment with the company's objectives and goals.

Diagnosing a Metric Change
Questions focusing on diagnosing a metric change aim to evaluate candidates' proficiency in identifying shifts in product metrics and determining the underlying causes behind these fluctuations. Candidates are challenged to demonstrate their analytical skills by examining data trends, conducting root cause analysis, and proposing hypotheses to explain the observed metric changes.

Effective responses showcase the candidate's ability to think critically, apply data-driven insights, and formulate strategic actions to address and mitigate the impacts of metric fluctuations within the product ecosystem.

Brainstorming Product Features
Questions centered around brainstorming product features assess candidates' creativity, strategic thinking, and understanding of user needs. Candidates are prompted to generate innovative ideas and propose new features or enhancements that align with the product's objectives and target audience.

Successful responses showcase the candidate's ability to identify opportunities for improvement, prioritize features based on impact and feasibility, and articulate compelling value propositions to enhance the product's competitiveness in the market.

Designing A/B Tests
In product sense interviews, questions regarding designing A/B tests evaluate candidates' proficiency in experimental design and their ability to validate hypotheses through controlled experiments. Candidates are tasked with outlining the methodology for conducting A/B tests to measure the impact of changes or features on user behavior or key metrics. Successful responses demonstrate the candidate's understanding of statistical principles, hypothesis formulation, sample sizing, and interpretation of results to make informed product decisions.

Effective communication of the A/B test design process and its alignment with product objectives showcases the candidate's analytical rigor and strategic mindset in optimizing product performance.

How to Answer a Product Sense Interview Question
Answering product sense interview questions involves understanding the context, analyzing thoroughly, and communicating effectively to demonstrate your strategic thinking and product intuition.

Product Sense Interview Preparation

Understand the Context: Begin by fully understanding the context of the question. Clarify any uncertainties and identify the problem's core elements, such as user needs, market dynamics, or product objectives.

Analyze and Strategize: Once you grasp the question's context, analyze it thoroughly and devise a strategic approach. Consider various factors, including potential solutions, feasibility, and anticipated outcomes. Think critically about the problem and prioritize key insights and considerations.

Communicate Effectively: Articulate your analysis and proposed strategy clearly and concisely. Provide relevant context, rationale, and supporting evidence to strengthen your response. Engage with the interviewer, address any follow-up questions, and demonstrate your product intuition and strategic thinking skills.

BTW nothing beats learning in person with your peers, check out these Data Science conferences and sign up to hear from amazing key note speakers!

13 Product Sense Interview Questions and Answers
1. How would you improve the user experience of our current product?
Sample Answer: I would start by conducting user research to identify pain points and gather feedback. Then, I would prioritize improvements based on impact and feasibility, possibly by implementing a more intuitive interface and streamlining the checkout process.

2. Can you provide an example of a successful product you've worked on in the past?
Sample Answer: In my previous role, I worked on a mobile app where I led a feature update that increased user engagement by 30% within three months. We achieved this by implementing personalized recommendations based on user preferences, leading to higher retention rates.

3. How would you determine the target market for a new product?
Sample Answer: I would start by conducting market research to understand demographics, preferences, and behavior patterns. Additionally, I would analyze competitors and identify gaps in the market to pinpoint the target audience that aligns with the product's value proposition.

4. What metrics would you track to measure the success of a subscription-based service?
Sample Answer: Key metrics for a subscription service include subscriber growth rate, churn rate, customer lifetime value, and average revenue per user. These metrics provide insights into user acquisition, retention, and overall revenue generation.

5. How would you prioritize features for a new product?
Sample Answer: I would prioritize features based on user feedback, market research, and strategic objectives. Using techniques like MoSCoW prioritization or impact vs. effort analysis, I would focus on high-impact features that align with the product's core value proposition.

6. Can you describe a time when you had to make a difficult product decision?
Sample Answer: In a previous project, we had to decide whether to invest resources in developing a new feature or improving an existing one. After conducting user research and analyzing data, we determined that improving the existing feature would deliver more value to users in the short term, leading to a unanimous decision among stakeholders.

7. How do you stay updated on industry trends and emerging technologies?
Sample Answer: I regularly attend industry conferences, participate in online forums, and subscribe to newsletters and blogs related to my field. Additionally, I make time for continuous learning through online courses and professional networking events.

8. How would you handle conflicting feedback from stakeholders about a product feature?
Sample Answer: I would first listen carefully to all stakeholders' perspectives to understand their concerns and priorities. Then, I would facilitate a discussion to find common ground and potentially conduct user testing or gather additional data to inform the decision-making process.

9. What factors would you consider when pricing a new product?
Sample Answer: Pricing a new product involves considering factors such as production costs, competitor pricing, perceived value, and target market demographics. By conducting pricing surveys, analyzing pricing models of similar products, and evaluating consumer willingness to pay, I would determine an optimal pricing strategy.

10. How would you approach launching a product in a new market?
Sample Answer: I would start by conducting market research to understand the cultural, economic, and regulatory landscape of the new market. Then, I would tailor the product messaging, distribution channels, and pricing strategy to resonate with the target audience and differentiate it from competitors.

11. Can you discuss a time when you successfully collaborated with cross-functional teams on a product initiative?
Sample Answer: In a previous project, I collaborated with the engineering, design, and marketing teams to launch a new feature within a tight deadline. By establishing clear communication channels, setting expectations, and fostering a collaborative environment, we successfully delivered the feature on time and exceeded user expectations.

12. How would you measure the success of a product beta test?
Sample Answer: I would measure the success of a product beta test by tracking key performance indicators such as user engagement, retention rate, and feedback quality. Additionally, I would compare these metrics against predefined benchmarks and conduct post-beta surveys to gather insights for further product refinement.

13. Can you describe a time when you had to pivot a product strategy based on market feedback?
Sample Answer: In a previous project, we initially planned to target enterprise customers but received overwhelming interest from individual consumers during beta testing. Based on this feedback, we pivoted our strategy to prioritize consumer-oriented features and adjust our marketing messaging accordingly, leading to a successful product launch and increased market penetration.
40 Probability & Statistics Data Science Interview Questions Asked By FAANG & Wall Street
Software Engineering Career Advice
We can't lie - Data Science Interviews are TOUGH. Especially tricky - probability and statistics questions asked by top tech companies & hedge funds during the Data Science, Data Analyst, and the Quant Trading Interview process.

That's why we put together 40 real probability & statistics data science interview questions asked by companies like Facebook, Amazon, Two Sigma, & Bloomberg. We have solutions to all 40 problems, and to 161 other data interview problems on SQL, Machine Learning, and Product/Business Sense  in our book, Ace The Data Science Interview. You can also practice some of these same exact questions on DataLemur's statistics interview questions section.

DataLemur has hundreds of real Statistics and Probability Interview questions, sourced from real Data Science and Data Analyst interviews at companies like Facebook and Google.  
So, without further ado, here are:
the probability & stat concepts to review before your DS interview
20 probability questions asked by top tech-companies & Wall Street
20 statistics questions asked by FANG & Hedge Funds
solutions to 5 of the probability questions
solutions to 5 of the statistics questions
links to more data science interview resources
Probability & Statistics Concepts To Review Before Your Data Science Interview
Because probability & statistics is foundational to the field of Data Science, before the interview you should review:

Probability Basics & Random Variables
Probability Distributions
Hypothesis Testing
Regression Analysis
In case these statistical concepts sound alien to you, check out some of our favorite Statistics Books for Data Analysts to get a gentle refresher.

Probability Basics and Random Variables
The beginnings of probability start with thinking about sample spaces, basic counting and combinatorial principles. Although it is not necessary to know all of the ins-and-outs of combinatorics, it is helpful to understand the basics for simplifying problems. One classic example here is the “stars and bars” counting method.

The other core topic to study is random variables. Knowing concepts related to expectation, variance, covariance, along with the basic probability distributions is crucial.

Probability Distributions
For modeling random variables, knowing the basics of various probability distributions is essential. Understanding both discrete and continuous examples, combined with expectations and variances, is crucial. The most common distributions discussed in interviews are the Uniform and Normal but there are plenty of other well-known distributions for particular use cases (Poisson, Binomial, Geometric).

Most of the time knowing the basics and their applications should suffice. For example, which distribution would flipping a coin be under? What about waiting for an event? It never hurts being able to do the derivations for expectation, variance, or other higher moments.

Hypothesis Testing
Hypothesis testing is the backbone behind statistical inference and can be broken down into a couple of topics. The first is the Central Limit Theorem, which plays an important role in studying large samples of data. Other core elements of hypothesis testing: sampling distributions, p-values, confidence intervals, type I and II errors. Lastly, it is worth looking at various tests involving proportions, and other hypothesis tests.

Most of these concepts play a crucial role in A/B testing, which is a commonly asked topic during interviews at consumer-tech companies like Facebook, Amazon, and Uber. It's useful to not only understand the technical details but also conceptually how A/B testing operates, what the assumptions are, possible pitfalls, and applications to real-life products.

Modeling
Modeling relies on a strong understanding of probability distributions and hypothesis testing. Since it is a broad term, we will refer to modeling as the areas which have a strong statistical intersection with Machine Learning. This includes topics such as: linear regression, maximum likelihood estimation, & bayesian statistics. For interviews focused on modeling and machine learning, knowing these topics is essential.

‍

20 Probability Interview Problems Asked By Top-Tech Companies & Wall Street
[Facebook - Easy] [Coin Fairness Test on DataLemur] There is a fair coin (one side heads, one side tails) and an unfair coin (both sides tails). You pick one at random, flip it 5 times, and observe that it comes up as tails all five times. What is the chance that you are flipping the unfair coin?
[Lyft - Easy] You and your friend are playing a game. The two of you will continue to toss a coin until the sequence HH or TH shows up. If HH shows up first, you win. If TH shows up first, your friend wins. What is the probability of you winning?
[Google - Easy] What is the probability that a seven-game series goes to 7 games?
[Facebook - Easy] Facebook has a content team that labels pieces of content on the platform as spam or not spam. 90% of them are diligent raters and will label 20% of the content as spam and 80% as non-spam. The remaining 10% are non-diligent raters and will label 0% of the content as spam and 100% as non-spam. Assume the pieces of content are labeled independently from one another, for every rater. Given that a rater has labeled 4 pieces of content as good, what is the probability that they are a diligent rater?
[Bloomberg - Easy] Say you draw a circle and choose two chords at random. What is the probability that those chords will intersect?
[Amazon - Easy] 1/1000 people have a particular disease, and there is a test that is 98% correct if you have the disease. If you don’t have the disease, there is a 1% error rate. If someone tests positive, what are the odds they have the disease?
[Facebook - Easy] There are 50 cards of 5 different colors. Each color has cards numbered between 1 to 10. You pick 2 cards at random. What is the probability that they are not of same color and also not of same number?
[Tesla - Easy] A fair six-sided die is rolled twice. What is the probability of getting 1 on the first roll and not getting 6 on the second roll?
[Facebook - Easy] What is the expected number of rolls needed to see all 6 sides of a fair die?
[Microsoft - Easy] Three friends in Seattle each told you it’s rainy, and each person has a 1/3 probability of lying. What is the probability that Seattle is rainy? Assume the probability of rain on any given day in Seattle is 0.25.
[Uber - Easy] Say you roll three dice, one by one. What is the probability that you obtain 3 numbers in a strictly increasing order?
[Bloomberg - Medium] Three ants are sitting at the corners of an equilateral triangle. Each ant randomly picks a direction and starts moving along the edge of the triangle. What is the probability that none of the ants collide? Now, what if it is k ants on all k corners of an equilateral polygon?
[Two Sigma - Medium] What is the expected number of coin flips needed to get two consecutive heads?
[Amazon - Medium] How many cards would you expect to draw from a standard deck before seeing the first ace?
[Robinhood - Medium] A and B are playing a game where A has n+1 coins, B has n coins, and they each flip all of their coins. What is the probability that A will have more heads than B?
[Airbnb - Medium] Say you are given an unfair coin, with an unknown bias towards heads or tails. How can you generate fair odds using this coin?
[Quora - Medium] Say you have N i.i.d. draws of a normal distribution with parameters μ and σ. What is the probability that k of those draws are larger than some value Y?
[Spotify - Hard] A fair die is rolled n times. What is the probability that the largest number rolled is r, for each r in 1..6?
[Snapchat - Hard] There are two groups of n users, A and B, and each user in A is friends with those in B and vice versa. Each user in A will randomly choose a user in B as their best friend and each user in B will randomly choose a user in A as their best friend. If two people have chosen each other, they are mutual best friends. What is the probability that there will be no mutual best friendships?
[Tesla - Hard] Suppose there is a new vehicle launch upcoming. Initial data suggests that any given day there is either a malfunction with some part of the vehicle or possibility of a crash, with probability p which then requires a replacement. Additionally, each vehicle that has been around for n days must be replaced. What is the long-term frequency of vehicle replacements?
20 Statistics Problems Asked By FAANG & Hedge Funds
[Facebook - Easy] How would you explain a confidence interval to a non-technical audience?
[Two Sigma - Easy] Say you are running a multiple linear regression and believe there are several predictors that are correlated. How will the results of the regression be affected if they are indeed correlated? How would you deal with this problem?
[Uber - Easy] Describe p-values in layman’s terms.
[Facebook - Easy] How would you build and test a metric to compare two user’s ranked lists of movie/tv show preferences?
[Microsoft - Easy] Explain the statistical background behind power.
[Twitter - Easy] Describe A/B testing. What are some common pitfalls?
[Google - Medium] How would you derive a confidence interval from a series of coin tosses?
[Stripe - Medium] Say you model the lifetime for a set of customers using an exponential distribution with parameter λ, and you have the lifetime history (in months) of n customers. What is your best guess for λ?
[Lyft - Medium] Derive the mean and variance of the uniform distribution U(a, b).
[Google - Medium] Say we have X ~ Uniform(0, 1) and Y ~ Uniform(0, 1). What is the expected value of the minimum of X and Y?
[Spotify - Medium] You sample from a uniform distribution [0, d] n times. What is your best estimate of d?
[Quora - Medium] You are drawing from a normally distributed random variable X ~ N(0, 1) once a day. What is the approximate expected number of days until you get a value of more than 2?
[Facebook - Medium] Derive the expectation for a geometric distributed random variable.
[Google - Medium] A coin was flipped 1000 times, and 550 times it showed up heads. Do you think the coin is biased? Why or why not?
[Robinhood - Medium] Say you have n integers 1…n and take a random permutation. For any integers i, j let a swap be defined as when the integer i is in the jth position, and vice versa. What is the expected value of the total number of swaps?
[Uber - Hard] What is the difference between MLE and MAP? Describe it mathematically.
[Google - Hard] Say you have two subsets of a dataset for which you know their means and standard deviations. How do you calculate the blended mean and standard deviation of the total dataset? Can you extend it to K subsets?
[Lyft - Hard] How do you randomly sample a point uniformly from a circle with radius 1?
[Two Sigma - Hard] Say you continually sample from some i.i.d. uniformly distributed (0, 1) random variables until the sum of the variables exceeds 1. How many times do you expect to sample?
[Uber - Hard] Given a random Bernoulli trial generator, how do you return a value sampled from a normal distribution
50 A/B Testing Interview Questions & Answers

By
Nick Singh

(Ex-Facebook & Best-Selling Data Science Author)




Updated on

April 28, 2025

A/B testing interview questions appear in about ~50% of data science interviews, especially for Product Data Science roles at consumer-tech companies like Meta, Airbnb, and Uber. To help you prepare, we've curated a list of 50 A/B Testing Interview Questions and Answers broken into the five main types of questions:

Experimental Design Questions
Metric Selection Questions
Interpretation of A/B Test Results Questions
Statistical Power Calculation Questions
Multiple Testing Questions
AB Testing

Section 1: Experimental Design Interview Questions
1. How do you determine the duration of an A/B test? To determine the duration of an A/B test, consider the following factors:

Sample size and statistical significance: The primary factor in determining test duration is reaching a statistically significant result. You need a large enough sample size in each variation to confidently conclude that the observed differences are not due to chance.
Business cycle and seasonality: Consider your business cycle and seasonality when determining test duration. For example, if you're an e-commerce site like Amazon, you may need to run tests for at least a full week to capture behavior across weekdays and weekends.
User behavior and purchasing cycle: Think about your typical user behavior and purchasing cycle. If you're testing a change related to a high-consideration purchase with a long decision cycle, you may need to run the test for several weeks to fully capture the impact on conversions.
Minimum detectable effect: The smaller the minimum improvement you want to be able to detect, the larger the sample size needed and thus the longer the test duration. If you only care about detecting large effects, you can reach significance faster.
2. What are some common pitfalls to avoid when designing an A/B test? Common pitfalls in A/B test design include:

inadequate sample sizes
biased sampling methods
insufficient randomization
running too many experiments at once
In an interview, you usually want to contextualize your answer about A/B testing pitfalls to the business & team at-hand. For example, if you were interviewing at Uber on the Driver Growth division, here are some specific A/B testing issues you might encounter:

Difficulty isolating variables: Driver behavior is influenced by many external factors like local market conditions, seasonality, competitor activity, etc. This can make it challenging to isolate the impact of a specific A/B test variable.
Long time to reach statistical significance: Given the long-term nature of driver acquisition and retention, it may take months for a test to reach statistically-significant results on metrics like driver retention and lifetime value
Potential interference between simultaneous tests: With multiple teams likely running A/B tests concurrently on different aspects of the driver experience (e.g. signup flow, incentives, app features), there is risk of tests interfering with each other and confounding results.
Ethical considerations with underserved segments: If an A/B test inadvertently provides a worse experience to certain underserved driver segments, even if unintentional, it could have outsized negative impact on those groups.
Uber A/B Testing Pitfalls

3. How would you ensure randomization in an A/B test? Randomization in an A/B test can be ensured by randomly assigning participants to treatment and control groups, thereby minimizing the risk of bias and ensuring that the groups are comparable.

4. Can you explain the concept of bucketing in the context of A/B testing? Bucketing refers to the process of assigning participants to treatment and control groups based on predetermined criteria, such as geographic location, device type, or user segment.

5. What considerations should be made when selecting the sample size for an A/B test? Sample size for an A/B test should be determined based on considerations such as the desired level of statistical power, expected effect size, baseline conversion rate, and significance level.

6. What is a control group, and why is it important in A/B testing? The control group serves as a baseline for comparison, allowing researchers to assess the impact of the treatment by comparing outcomes between the treatment and control groups.

7. How would you handle variations in user behavior over time during an A/B test? Variations in user behavior over time can be addressed by conducting the test over a sufficient duration, ensuring that the test period covers different days of the week, times of day, and user segments.

8. Describe the process of creating treatment groups for an A/B test. Treatment groups can be created by randomly assigning participants to different experimental conditions or by using stratified sampling methods to ensure that each group is representative of the population. Usually the in-house A/B testing framework at a company like Facebook or Uber is able to do this for you, automatically!

9. What measures can be taken to minimize the impact of external factors on the results of an A/B test? External factors can be minimized by conducting the test in a controlled environment, implementing safeguards to prevent interference, and monitoring external events that may impact the results.

10. How would you determine the statistical significance level for an A/B test? The statistical significance level, often denoted as alpha (α), is typically set at 0.05 or 0.01, indicating the acceptable probability of falsely rejecting the null hypothesis.

Section 2: Metric Selection Interview Questions
11. What criteria would you use to choose appropriate metrics for an A/B test? Appropriate metrics for an A/B test should be relevant to the business objectives, sensitive to changes in the treatment, reliable, and actionable.

12. Can you differentiate between primary and secondary metrics in A/B testing? Primary metrics are directly related to the primary goal of the experiment, while secondary metrics provide additional insights or context but are not the primary focus.

13. How would you prioritize metrics when they conflict with each other in an A/B test? Prioritization of metrics should consider their alignment with the primary goals, sensitivity to changes, reliability, and practical relevance to the business.

Vanity Metrics

14. What are vanity metrics, and why should they be avoided in A/B testing? Vanity metrics are superficial metrics that may be misleading or irrelevant to the business objectives and should be avoided in A/B testing.

For example, imagine you were interviewing for a Product Data Science role at Meta, and had a question about key metrics to track for Facebook Groups. Here's some potential vanity metrics to avoid mentioning to your interviewer:

Total number of Groups: Tracking the total number of Groups on the platform might seem important, but it doesn't necessarily reflect the health or engagement of those Groups. Many could be inactive or low-quality.
Total number of Group members: Similar to total number of Groups, tracking total Group membership doesn't account for member activity or engagement. A Group could have many members but low participation. Focusing on this could lead to tactics that drive superficial member growth without improving the Group experience.
Number of Group posts: Measuring the raw number of posts in Groups doesn't consider the quality, relevance, or value of those posts. This metric could be gamed by encouraging low-effort, spammy posting just to drive up the numbers, rather than facilitating meaningful conversations.
15. How do you ensure that the selected metrics are relevant to the business goals? Selected metrics should directly reflect the impact of the treatment on the desired outcomes, such as conversion rate, retention rate, revenue, or user satisfaction.

16. Explain the difference between leading and lagging indicators in the context of A/B testing. Leading indicators are predictive metrics that signal future outcomes, while lagging indicators are retrospective metrics that reflect past performance

For example, imagine you were interviewing to be a Data Scientist on Airbnb's Pricing Team. Some leading indicators you could bring up:

Number of hosts viewing the new pricing recommendations: This measures initial engagement with the new pricing feature and predicts future adoption.
Percentage of hosts accepting the pricing suggestions: This indicates the perceived relevance and trustworthiness of the recommendations, predicting future usage.
Change in average listing price: This immediate shift can predict the eventual impact on bookings and revenue.
Lagging Indicators to bring up for the Airbnb Data Scientist Interview:

Host retention and lifetime value: The long-term impact on host satisfaction and retention on the platform is crucial, but will significantly lag the initial pricing changes.
Guest reviews mentioning price: An eventual lagging indicator of guest price perception and satisfaction, which could impact rebookings and word of mouth.
17. How would you handle situations where the chosen metrics may be influenced by external factors? External factors influencing the metrics should be identified and controlled for, or alternative metrics should be selected that are less susceptible to external influences.

18. What role does statistical power play in metric selection for A/B testing? Statistical power considerations should be taken into account when selecting metrics to ensure that they are sensitive enough to detect meaningful differences.

19. Can you provide examples of quantitative and qualitative metrics used in A/B testing? Examples of quantitative metrics include conversion rate, revenue per user, and average session duration, while qualitative metrics include user satisfaction ratings and feedback.

20. How would you measure user engagement in an A/B test? User engagement can be measured using metrics such as session duration, number of page views, click-through rate, or interaction frequency.

Section 3: Interpretation of A/B Test Results Interview Questions
21. What steps would you take to validate the results of an A/B test Validation of A/B test results involves cross-checking with other data sources, conducting sensitivity analyses, and ensuring that the observed effects are consistent and robust.

22. How do you differentiate between statistically significant results and practical significance in A/B testing? Statistical significance alone does not guarantee practical significance; it is essential to consider the magnitude of the effect and its potential impact on the business objectives.

23. What factors could lead to false positives or false negatives in the results of an A/B test? False positives may occur due to random chance or multiple testing, while false negatives may result from inadequate sample sizes or insufficient statistical power.

24. Can you explain the concept of effect size and its relevance in interpreting A/B test results? Effect size quantifies the magnitude of the difference between treatment groups and provides context for interpreting the practical significance of the results.

25. How would you communicate the findings of an A/B test to stakeholders? Communication of A/B test findings should be clear, concise, and tailored to the audience, highlighting key insights, implications, and next steps.

26. What considerations should be made when comparing the performance of multiple variants in an A/B test? Comparison of multiple variants should consider both statistical significance and practical significance, as well as potential trade-offs between different performance metrics.

27. How do you assess the robustness of A/B test results against variations in data distribution? The robustness of A/B test results can be assessed by conducting sensitivity analyses, testing alternative hypotheses, and examining the consistency of results across subgroups.

28. What role does confidence interval play in interpreting the uncertainty of A/B test results? Confidence intervals provide a range of plausible values for the true effect size, accounting for uncertainty in the estimate.

29. How would you handle situations where the results of an A/B test are inconclusive? Inconclusive results may occur due to insufficient sample sizes, unexpected variations in user behavior, or limitations in the experimental design.

30. Can you discuss the importance of considering practical constraints and ethical implications in interpreting A/B test results? Consideration of practical constraints and ethical implications is crucial for interpreting A/B test results responsibly and making informed decisions.

Section 4: Statistical Power Calculation Interview Questions
31. What factors influence the statistical power of an A/B test? Factors influencing the statistical power include sample size, effect size, significance level, and variability in the data.

32. How would you calculate the statistical power for a given A/B test scenario? Statistical power can be calculated using statistical software or online calculators based on the desired level of significance, effect size, and sample size.

33. Can you explain the relationship between sample size, effect size, and statistical power? Sample size, effect size, and statistical power are interrelated, with larger sample sizes and effect sizes leading to higher statistical power.

34. How does the significance level affect the statistical power of an A/B test? The significance level, typically set at 0.05 or 0.01, determines the threshold for rejecting the null hypothesis and affects the statistical power.

35. What measures can be taken to increase the statistical power of an A/B test? Increasing the sample size, choosing more sensitive metrics, or reducing variability in the data can help increase the statistical power of an A/B test.

36. Can you discuss the trade-offs between statistical power and Type I error rate in A/B testing? Trade-offs between statistical power and Type I error rate involve balancing the risk of false positives with the risk of false negatives.

37. How would you determine the appropriate effect size for calculating the statistical power? The appropriate effect size for calculating statistical power depends on the context of the experiment and the magnitude of the expected difference between groups.

38. What role does variability in the data play in estimating the statistical power? Variability in the data, measured by standard deviation or variance, influences the precision of estimates and, consequently, the statistical power.

39. Can you provide examples of scenarios where a low statistical power could lead to misleading conclusions? Low statistical power increases the risk of Type II errors, where true effects may go undetected due to insufficient sample sizes.

40. How do you interpret the results of a power analysis in the context of A/B testing? Interpretation of power analysis results involves assessing whether the chosen sample size provides adequate sensitivity to detect meaningful differences with a desired level of confidence.

For more on Power Calculations read this publication by the Boston University School of Public Health.

Section 5: Handling Multiple Testing Interview Questions
41. What is multiple testing, and why is it a concern in A/B testing? Multiple testing refers to the practice of conducting multiple statistical comparisons simultaneously, leading to an increased risk of false positives..

42. How do you control the family-wise error rate in multiple testing scenarios? Family-wise error rate control methods, such as Bonferroni correction or Holm-Bonferroni method, adjust the significance threshold to account for multiple comparisons.

bonferroni correction formula

43. Can you explain the Bonferroni correction and its application in A/B testing? The Bonferroni correction divides the significance level by the number of comparisons to maintain the overall Type I error rate at the desired level.

44. What are some alternative methods for controlling the Type I error rate in multiple testing? Alternative methods for controlling Type I error rate include false discovery rate (FDR) control and sequential testing procedures.

45. How would you adjust the p-values for multiple comparisons in an A/B test? P-values can be adjusted using methods such as the Benjamini-Hochberg procedure or the Šidák correction to account for multiple comparisons.

46. Can you discuss the trade-offs between different approaches to multiple testing correction? Trade-offs in multiple testing correction involve balancing the risk of false positives with the potential loss of statistical power due to stringent correction methods.

47. What considerations should be made when interpreting results after multiple testing corrections? Interpretation of results after multiple testing corrections should consider both statistical significance and practical significance, as well as potential biases or confounding factors.

48. How do you determine the appropriate correction method based on the specific A/B test scenario? The appropriate correction method depends on factors such as the number of comparisons, the correlation structure of the data, and the desired balance between Type I and Type II error rates.

49. Can you provide examples of situations where failing to correct for multiple testing could lead to erroneous conclusions? Failure to correct for multiple testing can lead to an inflated Type I error rate and erroneous conclusions about the significance of the results.

50. How do you communicate the implications of multiple testing corrections to stakeholders? Communication of the implications of multiple testing corrections to stakeholders involves explaining the rationale behind the correction methods and the impact on the interpretation of the results.
50 Free LeetCode SQL Problems to Help Prepare for Your Next Coding Interview (Part One)
How many of them can you solve?
Jason Chong
Jason Chong

Follow
26 min read
·
Jan 8, 2024
140






Press enter or click to view image in full size

Photo by Max Duzij on Unsplash
If landing a data science role is one of your New Year’s resolutions for 2024, you have come to the right place.

It is no surprise that most companies these days have incorporated technical interviews as part of their recruitment process for data scientists. In these interviews, companies not only want to make sure that you can write SQL queries (as you will be doing a lot of that on the job itself) but more importantly, they are looking to test your ability to structure a problem and communicate your approach to solving it.

So, how do you go about preparing for these coding interviews? Well, fortunately, there is no shortage of websites that can help you do this such as StrataScratch and HackerRank. However, for this blog post, we will be using LeetCode, specifically one of their study plans called SQL 50.

Now, LeetCode is generally used by software engineers to prepare for data structures and algorithms interviews, but I actually found their SQL problems to be fairly reasonable and their website is also very intuitive to use. And the best part? You can solve all these problems completely for free!

SQL 50 is broken down into 7 sections:

Select (5 Questions)
Basic Joins (9 Questions)
Basic Aggregate Functions (8 Questions)
Sorting and Grouping (7 Questions)
Advanced Select and Joins (7 Questions)
Subqueries (7 Questions)
Advanced String Functions, Regex and Clause (7 Questions)
For each problem, I have outlined the question, the input and output tables as well as the solution query. Additional explanation is also included for more complex problems.

Before we get started, it is important to note that this post assumes fundamental knowledge of SQL. If you are not yet familiar with SQL, I would highly encourage first brushing up on the fundamentals before attempting these problems.

Select (5 Questions)
1757. Recyclable and low-fat products
Write a solution to find the IDs of products that are both low-fat and recyclable.

Return the result table in any order.

Input: 
Products table:
+-------------+----------+------------+
| product_id  | low_fats | recyclable |
+-------------+----------+------------+
| 0           | Y        | N          |
| 1           | Y        | Y          |
| 2           | N        | Y          |
| 3           | Y        | Y          |
| 4           | N        | N          |
+-------------+----------+------------+
Output: 
+-------------+
| product_id  |
+-------------+
| 1           |
| 3           |
+-------------+
Explanation: Only products 1 and 3 are both low fat and recyclable.
SELECT product_id
FROM Products
WHERE low_fats='Y'
  AND recyclable='Y'
584. Find customer referee
Find the names of the customers that are not referred by the customer with id=2.

Return the result table in any order.

Input: 
Customer table:
+----+------+------------+
| id | name | referee_id |
+----+------+------------+
| 1  | Will | null       |
| 2  | Jane | null       |
| 3  | Alex | 2          |
| 4  | Bill | null       |
| 5  | Zack | 1          |
| 6  | Mark | 2          |
+----+------+------------+
Output: 
+------+
| name |
+------+
| Will |
| Jane |
| Bill |
| Zack |
+------+
SELECT name
FROM Customer
WHERE referee_id <> 2
  OR referee_id IS NULL
595. Big countries
A country is big if it has an area of at least three million or a population of at least 25 million. Write a solution to find the name, population and area of the big countries.

Return the table in any order.

Input: 
World table:
+-------------+-----------+---------+------------+--------------+
| name        | continent | area    | population | gdp          |
+-------------+-----------+---------+------------+--------------+
| Afghanistan | Asia      | 652230  | 25500100   | 20343000000  |
| Albania     | Europe    | 28748   | 2831741    | 12960000000  |
| Algeria     | Africa    | 2381741 | 37100000   | 188681000000 |
| Andorra     | Europe    | 468     | 78115      | 3712000000   |
| Angola      | Africa    | 1246700 | 20609294   | 100990000000 |
+-------------+-----------+---------+------------+--------------+
Output: 
+-------------+------------+---------+
| name        | population | area    |
+-------------+------------+---------+
| Afghanistan | 25500100   | 652230  |
| Algeria     | 37100000   | 2381741 |
+-------------+------------+---------+
SELECT
  name
  , population
  , area
FROM World 
WHERE area>=3000000
  OR population>=25000000
1148. Article views I
Write a solution to find all the authors who viewed at least one of their own articles. Return the result table sorted by id in ascending order.

Input: 
Views table:
+------------+-----------+-----------+------------+
| article_id | author_id | viewer_id | view_date  |
+------------+-----------+-----------+------------+
| 1          | 3         | 5         | 2019-08-01 |
| 1          | 3         | 6         | 2019-08-02 |
| 2          | 7         | 7         | 2019-08-01 |
| 2          | 7         | 6         | 2019-08-02 |
| 4          | 7         | 1         | 2019-07-22 |
| 3          | 4         | 4         | 2019-07-21 |
| 3          | 4         | 4         | 2019-07-21 |
+------------+-----------+-----------+------------+
Output: 
+------+
| id   |
+------+
| 4    |
| 7    |
+------+
SELECT DISTINCT author_id AS id
FROM Views 
WHERE author_id=viewer_id
ORDER BY author_id
1683. Invalid tweets
Write a solution to find the IDs of the invalid tweets. The tweet is invalid if the number of characters used in the content of the tweet is strictly greater than 15.

Return the result table in any order.

Input: 
Tweets table:
+----------+----------------------------------+
| tweet_id | content                          |
+----------+----------------------------------+
| 1        | Vote for Biden                   |
| 2        | Let us make America great again! |
+----------+----------------------------------+
Output: 
+----------+
| tweet_id |
+----------+
| 2        |
+----------+
Explanation: 
Tweet 1 has length = 14. It is a valid tweet.
Tweet 2 has length = 32. It is an invalid tweet.
SELECT tweet_id
FROM Tweets
WHERE LENGTH(content)>15
Basic Joins (9 Questions)
1378. Replace employee ID with the unique identifier
Write a solution to show the unique ID of each user. If a user does not have a unique ID, replace just null.

Return the result table in any order.

Input: 
Employees table:
+----+----------+
| id | name     |
+----+----------+
| 1  | Alice    |
| 7  | Bob      |
| 11 | Meir     |
| 90 | Winston  |
| 3  | Jonathan |
+----+----------+
EmployeeUNI table:
+----+-----------+
| id | unique_id |
+----+-----------+
| 3  | 1         |
| 11 | 2         |
| 90 | 3         |
+----+-----------+
Output: 
+-----------+----------+
| unique_id | name     |
+-----------+----------+
| null      | Alice    |
| null      | Bob      |
| 2         | Meir     |
| 3         | Winston  |
| 1         | Jonathan |
+-----------+----------+
Explanation: 
Alice and Bob do not have a unique ID, We will show null instead.
The unique ID of Meir is 2.
The unique ID of Winston is 3.
The unique ID of Jonathan is 1.
SELECT
  eu.unique_id
  , e.name
FROM Employees e
LEFT JOIN EmployeeUNI eu
  ON e.id=eu.id
1068. Product sales analysis I
Write a solution to report the product_name, year and price for each sale_id in the Sales table.

Return the resulting table in any order.

Input: 
Sales table:
+---------+------------+------+----------+-------+
| sale_id | product_id | year | quantity | price |
+---------+------------+------+----------+-------+ 
| 1       | 100        | 2008 | 10       | 5000  |
| 2       | 100        | 2009 | 12       | 5000  |
| 7       | 200        | 2011 | 15       | 9000  |
+---------+------------+------+----------+-------+
Product table:
+------------+--------------+
| product_id | product_name |
+------------+--------------+
| 100        | Nokia        |
| 200        | Apple        |
| 300        | Samsung      |
+------------+--------------+
Output: 
+--------------+-------+-------+
| product_name | year  | price |
+--------------+-------+-------+
| Nokia        | 2008  | 5000  |
| Nokia        | 2009  | 5000  |
| Apple        | 2011  | 9000  |
+--------------+-------+-------+
Explanation: 
From sale_id = 1, we can conclude that Nokia was sold for 5000 in the year 2008.
From sale_id = 2, we can conclude that Nokia was sold for 5000 in the year 2009.
From sale_id = 7, we can conclude that Apple was sold for 9000 in the year 2011.
SELECT
  p.product_name
  , s.year
  , s.price 
FROM Sales s
LEFT JOIN Product p
  ON s.product_id=p.product_id
1581. Customers who visited but did not make any transactions
Write a solution to find the IDs of the users who visited without making any transactions and the number of times they made these types of visits.

Return the result table sorted in any order.

Input: 
Visits
+----------+-------------+
| visit_id | customer_id |
+----------+-------------+
| 1        | 23          |
| 2        | 9           |
| 4        | 30          |
| 5        | 54          |
| 6        | 96          |
| 7        | 54          |
| 8        | 54          |
+----------+-------------+
Transactions
+----------------+----------+--------+
| transaction_id | visit_id | amount |
+----------------+----------+--------+
| 2              | 5        | 310    |
| 3              | 5        | 300    |
| 9              | 5        | 200    |
| 12             | 1        | 910    |
| 13             | 2        | 970    |
+----------------+----------+--------+
Output: 
+-------------+----------------+
| customer_id | count_no_trans |
+-------------+----------------+
| 54          | 2              |
| 30          | 1              |
| 96          | 1              |
+-------------+----------------+
Explanation: 
Customer with id = 23 visited the mall once and made one transaction during the visit with id = 12.
Customer with id = 9 visited the mall once and made one transaction during the visit with id = 13.
Customer with id = 30 visited the mall once and did not make any transactions.
Customer with id = 54 visited the mall three times. During 2 visits they did not make any transactions, and during one visit they made 3 transactions.
Customer with id = 96 visited the mall once and did not make any transactions.
As we can see, users with IDs 30 and 96 visited the mall one time without making any transactions. Also, user 54 visited the mall twice and did not make any transactions.
SELECT
  v.customer_id
  , COUNT(*) AS count_no_trans
FROM Visits v
LEFT JOIN Transactions t
  ON v.visit_id=t.visit_id
WHERE transaction_id IS NULL
GROUP BY 1
197. Rising temperature
Write a solution to find all date IDs with higher temperatures compared to its previous dates (yesterday).

Return the result table in any order.

Input: 
Weather table:
+----+------------+-------------+
| id | recordDate | temperature |
+----+------------+-------------+
| 1  | 2015-01-01 | 10          |
| 2  | 2015-01-02 | 25          |
| 3  | 2015-01-03 | 20          |
| 4  | 2015-01-04 | 30          |
+----+------------+-------------+
Output: 
+----+
| id |
+----+
| 2  |
| 4  |
+----+
Explanation: 
In 2015-01-02, the temperature was higher than the previous day (10 -> 25).
In 2015-01-04, the temperature was higher than the previous day (20 -> 30).
SELECT w1.id
FROM Weather w1, Weather w2
WHERE w1.temperature > w2.temperature
  AND DATEDIFF(w1.recordDate, w2.recordDate) = 1
1661. Average time of process per machine
There is a factory website that has several machines each running the same number of processes. Write a solution to find the average time each machine takes to complete a process.

The time to complete a process is the end timestamp minus the start timestamp. The average time is calculated by the total time to complete every process on the machine divided by the number of processes that were run.

The resulting table should have the machine_id along with the average time as processing_time, which should be rounded to 3 decimal places.

Return the result table in any order.

Input: 
Activity table:
+------------+------------+---------------+-----------+
| machine_id | process_id | activity_type | timestamp |
+------------+------------+---------------+-----------+
| 0          | 0          | start         | 0.712     |
| 0          | 0          | end           | 1.520     |
| 0          | 1          | start         | 3.140     |
| 0          | 1          | end           | 4.120     |
| 1          | 0          | start         | 0.550     |
| 1          | 0          | end           | 1.550     |
| 1          | 1          | start         | 0.430     |
| 1          | 1          | end           | 1.420     |
| 2          | 0          | start         | 4.100     |
| 2          | 0          | end           | 4.512     |
| 2          | 1          | start         | 2.500     |
| 2          | 1          | end           | 5.000     |
+------------+------------+---------------+-----------+
Output: 
+------------+-----------------+
| machine_id | processing_time |
+------------+-----------------+
| 0          | 0.894           |
| 1          | 0.995           |
| 2          | 1.456           |
+------------+-----------------+
Explanation: 
There are 3 machines running 2 processes each.
Machine 0's average time is ((1.520 - 0.712) + (4.120 - 3.140)) / 2 = 0.894
Machine 1's average time is ((1.550 - 0.550) + (1.420 - 0.430)) / 2 = 0.995
Machine 2's average time is ((4.512 - 4.100) + (5.000 - 2.500)) / 2 = 1.456
SELECT 
  a1.machine_id
  , ROUND(AVG(a2.timestamp - a1.timestamp), 3) AS processing_time
FROM Activity a1
INNER JOIN Activity a2
ON a1.machine_id=a2.machine_id
  AND a1.process_id=a2.process_id
  AND a1.activity_type='start'
  AND a2.activity_type='end'
GROUP BY a1.machine_id
577. Employee bonus
Write a solution to report the name and bonus amount of each employee with a bonus of less than 1,000.

Return the result table in any order.

Input: 
Employee table:
+-------+--------+------------+--------+
| empId | name   | supervisor | salary |
+-------+--------+------------+--------+
| 3     | Brad   | null       | 4000   |
| 1     | John   | 3          | 1000   |
| 2     | Dan    | 3          | 2000   |
| 4     | Thomas | 3          | 4000   |
+-------+--------+------------+--------+
Bonus table:
+-------+-------+
| empId | bonus |
+-------+-------+
| 2     | 500   |
| 4     | 2000  |
+-------+-------+
Output: 
+------+-------+
| name | bonus |
+------+-------+
| Brad | null  |
| John | null  |
| Dan  | 500   |
+------+-------+
SELECT
  e.name
  , b.bonus
FROM Employee e
LEFT JOIN Bonus b
  ON e.empId=b.empId
WHERE bonus<1000
  OR bonus IS NULL
1280. Students and examinations
Write a solution to find the number of times each student attended each exam.

Return the result table ordered by student_id and subject_name.

Input: 
Students table:
+------------+--------------+
| student_id | student_name |
+------------+--------------+
| 1          | Alice        |
| 2          | Bob          |
| 13         | John         |
| 6          | Alex         |
+------------+--------------+
Subjects table:
+--------------+
| subject_name |
+--------------+
| Math         |
| Physics      |
| Programming  |
+--------------+
Examinations table:
+------------+--------------+
| student_id | subject_name |
+------------+--------------+
| 1          | Math         |
| 1          | Physics      |
| 1          | Programming  |
| 2          | Programming  |
| 1          | Physics      |
| 1          | Math         |
| 13         | Math         |
| 13         | Programming  |
| 13         | Physics      |
| 2          | Math         |
| 1          | Math         |
+------------+--------------+
Output: 
+------------+--------------+--------------+----------------+
| student_id | student_name | subject_name | attended_exams |
+------------+--------------+--------------+----------------+
| 1          | Alice        | Math         | 3              |
| 1          | Alice        | Physics      | 2              |
| 1          | Alice        | Programming  | 1              |
| 2          | Bob          | Math         | 1              |
| 2          | Bob          | Physics      | 0              |
| 2          | Bob          | Programming  | 1              |
| 6          | Alex         | Math         | 0              |
| 6          | Alex         | Physics      | 0              |
| 6          | Alex         | Programming  | 0              |
| 13         | John         | Math         | 1              |
| 13         | John         | Physics      | 1              |
| 13         | John         | Programming  | 1              |
+------------+--------------+--------------+----------------+
Explanation: 
The result table should contain all students and all subjects.
Alice attended the Math exam 3 times, the Physics exam 2 times, and the Programming exam 1 time.
Bob attended the Math exam 1 time, the Programming exam 1 time, and did not attend the Physics exam.
Alex did not attend any exams.
John attended the Math exam 1 time, the Physics exam 1 time, and the Programming exam 1 time.
SELECT 
  s.student_id
  , s.student_name
  , sub.subject_name
  , COUNT(e.student_id) AS attended_exams
FROM Students s
CROSS JOIN Subjects sub
LEFT JOIN Examinations e
  ON s.student_id=e.student_id
  AND sub.subject_name=e.subject_name
GROUP BY 1, 2, 3
ORDER BY 1, 3
The cross join here is to ensure that we have every unique combination of student and subject. In the event a student has not attended an exam for a particular subject, it will have a value of zero.

570. Managers with at least 5 direct reports
Write a solution to find managers with at least five direct reports.

Return the result table in any order.

Input: 
Employee table:
+-----+-------+------------+-----------+
| id  | name  | department | managerId |
+-----+-------+------------+-----------+
| 101 | John  | A          | null      |
| 102 | Dan   | A          | 101       |
| 103 | James | A          | 101       |
| 104 | Amy   | A          | 101       |
| 105 | Anne  | A          | 101       |
| 106 | Ron   | B          | 101       |
+-----+-------+------------+-----------+
Output: 
+------+
| name |
+------+
| John |
+------+
SELECT name
FROM Employee 
WHERE id IN (SELECT managerId FROM Employee GROUP BY 1 HAVING COUNT(1) >= 5)
SELECT e.name
FROM Employee e 
INNER JOIN Employee m
  ON e.id=m.managerId 
GROUP BY m.managerId 
HAVING COUNT(m.managerId) >= 5
There are two slightly different ways to approach this problem, but both use a similar logic.

1934. Confirmation rate
The confirmation rate of a user is the number of confirmed messages divided by the total number of requested confirmation messages. The confirmation rate of a user that did not request any confirmation messages is 0. Round the confirmation rate to two decimal places.

Write a solution to find the confirmation rate of each user.

Return the result table in any order.

Input: 
Signups table:
+---------+---------------------+
| user_id | time_stamp          |
+---------+---------------------+
| 3       | 2020-03-21 10:16:13 |
| 7       | 2020-01-04 13:57:59 |
| 2       | 2020-07-29 23:09:44 |
| 6       | 2020-12-09 10:39:37 |
+---------+---------------------+
Confirmations table:
+---------+---------------------+-----------+
| user_id | time_stamp          | action    |
+---------+---------------------+-----------+
| 3       | 2021-01-06 03:30:46 | timeout   |
| 3       | 2021-07-14 14:00:00 | timeout   |
| 7       | 2021-06-12 11:57:29 | confirmed |
| 7       | 2021-06-13 12:58:28 | confirmed |
| 7       | 2021-06-14 13:59:27 | confirmed |
| 2       | 2021-01-22 00:00:00 | confirmed |
| 2       | 2021-02-28 23:59:59 | timeout   |
+---------+---------------------+-----------+
Output: 
+---------+-------------------+
| user_id | confirmation_rate |
+---------+-------------------+
| 6       | 0.00              |
| 3       | 0.00              |
| 7       | 1.00              |
| 2       | 0.50              |
+---------+-------------------+
Explanation: 
User 6 did not request any confirmation messages. The confirmation rate is 0.
User 3 made 2 requests and both timed out. The confirmation rate is 0.
User 7 made 3 requests and all were confirmed. The confirmation rate is 1.
User 2 made 2 requests where one was confirmed and the other timed out. The confirmation rate is 1 / 2 = 0.5.
SELECT 
  s.user_id,
  ROUND(AVG(IF(c.action='confirmed', 1, 0)), 2) AS confirmation_rate
FROM Signups s
LEFT JOIN Confirmations c
  ON s.user_id=c.user_id
GROUP BY user_id
Basic Aggregate Functions (8 Questions)
620. Not boring movies
Write a solution to report the movies with an odd-numbered ID and a description that is not boring.

Return the result table ordered by rating in descending order.

Input: 
Cinema table:
+----+------------+-------------+--------+
| id | movie      | description | rating |
+----+------------+-------------+--------+
| 1  | War        | great 3D    | 8.9    |
| 2  | Science    | fiction     | 8.5    |
| 3  | irish      | boring      | 6.2    |
| 4  | Ice song   | Fantacy     | 8.6    |
| 5  | House card | Interesting | 9.1    |
+----+------------+-------------+--------+
Output: 
+----+------------+-------------+--------+
| id | movie      | description | rating |
+----+------------+-------------+--------+
| 5  | House card | Interesting | 9.1    |
| 1  | War        | great 3D    | 8.9    |
+----+------------+-------------+--------+
Explanation: 
We have three movies with odd-numbered IDs: 1, 3, and 5. The movie with ID = 3 is boring so we do not include it in the answer.
SELECT *
FROM Cinema
WHERE MOD(id, 2) = 1
  AND description <> 'boring'
ORDER BY rating DESC
1251. Average selling price
Write a solution to find the average selling price for each product. average_price should be rounded to two decimal places.

Return the result table in any order.

Input: 
Prices table:
+------------+------------+------------+--------+
| product_id | start_date | end_date   | price  |
+------------+------------+------------+--------+
| 1          | 2019-02-17 | 2019-02-28 | 5      |
| 1          | 2019-03-01 | 2019-03-22 | 20     |
| 2          | 2019-02-01 | 2019-02-20 | 15     |
| 2          | 2019-02-21 | 2019-03-31 | 30     |
+------------+------------+------------+--------+
UnitsSold table:
+------------+---------------+-------+
| product_id | purchase_date | units |
+------------+---------------+-------+
| 1          | 2019-02-25    | 100   |
| 1          | 2019-03-01    | 15    |
| 2          | 2019-02-10    | 200   |
| 2          | 2019-03-22    | 30    |
+------------+---------------+-------+
Output: 
+------------+---------------+
| product_id | average_price |
+------------+---------------+
| 1          | 6.96          |
| 2          | 16.96         |
+------------+---------------+
Explanation: 
Average selling price = Total Price of Product / Number of products sold.
Average selling price for product 1 = ((100 * 5) + (15 * 20)) / 115 = 6.96
Average selling price for product 2 = ((200 * 15) + (30 * 30)) / 230 = 16.96
SELECT 
  p.product_id
  , COALESCE(round(SUM(units*price)/SUM(units), 2), 0) AS average_price
FROM Prices p
LEFT JOIN UnitsSold u
  ON p.product_id=u.product_id
  AND u.purchase_date BETWEEN p.start_date AND p.end_date
GROUP BY 1
1075. Project employees I
Write an SQL query that reports the average experience years of all the employees for each project, rounded to two digits.

Return the result table in any order.

Input: 
Project table:
+-------------+-------------+
| project_id  | employee_id |
+-------------+-------------+
| 1           | 1           |
| 1           | 2           |
| 1           | 3           |
| 2           | 1           |
| 2           | 4           |
+-------------+-------------+
Employee table:
+-------------+--------+------------------+
| employee_id | name   | experience_years |
+-------------+--------+------------------+
| 1           | Khaled | 3                |
| 2           | Ali    | 2                |
| 3           | John   | 1                |
| 4           | Doe    | 2                |
+-------------+--------+------------------+
Output: 
+-------------+---------------+
| project_id  | average_years |
+-------------+---------------+
| 1           | 2.00          |
| 2           | 2.50          |
+-------------+---------------+
Explanation: The average experience years for the first project is (3 + 2 + 1) / 3 = 2.00 and for the second project is (3 + 2) / 2 = 2.50
SELECT
  project_id
  , ROUND(AVG(experience_years), 2) AS average_years
FROM Project p
LEFT JOIN Employee e
  ON p.employee_id=e.employee_id  
GROUP BY 1
1633. Percentage of users who attended a contest
Write a solution to find the percentage of the users registered in each contest rounded to two decimals.

Return the result table ordered by percentage in descending order. In case of a tie, order it by contest_id in ascending order.

Input: 
Users table:
+---------+-----------+
| user_id | user_name |
+---------+-----------+
| 6       | Alice     |
| 2       | Bob       |
| 7       | Alex      |
+---------+-----------+
Register table:
+------------+---------+
| contest_id | user_id |
+------------+---------+
| 215        | 6       |
| 209        | 2       |
| 208        | 2       |
| 210        | 6       |
| 208        | 6       |
| 209        | 7       |
| 209        | 6       |
| 215        | 7       |
| 208        | 7       |
| 210        | 2       |
| 207        | 2       |
| 210        | 7       |
+------------+---------+
Output: 
+------------+------------+
| contest_id | percentage |
+------------+------------+
| 208        | 100.0      |
| 209        | 100.0      |
| 210        | 100.0      |
| 215        | 66.67      |
| 207        | 33.33      |
+------------+------------+
Explanation: 
All the users registered in contests 208, 209, and 210. The percentage is 100% and we sort them in the answer table by contest_id in ascending order.
Alice and Alex registered in contest 215 and the percentage is ((2/3) * 100) = 66.67%
Bob registered in contest 207 and the percentage is ((1/3) * 100) = 33.33%
SELECT 
  contest_id
  , ROUND(100*COUNT(1)/(SELECT COUNT(DISTINCT user_id) FROM Users), 2) AS percentage
FROM Register
GROUP BY contest_id
ORDER BY percentage DESC, contest_id
1211. Queries quality and percentage
We define query quality as the average of the ratio between query rating and its position. We also define poor_query_percentage as the percentage of all queries with rating less than three.

Both quality and poor_query_percentage should be rounded to two decimal places.

Return the result table in any order.

Input: 
Queries table:
+------------+-------------------+----------+--------+
| query_name | result            | position | rating |
+------------+-------------------+----------+--------+
| Dog        | Golden Retriever  | 1        | 5      |
| Dog        | German Shepherd   | 2        | 5      |
| Dog        | Mule              | 200      | 1      |
| Cat        | Shirazi           | 5        | 2      |
| Cat        | Siamese           | 3        | 3      |
| Cat        | Sphynx            | 7        | 4      |
+------------+-------------------+----------+--------+
Output: 
+------------+---------+-----------------------+
| query_name | quality | poor_query_percentage |
+------------+---------+-----------------------+
| Dog        | 2.50    | 33.33                 |
| Cat        | 0.66    | 33.33                 |
+------------+---------+-----------------------+
Explanation: 
Dog queries quality is ((5 / 1) + (5 / 2) + (1 / 200)) / 3 = 2.50
Dog queries poor_ query_percentage is (1 / 3) * 100 = 33.33

Cat queries quality equals ((2 / 5) + (3 / 3) + (4 / 7)) / 3 = 0.66
Cat queries poor_ query_percentage is (1 / 3) * 100 = 33.33
SELECT 
  query_name
  , ROUND(AVG(rating/position), 2) AS quality
  , ROUND(100*AVG(IF(rating<3, 1, 0)), 2) AS poor_query_percentage
FROM Queries
WHERE query_name IS NOT NULL 
GROUP BY 1
1193. Monthly transactions I
Write an SQL query to find for each month and country, the number of transactions and their total amount, the number of approved transactions and their total amount.

Return the result table in any order.

Input: 
Transactions table:
+------+---------+----------+--------+------------+
| id   | country | state    | amount | trans_date |
+------+---------+----------+--------+------------+
| 121  | US      | approved | 1000   | 2018-12-18 |
| 122  | US      | declined | 2000   | 2018-12-19 |
| 123  | US      | approved | 2000   | 2019-01-01 |
| 124  | DE      | approved | 2000   | 2019-01-07 |
+------+---------+----------+--------+------------+
Output: 
+----------+---------+-------------+----------------+--------------------+-----------------------+
| month    | country | trans_count | approved_count | trans_total_amount | approved_total_amount |
+----------+---------+-------------+----------------+--------------------+-----------------------+
| 2018-12  | US      | 2           | 1              | 3000               | 1000                  |
| 2019-01  | US      | 1           | 1              | 2000               | 2000                  |
| 2019-01  | DE      | 1           | 1              | 2000               | 2000                  |
+----------+---------+-------------+----------------+--------------------+-----------------------+
SELECT 
  date_format(trans_date, '%Y-%m') AS month
  , country
  , COUNT(1) AS trans_count
  , SUM(IF(state='approved', 1, 0)) AS approved_count
  , SUM(amount) as trans_total_amount
  , SUM(IF(state='approved', amount, 0)) AS approved_total_amount
FROM Transactions
GROUP BY 1, 2
1174. Immediate food delivery II
If the customer’s preferred delivery date is the same as the order date, then the order is called immediate. Otherwise, it is called scheduled.

The first order of a customer is the order with the earliest order date that the customer made. It is guaranteed that a customer has precisely one first order.

Write a solution to find the percentage of immediate orders in the first orders of all customers, rounded to two decimal places.

Input: 
Delivery table:
+-------------+-------------+------------+-----------------------------+
| delivery_id | customer_id | order_date | customer_pref_delivery_date |
+-------------+-------------+------------+-----------------------------+
| 1           | 1           | 2019-08-01 | 2019-08-02                  |
| 2           | 2           | 2019-08-02 | 2019-08-02                  |
| 3           | 1           | 2019-08-11 | 2019-08-12                  |
| 4           | 3           | 2019-08-24 | 2019-08-24                  |
| 5           | 3           | 2019-08-21 | 2019-08-22                  |
| 6           | 2           | 2019-08-11 | 2019-08-13                  |
| 7           | 4           | 2019-08-09 | 2019-08-09                  |
+-------------+-------------+------------+-----------------------------+
Output: 
+----------------------+
| immediate_percentage |
+----------------------+
| 50.00                |
+----------------------+
Explanation: 
The customer id 1 has a first order with delivery id 1 and it is scheduled.
The customer id 2 has a first order with delivery id 2 and it is immediate.
The customer id 3 has a first order with delivery id 5 and it is scheduled.
The customer id 4 has a first order with delivery id 7 and it is immediate.
Hence, half the customers have immediate first orders.
SELECT ROUND(100*AVG(order_date=customer_pref_delivery_date), 2) AS immediate_percentage
FROM Delivery
WHERE (customer_id, order_date) IN (SELECT customer_id, MIN(order_date) FROM Delivery GROUP BY 1)
550. Game play analysis IV
Write a solution to report the fraction of players that logged in again on the day after the day they first logged in, rounded to two decimal places. In other words, you need to count the number of players that logged in for at least two consecutive days starting from their first login date, then divide that number by the total number of players.

Input: 
Activity table:
+-----------+-----------+------------+--------------+
| player_id | device_id | event_date | games_played |
+-----------+-----------+------------+--------------+
| 1         | 2         | 2016-03-01 | 5            |
| 1         | 2         | 2016-03-02 | 6            |
| 2         | 3         | 2017-06-25 | 1            |
| 3         | 1         | 2016-03-02 | 0            |
| 3         | 4         | 2018-07-03 | 5            |
+-----------+-----------+------------+--------------+
Output: 
+-----------+
| fraction  |
+-----------+
| 0.33      |
+-----------+
Explanation: 
Only the player with id 1 logged back in after the first day he had logged in so the answer is 1/3 = 0.33
SELECT ROUND(COUNT(DISTINCT player_id)/(SELECT COUNT(DISTINCT player_id) FROM Activity), 2) AS fraction
FROM Activity
WHERE (player_id, date_sub(event_date, INTERVAL 1 DAY)) IN (SELECT player_id, MIN(event_date) FROM Activity GROUP BY 1)
Sorting and Grouping (7 Questions)
2356. Number of unique subjects taught by each teacher
Write a solution to calculate the number of unique subjects each teacher teaches in the university.

Return the result table in any order.

Input: 
Teacher table:
+------------+------------+---------+
| teacher_id | subject_id | dept_id |
+------------+------------+---------+
| 1          | 2          | 3       |
| 1          | 2          | 4       |
| 1          | 3          | 3       |
| 2          | 1          | 1       |
| 2          | 2          | 1       |
| 2          | 3          | 1       |
| 2          | 4          | 1       |
+------------+------------+---------+
Output:  
+------------+-----+
| teacher_id | cnt |
+------------+-----+
| 1          | 2   |
| 2          | 4   |
+------------+-----+
Explanation: 
Teacher 1:
  - They teach subject 2 in departments 3 and 4.
  - They teach subject 3 in department 3.
Teacher 2:
  - They teach subject 1 in department 1.
  - They teach subject 2 in department 1.
  - They teach subject 3 in department 1.
  - They teach subject 4 in department 1.
SELECT
  teacher_id
  , COUNT(DISTINCT subject_id) AS cnt
FROM Teacher
GROUP BY 1
1141. User activity for the past 30 days I
Write a solution to find the daily active user count for a period of 30 days ending 2019–07–27 inclusively. A user was active on someday if they made at least one activity on that day.

Return the result table in any order.

Input: 
Activity table:
+---------+------------+---------------+---------------+
| user_id | session_id | activity_date | activity_type |
+---------+------------+---------------+---------------+
| 1       | 1          | 2019-07-20    | open_session  |
| 1       | 1          | 2019-07-20    | scroll_down   |
| 1       | 1          | 2019-07-20    | end_session   |
| 2       | 4          | 2019-07-20    | open_session  |
| 2       | 4          | 2019-07-21    | send_message  |
| 2       | 4          | 2019-07-21    | end_session   |
| 3       | 2          | 2019-07-21    | open_session  |
| 3       | 2          | 2019-07-21    | send_message  |
| 3       | 2          | 2019-07-21    | end_session   |
| 4       | 3          | 2019-06-25    | open_session  |
| 4       | 3          | 2019-06-25    | end_session   |
+---------+------------+---------------+---------------+
Output: 
+------------+--------------+ 
| day        | active_users |
+------------+--------------+ 
| 2019-07-20 | 2            |
| 2019-07-21 | 2            |
+------------+--------------+ 
Explanation: Note that we do not care about days with zero active users.
SELECT 
  activity_date AS day,
  COUNT(DISTINCT user_id) AS active_users
FROM Activity
WHERE activity_date BETWEEN '2019-06-28' AND '2019-07-27'
GROUP BY 1
1070. Product sales analysis III
Write a solution to select the product_id, year, quantity and price for the first year of every product sold.

Return the resulting table in any order.

Input: 
Sales table:
+---------+------------+------+----------+-------+
| sale_id | product_id | year | quantity | price |
+---------+------------+------+----------+-------+ 
| 1       | 100        | 2008 | 10       | 5000  |
| 2       | 100        | 2009 | 12       | 5000  |
| 7       | 200        | 2011 | 15       | 9000  |
+---------+------------+------+----------+-------+
Product table:
+------------+--------------+
| product_id | product_name |
+------------+--------------+
| 100        | Nokia        |
| 200        | Apple        |
| 300        | Samsung      |
+------------+--------------+
Output: 
+------------+------------+----------+-------+
| product_id | first_year | quantity | price |
+------------+------------+----------+-------+ 
| 100        | 2008       | 10       | 5000  |
| 200        | 2011       | 15       | 9000  |
+------------+------------+----------+-------+
SELECT
  product_id
  , year AS first_year
  , quantity
  , price
FROM Sales
WHERE (product_id, year) IN (SELECT product_id, MIN(year) FROM Sales GROUP BY 1)
596. Classes with more than 5 students
Write a solution to find all the classes that have at least five students.

Return the result table in any order.

Input: 
Courses table:
+---------+----------+
| student | class    |
+---------+----------+
| A       | Math     |
| B       | English  |
| C       | Math     |
| D       | Biology  |
| E       | Math     |
| F       | Computer |
| G       | Math     |
| H       | Math     |
| I       | Math     |
+---------+----------+
Output: 
+---------+
| class   |
+---------+
| Math    |
+---------+
Explanation: 
- Math has 6 students, so we include it.
- English has 1 student, so we do not include it.
- Biology has 1 student, so we do not include it.
- Computer has 1 student, so we do not include it.
SELECT class
FROM Courses
GROUP BY 1
HAVING COUNT(*) >= 5
1729. Find followers count
Write a solution that will, for each user, return the number of followers.

Return the result table ordered by user_id in ascending order.

Input: 
Followers table:
+---------+-------------+
| user_id | follower_id |
+---------+-------------+
| 0       | 1           |
| 1       | 0           |
| 2       | 0           |
| 2       | 1           |
+---------+-------------+
Output: 
+---------+----------------+
| user_id | followers_count|
+---------+----------------+
| 0       | 1              |
| 1       | 1              |
| 2       | 2              |
+---------+----------------+
Explanation: 
The followers of 0 are {1}
The followers of 1 are {0}
The followers of 2 are {0,1}
SELECT
  user_id
  , COUNT(*) AS followers_count
FROM Followers
GROUP BY 1
ORDER BY 1
619. Biggest single number
A single number is a number that appeared only once in the MyNumbers table.

Find the largest single number. If there is no single number, report null.

Example 1:

Input: 
MyNumbers table:
+-----+
| num |
+-----+
| 8   |
| 8   |
| 3   |
| 3   |
| 1   |
| 4   |
| 5   |
| 6   |
+-----+
Output: 
+-----+
| num |
+-----+
| 6   |
+-----+
Explanation: The single numbers are 1, 4, 5, and 6.
Since 6 is the largest single number, we return it.

Example 2:

Input: 
MyNumbers table:
+-----+
| num |
+-----+
| 8   |
| 8   |
| 7   |
| 7   |
| 3   |
| 3   |
| 3   |
+-----+
Output: 
+------+
| num  |
+------+
| null |
+------+
Explanation: There are no single numbers in the input table so we return null.
SELECT MAX(num) AS num
FROM (
  SELECT *
  FROM MyNumbers
  GROUP BY 1
  HAVING COUNT(*)=1
) _
1045. Customers who bought all products
Write a solution to report the customer ids from the Customer table that bought all the products in the Product table.

Return the result table in any order.

Input: 
Customer table:
+-------------+-------------+
| customer_id | product_key |
+-------------+-------------+
| 1           | 5           |
| 2           | 6           |
| 3           | 5           |
| 3           | 6           |
| 1           | 6           |
+-------------+-------------+
Product table:
+-------------+
| product_key |
+-------------+
| 5           |
| 6           |
+-------------+
Output: 
+-------------+
| customer_id |
+-------------+
| 1           |
| 3           |
+-------------+
Explanation: 
The customers who bought all the products (5 and 6) are customers with IDs 1 and 3.
SELECT customer_id
FROM Customer 
GROUP BY 1
HAVING COUNT(DISTINCT product_key)=(SELECT COUNT(*) FROM Product)
That’s a wrap for part one. Generally speaking, the problems here are relatively straightforward and they offer great practice for writing basic SQL queries.

Advanced Select and Joins (7 Questions)
1731. The number of employees which report to each employee
For this problem, we will consider a manager an employee who has at least 1 other employee reporting to them.

Write a solution to report the ids and the names of all managers, the number of employees who report directly to them, and the average age of the reports rounded to the nearest integer.

Return the result table ordered by employee_id.

Input: 
Employees table:
+-------------+---------+------------+-----+
| employee_id | name    | reports_to | age |
+-------------+---------+------------+-----+
| 9           | Hercy   | null       | 43  |
| 6           | Alice   | 9          | 41  |
| 4           | Bob     | 9          | 36  |
| 2           | Winston | null       | 37  |
+-------------+---------+------------+-----+
Output: 
+-------------+-------+---------------+-------------+
| employee_id | name  | reports_count | average_age |
+-------------+-------+---------------+-------------+
| 9           | Hercy | 2             | 39          |
+-------------+-------+---------------+-------------+
Explanation: Hercy has 2 people report directly to him, Alice and Bob. Their average age is (41+36)/2 = 38.5, which is 39 after rounding it to the nearest integer.
SELECT 
  e1.employee_id
  , e1.name
  , COUNT(*) AS reports_count
  , ROUND(AVG(e2.age), 0) AS average_age
FROM Employees e1, Employees e2
WHERE e1.employee_id=e2.reports_to
GROUP BY 1, 2
ORDER BY 1
1789. Primary department for each employee
Employees can belong to multiple departments. When the employee joins other departments, they need to decide which department is their primary department. Note that when an employee belongs to only one department, their primary column is N.

Write a solution to report all the employees with their primary department. For employees who belong to one department, report their only department.

Return the result table in any order.

Input: 
Employee table:
+-------------+---------------+--------------+
| employee_id | department_id | primary_flag |
+-------------+---------------+--------------+
| 1           | 1             | N            |
| 2           | 1             | Y            |
| 2           | 2             | N            |
| 3           | 3             | N            |
| 4           | 2             | N            |
| 4           | 3             | Y            |
| 4           | 4             | N            |
+-------------+---------------+--------------+
Output: 
+-------------+---------------+
| employee_id | department_id |
+-------------+---------------+
| 1           | 1             |
| 2           | 1             |
| 3           | 3             |
| 4           | 3             |
+-------------+---------------+
Explanation: 
- The Primary department for employee 1 is 1.
- The Primary department for employee 2 is 1.
- The Primary department for employee 3 is 3.
- The Primary department for employee 4 is 3.
SELECT employee_id, department_id
FROM Employee
WHERE primary_flag='Y'
UNION
SELECT employee_id, department_id
FROM Employee
GROUP BY 1
HAVING COUNT(*)=1
The trick here is to separate employees into two buckets: those with multiple departments and those with only one.

Each bucket would have its query and resulting table. Once you have the two tables, you simply need to union them together as seen in the solution above.

610. Triangle judgement
Report for every three line segments whether they can form a triangle.

Return the result table in any order.

Triangle table:
+----+----+----+
| x  | y  | z  |
+----+----+----+
| 13 | 15 | 30 |
| 10 | 20 | 15 |
+----+----+----+
Output: 
+----+----+----+----------+
| x  | y  | z  | triangle |
+----+----+----+----------+
| 13 | 15 | 30 | No       |
| 10 | 20 | 15 | Yes      |
+----+----+----+----------+
SELECT 
  x
  , y
  , z
  , CASE WHEN (x + y <= z OR x + z <= y OR y + z <= x) THEN 'No' ELSE 'Yes' END AS triangle
FROM Triangle
180. Consecutive numbers
Find all numbers that appear at least three times consecutively.

Return the result table in any order.

Input: 
Logs table:
+----+-----+
| id | num |
+----+-----+
| 1  | 1   |
| 2  | 1   |
| 3  | 1   |
| 4  | 2   |
| 5  | 1   |
| 6  | 2   |
| 7  | 2   |
+----+-----+
Output: 
+-----------------+
| ConsecutiveNums |
+-----------------+
| 1               |
+-----------------+
Explanation: 1 is the only number that appears consecutively for at least three times.
WITH cte AS (
  SELECT
    num
    , LEAD(num, 1) OVER() AS lead1
    , LEAD(num, 2) OVER() AS lead2
  FROM logs
)
SELECT DISTINCT num AS ConsecutiveNums
FROM cte
WHERE num=lead1 AND lead1=lead2
Create two columns for leading values, one succeeding by one row and the other succeeding by two rows.

If the current number equals the next two, then the number is considered to appear consecutively at least three times.

1164. Product price at a given date
Write a solution to find the prices of all products on 2019–08–16. Assume the price of all products before any change is 10.

Return the result table in any order.

Input: 
Products table:
+------------+-----------+-------------+
| product_id | new_price | change_date |
+------------+-----------+-------------+
| 1          | 20        | 2019-08-14  |
| 2          | 50        | 2019-08-14  |
| 1          | 30        | 2019-08-15  |
| 1          | 35        | 2019-08-16  |
| 2          | 65        | 2019-08-17  |
| 3          | 20        | 2019-08-18  |
+------------+-----------+-------------+
Output: 
+------------+-------+
| product_id | price |
+------------+-------+
| 2          | 50    |
| 1          | 35    |
| 3          | 10    |
+------------+-------+
SELECT product_id, 10 AS price
FROM Products
WHERE product_id NOT IN (SELECT DISTINCT product_id FROM Products WHERE change_date<='2019-08-16')
UNION
SELECT product_id, new_price AS price
FROM Products
WHERE (product_id, change_date) IN (SELECT product_id, MAX(change_date) FROM Products WHERE change_date<='2019-08-16' GROUP BY 1)
Similar to the employee department question above, the trick here is to split up your query into two parts: one for products with price change before 2019–08–16, and one for after.

For products with price change after 2019–08–16, set the default price as 10. On the other hand, for products with price change before 2019–08–16, return the most recent price.

1204. Last person to fit in the bus
There is a queue of people waiting to board a bus. However, the bus has a weight limit of 1,000 kilograms, so there may be some people who cannot board.

Write a solution to find the person_name of the last person that can fit on the bus without exceeding the weight limit. The test cases are generated such that the first person does not exceed the weight limit.

Input: 
Queue table:
+-----------+-------------+--------+------+
| person_id | person_name | weight | turn |
+-----------+-------------+--------+------+
| 5         | Alice       | 250    | 1    |
| 4         | Bob         | 175    | 5    |
| 3         | Alex        | 350    | 2    |
| 6         | John Cena   | 400    | 3    |
| 1         | Winston     | 500    | 6    |
| 2         | Marie       | 200    | 4    |
+-----------+-------------+--------+------+
Output: 
+-------------+
| person_name |
+-------------+
| John Cena   |
+-------------+
Explanation: The folowing table is ordered by the turn for simplicity.
+------+----+-----------+--------+--------------+
| Turn | ID | Name      | Weight | Total Weight |
+------+----+-----------+--------+--------------+
| 1    | 5  | Alice     | 250    | 250          |
| 2    | 3  | Alex      | 350    | 600          |
| 3    | 6  | John Cena | 400    | 1000         | (last person to board)
| 4    | 2  | Marie     | 200    | 1200         | (cannot board)
| 5    | 4  | Bob       | 175    | ___          |
| 6    | 1  | Winston   | 500    | ___          |
+------+----+-----------+--------+--------------+
SELECT q1.person_name
FROM Queue q1
LEFT JOIN Queue q2
  ON q1.turn >= q2.turn
GROUP BY q1.turn
HAVING SUM(q2.weight) <= 1000
ORDER BY SUM(q2.weight) DESC
LIMIT 1
For each turn, get the weights of all the preceding turns and the current turn. If the sum of all preceding turns and current turn is less than or equal to a thousand, then return the passenger name.

1907. Count salary categories
Write a solution to calculate the number of bank accounts for each salary category. The salary categories are

Low Salary: All the salaries strictly less than 20,000
Average Salary: All the salaries in the inclusive range [20,000, 50,000]
High Salary: All the salaries strictly greater than 50,000
The result table must contain all three categories. If there are no accounts in a category, return 0.

Return the result table in any order.

Input: 
Accounts table:
+------------+--------+
| account_id | income |
+------------+--------+
| 3          | 108939 |
| 2          | 12747  |
| 8          | 87709  |
| 6          | 91796  |
+------------+--------+
Output: 
+----------------+----------------+
| category       | accounts_count |
+----------------+----------------+
| Low Salary     | 1              |
| Average Salary | 0              |
| High Salary    | 3              |
+----------------+----------------+
Explanation: 
Low Salary: Account 2.
Average Salary: No accounts.
High Salary: Accounts 3, 6, and 8.
SELECT 'Low Salary' AS category, SUM(income<20000) AS accounts_count
FROM Accounts
UNION
SELECT 'Average Salary' AS category, SUM(income>=20000 AND income<=50000) AS accounts_count
FROM Accounts
UNION
SELECT 'High Salary' AS category, SUM(income>50000) AS accounts_count
FROM Accounts
The reason a union is needed here and not a simple case when followed by a group by count is because we want the category without any accounts to still return a value of zero.

Subqueries (7 Questions)
1978. Employees whose manager left the company
Find the IDs of the employees whose salary is strictly less than $30,000 and whose manager left the company. When a manager leaves the company, their information is deleted from the Employees table, but the reports still have their manager_id set to the manager that left.

Return the result table ordered by employee_id.

Input:  
Employees table:
+-------------+-----------+------------+--------+
| employee_id | name      | manager_id | salary |
+-------------+-----------+------------+--------+
| 3           | Mila      | 9          | 60301  |
| 12          | Antonella | null       | 31000  |
| 13          | Emery     | null       | 67084  |
| 1           | Kalel     | 11         | 21241  |
| 9           | Mikaela   | null       | 50937  |
| 11          | Joziah    | 6          | 28485  |
+-------------+-----------+------------+--------+
Output: 
+-------------+
| employee_id |
+-------------+
| 11          |
+-------------+
Explanation: 
The employees with a salary less than $30000 are 1 (Kalel) and 11 (Joziah).
Kalel's manager is employee 11, who is still in the company (Joziah).
Joziah's manager is employee 6, who left the company because there is no row for employee 6 as it was deleted.
SELECT employee_id
FROM Employees 
WHERE salary<30000
  AND manager_id NOT IN (SELECT DISTINCT employee_id FROM Employees)
ORDER BY 1
626. Exchange seats
Write a solution to swap the seat id of every two consecutive students. If the number of students is odd, the id of the last student is not swapped.

Return the result table ordered by id in ascending order.

Input: 
Seat table:
+----+---------+
| id | student |
+----+---------+
| 1  | Abbot   |
| 2  | Doris   |
| 3  | Emerson |
| 4  | Green   |
| 5  | Jeames  |
+----+---------+
Output: 
+----+---------+
| id | student |
+----+---------+
| 1  | Doris   |
| 2  | Abbot   |
| 3  | Green   |
| 4  | Emerson |
| 5  | Jeames  |
+----+---------+
Explanation: 
Note that if the number of students is odd, there is no need to change the last one's seat.
WITH cte AS (
  SELECT 
    *
    , LAG(student) OVER() AS prev_student
    , LEAD(student) OVER() AS next_student
  FROM Seat
)
SELECT 
  id
  , CASE WHEN MOD(id, 2)=1 AND next_student IS NOT NULL THEN next_student
         WHEN MOD(id, 2)=0 THEN prev_student
         ELSE student
    END AS student
FROM cte
For each ID, get the previous and next student if any. If the ID is odd and there is a next student, swap places with the next student. If the ID is even, then swap places with the previous student. Otherwise, the remaining ID has to be the last student in an odd-numbered group, in which case the student remains in their original place.

SELECT
  ROW_NUMBER() OVER() AS id
  , student
FROM seat
ORDER BY IF(MOD(id, 2)=0, id-1, id+1)
There is also a more algorithmically elegant way to do what we just did, which at first glance, may seem a little difficult to comprehend. Let’s break this down.

First, if the original ID is even, then minus one and conversely, if the original ID is odd, then plus one. We want to then sort the table based on the newly created IDs which indirectly swaps two consecutive seats.

But now that the original IDs are swapped, how do we make sure they are still sorted based on the new student order? This is where we use the row number window function to re-assign the new IDs.

1341. Movie rating
Write a solution to:

Find the name of the user who has rated the greatest number of movies. In case of a tie, return the lexicographically smaller user name.
Find the movie name with the highest average rating in February 2020. In case of a tie, return the lexicographically smaller movie name.
Input: 
Movies table:
+-------------+--------------+
| movie_id    |  title       |
+-------------+--------------+
| 1           | Avengers     |
| 2           | Frozen 2     |
| 3           | Joker        |
+-------------+--------------+
Users table:
+-------------+--------------+
| user_id     |  name        |
+-------------+--------------+
| 1           | Daniel       |
| 2           | Monica       |
| 3           | Maria        |
| 4           | James        |
+-------------+--------------+
MovieRating table:
+-------------+--------------+--------------+-------------+
| movie_id    | user_id      | rating       | created_at  |
+-------------+--------------+--------------+-------------+
| 1           | 1            | 3            | 2020-01-12  |
| 1           | 2            | 4            | 2020-02-11  |
| 1           | 3            | 2            | 2020-02-12  |
| 1           | 4            | 1            | 2020-01-01  |
| 2           | 1            | 5            | 2020-02-17  | 
| 2           | 2            | 2            | 2020-02-01  | 
| 2           | 3            | 2            | 2020-03-01  |
| 3           | 1            | 3            | 2020-02-22  | 
| 3           | 2            | 4            | 2020-02-25  | 
+-------------+--------------+--------------+-------------+
Output: 
+--------------+
| results      |
+--------------+
| Daniel       |
| Frozen 2     |
+--------------+
Explanation: 
Daniel and Monica have rated 3 movies ("Avengers", "Frozen 2" and "Joker") but Daniel is smaller lexicographically.
Frozen 2 and Joker have a rating average of 3.5 in February but Frozen 2 is smaller lexicographically.
(SELECT u.name AS results
FROM MovieRating r
LEFT JOIN Users u
  ON r.user_id=u.user_id
GROUP BY u.user_id 
ORDER BY count(*) DESC, u.name
LIMIT 1)
UNION ALL 
(SELECT m.title AS results 
FROM MovieRating r
LEFT JOIN Movies m
  ON r.movie_id=m.movie_id
WHERE r.created_at like '2020-02%'
GROUP BY r.movie_id
ORDER BY AVG(r.rating) DESC, m.title
LIMIT 1)
1321. Restaurant growth
You are the restaurant owner and you want to analyze a possible expansion (there will be at least one customer every day).

Compute the moving average of how much the customer paid in a seven-day window (current day plus six days before). average_amount should be rounded to two decimal places.

Return the result table ordered by visited_on in ascending order.

Input: 
Customer table:
+-------------+--------------+--------------+-------------+
| customer_id | name         | visited_on   | amount      |
+-------------+--------------+--------------+-------------+
| 1           | Jhon         | 2019-01-01   | 100         |
| 2           | Daniel       | 2019-01-02   | 110         |
| 3           | Jade         | 2019-01-03   | 120         |
| 4           | Khaled       | 2019-01-04   | 130         |
| 5           | Winston      | 2019-01-05   | 110         | 
| 6           | Elvis        | 2019-01-06   | 140         | 
| 7           | Anna         | 2019-01-07   | 150         |
| 8           | Maria        | 2019-01-08   | 80          |
| 9           | Jaze         | 2019-01-09   | 110         | 
| 1           | Jhon         | 2019-01-10   | 130         | 
| 3           | Jade         | 2019-01-10   | 150         | 
+-------------+--------------+--------------+-------------+
Output: 
+--------------+--------------+----------------+
| visited_on   | amount       | average_amount |
+--------------+--------------+----------------+
| 2019-01-07   | 860          | 122.86         |
| 2019-01-08   | 840          | 120            |
| 2019-01-09   | 840          | 120            |
| 2019-01-10   | 1000         | 142.86         |
+--------------+--------------+----------------+
Explanation: 
1st moving average from 2019-01-01 to 2019-01-07 has an average_amount of (100 + 110 + 120 + 130 + 110 + 140 + 150)/7 = 122.86
2nd moving average from 2019-01-02 to 2019-01-08 has an average_amount of (110 + 120 + 130 + 110 + 140 + 150 + 80)/7 = 120
3rd moving average from 2019-01-03 to 2019-01-09 has an average_amount of (120 + 130 + 110 + 140 + 150 + 80 + 110)/7 = 120
4th moving average from 2019-01-04 to 2019-01-10 has an average_amount of (130 + 110 + 140 + 150 + 80 + 110 + 130 + 150)/7 = 142.86
SELECT
  a.visited_on AS visited_on
  , SUM(b.day_sum) AS amount
  , ROUND(AVG(b.day_sum), 2) AS average_amount
FROM
  (SELECT visited_on, SUM(amount) AS day_sum FROM Customer GROUP BY visited_on) a
  , (SELECT visited_on, SUM(amount) AS day_sum FROM Customer GROUP BY visited_on) b
WHERE DATEDIFF(a.visited_on, b.visited_on) BETWEEN 0 AND 6
GROUP BY a.visited_on
HAVING COUNT(b.visited_on) = 7
Use inner join to get the preceding six days, if applicable, for each visit date and calculate the average of the seven days total (the current day plus the six preceding days).

602. Friend request II: who has the most friends
Write a solution to find the people who have the most friends and the most friends number.

Input: 
RequestAccepted table:
+--------------+-------------+-------------+
| requester_id | accepter_id | accept_date |
+--------------+-------------+-------------+
| 1            | 2           | 2016/06/03  |
| 1            | 3           | 2016/06/08  |
| 2            | 3           | 2016/06/08  |
| 3            | 4           | 2016/06/09  |
+--------------+-------------+-------------+
Output: 
+----+-----+
| id | num |
+----+-----+
| 3  | 3   |
+----+-----+
Explanation: 
The person with id 3 is a friend of people 1, 2, and 4, so he has three friends in total, which is the most number than any others.
WITH cte AS (
  SELECT requester_id AS id FROM RequestAccepted
  UNION ALL 
  SELECT accepter_id id FROM RequestAccepted
)
SELECT id, COUNT(*) AS num
FROM cte
GROUP BY 1
ORDER BY 2 DESC
LIMIT 1
Every ID has the same number of friends as they are the number of times they appear as a row in the table. A person forms a new friend by either sending or accepting a request.

Once you learn this pattern, this problem should be very easy to crack.

585. Investments in 2016
Write a solution to report the sum of all total investment values in 2016 tiv_2016, for all policyholders who:

Have the same tiv_2015 value as one or more other policyholders
Are not located in the same city as any other policyholder (lat, lon attribute pairs must be unique)
Input: 
Insurance table:
+-----+----------+----------+-----+-----+
| pid | tiv_2015 | tiv_2016 | lat | lon |
+-----+----------+----------+-----+-----+
| 1   | 10       | 5        | 10  | 10  |
| 2   | 20       | 20       | 20  | 20  |
| 3   | 10       | 30       | 20  | 20  |
| 4   | 10       | 40       | 40  | 40  |
+-----+----------+----------+-----+-----+
Output: 
+----------+
| tiv_2016 |
+----------+
| 45.00    |
+----------+
Explanation: 
The first record in the table, like the last record, meets both of the two criteria.
The tiv_2015 value 10 is the same as the third and fourth records, and its location is unique.
The second record does not meet any of the two criteria. Its tiv_2015 is not like any other policyholders and its location is the same as the third record, which makes the third record fail, too.
So, the result is the sum of tiv_2016 of the first and last record, which is 45.
WITH cte AS (
  SELECT 
    *
    , COUNT(*) OVER(PARTITION BY tiv_2015) AS cnt1
    , COUNT(*) OVER(PARTITION BY lat, lon) AS cnt2
  FROM Insurance
)
SELECT ROUND(SUM(tiv_2016), 2) AS tiv_2016
FROM cte 
WHERE cnt1>1 AND cnt2=1
185. Department top three salaries
A company’s executives are interested in seeing who earns the most money in each of the company’s departments. A high earner in a department is an employee who has a salary in the top three unique salaries for that department.

Write a solution to find the employees who are high earners in each of the departments.

Return the result table in any order.

Input: 
Employee table:
+----+-------+--------+--------------+
| id | name  | salary | departmentId |
+----+-------+--------+--------------+
| 1  | Joe   | 85000  | 1            |
| 2  | Henry | 80000  | 2            |
| 3  | Sam   | 60000  | 2            |
| 4  | Max   | 90000  | 1            |
| 5  | Janet | 69000  | 1            |
| 6  | Randy | 85000  | 1            |
| 7  | Will  | 70000  | 1            |
+----+-------+--------+--------------+
Department table:
+----+-------+
| id | name  |
+----+-------+
| 1  | IT    |
| 2  | Sales |
+----+-------+
Output: 
+------------+----------+--------+
| Department | Employee | Salary |
+------------+----------+--------+
| IT         | Max      | 90000  |
| IT         | Joe      | 85000  |
| IT         | Randy    | 85000  |
| IT         | Will     | 70000  |
| Sales      | Henry    | 80000  |
| Sales      | Sam      | 60000  |
+------------+----------+--------+
Explanation: 
In the IT department:
- Max earns the highest unique salary
- Both Randy and Joe earn the second-highest unique salary
- Will earns the third-highest unique salary
In the Sales department:
- Henry earns the highest salary
- Sam earns the second-highest salary
- There is no third-highest salary as there are only two employees
WITH cte AS (
  SELECT
    e.id
    , e.name AS Employee
    , e.salary AS Salary
    , DENSE_RANK() OVER(PARTITION BY d.name ORDER BY e.salary DESC) AS dept_rank
    , d.name AS Department
  FROM Employee e
  LEFT JOIN Department d
    ON e.departmentId=d.id
)
SELECT
  Department
  , Employee
  , Salary
FROM cte
WHERE dept_rank<=3
Advanced String Functions, Regex and Clause (7 Questions)
Now, I should note that you will rarely encounter regex-type SQL problems during real-life interviews unless you are interviewing at a company that is focused on text mining.

Nevertheless, here are a few questions that you can learn or practise applying regular expressions in SQL.

1667. Fix names in a table
Write a solution to fix the names so that only the first character is uppercase and the rest are lowercase.

Return the result table ordered by user_id.

Input: 
Users table:
+---------+-------+
| user_id | name  |
+---------+-------+
| 1       | aLice |
| 2       | bOB   |
+---------+-------+
Output: 
+---------+-------+
| user_id | name  |
+---------+-------+
| 1       | Alice |
| 2       | Bob   |
+---------+-------+
SELECT 
  user_id
  , CONCAT(UPPER(SUBSTR(name, 1, 1)), LOWER(SUBSTR(name, 2))) AS name
FROM Users
ORDER BY 1
1527. Patients with a condition
Write a solution to find the patient_id, patient_name, and conditions of the patients who have Type I Diabetes. Type I Diabetes always starts with DIAB1 prefix.

Return the result table in any order.

Input: 
Patients table:
+------------+--------------+--------------+
| patient_id | patient_name | conditions   |
+------------+--------------+--------------+
| 1          | Daniel       | YFEV COUGH   |
| 2          | Alice        |              |
| 3          | Bob          | DIAB100 MYOP |
| 4          | George       | ACNE DIAB100 |
| 5          | Alain        | DIAB201      |
+------------+--------------+--------------+
Output: 
+------------+--------------+--------------+
| patient_id | patient_name | conditions   |
+------------+--------------+--------------+
| 3          | Bob          | DIAB100 MYOP |
| 4          | George       | ACNE DIAB100 | 
+------------+--------------+--------------+
Explanation: Bob and George both have a condition that starts with DIAB1.
SELECT *
FROM Patients
WHERE conditions LIKE 'DIAB1%'
  OR conditions LIKE '% DIAB1%'
196. Delete duplicate emails
Write a solution to delete all duplicate emails, keeping only one unique email with the smallest id. For SQL users, please note that you are supposed to write a DELETE statement and not a SELECT one.

Input: 
Person table:
+----+------------------+
| id | email            |
+----+------------------+
| 1  | john@example.com |
| 2  | bob@example.com  |
| 3  | john@example.com |
+----+------------------+
Output: 
+----+------------------+
| id | email            |
+----+------------------+
| 1  | john@example.com |
| 2  | bob@example.com  |
+----+------------------+
Explanation: john@example.com is repeated two times. We keep the row with the smallest Id = 1.
DELETE p1
FROM person p1, person p2
WHERE p1.email=p2.email
  AND p1.id>p2.id
176. Second highest salary
Write a solution to find the second-highest salary from the Employee table. If there is no second-highest salary, return null.

Example 1
Input: 
Employee table:
+----+--------+
| id | salary |
+----+--------+
| 1  | 100    |
| 2  | 200    |
| 3  | 300    |
+----+--------+
Output: 
+---------------------+
| SecondHighestSalary |
+---------------------+
| 200                 |
+---------------------+
Example 2
Input: 
Employee table:
+----+--------+
| id | salary |
+----+--------+
| 1  | 100    |
+----+--------+
Output: 
+---------------------+
| SecondHighestSalary |
+---------------------+
| null                |
+---------------------+
SELECT MAX(salary) AS SecondHighestSalary
FROM Employee
WHERE salary < (SELECT MAX(salary) from Employee)
1484. Group sold products by the date
Write a solution to find for each date the number of different products sold and their names.

The sold product names for each date should be sorted lexicographically.

Return the result ordered by sell_date.

Input: 
Activities table:
+------------+------------+
| sell_date  | product     |
+------------+------------+
| 2020-05-30 | Headphone  |
| 2020-06-01 | Pencil     |
| 2020-06-02 | Mask       |
| 2020-05-30 | Basketball |
| 2020-06-01 | Bible      |
| 2020-06-02 | Mask       |
| 2020-05-30 | T-Shirt    |
+------------+------------+
Output: 
+------------+----------+------------------------------+
| sell_date  | num_sold | products                     |
+------------+----------+------------------------------+
| 2020-05-30 | 3        | Basketball,Headphone,T-shirt |
| 2020-06-01 | 2        | Bible,Pencil                 |
| 2020-06-02 | 1        | Mask                         |
+------------+----------+------------------------------+
Explanation: 
For 2020-05-30, Sold items were (Headphone, Basketball, T-shirt), we sort them lexicographically and separate them by a comma.
For 2020-06-01, Sold items were (Pencil, Bible), we sort them lexicographically and separate them by a comma.
For 2020-06-02, the Sold item is (Mask), we just return it.
SELECT 
  sell_date
  , COUNT(DISTINCT product) AS num_sold
  , GROUP_CONCAT(DISTINCT product ORDER BY product separator ',') AS products
FROM Activities
GROUP BY 1
1327. List the products ordered in a period
Write a solution to get the names of the products that have at least 100 units ordered in February 2020 and their amount.

Return the result table in any order.

Input: 
Products table:
+-------------+-----------------------+------------------+
| product_id  | product_name          | product_category |
+-------------+-----------------------+------------------+
| 1           | Leetcode Solutions    | Book             |
| 2           | Jewels of Stringology | Book             |
| 3           | HP                    | Laptop           |
| 4           | Lenovo                | Laptop           |
| 5           | Leetcode Kit          | T-shirt          |
+-------------+-----------------------+------------------+
Orders table:
+--------------+--------------+----------+
| product_id   | order_date   | unit     |
+--------------+--------------+----------+
| 1            | 2020-02-05   | 60       |
| 1            | 2020-02-10   | 70       |
| 2            | 2020-01-18   | 30       |
| 2            | 2020-02-11   | 80       |
| 3            | 2020-02-17   | 2        |
| 3            | 2020-02-24   | 3        |
| 4            | 2020-03-01   | 20       |
| 4            | 2020-03-04   | 30       |
| 4            | 2020-03-04   | 60       |
| 5            | 2020-02-25   | 50       |
| 5            | 2020-02-27   | 50       |
| 5            | 2020-03-01   | 50       |
+--------------+--------------+----------+
Output: 
+--------------------+---------+
| product_name       | unit    |
+--------------------+---------+
| Leetcode Solutions | 130     |
| Leetcode Kit       | 100     |
+--------------------+---------+
Explanation: 
Products with product_id = 1 is ordered in February a total of (60 + 70) = 130.
Products with product_id = 2 is ordered in February a total of 80.
Products with product_id = 3 is ordered in February a total of (2 + 3) = 5.
Products with product_id = 4 was not ordered in February 2020.
Products with product_id = 5 is ordered in February a total of (50 + 50) = 100.
SELECT
  product_name
  , SUM(unit) AS unit
FROM Orders o
LEFT JOIN Products p
  ON o.product_id=p.product_id
WHERE order_date LIKE '2020-02%' 
GROUP BY 1 
HAVING SUM(unit)>=100
1517. Find users with valid e-mails
Write a solution to find the users who have valid emails.

A valid e-mail has a prefix name and a domain where:

The prefix name is a string that may contain letters (upper or lower case), digits, underscore, period, and/or dash. The prefix name must start with a letter
The domain is leetcode.com
Return the result table in any order.

Input: 
Users table:
+---------+-----------+-------------------------+
| user_id | name      | mail                    |
+---------+-----------+-------------------------+
| 1       | Winston   | winston@leetcode.com    |
| 2       | Jonathan  | jonathanisgreat         |
| 3       | Annabelle | bella-@leetcode.com     |
| 4       | Sally     | sally.come@leetcode.com |
| 5       | Marwan    | quarz#2020@leetcode.com |
| 6       | David     | david69@gmail.com       |
| 7       | Shapiro   | .shapo@leetcode.com     |
+---------+-----------+-------------------------+
Output: 
+---------+-----------+-------------------------+
| user_id | name      | mail                    |
+---------+-----------+-------------------------+
| 1       | Winston   | winston@leetcode.com    |
| 3       | Annabelle | bella-@leetcode.com     |
| 4       | Sally     | sally.come@leetcode.com |
+---------+-----------+-------------------------+
Explanation: 
The mail of user 2 does not have a domain.
The mail of user 5 has the # sign which is not allowed.
The mail of user 6 does not have the leetcode domain.
The mail of user 7 starts with a period.
SELECT *
FROM Users
WHERE mail REGEXP '^[a-zA-Z][a-zA-Z0-9_.-]*@leetcode[.]com'
And that’s a wrap! I hope these 50 SQL problems have given some insights into what to expect from a coding interview and how you would tackle them.
90+ Most Common SQL Interview Questions You Need to Know
Ruiying Li
Written by
Ruiying Li
Jaya Muvania
Edited by
Jaya Muvania
Kaivan Dave
Reviewed by
Kaivan Dave
Updated on
Jul 27, 2025
Read time
30 min read
Preparing for an SQL interview can be daunting, but having a solid grasp of common questions and their answers can make all the difference. In this article, we’ve compiled 90+ essential SQL interview questions and answers to help you ace your next interview with confidence. Whether you're a beginner or an experienced professional, these questions will cover the key concepts you need to know.

What are SQL interview questions?
SQL interview questions are designed to assess a candidate's knowledge and proficiency in Structured Query Language (SQL). These questions typically cover a range of topics, including database design, data manipulation, and query optimization, to evaluate the candidate's ability to work with relational databases effectively.

Why do interviewers ask SQL questions?
The main purpose of SQL interview questions is to evaluate a candidate's ability to manage and manipulate data within relational databases. Interviewers ask these questions to gauge the candidate's technical skills, problem-solving abilities, and understanding of database concepts, ensuring they can effectively handle real-world data challenges.

91 SQL Interview Questions
What is SQL and how does it differ from other programming languages?
Explain the difference between INNER JOIN and LEFT JOIN with examples.
Write a SQL query to find the second highest salary from a table named Employees.
What are primary keys and foreign keys? Provide examples.
Write a SQL query to retrieve all records from a table named Products where the price is greater than 100.
Explain the concept of normalization and its types.
Write a SQL query to count the number of employees in each department from a table named Employees.
What is a subquery? Provide an example of a subquery in a SELECT statement.
Write a SQL query to find all customers who have placed more than 5 orders from a table named Orders.
Explain the difference between UNION and UNION ALL.
Write a SQL query to update the email address of a customer in a table named Customers.
What are indexes in SQL? How do they improve query performance?
Write a SQL query to delete all records from a table named Logs where the created_at date is older than 1 year.
Explain the ACID properties in the context of database transactions.
Write a SQL query to retrieve the top 3 highest-paid employees from a table named Employees.
What is a view in SQL? How is it different from a table?
Write a SQL query to find the total sales amount from a table named Sales grouped by product.
Explain the concept of stored procedures and their advantages.
Write a SQL query to find all products that have not been sold from a table named Products and a table named Sales.
What is the purpose of the GROUP BY clause? Provide an example.
Write a SQL query to retrieve the names of employees who have the same job title as 'Manager'.
Explain the difference between a clustered index and a non-clustered index.
Write a SQL query to find the average salary of employees in each department from a table named Employees.
What is a trigger in SQL? Provide an example of when you might use one.
Write a SQL query to retrieve all distinct values from a column named Category in a table named Products.
What are window functions and how do they differ from aggregate functions?
Write a SQL query to find the nth highest salary using window functions.
Explain the difference between HAVING and WHERE clauses.
What is a Common Table Expression (CTE) and when would you use it?
Write a SQL query to find duplicate records in a table.
What is the difference between DELETE, TRUNCATE, and DROP?
How do you handle NULL values in SQL queries?
Write a SQL query to calculate a running total.
What are the different types of constraints in SQL?
Explain the concept of database transactions and isolation levels.
Write a SQL query to pivot data from rows to columns.
What is the difference between RANK(), DENSE_RANK(), and ROW_NUMBER()?
How do you optimize SQL query performance?
What are user-defined functions and how do they differ from stored procedures?
Write a SQL query to find the percentage of total sales for each product.
What is the difference between correlated and non-correlated subqueries?
How do you handle date and time operations in SQL?
Write a SQL query to find records that exist in one table but not in another.
What is database denormalization and when would you use it?
Explain the difference between a primary key and a unique key.
Write a SQL query to find the median value from a column.
What are the advantages and disadvantages of using views?
How do you implement full-text search in SQL?
Write a SQL query to find the first and last record for each group.
What is the purpose of the COALESCE function?
Explain the concept of recursive queries and provide an example.
What are materialized views and how do they differ from regular views?
Write a SQL query to calculate year-over-year growth percentage.
How do you handle large dataset queries efficiently?
What is the difference between CHAR and VARCHAR data types?
Write a SQL query to find gaps in sequential data.
Explain the concept of database partitioning and its types.
What are database locks and how do they work?
Write a SQL query to transpose rows to columns dynamically.
How do you implement audit trails in SQL databases?
What is the difference between OLTP and OLAP systems?
Write a SQL query to find the top N customers by sales in each region.
How do you handle database schema migrations?
What are the considerations for choosing appropriate data types?
Write a SQL query to calculate moving averages.
How do you implement row-level security in SQL databases?
What is the purpose of database connection pooling?
Write a SQL query to find customers who haven't placed orders in the last 6 months.
How do you handle database backup and recovery strategies?
What are the best practices for SQL query writing?
Write a SQL query to find overlapping date ranges.
How do you implement database replication and what are its types?
What is database sharding and when would you implement it?
Write a SQL query to calculate retention rates.
How do you monitor and troubleshoot database performance issues?
What are database constraints and how do they ensure data integrity?
Write a SQL query to generate a calendar table.
How do you implement database versioning and change management?
What are the security considerations for SQL databases?
Write a SQL query to find the longest consecutive sequence.
How do you handle time zone considerations in global applications?
What is the difference between database clustering and replication?
Write a SQL query to implement a recommendation system using collaborative filtering.
How do you implement data archiving strategies?
What are the considerations for database capacity planning?
Write a SQL query to detect anomalies in time-series data.
How do you implement database disaster recovery procedures?
What are the emerging trends in database technology?
Write a SQL query to implement a simple rating system with weighted averages.
How do you optimize database storage and reduce costs?
What are the key considerations for migrating from legacy database systems?
1. What is SQL and how does it differ from other programming languages?
Why you might get asked this: Understanding the fundamental differences between SQL and other programming languages is crucial for roles that involve database management and data manipulation, such as a Database Administrator or Data Analyst.

How to answer:

Define SQL as a domain-specific language used for managing and manipulating relational databases.
Highlight that SQL is declarative, focusing on what data to retrieve rather than how to retrieve it.
Contrast SQL with general-purpose programming languages, emphasizing its specialized use for database operations.
Example answer:

"SQL, or Structured Query Language, is a domain-specific language designed for managing and manipulating relational databases. Unlike general-purpose programming languages, SQL is declarative, meaning it focuses on what data to retrieve rather than how to retrieve it."

2. Explain the difference between INNER JOIN and LEFT JOIN with examples.
Why you might get asked this: Understanding the difference between INNER JOIN and LEFT JOIN is essential for roles that require complex data retrieval and manipulation, such as a Data Engineer or SQL Developer.

How to answer:

Define INNER JOIN as a join that returns only the matching rows from both tables.
Explain LEFT JOIN as a join that returns all rows from the left table and the matching rows from the right table, with NULLs for non-matching rows.
Provide a simple example query for each join to illustrate the differences.
Example answer:

"An INNER JOIN returns only the rows that have matching values in both tables, while a LEFT JOIN returns all rows from the left table and the matching rows from the right table, with NULL values for non-matching rows. For example, SELECT * FROM Orders INNER JOIN Customers ON Orders.CustomerID = Customers.CustomerID retrieves only the orders with matching customers, whereas SELECT * FROM Orders LEFT JOIN Customers ON Orders.CustomerID = Customers.CustomerID retrieves all orders, including those without matching customers."

3. Write a SQL query to find the second highest salary from a table named Employees.
Why you might get asked this: This question tests your ability to write complex SQL queries and demonstrates your problem-solving skills, which are crucial for roles like Data Analyst or Database Developer.

How to answer:

Explain the use of the LIMIT clause to restrict the number of rows returned.
Describe the use of a subquery to find the highest salary and exclude it from the results.
Provide a sample query using ORDER BY and LIMIT to retrieve the second highest salary.
Example answer:

"To find the second highest salary from a table named Employees, you can use a subquery to first identify the highest salary and then exclude it from the results. The query would look like this: SELECT MAX(Salary) FROM Employees WHERE Salary < (SELECT MAX(Salary) FROM Employees)."

4. What are primary keys and foreign keys? Provide examples.
Why you might get asked this: Understanding primary keys and foreign keys is fundamental for ensuring data integrity and establishing relationships between tables, which is crucial for roles like Database Administrator or Data Architect.

How to answer:

Define a primary key as a unique identifier for each record in a table.
Explain a foreign key as a field in one table that uniquely identifies a row of another table.
Provide examples using simple table structures, such as an EmployeeID in an Employees table and a DepartmentID in a Departments table.
Example answer:

"A primary key is a unique identifier for each record in a table, ensuring that no two rows have the same key. A foreign key is a field in one table that uniquely identifies a row of another table, establishing a relationship between the two tables, such as EmployeeID in an Employees table and DepartmentID in a Departments table."

5. Write a SQL query to retrieve all records from a table named Products where the price is greater than 100.
Why you might get asked this: This question assesses your ability to write basic SQL queries for data retrieval, a fundamental skill for any role involving database management, such as a Data Analyst or SQL Developer.

How to answer:

Explain the use of the SELECT statement to retrieve data from the table.
Describe the WHERE clause to filter records based on the price condition.
Provide a sample query using SELECT * FROM Products WHERE price > 100.
Example answer:

"To retrieve all records from a table named Products where the price is greater than 100, you can use the following SQL query: SELECT * FROM Products WHERE price > 100; This query selects all columns from the Products table where the price column has a value greater than 100."

6. Explain the concept of normalization and its types.
Why you might get asked this: Understanding the concept of normalization and its types is crucial for ensuring efficient database design and data integrity, which is essential for roles like Database Administrator or Data Architect, for example.

How to answer:

Define normalization as the process of organizing data to reduce redundancy and improve data integrity.
Briefly describe the different normal forms, such as 1NF, 2NF, and 3NF, and their purposes.
Provide a simple example to illustrate how normalization is applied in a database.
Example answer:

"Normalization is the process of organizing data in a database to reduce redundancy and improve data integrity. It involves dividing large tables into smaller, related tables and defining relationships between them to ensure data consistency."

7. Write a SQL query to count the number of employees in each department from a table named Employees.
Why you might get asked this: This question evaluates your ability to write aggregate queries, a fundamental skill for roles that involve data analysis and reporting, such as a Data Analyst or Business Intelligence Developer, for example.

How to answer:

Explain the use of the GROUP BY clause to group records by department.
Describe the use of the COUNT function to count the number of employees in each group.
Provide a sample query using SELECT Department, COUNT(*) FROM Employees GROUP BY Department.
Example answer:

"To count the number of employees in each department from a table named Employees, you can use the GROUP BY clause along with the COUNT function. The query would look like this: SELECT Department, COUNT(*) FROM Employees GROUP BY Department;"

8. What is a subquery? Provide an example of a subquery in a SELECT statement.
Why you might get asked this: Understanding subqueries and their application in SELECT statements is essential for roles that require complex data retrieval and manipulation, such as a Data Analyst or SQL Developer, for example.

How to answer:

Define a subquery as a query nested within another query.
Explain that subqueries can be used in SELECT, INSERT, UPDATE, or DELETE statements.
Provide a simple example, such as SELECT * FROM Employees WHERE Salary > (SELECT AVG(Salary) FROM Employees).
Example answer:

"A subquery is a query nested within another query, often used to perform operations that require multiple steps. For example, SELECT * FROM Employees WHERE Salary > (SELECT AVG(Salary) FROM Employees) retrieves employees with salaries above the average."

9. Write a SQL query to find all customers who have placed more than 5 orders from a table named Orders.
Why you might get asked this: This question tests your ability to write complex SQL queries involving aggregate functions and conditional logic, which are crucial skills for roles that require data analysis and reporting, such as a Data Analyst or SQL Developer, for example.

How to answer:

Explain the use of the GROUP BY clause to group records by customer.
Describe the use of the HAVING clause to filter groups with more than 5 orders.
Provide a sample query using SELECT CustomerID FROM Orders GROUP BY CustomerID HAVING COUNT(*) > 5.
Example answer:

"To find all customers who have placed more than 5 orders from a table named Orders, you can use the GROUP BY clause to group records by customer and the HAVING clause to filter groups with more than 5 orders. The query would look like this: SELECT CustomerID FROM Orders GROUP BY CustomerID HAVING COUNT(*) > 5;"

10. Explain the difference between UNION and UNION ALL.
Why you might get asked this: Understanding the difference between UNION and UNION ALL is crucial for roles that require data consolidation and query optimization, such as a Data Analyst or SQL Developer, for example.

How to answer:

Define UNION as a command that combines the results of two queries and removes duplicate rows.
Explain UNION ALL as a command that combines the results of two queries without removing duplicates.
Provide a simple example query for each to illustrate the differences.
Example answer:

"A UNION combines the results of two queries and removes duplicate rows, ensuring each row is unique in the final result set. In contrast, UNION ALL combines the results of two queries without removing duplicates, which can be more efficient when duplicates are not a concern."

11. Write a SQL query to update the email address of a customer in a table named Customers.
Why you might get asked this: This question assesses your ability to perform data updates, a fundamental skill for roles that involve database management and maintenance, such as a Database Administrator or SQL Developer, for example.

How to answer:

Explain the use of the UPDATE statement to modify existing records in the table.
Describe the SET clause to specify the new email address.
Include the WHERE clause to target the specific customer whose email address needs updating.
Example answer:

"To update the email address of a customer in a table named Customers, you can use the UPDATE statement along with the SET clause to specify the new email address. The query would look like this: UPDATE Customers SET email = 'newemail@example.com' WHERE customer_id = 1;"

12. What are indexes in SQL? How do they improve query performance?
Why you might get asked this: Understanding indexes and their impact on query performance is crucial for optimizing database operations, which is essential for roles that involve database management and optimization, such as a Database Administrator or SQL Developer, for example.

How to answer:

Define indexes as database objects that improve the speed of data retrieval operations.
Explain that indexes work by creating a data structure that allows for faster searches.
Highlight that while indexes improve read performance, they can slow down write operations due to the overhead of maintaining the index.
Example answer:

"Indexes in SQL are special data structures that improve the speed of data retrieval operations on a database table. They work by creating a quick lookup reference for the database, significantly reducing the time it takes to find specific rows."

13. Write a SQL query to delete all records from a table named Logs where the created_at date is older than 1 year.
Why you might get asked this: This question evaluates your ability to perform data maintenance tasks, which are crucial for roles that involve database management and optimization, such as a Database Administrator or SQL Developer, for example.

How to answer:

Explain the use of the DELETE statement to remove records from the table.
Describe the WHERE clause to filter records based on the created_at date.
Provide a sample query using DELETE FROM Logs WHERE created_at < NOW() - INTERVAL 1 YEAR.
Example answer:

"To delete all records from a table named Logs where the created_at date is older than 1 year, you can use the following SQL query: DELETE FROM Logs WHERE created_at < NOW() - INTERVAL 1 YEAR; This query ensures that only records older than one year are removed, keeping your table up-to-date."

14. Explain the ACID properties in the context of database transactions.
Why you might get asked this: Understanding the ACID properties is crucial for ensuring data integrity and reliability in database transactions, which is essential for roles that involve database management and optimization, such as a Database Administrator or SQL Developer, for example.

How to answer:

Define ACID as a set of properties that ensure reliable database transactions.
Briefly describe each property: Atomicity, Consistency, Isolation, and Durability.
Provide a simple example to illustrate how these properties maintain data integrity.
Example answer:

"ACID properties ensure reliable database transactions by maintaining data integrity. They stand for Atomicity, Consistency, Isolation, and Durability, which collectively guarantee that transactions are processed reliably."

15. Write a SQL query to retrieve the top 3 highest-paid employees from a table named Employees.
Why you might get asked this: This question tests your ability to write queries that involve sorting and limiting results, which is crucial for roles that require data analysis and reporting, such as a Data Analyst or SQL Developer, for example.

How to answer:

Explain the use of the ORDER BY clause to sort the salaries in descending order.
Describe the use of the LIMIT clause to restrict the number of rows returned to three.
Provide a sample query using SELECT * FROM Employees ORDER BY Salary DESC LIMIT 3.
Example answer:

"To retrieve the top 3 highest-paid employees from a table named Employees, you can use the ORDER BY clause to sort the salaries in descending order and the LIMIT clause to restrict the number of rows returned to three. The query would look like this: SELECT * FROM Employees ORDER BY Salary DESC LIMIT 3;"

16. What is a view in SQL? How is it different from a table?
Why you might get asked this: Understanding the concept of views and their differences from tables is crucial for roles that involve database management and optimization, such as a Database Administrator or SQL Developer, for example.

How to answer:

Define a view as a virtual table based on the result set of a SQL query.
Explain that views do not store data physically but provide a way to simplify complex queries.
Highlight that unlike tables, views are dynamic and reflect changes in the underlying data in real-time.
Example answer:

"A view in SQL is a virtual table created based on the result set of a SQL query. Unlike a table, a view does not store data physically but provides a way to simplify complex queries."

17. Write a SQL query to find the total sales amount from a table named Sales grouped by product.
Why you might get asked this: This question evaluates your ability to perform aggregate calculations and group data, which are essential skills for roles that involve data analysis and reporting, such as a Data Analyst or Business Intelligence Developer, for example.

How to answer:

Explain the use of the SUM function to calculate the total sales amount.
Describe the GROUP BY clause to group the results by product.
Provide a sample query using SELECT product, SUM(amount) FROM Sales GROUP BY product.
Example answer:

"To find the total sales amount from a table named Sales grouped by product, you can use the SUM function along with the GROUP BY clause. The query would look like this: SELECT product, SUM(amount) FROM Sales GROUP BY product;"

18. Explain the concept of stored procedures and their advantages.
Why you might get asked this: Understanding stored procedures and their advantages is crucial for roles that involve database management and optimization, such as a Database Administrator or SQL Developer, for example.

How to answer:

Define a stored procedure as a precompiled collection of SQL statements.
Explain that stored procedures improve performance by reducing the need for repeated parsing and compilation.
Highlight that they enhance security by encapsulating business logic and restricting direct access to data.
Example answer:

"A stored procedure is a precompiled collection of SQL statements that can be executed as a single unit. They improve performance by reducing the need for repeated parsing and compilation, and enhance security by encapsulating business logic and restricting direct access to data."

19. Write a SQL query to find all products that have not been sold from a table named Products and a table named Sales.
Why you might get asked this: This question tests your ability to perform complex data retrieval operations involving multiple tables, which is crucial for roles that require advanced SQL skills, such as a Data Analyst or SQL Developer, for example.

How to answer:

Explain the use of a subquery to identify products that have been sold.
Describe the use of the NOT IN clause to filter out sold products from the Products table.
Provide a sample query using SELECT * FROM Products WHERE ProductID NOT IN (SELECT ProductID FROM Sales).
Example answer:

"To find all products that have not been sold, you can use a subquery to identify sold products and then filter them out from the Products table. The query would look like this: SELECT * FROM Products WHERE ProductID NOT IN (SELECT ProductID FROM Sales);"

20. What is the purpose of the GROUP BY clause? Provide an example.
Why you might get asked this: Understanding the purpose of the GROUP BY clause is essential for roles that involve data aggregation and reporting, such as a Data Analyst or Business Intelligence Developer, for example.

How to answer:

Explain that the GROUP BY clause is used to group rows that have the same values in specified columns.
Highlight that it allows aggregate functions like SUM, COUNT, and AVG to be applied to each group.
Provide a simple example, such as SELECT department, COUNT(*) FROM Employees GROUP BY department.
Example answer:

"The GROUP BY clause is used to group rows that have the same values in specified columns, allowing aggregate functions to be applied to each group. For example, SELECT department, COUNT(*) FROM Employees GROUP BY department counts the number of employees in each department."

21. Write a SQL query to retrieve the names of employees who have the same job title as 'Manager'.
Why you might get asked this: This question tests your ability to write SQL queries that involve string matching and filtering, which is crucial for roles that require data retrieval and manipulation, such as a Data Analyst or SQL Developer, for example.

How to answer:

Explain the use of the SELECT statement to retrieve employee names.
Describe the WHERE clause to filter employees with the job title 'Manager'.
Provide a sample query using SELECT name FROM Employees WHERE job_title = 'Manager'.
Example answer:

"To retrieve the names of employees who have the same job title as 'Manager', you can use the following SQL query: SELECT name FROM Employees WHERE job_title = 'Manager'; This query selects the names of all employees whose job title is 'Manager' from the Employees table."

22. Explain the difference between a clustered index and a non-clustered index.
Why you might get asked this: Understanding the difference between a clustered index and a non-clustered index is crucial for optimizing database performance, which is essential for roles that involve database management and optimization, such as a Database Administrator or SQL Developer, for example.

How to answer:

Define a clustered index as a type of index that sorts and stores the data rows in the table based on the index key.
Explain that a non-clustered index creates a separate structure to store the index and includes a pointer to the data rows.
Highlight that a table can have only one clustered index but multiple non-clustered indexes.
Example answer:

"A clustered index sorts and stores the data rows in the table based on the index key, making data retrieval faster. In contrast, a non-clustered index creates a separate structure to store the index and includes a pointer to the data rows, allowing for multiple non-clustered indexes on a table."

23. Write a SQL query to find the average salary of employees in each department from a table named Employees.
Why you might get asked this: This question evaluates your ability to perform aggregate calculations and group data, which are essential skills for roles that involve data analysis and reporting, such as a Data Analyst or Business Intelligence Developer, for example.

How to answer:

Explain the use of the AVG function to calculate the average salary.
Describe the GROUP BY clause to group the results by department.
Provide a sample query using SELECT department, AVG(salary) FROM Employees GROUP BY department.
Example answer:

"To find the average salary of employees in each department from a table named Employees, you can use the AVG function along with the GROUP BY clause. The query would look like this: SELECT department, AVG(salary) FROM Employees GROUP BY department;"

24. What is a trigger in SQL? Provide an example of when you might use one.
Why you might get asked this: Understanding triggers and their applications is crucial for automating database tasks and ensuring data integrity, which is essential for roles that involve database management and optimization, such as a Database Administrator or SQL Developer, for example.

How to answer:

Define a trigger as a special type of stored procedure that automatically executes in response to certain events on a table or view.
Explain that triggers can be used to enforce business rules, maintain audit trails, or synchronize tables.
Provide an example, such as using a trigger to automatically update a stock quantity in an inventory table when a new order is inserted.
Example answer:

"A trigger in SQL is a special type of stored procedure that automatically executes in response to certain events on a table or view. For example, you might use a trigger to automatically update a stock quantity in an inventory table when a new order is inserted."

25. Write a SQL query to retrieve all distinct values from a column named Category in a table named Products.
Why you might get asked this: This question tests your ability to write queries that retrieve unique values, a fundamental skill for roles that involve data analysis and reporting, such as a Data Analyst or SQL Developer, for example.

How to answer:

Explain the use of the SELECT DISTINCT statement to retrieve unique values.
Describe the FROM clause to specify the table name.
Provide a sample query using SELECT DISTINCT Category FROM Products.
Example answer:

"To retrieve all distinct values from a column named Category in a table named Products, you can use the SELECT DISTINCT statement. The query would look like this: SELECT DISTINCT Category FROM Products;"

26. What are window functions and how do they differ from aggregate functions?
Why you might get asked this: Understanding window functions is crucial for advanced data analysis and reporting tasks, which is essential for roles like Data Analyst or Business Intelligence Developer.

How to answer:

Define window functions as functions that perform calculations across related rows within a result set.
Explain that unlike aggregate functions, window functions don't group rows into a single output row.
Mention that window functions use the OVER clause to define the window of rows.
Example answer: "Window functions perform calculations across a set of related rows without collapsing them into a single row like aggregate functions do. They use the OVER clause to define the window of rows, enabling calculations like running totals, rankings, and moving averages while preserving individual row details."

27. Write a SQL query to find the nth highest salary using window functions.
Why you might get asked this: This question tests your understanding of advanced SQL techniques for ranking and ordering data, which is crucial for roles involving complex data analysis.

How to answer:

Explain the use of the ROW_NUMBER() or DENSE_RANK() window function.
Describe how to order the data and filter for the specific rank.
Provide a sample query using window functions to find the nth highest salary.
Example answer: "To find the nth highest salary using window functions, you can use DENSE_RANK() to rank salaries and filter for the specific position. The query would look like: SELECT * FROM (SELECT *, DENSE_RANK() OVER (ORDER BY salary DESC) as rank FROM Employees) ranked WHERE rank = n;"

28. Explain the difference between HAVING and WHERE clauses.
Why you might get asked this: Understanding the distinction between HAVING and WHERE is fundamental for proper query construction, especially important for roles involving data filtering and aggregation.

How to answer:

Explain that WHERE filters rows before grouping occurs.
Describe that HAVING filters groups after GROUP BY has been applied.
Mention that HAVING can work with aggregate functions while WHERE cannot.
Example answer: "The WHERE clause filters individual rows before any grouping occurs, while the HAVING clause filters groups after the GROUP BY operation. WHERE cannot use aggregate functions, but HAVING can, making it essential for filtering grouped data based on aggregate conditions."

29. What is a Common Table Expression (CTE) and when would you use it?
Why you might get asked this: Understanding CTEs is important for writing readable and maintainable complex queries, which is valuable for roles involving advanced SQL development.

How to answer:

Define CTE as a named temporary result set that exists within the scope of a single statement.
Explain that CTEs improve query readability and can be referenced multiple times.
Mention use cases like recursive queries, complex joins, and breaking down complex logic.
Example answer: "A Common Table Expression (CTE) is a named temporary result set defined using the WITH clause that exists only for the duration of a query. CTEs improve readability, enable recursive operations, and can be referenced multiple times within the same query, making complex queries more maintainable."

30. Write a SQL query to find duplicate records in a table.
Why you might get asked this: Identifying duplicate data is a common data quality task essential for roles involving data cleaning and database maintenance.

How to answer:

Explain using GROUP BY with the columns to check for duplicates.
Describe using HAVING with COUNT to filter groups with more than one record.
Provide a sample query that identifies duplicate records.
Example answer: "To find duplicate records, you can group by the columns that should be unique and use HAVING to filter groups with more than one record: SELECT column1, column2, COUNT() FROM table_name GROUP BY column1, column2 HAVING COUNT() > 1;"

31. What is the difference between DELETE, TRUNCATE, and DROP?
Why you might get asked this: Understanding different methods of removing data is crucial for database management and maintenance roles.

How to answer:

Explain DELETE removes specific rows and can use WHERE clause.
Describe TRUNCATE removes all rows but keeps table structure.
Mention DROP removes the entire table including structure.
Example answer: "DELETE removes specific rows based on conditions and can be rolled back, TRUNCATE removes all rows quickly but cannot be rolled back in most databases, and DROP removes the entire table structure and data permanently. TRUNCATE is faster than DELETE for removing all data."

32. How do you handle NULL values in SQL queries?
Why you might get asked this: Proper NULL handling is essential for accurate data analysis and preventing unexpected query results.

How to answer:

Explain using IS NULL and IS NOT NULL for checking NULL values.
Mention functions like COALESCE, ISNULL, or IFNULL for handling NULLs.
Describe how NULLs behave in comparisons and aggregate functions.
Example answer: "NULL values require special handling using IS NULL or IS NOT NULL operators. Functions like COALESCE can replace NULLs with default values. NULLs in aggregate functions are typically ignored, and any comparison with NULL returns unknown, not true or false."

33. Write a SQL query to calculate a running total.
Why you might get asked this: Running totals are common in financial and analytical reports, making this skill valuable for data analysis roles.

How to answer:

Explain using window functions with the SUM function.
Describe the ORDER BY clause within the OVER clause.
Provide a sample query showing running total calculation.
Example answer: "To calculate a running total, use the SUM window function with an ORDER BY clause: SELECT date, amount, SUM(amount) OVER (ORDER BY date) as running_total FROM sales ORDER BY date;"

34. What are the different types of constraints in SQL?
Why you might get asked this: Understanding constraints is fundamental for maintaining data integrity, which is crucial for database design and administration roles.

How to answer:

List the main constraint types: PRIMARY KEY, FOREIGN KEY, UNIQUE, NOT NULL, CHECK.
Briefly explain the purpose of each constraint type.
Mention how constraints ensure data integrity and business rules.
Example answer: "SQL constraints include PRIMARY KEY (unique identifier), FOREIGN KEY (referential integrity), UNIQUE (no duplicates), NOT NULL (required values), and CHECK (custom validation rules). These constraints ensure data integrity and enforce business rules at the database level."

35. Explain the concept of database transactions and isolation levels.
Why you might get asked this: Understanding transactions is crucial for maintaining data consistency, especially important for roles involving financial or critical business data.

How to answer:

Define a transaction as a unit of work that either completes entirely or fails entirely.
Explain the four isolation levels: READ UNCOMMITTED, READ COMMITTED, REPEATABLE READ, SERIALIZABLE.
Mention the trade-offs between isolation levels and performance.
Example answer: "A database transaction is a logical unit of work that maintains data consistency. Isolation levels control how transaction changes are visible to other concurrent transactions, ranging from READ UNCOMMITTED (lowest isolation, highest concurrency) to SERIALIZABLE (highest isolation, lowest concurrency)."

36. Write a SQL query to pivot data from rows to columns.
Why you might get asked this: Data pivoting is essential for creating reports and transforming data for analysis, valuable for business intelligence roles.

How to answer:

Explain the concept of pivoting data using CASE statements or PIVOT function.
Describe how to transform row data into column format.
Provide a sample query showing data transformation.
Example answer: "To pivot data, you can use CASE statements with aggregate functions: SELECT product, SUM(CASE WHEN month = 'Jan' THEN sales ELSE 0 END) as Jan_Sales, SUM(CASE WHEN month = 'Feb' THEN sales ELSE 0 END) as Feb_Sales FROM sales_data GROUP BY product;"

37. What is the difference between RANK(), DENSE_RANK(), and ROW_NUMBER()?
Why you might get asked this: Understanding ranking functions is important for data analysis and reporting tasks that require ordering and positioning data.

How to answer:

Explain ROW_NUMBER() assigns unique sequential numbers.
Describe RANK() leaves gaps after ties.
Mention DENSE_RANK() doesn't leave gaps after ties.
Example answer: "ROW_NUMBER() assigns unique sequential numbers regardless of ties. RANK() assigns the same rank to tied values but leaves gaps in subsequent rankings. DENSE_RANK() assigns the same rank to tied values without leaving gaps in the sequence."

38. How do you optimize SQL query performance?
Why you might get asked this: Query optimization is crucial for maintaining application performance, especially important for senior database roles.

How to answer:

Mention proper indexing strategies and query structure.
Explain analyzing execution plans and identifying bottlenecks.
Describe techniques like avoiding SELECT *, using appropriate joins, and limiting result sets.
Example answer: "SQL query optimization involves proper indexing, analyzing execution plans, avoiding SELECT *, using appropriate join types, limiting result sets with WHERE clauses, and ensuring statistics are up to date. Regular monitoring and profiling help identify performance bottlenecks."

39. What are user-defined functions and how do they differ from stored procedures?
Why you might get asked this: Understanding different database objects is important for advanced database development and maintenance roles.

How to answer:

Define user-defined functions as reusable code blocks that return values.
Explain that functions return values while stored procedures may not.
Mention that functions can be used in SELECT statements while procedures cannot.
Example answer: "User-defined functions are reusable code blocks that return values and can be used within SQL statements like SELECT queries. Unlike stored procedures, functions must return a value and cannot perform operations like INSERT, UPDATE, or DELETE on the same database."

40. Write a SQL query to find the percentage of total sales for each product.
Why you might get asked this: Calculating percentages is common in business analysis and reporting, valuable for analytical roles.

How to answer:

Explain using window functions to calculate total sales.
Describe how to calculate individual product sales as a percentage of total.
Provide a sample query showing percentage calculation.
Example answer: "To find the percentage of total sales for each product: SELECT product, sales, (sales * 100.0 / SUM(sales) OVER()) as percentage_of_total FROM product_sales;"

41. What is the difference between correlated and non-correlated subqueries?
Why you might get asked this: Understanding subquery types is important for writing efficient complex queries, essential for advanced SQL development roles.

How to answer:

Define non-correlated subqueries as independent of the outer query.
Explain correlated subqueries reference columns from the outer query.
Mention performance implications of each type.
Example answer: "Non-correlated subqueries are independent and execute once, returning results used by the outer query. Correlated subqueries reference the outer query and execute once for each row of the outer query, often making them slower but more flexible for row-by-row comparisons."

42. How do you handle date and time operations in SQL?
Why you might get asked this: Date and time manipulation is common in business applications, important for roles involving temporal data analysis.

How to answer:

Mention common date functions like DATE_ADD, DATEDIFF, EXTRACT.
Explain formatting dates and handling time zones.
Describe best practices for date storage and querying.
Example answer: "Date and time operations use functions like DATE_ADD for arithmetic, DATEDIFF for calculating differences, and EXTRACT for getting specific parts. Always consider time zones, use appropriate data types (DATE, DATETIME, TIMESTAMP), and be careful with date formatting for consistent results."

43. Write a SQL query to find records that exist in one table but not in another.
Why you might get asked this: Finding data differences between tables is common in data validation and migration tasks.

How to answer:

Explain using LEFT JOIN with IS NULL condition.
Describe alternative approaches using NOT EXISTS or EXCEPT.
Provide sample queries showing different methods.
Example answer: "To find records in table A but not in table B, use: SELECT a.* FROM tableA a LEFT JOIN tableB b ON a.id = b.id WHERE b.id IS NULL; Alternatively, use NOT EXISTS: SELECT * FROM tableA WHERE NOT EXISTS (SELECT 1 FROM tableB WHERE tableB.id = tableA.id);"

44. What is database denormalization and when would you use it?
Why you might get asked this: Understanding when to denormalize is important for performance optimization in large-scale systems.

How to answer:

Define denormalization as intentionally introducing redundancy for performance.
Explain scenarios where denormalization is beneficial.
Mention trade-offs between performance and data integrity.
Example answer: "Denormalization intentionally introduces redundancy to improve query performance by reducing joins. It's useful for read-heavy systems, data warehouses, and reporting databases where query speed is more important than storage efficiency, but it requires careful maintenance to ensure data consistency."

45. Explain the difference between a primary key and a unique key.
Why you might get asked this: Understanding key constraints is fundamental for database design and data integrity.

How to answer:

Explain that primary keys cannot be NULL and there can be only one per table.
Describe that unique keys can be NULL and there can be multiple per table.
Mention their roles in indexing and referential integrity.
Example answer: "A primary key uniquely identifies rows, cannot contain NULL values, and there can be only one per table. A unique key also ensures uniqueness but can contain NULL values and there can be multiple unique keys per table. Both automatically create indexes."

46. Write a SQL query to find the median value from a column.
Why you might get asked this: Calculating statistical measures like median is important for data analysis roles.

How to answer:

Explain using window functions with PERCENTILE_CONT or similar functions.
Describe alternative approaches using ROW_NUMBER for databases without built-in median functions.
Provide sample queries for calculating median.
Example answer: "To find the median, use PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY column_name) or for databases without this function: SELECT AVG(salary) FROM (SELECT salary, ROW_NUMBER() OVER (ORDER BY salary) as rn, COUNT(*) OVER() as cnt FROM employees) WHERE rn IN ((cnt+1)/2, (cnt+2)/2);"

47. What are the advantages and disadvantages of using views?
Why you might get asked this: Understanding views is important for database design and security implementation.

How to answer:

List advantages: security, simplification, data abstraction.
Mention disadvantages: performance overhead, dependency issues.
Explain when views are most beneficial.
Example answer: "Views provide security by hiding sensitive columns, simplify complex queries, and offer data abstraction. However, they can have performance overhead, create dependencies, and complex views may not be updatable. They're best for frequently used complex queries and security requirements."

48. How do you implement full-text search in SQL?
Why you might get asked this: Full-text search capabilities are important for applications requiring advanced search functionality.

How to answer:

Explain full-text indexes and search functions like MATCH AGAINST.
Mention different search modes (natural language, Boolean).
Describe limitations and alternatives.
Example answer: "Full-text search uses specialized indexes and functions like MATCH() AGAINST() in MySQL or CONTAINS() in SQL Server. It supports natural language and Boolean search modes, ranking results by relevance. For complex requirements, dedicated search engines like Elasticsearch might be more appropriate."

49. Write a SQL query to find the first and last record for each group.
Why you might get asked this: Finding boundary records is common in time-series analysis and reporting.

How to answer:

Explain using window functions with FIRST_VALUE and LAST_VALUE.
Describe alternative approaches using subqueries and joins.
Provide sample queries showing different methods.
Example answer: "To find first and last records per group: SELECT *, FIRST_VALUE(value) OVER (PARTITION BY group_id ORDER BY date), LAST_VALUE(value) OVER (PARTITION BY group_id ORDER BY date ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) FROM table_name;"

50. What is the purpose of the COALESCE function?
Why you might get asked this: Understanding NULL handling functions is important for data quality and accurate reporting.

How to answer:

Define COALESCE as a function that returns the first non-NULL value.
Explain its use in handling NULL values and providing defaults.
Provide examples of practical applications.
Example answer: "COALESCE returns the first non-NULL value from a list of expressions, making it useful for handling NULL values and providing default values. For example, COALESCE(middle_name, '') returns an empty string if middle_name is NULL, ensuring consistent output formatting."

51. Explain the concept of recursive queries and provide an example.
Why you might get asked this: Recursive queries are important for hierarchical data processing, valuable for advanced database development roles.

How to answer:

Define recursive queries as queries that reference themselves.
Explain the structure with anchor and recursive members.
Provide an example using hierarchical data like organizational charts.
Example answer: "Recursive queries use Common Table Expressions to process hierarchical data by referencing themselves. They consist of an anchor member (base case) and recursive member. Example: WITH RECURSIVE emp_hierarchy AS (SELECT id, name, manager_id, 1 as level FROM employees WHERE manager_id IS NULL UNION ALL SELECT e.id, e.name, e.manager_id, eh.level+1 FROM employees e JOIN emp_hierarchy eh ON e.manager_id = eh.id) SELECT * FROM emp_hierarchy;"

52. What are materialized views and how do they differ from regular views?
Why you might get asked this: Understanding materialized views is important for performance optimization in data warehousing and analytics.

How to answer:

Define materialized views as physical storage of view results.
Explain performance benefits and refresh mechanisms.
Describe trade-offs with storage and data freshness.
Example answer: "Materialized views physically store query results, providing faster access than regular views that execute queries each time. They're ideal for complex aggregations and reporting but require refresh strategies to maintain data currency and consume additional storage space."

53. Write a SQL query to calculate year-over-year growth percentage.
Why you might get asked this: Year-over-year calculations are common in business analytics and financial reporting.

How to answer:

Explain using window functions with LAG to get previous year values.
Describe the formula for calculating growth percentage.
Provide a sample query showing the calculation.
Example answer: "To calculate year-over-year growth: SELECT year, revenue, LAG(revenue) OVER (ORDER BY year) as prev_year_revenue, ((revenue - LAG(revenue) OVER (ORDER BY year)) * 100.0 / LAG(revenue) OVER (ORDER BY year)) as yoy_growth_pct FROM annual_revenue ORDER BY year;"

54. How do you handle large dataset queries efficiently?
Why you might get asked this: Optimizing queries for large datasets is crucial for enterprise-level database performance.

How to answer:

Mention partitioning strategies and proper indexing.
Explain query optimization techniques and pagination.
Describe using appropriate hardware and configuration.
Example answer: "For large datasets, use table partitioning, proper indexing, query optimization with execution plan analysis, pagination for result sets, and consider parallel processing. Also implement appropriate WHERE clauses to limit data scanned and use summary tables for frequently accessed aggregations."

55. What is the difference between CHAR and VARCHAR data types?
Why you might get asked this: Understanding data types is fundamental for efficient database design and storage optimization.

How to answer:

Explain CHAR as fixed-length and VARCHAR as variable-length.
Mention storage implications and performance considerations.
Describe when to use each type.
Example answer: "CHAR is fixed-length and pads shorter values with spaces, while VARCHAR is variable-length and stores only the actual characters. CHAR is faster for fixed-size data but wastes space, while VARCHAR is more storage-efficient but has slight performance overhead for variable-length data."

56. Write a SQL query to find gaps in sequential data.
Why you might get asked this: Finding gaps in sequences is common in inventory management and audit scenarios.

How to answer:

Explain using window functions to identify missing sequences.
Describe comparing expected vs actual sequence values.
Provide a sample query for gap detection.
Example answer: "To find gaps in sequential data: SELECT (LAG(id) OVER (ORDER BY id) + 1) as gap_start, (id - 1) as gap_end FROM table_name WHERE id - LAG(id) OVER (ORDER BY id) > 1;"

57. Explain the concept of database partitioning and its types.
Why you might get asked this: Understanding partitioning is important for managing large databases and improving query performance.

How to answer:

Define partitioning as dividing large tables into smaller manageable pieces.
Explain different types: range, list, hash, and composite partitioning.
Mention benefits like improved performance and maintenance.
Example answer: "Database partitioning divides large tables into smaller, manageable segments based on partition keys. Types include range (date ranges), list (specific values), hash (even distribution), and composite (combination). Benefits include improved query performance, easier maintenance, and parallel processing capabilities."

58. What are database locks and how do they work?
Why you might get asked this: Understanding locking mechanisms is crucial for database concurrency and performance management.

How to answer:

Define locks as mechanisms to control concurrent access to data.
Explain different lock types: shared, exclusive, update locks.
Mention deadlocks and lock escalation.
Example answer: "Database locks control concurrent access to data, preventing conflicts between transactions. Types include shared locks (allow multiple reads), exclusive locks (prevent all other access), and update locks (prevent deadlocks during updates). Proper lock management prevents data corruption while maintaining concurrency."

59. Write a SQL query to transpose rows to columns dynamically.
Why you might get asked this: Dynamic transposition is useful for flexible reporting and data presentation.

How to answer:

Explain challenges of dynamic pivoting without knowing column names.
Describe using dynamic SQL or specific database features.
Provide an approach using conditional aggregation.
Example answer: "Dynamic transposition requires dynamic SQL since column names aren't known beforehand. Example approach: Build the SQL string dynamically based on distinct values in the pivot column, then execute it. Some databases offer PIVOT operators, but most require conditional aggregation with dynamically built CASE statements."

60. How do you implement audit trails in SQL databases?
Why you might get asked this: Audit trails are important for compliance and security requirements in enterprise applications.

How to answer:

Explain using triggers to capture data changes.
Mention audit table design with old/new values and metadata.
Describe alternative approaches like Change Data Capture.
Example answer: "Audit trails track data changes using triggers that insert records into audit tables containing old values, new values, operation type, user, and timestamp. Alternative approaches include Change Data Capture (CDC) features, database logs analysis, or application-level logging for better performance."

61. What is the difference between OLTP and OLAP systems?
Why you might get asked this: Understanding different database architectures is important for system design and data architecture roles.

How to answer:

Define OLTP as transaction-focused systems for daily operations.
Explain OLAP as analytics-focused systems for decision support.
Mention design differences and optimization strategies.
Example answer: "OLTP (Online Transaction Processing) systems handle high-volume transactions with normalized databases optimized for INSERT, UPDATE, DELETE operations. OLAP (Online Analytical Processing) systems are designed for complex queries and reporting with denormalized, dimensional models optimized for SELECT operations and aggregations."

62. Write a SQL query to find the top N customers by sales in each region.
Why you might get asked this: Ranking within groups is common in business analytics and competitive analysis.

How to answer:

Explain using window functions with partitioning.
Describe ranking and filtering for top N results.
Provide a sample query using ROW_NUMBER or RANK.
Example answer: "To find top N customers by sales in each region: SELECT * FROM (SELECT customer, region, sales, ROW_NUMBER() OVER (PARTITION BY region ORDER BY sales DESC) as rn FROM customer_sales) ranked WHERE rn <= N;"

63. How do you handle database schema migrations?
Why you might get asked this: Schema migrations are crucial for application deployment and database evolution management.

How to answer:

Explain version control for database schemas.
Mention migration tools and rollback strategies.
Describe testing and deployment best practices.
Example answer: "Database schema migrations use version-controlled scripts with tools like Flyway or Liquibase. Each migration is numbered sequentially, tested thoroughly, and includes rollback procedures. Best practices include backward compatibility, data migration validation, and coordination with application deployments."

64. What are the considerations for choosing appropriate data types?
Why you might get asked this: Proper data type selection affects storage efficiency, performance, and data integrity.

How to answer:

Mention storage requirements and performance implications.
Explain precision needs and range considerations.
Describe future scalability and standardization.
Example answer: "Data type selection considers storage efficiency, query performance, data range and precision requirements, and future scalability. Choose the smallest appropriate type, consider indexing implications, ensure proper precision for calculations, and maintain consistency across similar fields in the database."

65. Write a SQL query to calculate moving averages.
Why you might get asked this: Moving averages are important for trend analysis and financial calculations.

How to answer:

Explain using window functions with frame specifications.
Describe different frame options like ROWS and RANGE.
Provide sample queries for different moving average periods.
Example answer: "To calculate a moving average: SELECT date, value, AVG(value) OVER (ORDER BY date ROWS BETWEEN 2 PRECEDING AND CURRENT ROW) as moving_avg_3_days FROM sales_data ORDER BY date;"

66. How do you implement row-level security in SQL databases?
Why you might get asked this: Row-level security is important for multi-tenant applications and data privacy compliance.

How to answer:

Explain using security policies and predicates.
Mention role-based access control implementation.
Describe alternative approaches using views and functions.
Example answer: "Row-level security uses security policies with predicates to filter rows based on user context. Implementation involves creating security functions that return filter conditions, then applying them as policies. Alternative approaches include using views with user-based WHERE clauses or application-level filtering."

67. What is the purpose of database connection pooling?
Why you might get asked this: Connection pooling is crucial for application performance and resource management.

How to answer:

Explain reducing connection overhead and resource usage.
Mention improved application scalability and performance.
Describe configuration considerations and best practices.
Example answer: "Database connection pooling reuses existing connections instead of creating new ones for each request, reducing overhead and improving performance. It manages a pool of connections shared among application threads, preventing connection exhaustion and providing better resource utilization and scalability."

68. Write a SQL query to find customers who haven't placed orders in the last 6 months.
Why you might get asked this: Identifying inactive customers is important for customer retention and marketing analytics.

How to answer:

Explain using LEFT JOIN with date filtering.
Describe alternative approaches with NOT EXISTS.
Provide sample queries showing different methods.
Example answer: "To find inactive customers: SELECT c.* FROM customers c LEFT JOIN orders o ON c.customer_id = o.customer_id AND o.order_date >= DATE_SUB(NOW(), INTERVAL 6 MONTH) WHERE o.customer_id IS NULL;"

69. How do you handle database backup and recovery strategies?
Why you might get asked this: Backup and recovery planning is critical for database administration and business continuity.

How to answer:

Explain different backup types: full, incremental, differential.
Mention recovery models and point-in-time recovery.
Describe testing and automation strategies.
Example answer: "Database backup strategies include full backups (complete database), incremental (changes since last backup), and differential (changes since last full backup). Recovery planning involves defining RTO/RPO requirements, implementing automated backup schedules, regular restore testing, and maintaining both local and offsite copies."

70. What are the best practices for SQL query writing?
Why you might get asked this: Following best practices is important for maintainable, efficient, and secure database code.

How to answer:

Mention code formatting, naming conventions, and documentation.
Explain performance optimization and security considerations.
Describe testing and code review practices.
Example answer: "SQL best practices include consistent formatting and naming conventions, avoiding SELECT *, using proper indexing, parameterized queries for security, meaningful aliases, appropriate JOINs, query optimization with execution plans, comprehensive testing, and thorough documentation for complex logic."

71. Write a SQL query to find overlapping date ranges.
Why you might get asked this: Finding overlapping periods is common in scheduling, reservation, and temporal data analysis.

How to answer:

Explain the logic for detecting overlapping intervals.
Describe using self-joins or window functions.
Provide sample queries for overlap detection.
Example answer: "To find overlapping date ranges: SELECT a., b. FROM reservations a JOIN reservations b ON a.id < b.id AND a.start_date <= b.end_date AND a.end_date >= b.start_date;"

72. How do you implement database replication and what are its types?
Why you might get asked this: Database replication is important for high availability, scalability, and disaster recovery.

How to answer:

Explain master-slave and master-master replication.
Mention synchronous vs asynchronous replication.
Describe use cases and trade-offs.
Example answer: "Database replication types include master-slave (read replicas for scaling reads), master-master (bi-directional for high availability), and cluster replication. Synchronous replication ensures consistency but affects performance, while asynchronous replication provides better performance but potential data lag."

73. What is database sharding and when would you implement it?
Why you might get asked this: Sharding is important for scaling very large databases beyond single-server capabilities.

How to answer:

Define sharding as horizontal partitioning across servers.
Explain sharding strategies and key selection.
Mention complexity and trade-offs.
Example answer: "Database sharding horizontally partitions data across multiple servers using a shard key. It's implemented when vertical scaling limits are reached, enabling linear scaling. However, it adds complexity for cross-shard queries, transactions, and requires careful shard key selection to avoid hotspots."

74. Write a SQL query to calculate retention rates.
Why you might get asked this: Retention analysis is crucial for understanding customer behavior and business performance.

How to answer:

Explain defining cohorts and retention periods.
Describe calculating percentage of returning users.
Provide sample queries for retention calculation.
Example answer: "To calculate retention rates: SELECT cohort_month, period_number, COUNT(DISTINCT user_id) * 100.0 / first_month_users as retention_rate FROM user_activity_cohorts GROUP BY cohort_month, period_number ORDER BY cohort_month, period_number;"

75. How do you monitor and troubleshoot database performance issues?
Why you might get asked this: Performance monitoring is crucial for maintaining database health and application performance.

How to answer:

Mention monitoring tools and key metrics.
Explain query analysis and execution plans.
Describe proactive monitoring and alerting.
Example answer: "Database performance monitoring involves tracking metrics like CPU, memory, I/O, and query response times using tools like performance dashboards, slow query logs, and execution plan analysis. Implement alerts for threshold breaches, regular index analysis, and maintain baseline performance metrics for comparison."

76. What are database constraints and how do they ensure data integrity?
Why you might get asked this: Understanding constraints is fundamental for maintaining data quality and business rule enforcement.

How to answer:

List constraint types and their purposes.
Explain how constraints prevent invalid data.
Mention performance implications and best practices.
Example answer: "Database constraints enforce data integrity rules: PRIMARY KEY ensures unique identification, FOREIGN KEY maintains referential integrity, UNIQUE prevents duplicates, NOT NULL requires values, and CHECK validates business rules. They prevent invalid data entry and maintain consistency across the database."

77. Write a SQL query to generate a calendar table.
Why you might get asked this: Calendar tables are useful for date-based reporting and analytics applications.

How to answer:

Explain using recursive CTEs or number sequences.
Describe adding date attributes and business logic.
Provide sample queries for calendar generation.
Example answer: "To generate a calendar table: WITH RECURSIVE calendar AS (SELECT '2024-01-01' as date UNION ALL SELECT DATE_ADD(date, INTERVAL 1 DAY) FROM calendar WHERE date < '2024-12-31') SELECT date, YEAR(date) as year, MONTH(date) as month, DAY(date) as day, DAYNAME(date) as day_name FROM calendar;"

78. How do you implement database versioning and change management?
Why you might get asked this: Version control for databases is important for team collaboration and deployment management.

How to answer:

Explain database migration tools and version control.
Mention change scripts and rollback procedures.
Describe integration with application deployment.
Example answer: "Database versioning uses migration tools like Flyway or Liquibase with version-controlled SQL scripts. Each change is numbered sequentially, includes rollback procedures, and integrates with CI/CD pipelines. This ensures consistent schema evolution across environments and enables reliable deployments."

79. What are the security considerations for SQL databases?
Why you might get asked this: Database security is critical for protecting sensitive data and preventing breaches.

How to answer:

Mention access control, encryption, and SQL injection prevention.
Explain audit logging and network security.
Describe principle of least privilege and regular security updates.
Example answer: "SQL database security includes access control with role-based permissions, encryption at rest and in transit, SQL injection prevention through parameterized queries, comprehensive audit logging, network security with firewalls, regular security updates, and implementing the principle of least privilege for user access."

80. Write a SQL query to find the longest consecutive sequence.
Why you might get asked this: Finding consecutive sequences is useful in gaming, finance, and behavioral analysis.

How to answer:

Explain using window functions to identify sequence breaks.
Describe grouping consecutive elements.
Provide sample queries for sequence analysis.
Example answer: "To find longest consecutive sequence: SELECT MAX(consecutive_count) FROM (SELECT COUNT(*) as consecutive_count FROM (SELECT *, ROW_NUMBER() OVER (ORDER BY value) - ROW_NUMBER() OVER (PARTITION BY value ORDER BY value) as grp FROM sequences) grouped GROUP BY grp) counts;"

81. How do you handle time zone considerations in global applications?
Why you might get asked this: Time zone handling is important for international applications and accurate temporal data management.

How to answer:

Explain storing times in UTC and converting for display.
Mention time zone data types and functions.
Describe best practices for global applications.
Example answer: "Handle time zones by storing all timestamps in UTC and converting to local time zones for display. Use timezone-aware data types like TIMESTAMPTZ, implement proper conversion functions, maintain time zone reference data, and consider daylight saving time changes in business logic."

82. What is the difference between database clustering and replication?
Why you might get asked this: Understanding different high-availability architectures is important for system design decisions.

How to answer:

Define clustering as multiple servers acting as one system.
Explain replication as copying data to multiple servers.
Mention use cases and implementation differences.
Example answer: "Database clustering involves multiple servers working together as a single system with shared storage, providing high availability and load distribution. Replication copies data across separate database instances for read scaling and disaster recovery. Clustering offers automatic failover, while replication requires manual or automated failover management."

83. Write a SQL query to implement a recommendation system using collaborative filtering.
Why you might get asked this: Basic recommendation logic is useful for understanding how data-driven features work in applications.

How to answer:

Explain finding users with similar preferences.
Describe recommending items based on similar users' choices.
Provide sample queries for collaborative filtering.
Example answer: "Basic collaborative filtering: SELECT p2.product_id, COUNT(*) as similarity_score FROM purchases p1 JOIN purchases p2 ON p1.user_id = p2.user_id JOIN purchases p3 ON p3.product_id = p1.product_id WHERE p3.user_id = @target_user AND p2.product_id NOT IN (SELECT product_id FROM purchases WHERE user_id = @target_user) GROUP BY p2.product_id ORDER BY similarity_score DESC;"

84. How do you implement data archiving strategies?
Why you might get asked this: Data archiving is important for managing database size and maintaining performance while preserving historical data.

How to answer:

Explain identifying data for archiving based on age or usage.
Mention archival storage options and retrieval methods.
Describe automation and compliance considerations.
Example answer: "Data archiving strategies involve identifying old or infrequently accessed data, moving it to cheaper storage while maintaining accessibility. Implement automated archiving jobs based on date or usage patterns, use compressed storage formats, maintain indexes for archived data retrieval, and ensure compliance with data retention policies."

85. What are the considerations for database capacity planning?
Why you might get asked this: Capacity planning is crucial for maintaining database performance and avoiding resource constraints.

How to answer:

Mention monitoring current usage trends and growth patterns.
Explain projecting future requirements and resource needs.
Describe planning for peak loads and scalability.
Example answer: "Database capacity planning involves monitoring current storage, CPU, memory, and I/O usage, analyzing growth trends, projecting future requirements based on business growth, planning for peak loads, considering data retention policies, and implementing monitoring alerts for proactive scaling decisions."

86. Write a SQL query to detect anomalies in time-series data.
Why you might get asked this: Anomaly detection is important for monitoring systems and identifying unusual patterns in business data.

How to answer:

Explain using statistical functions to identify outliers.
Describe comparing current values to historical averages.
Provide sample queries for anomaly detection.
Example answer: "To detect anomalies using standard deviation: SELECT *, CASE WHEN ABS(value - avg_value) > 2 * stddev_value THEN 'Anomaly' ELSE 'Normal' END as status FROM (SELECT *, AVG(value) OVER (ORDER BY timestamp ROWS BETWEEN 30 PRECEDING AND 1 PRECEDING) as avg_value, STDDEV(value) OVER (ORDER BY timestamp ROWS BETWEEN 30 PRECEDING AND 1 PRECEDING) as stddev_value FROM time_series_data) analyzed;"

87. How do you implement database disaster recovery procedures?
Why you might get asked this: Disaster recovery planning is critical for business continuity and data protection.

How to answer:

Explain backup strategies and offsite storage.
Mention recovery time and point objectives.
Describe testing and documentation requirements.
Example answer: "Disaster recovery involves regular backups with offsite storage, defining RTO/RPO objectives, maintaining secondary sites or cloud infrastructure, implementing automated failover procedures, regular disaster recovery testing, comprehensive documentation, and staff training for emergency procedures."

88. What are the emerging trends in database technology?
Why you might get asked this: Understanding technology trends is important for making informed architectural decisions and career development.

How to answer:

Mention cloud-native databases and serverless architectures.
Explain NewSQL and distributed database systems.
Describe AI/ML integration and real-time analytics.
Example answer: "Emerging database trends include cloud-native and serverless databases for auto-scaling, NewSQL systems combining ACID guarantees with horizontal scaling, AI/ML integration for automated optimization, real-time analytics with streaming databases, multi-model databases supporting various data types, and edge computing databases for IoT applications."

89. Write a SQL query to implement a simple rating system with weighted averages.
Why you might get asked this: Rating systems are common in e-commerce and review applications.

How to answer:

Explain weighting ratings by factors like recency or reviewer credibility.
Describe calculating weighted averages and handling edge cases.
Provide sample queries for rating calculations.
Example answer: "To implement weighted ratings: SELECT product_id, SUM(rating * weight) / SUM(weight) as weighted_avg_rating FROM (SELECT product_id, rating, CASE WHEN reviewer_level = 'expert' THEN 2.0 WHEN review_date > DATE_SUB(NOW(), INTERVAL 30 DAY) THEN 1.5 ELSE 1.0 END as weight FROM reviews) weighted_reviews GROUP BY product_id;"

90. How do you optimize database storage and reduce costs?
Why you might get asked this: Storage optimization is important for managing database costs and improving performance.

How to answer:

Mention data compression and archiving strategies.
Explain proper data type selection and index optimization.
Describe partitioning and storage tiering approaches.
Example answer: "Database storage optimization involves implementing data compression, choosing appropriate data types, archiving old data, optimizing indexes by removing unused ones, implementing table partitioning, using storage tiering for different data access patterns, and regularly analyzing storage usage patterns to identify optimization opportunities."

91. What are the key considerations for migrating from legacy database systems?
Why you might get asked this: Database migration is common in modernization projects and requires careful planning and execution.

How to answer:

Explain assessing current system and defining migration strategy.
Mention data mapping, testing, and rollback procedures.
Describe minimizing downtime and ensuring data integrity.
Example answer: "Legacy database migration requires thorough assessment of current system, data mapping and transformation planning, choosing appropriate migration tools, extensive testing with production-like data, implementing rollback procedures, planning for minimal downtime, training staff on new systems, and post-migration monitoring to ensure performance and data integrity."
175. Combine Two Tables | Easy | LeetCode
Table: Person


+-------------+---------+
| Column Name | Type    |
+-------------+---------+
| PersonId    | int     |
| FirstName   | varchar |
| LastName    | varchar |
+-------------+---------+
PersonId is the primary key column for this table.
Table: Address


+-------------+---------+
| Column Name | Type    |
+-------------+---------+
| AddressId   | int     |
| PersonId    | int     |
| City        | varchar |
| State       | varchar |
+-------------+---------+
AddressId is the primary key column for this table.
Write a SQL query for a report that provides the following information for each person in the Person table, regardless if there is an address for each of those people:


FirstName, LastName, City, State
Solution
sql

SELECT p.FirstName, p.LastName, a.City, a.State
FROM Person p
LEFT JOIN Address a
ON p.PersonId = a.PersonId;
176. Second Highest Salary | Easy | LeetCode
Write a SQL query to get the second highest salary from the Employee table.


+----+--------+
| Id | Salary |
+----+--------+
| 1  | 100    |
| 2  | 200    |
| 3  | 300    |
+----+--------+
For example, given the above Employee table, the query should return 200 as the second highest salary. If there is no second highest salary, then the query should return null.


+---------------------+
| SecondHighestSalary |
+---------------------+
| 200                 |
+---------------------+
Solution
sql

#Solution 1:
SELECT Max(Salary) SecondHighestSalary
FROM Employee WHERE Salary < (SELECT MAX(Salary) FROM Employee)

#Solution 2:
WITH CTE AS (SELECT DISTINCT Salary
FROM Employee
ORDER BY Salary DESC
LIMIT 2)

SELECT Salary as SecondHighestSalary
FROM CTE
ORDER BY Salary Asc
LIMIT 1;

#Solution 3:
WITH CTE AS
(
    SELECT Salary,
           DENSE_RANK() OVER (ORDER BY Salary DESC) AS DENSERANK
    FROM Employee
)
SELECT Salary SecondHighestSalary
FROM CTE
WHERE DENSERANK = 2;
177. Nth Highest Salary | Medium | LeetCode
Write a SQL query to get the nth highest salary from the Employee table.


+----+--------+
| Id | Salary |
+----+--------+
| 1  | 100    |
| 2  | 200    |
| 3  | 300    |
+----+--------+
For example, given the above Employee table, the nth highest salary where n = 2 is 200. If there is no nth highest salary, then the query should return null.


+------------------------+
| getNthHighestSalary(2) |
+------------------------+
| 200                    |
+------------------------+
Solution
sql

CREATE FUNCTION getNthHighestSalary(N INT) RETURNS INT
BEGIN
SET N = N-1;
    RETURN(
        SELECT DISTINCT Salary FROM Employee ORDER BY Salary DESC
        LIMIT 1 OFFSET N
    );
END
178. Rank Scores | Medium | LeetCode
Write a SQL query to rank scores. If there is a tie between two scores, both should have the same ranking. Note that after a tie, the next ranking number should be the next consecutive integer value. In other words, there should be no "holes" between ranks.


+----+-------+
| Id | Score |
+----+-------+
| 1  | 3.50  |
| 2  | 3.65  |
| 3  | 4.00  |
| 4  | 3.85  |
| 5  | 4.00  |
| 6  | 3.65  |
+----+-------+
For example, given the above Scores table, your query should generate the following report (order by highest score):


+-------+---------+
| score | Rank    |
+-------+---------+
| 4.00  | 1       |
| 4.00  | 1       |
| 3.85  | 2       |
| 3.65  | 3       |
| 3.65  | 3       |
| 3.50  | 4       |
+-------+---------+
Important Note: For MySQL solutions, to escape reserved words used as column names, you can use an apostrophe before and after the keyword. For example Rank.

Solution
sql

SELECT score, DENSE_RANK() OVER (ORDER By Score DESC) AS "Rank"
FROM Scores;
180. Consecutive Numbers | Medium | LeetCode
Table: Logs


+-------------+---------+
| Column Name | Type    |
+-------------+---------+
| id          | int     |
| num         | varchar |
+-------------+---------+
id is the primary key for this table.
Write an SQL query to find all numbers that appear at least three times consecutively.

Return the result table in any order.

The query result format is in the following example:


Logs table:
+----+-----+
| Id | Num |
+----+-----+
| 1  | 1   |
| 2  | 1   |
| 3  | 1   |
| 4  | 2   |
| 5  | 1   |
| 6  | 2   |
| 7  | 2   |
+----+-----+

Result table:
+-----------------+
| ConsecutiveNums |
+-----------------+
| 1               |
+-----------------+
1 is the only number that appears consecutively for at least three times.
Solution
sql

SELECT a.Num as ConsecutiveNums
FROM Logs a
JOIN Logs b
ON a.id = b.id+1 AND a.num = b.num
JOIN Logs c
ON a.id = c.id+2 AND a.num = c.num;
181. Employees Earning More Than Their Managers | Easy | LeetCode
The Employee table holds all employees including their managers. Every employee has an Id, and there is also a column for the manager Id.


+----+-------+--------+-----------+
| Id | Name  | Salary | ManagerId |
+----+-------+--------+-----------+
| 1  | Joe   | 70000  | 3         |
| 2  | Henry | 80000  | 4         |
| 3  | Sam   | 60000  | NULL      |
| 4  | Max   | 90000  | NULL      |
+----+-------+--------+-----------+
Given the Employee table, write a SQL query that finds out employees who earn more than their managers. For the above table, Joe is the only employee who earns more than his manager.


+----------+
| Employee |
+----------+
| Joe      |
+----------+
Solution
sql

SELECT E.Name as "Employee"
FROM Employee E
JOIN Employee M
ON E.ManagerId = M.Id
AND E.Salary > M.Salary;
182. Duplicate Emails | Easy | LeetCode
Write a SQL query to find all duplicate emails in a table named Person.


+----+---------+
| Id | Email   |
+----+---------+
| 1  | a@b.com |
| 2  | c@d.com |
| 3  | a@b.com |
+----+---------+
For example, your query should return the following for the above table:


+---------+
| Email   |
+---------+
| a@b.com |
+---------+
Note: All emails are in lowercase.

Solution
sql

#Solution- 1:
SELECT Email
FROM Person
GROUP BY Email
HAVING count(*) > 1

#Solution- 2:
WITH CTE AS(
SELECT Email, ROW_NUMBER() OVER(PARTITION BY Email ORDER BY Email) AS RN
    FROM Person
)

SELECT Email
FROM CTE
WHERE RN > 1;
183. Customers Who Never Order | Easy | LeetCode
Suppose that a website contains two tables, the Customers table and the Orders table. Write a SQL query to find all customers who never order anything.

Table: Customers.


+----+-------+
| Id | Name  |
+----+-------+
| 1  | Joe   |
| 2  | Henry |
| 3  | Sam   |
| 4  | Max   |
+----+-------+
Table: Orders.


+----+------------+
| Id | CustomerId |
+----+------------+
| 1  | 3          |
| 2  | 1          |
+----+------------+
Using the above tables as example, return the following:


+-----------+
| Customers |
+-----------+
| Henry     |
| Max       |
+-----------+
Solution
sql

#Solution- 1:
SELECT Name AS Customers
FROM Customers
LEFT JOIN Orders
ON Customers.Id = Orders.CustomerId
WHERE CustomerId IS NULL;

#Solution- 2:
SELECT Name as Customers
FROM Customers
WHERE Id NOT IN(
    SELECT CustomerId
    FROM Orders
)
184. Department Highest Salary | Medium | LeetCode
The Employee table holds all employees. Every employee has an Id, a salary, and there is also a column for the department Id.


+----+-------+--------+--------------+
| Id | Name  | Salary | DepartmentId |
+----+-------+--------+--------------+
| 1  | Joe   | 70000  | 1            |
| 2  | Jim   | 90000  | 1            |
| 3  | Henry | 80000  | 2            |
| 4  | Sam   | 60000  | 2            |
| 5  | Max   | 90000  | 1            |
+----+-------+--------+--------------+
The Department table holds all departments of the company.


+----+----------+
| Id | Name     |
+----+----------+
| 1  | IT       |
| 2  | Sales    |
+----+----------+
Write a SQL query to find employees who have the highest salary in each of the departments. For the above tables, your SQL query should return the following rows (order of rows does not matter).


+------------+----------+--------+
| Department | Employee | Salary |
+------------+----------+--------+
| IT         | Max      | 90000  |
| IT         | Jim      | 90000  |
| Sales      | Henry    | 80000  |
+------------+----------+--------+
Explanation:
Max and Jim both have the highest salary in the IT department and Henry has the highest salary in the Sales department.

Solution
sql

SELECT Department.Name AS Department, Employee.Name AS Employee, Salary
FROM Employee
JOIN Department
ON Employee.DepartmentId = Department.Id
WHERE (DepartmentId, Salary) IN(
        SELECT  DepartmentId, MAX(Salary) AS Salary
        FROM Employee
        GROUP BY DepartmentId
        );
185. Department Top Three Salaries | Hard | LeetCode
The Employee table holds all employees. Every employee has an Id, and there is also a column for the department Id.


+----+-------+--------+--------------+
| Id | Name` | Salary | DepartmentId |
+----+-------+--------+--------------+
| 1  | Joe   | 85000  | 1            |
| 2  | Henry | 80000  | 2            |
| 3  | Sam   | 60000  | 2            |
| 4  | Max   | 90000  | 1            |
| 5  | Janet | 69000  | 1            |
| 6  | Randy | 85000  | 1            |
| 7  | Will  | 70000  | 1            |
+----+-------+--------+--------------+
The Department table holds all departments of the company.


+----+----------+
| Id | Name     |
+----+----------+
| 1  | IT       |
| 2  | Sales    |
+----+----------+
Write a SQL query to find employees who earn the top three salaries in each of the department. For the above tables, your SQL query should return the following rows (order of rows does not matter).


+------------+----------+--------+
| Department | Employee | Salary |
+------------+----------+--------+
| IT         | Max      | 90000  |
| IT         | Randy    | 85000  |
| IT         | Joe      | 85000  |
| IT         | Will     | 70000  |
| Sales      | Henry    | 80000  |
| Sales      | Sam      | 60000  |
+------------+----------+--------+
Explanation:
In IT department, Max earns the highest salary, both Randy and Joe earn the second highest salary, and Will earns the third highest salary. There are only two employees in the Sales department, Henry earns the highest salary while Sam earns the second highest salary.

Solution
sql

WITH department_ranking AS (
SELECT Name AS Employee, Salary ,DepartmentId
  ,DENSE_RANK() OVER (PARTITION BY DepartmentId ORDER BY Salary DESC) AS rnk
FROM Employee
)

SELECT d.Name AS Department, r.Employee, r.Salary
FROM department_ranking AS r
JOIN Department AS d
ON r.DepartmentId = d.Id
WHERE r.rnk <= 3
ORDER BY d.Name ASC, r.Salary DESC;
196. Delete Duplicate Emails | Easy | LeetCode
Write a SQL query to delete all duplicate email entries in a table named Person, keeping only unique emails based on its smallest Id.


+----+------------------+
| Id | Email            |
+----+------------------+
| 1  | john@example.com |
| 2  | bob@example.com  |
| 3  | john@example.com |
+----+------------------+
Id is the primary key column for this table. For example, after running your query, the above Person table should have the following rows:


+----+------------------+
| Id | Email            |
+----+------------------+
| 1  | john@example.com |
| 2  | bob@example.com  |
+----+------------------+
Note:
Your output is the whole Person table after executing your sql. Use delete statement.

Solution
sql

DELETE p2
FROM Person p1
JOIN Person p2
ON p1.Email = p2.Email
AND p1.id < p2.id
197. Rising Temperature | Easy | LeetCode
Table: Weather


+---------------+---------+
| Column Name   | Type    |
+---------------+---------+
| id            | int     |
| recordDate    | date    |
| temperature   | int     |
+---------------+---------+

id is the primary key for this table.
This table contains information about the temperature in a certain day.
Write an SQL query to find all dates' id with higher temperature compared to its previous dates (yesterday).

Return the result table in any order.

The query result format is in the following example:


Weather
+----+------------+-------------+
| id | recordDate | Temperature |
+----+------------+-------------+
| 1  | 2015-01-01 | 10          |
| 2  | 2015-01-02 | 25          |
| 3  | 2015-01-03 | 20          |
| 4  | 2015-01-04 | 30          |
+----+------------+-------------+

Result table:
+----+
| id |
+----+
| 2  |
| 4  |
+----+

In 2015-01-02, temperature was higher than the previous day (10 -> 25).
In 2015-01-04, temperature was higher than the previous day (20 -> 30).
Solution
sql

#Solution- 1:
SELECT t.Id
FROM Weather AS t, Weather AS y
WHERE DATEDIFF(t.RecordDate, y.RecordDate) = 1
AND t.Temperature > y.Temperature;

#Solution- 2:
SELECT t.Id
FROM Weather t
JOIN Weather y
ON DATEDIFF(t.recordDate, y.recordDate) = 1 AND
t.temperature > y.temperature;
262. Trips and Users | Hard | LeetCode
Table: Trips


+-------------+----------+
| Column Name | Type     |
+-------------+----------+
| Id          | int      |
| Client_Id   | int      |
| Driver_Id   | int      |
| City_Id     | int      |
| Status      | enum     |
| Request_at  | date     |
+-------------+----------+
Id is the primary key for this table.
The table holds all taxi trips. Each trip has a unique Id, while Client_Id and Driver_Id are foreign keys to the Users_Id at the Users table.
Status is an ENUM type of (‘completed’, ‘cancelled_by_driver’, ‘cancelled_by_client’).
Table: Users


+-------------+----------+
| Column Name | Type     |
+-------------+----------+
| Users_Id    | int      |
| Banned      | enum     |
| Role        | enum     |
+-------------+----------+
Users_Id is the primary key for this table.
The table holds all users. Each user has a unique Users_Id, and Role is an ENUM type of (‘client’, ‘driver’, ‘partner’).
Status is an ENUM type of (‘Yes’, ‘No’).
Write a SQL query to find the cancellation rate of requests with unbanned users (both client and driver must not be banned) each day between "2013-10-01" and "2013-10-03".

The cancellation rate is computed by dividing the number of canceled (by client or driver) requests with unbanned users by the total number of requests with unbanned users on that day.

Return the result table in any order. Round Cancellation Rate to two decimal points.

The query result format is in the following example:


Trips table:
+----+-----------+-----------+---------+---------------------+------------+
| Id | Client_Id | Driver_Id | City_Id | Status              | Request_at |
+----+-----------+-----------+---------+---------------------+------------+
| 1  | 1         | 10        | 1       | completed           | 2013-10-01 |
| 2  | 2         | 11        | 1       | cancelled_by_driver | 2013-10-01 |
| 3  | 3         | 12        | 6       | completed           | 2013-10-01 |
| 4  | 4         | 13        | 6       | cancelled_by_client | 2013-10-01 |
| 5  | 1         | 10        | 1       | completed           | 2013-10-02 |
| 6  | 2         | 11        | 6       | completed           | 2013-10-02 |
| 7  | 3         | 12        | 6       | completed           | 2013-10-02 |
| 8  | 2         | 12        | 12      | completed           | 2013-10-03 |
| 9  | 3         | 10        | 12      | completed           | 2013-10-03 |
| 10 | 4         | 13        | 12      | cancelled_by_driver | 2013-10-03 |
+----+-----------+-----------+---------+---------------------+------------+

Users table:
+----------+--------+--------+
| Users_Id | Banned | Role   |
+----------+--------+--------+
| 1        | No     | client |
| 2        | Yes    | client |
| 3        | No     | client |
| 4        | No     | client |
| 10       | No     | driver |
| 11       | No     | driver |
| 12       | No     | driver |
| 13       | No     | driver |
+----------+--------+--------+

Result table:
+------------+-------------------+
| Day        | Cancellation Rate |
+------------+-------------------+
| 2013-10-01 | 0.33              |
| 2013-10-02 | 0.00              |
| 2013-10-03 | 0.50              |
+------------+-------------------+

On 2013-10-01:

- There were 4 requests in total, 2 of which were canceled.
- However, the request with Id=2 was made by a banned client (User_Id=2), so it is ignored in the calculation.
- Hence there are 3 unbanned requests in total, 1 of which was canceled.
- The Cancellation Rate is (1 / 3) = 0.33
  On 2013-10-02:
- There were 3 requests in total, 0 of which were canceled.
- The request with Id=6 was made by a banned client, so it is ignored.
- Hence there are 2 unbanned requests in total, 0 of which were canceled.
- The Cancellation Rate is (0 / 2) = 0.00
  On 2013-10-03:
- There were 3 requests in total, 1 of which was canceled.
- The request with Id=8 was made by a banned client, so it is ignored.
- Hence there are 2 unbanned request in total, 1 of which were canceled.
- The Cancellation Rate is (1 / 2) = 0.50
Solution
sql

SELECT Request_at AS Day,
ROUND(SUM(IF(Status<>"completed", 1, 0))/COUNT(Status),2) AS "Cancellation Rate"
FROM Trips
WHERE Request_at BETWEEN "2013-10-01" AND "2013-10-03"
AND Client_Id NOT IN (SELECT Users_Id FROM Users WHERE Banned = 'Yes')
AND Driver_Id NOT IN (SELECT Users_Id FROM Users WHERE Banned = 'Yes')
GROUP BY Request_at;
511. Game Play Analysis I | Easy | 🔒 LeetCode
Table: Activity


+--------------+---------+
| Column Name  | Type    |
+--------------+---------+
| player_id    | int     |
| device_id    | int     |
| event_date   | date    |
| games_played | int     |
+--------------+---------+
(player_id, event_date) is the primary key of this table.
This table shows the activity of players of some game.
Each row is a record of a player who logged in and played a number of games (possibly 0) before logging out on some day using some device.
Write an SQL query that reports the first login date for each player.

The query result format is in the following example:


Activity table:
+-----------+-----------+------------+--------------+
| player_id | device_id | event_date | games_played |
+-----------+-----------+------------+--------------+
| 1         | 2         | 2016-03-01 | 5            |
| 1         | 2         | 2016-05-02 | 6            |
| 2         | 3         | 2017-06-25 | 1            |
| 3         | 1         | 2016-03-02 | 0            |
| 3         | 4         | 2018-07-03 | 5            |
+-----------+-----------+------------+--------------+

Result table:
+-----------+-------------+
| player_id | first_login |
+-----------+-------------+
| 1         | 2016-03-01  |
| 2         | 2017-06-25  |
| 3         | 2016-03-02  |
+-----------+-------------+
Solution
sql

SELECT player_id, MIN(event_date) as first_login
FROM Activity
GROUP BY player_id
512. Game Play Analysis II | Easy | 🔒 LeetCode
Table: Activity


+--------------+---------+
| Column Name  | Type    |
+--------------+---------+
| player_id    | int     |
| device_id    | int     |
| event_date   | date    |
| games_played | int     |
+--------------+---------+
(player_id, event_date) is the primary key of this table.
This table shows the activity of players of some game.
Each row is a record of a player who logged in and played a number of games (possibly 0) before logging out on some day using some device.
Write a SQL query that reports the device that is first logged in for each player.

The query result format is in the following example:


Activity table:
+-----------+-----------+------------+--------------+
| player_id | device_id | event_date | games_played |
+-----------+-----------+------------+--------------+
| 1         | 2         | 2016-03-01 | 5            |
| 1         | 2         | 2016-05-02 | 6            |
| 2         | 3         | 2017-06-25 | 1            |
| 3         | 1         | 2016-03-02 | 0            |
| 3         | 4         | 2018-07-03 | 5            |
+-----------+-----------+------------+--------------+

Result table:
+-----------+-----------+
| player_id | device_id |
+-----------+-----------+
| 1         | 2         |
| 2         | 3         |
| 3         | 1         |
+-----------+-----------+
Solution
sql

#Solution- 1:
SELECT DISTINCT player_id, device_id
FROM Activity
WHERE (player_id, event_date) in (
    SELECT player_id, min(event_date)
    FROM Activity
    GROUP BY player_id)

#Solution- 2:
SELECT a.player_id, b.device_id
FROM
(SELECT player_id, MIN(event_date) AS event_date FROM Activity
GROUP BY player_id) a
JOIN Activity b
ON a.player_id = b.player_id AND a.event_date = b.event_date;

#Solution- 3:
SELECT player_id, device_id
FROM
(SELECT player_id, device_id, event_date,
ROW_NUMBER() OVER (PARTITION BY player_id ORDER BY event_date) AS r
FROM Activity) lookup
WHERE r = 1;
534. Game Play Analysis III | Medium | 🔒 LeetCode
Table: Activity


+--------------+---------+
| Column Name  | Type    |
+--------------+---------+
| player_id    | int     |
| device_id    | int     |
| event_date   | date    |
| games_played | int     |
+--------------+---------+
(player_id, event_date) is the primary key of this table.
This table shows the activity of players of some game.
Each row is a record of a player who logged in and played a number of games (possibly 0) before logging out on some day using some device.
Write an SQL query that reports for each player and date, how many games played so far by the player. That is, the total number of games played by the player until that date. Check the example for clarity.

The query result format is in the following example:


Activity table:
+-----------+-----------+------------+--------------+
| player_id | device_id | event_date | games_played |
+-----------+-----------+------------+--------------+
| 1         | 2         | 2016-03-01 | 5            |
| 1         | 2         | 2016-05-02 | 6            |
| 1         | 3         | 2017-06-25 | 1            |
| 3         | 1         | 2016-03-02 | 0            |
| 3         | 4         | 2018-07-03 | 5            |
+-----------+-----------+------------+--------------+

Result table:
+-----------+------------+---------------------+
| player_id | event_date | games_played_so_far |
+-----------+------------+---------------------+
| 1         | 2016-03-01 | 5                   |
| 1         | 2016-05-02 | 11                  |
| 1         | 2017-06-25 | 12                  |
| 3         | 2016-03-02 | 0                   |
| 3         | 2018-07-03 | 5                   |
+-----------+------------+---------------------+
For the player with id 1, 5 + 6 = 11 games played by 2016-05-02, and 5 + 6 + 1 = 12 games played by 2017-06-25.
For the player with id 3, 0 + 5 = 5 games played by 2018-07-03.
Note that for each player we only care about the days when the player logged in.
Solution
sql

#Solution- 1:
SELECT t1.player_id, t1.event_date, SUM(t2.games_played) as games_played_so_far
FROM Activity t1
JOIN Activity t2
ON t1.player_id = t2.player_id
WHERE t1.event_date >= t2.event_date
GROUP BY t1.player_id, t1.event_date;

#Solution- 2:
SELECT player_id, event_date,
SUM(games_played) OVER (PARTITION BY player_id ORDER BY event_date) AS games_played_so_far
FROM Activity;
550. Game Play Analysis IV | Medium | 🔒 LeetCode
Table: Activity


+--------------+---------+
| Column Name  | Type    |
+--------------+---------+
| player_id    | int     |
| device_id    | int     |
| event_date   | date    |
| games_played | int     |
+--------------+---------+
(player_id, event_date) is the primary key of this table.
This table shows the activity of players of some game.
Each row is a record of a player who logged in and played a number of games (possibly 0) before logging out on some day using some device.
Write an SQL query that reports the fraction of players that logged in again on the day after the day they first logged in, rounded to 2 decimal places. In other words, you need to count the number of players that logged in for at least two consecutive days starting from their first login date, then divide that number by the total number of players.

The query result format is in the following example:


Activity table:
+-----------+-----------+------------+--------------+
| player_id | device_id | event_date | games_played |
+-----------+-----------+------------+--------------+
| 1         | 2         | 2016-03-01 | 5            |
| 1         | 2         | 2016-03-02 | 6            |
| 2         | 3         | 2017-06-25 | 1            |
| 3         | 1         | 2016-03-02 | 0            |
| 3         | 4         | 2018-07-03 | 5            |
+-----------+-----------+------------+--------------+

Result table:
+-----------+
| fraction  |
+-----------+
| 0.33      |
+-----------+
Only the player with id 1 logged back in after the first day he had logged in so the answer is 1/3 = 0.33
Solution
sql

#Solution- 1:
SELECT ROUND(sum(CASE WHEN t1.event_date = t2.first_event+1 THEN 1 ELSE 0 END)/COUNT(DISTINCT t1.player_id), 2) AS fraction
FROM Activity t1
JOIN
    (SELECT player_id, MIN(event_date) AS first_event
    FROM Activity
    GROUP BY player_id) t2
    ON t1.player_id = t2.player_id;

#Solution- 2:
SELECT ROUND(COUNT(DISTINCT b.player_id)/COUNT(DISTINCT a.player_id),2) AS fraction
FROM
  (SELECT player_id, MIN(event_date) AS event_date FROM Activity
  GROUP BY player_id) a
  LEFT JOIN Activity b
  ON a.player_id = b.player_id AND a.event_date+1 = b.event_date;
569. Median Employee Salary | Hard | 🔒 LeetCode
The Employee table holds all employees. The employee table has three columns: Employee Id, Company Name, and Salary.


+-----+------------+--------+
|Id   | Company    | Salary |
+-----+------------+--------+
|1    | A          | 2341   |
|2    | A          | 341    |
|3    | A          | 15     |
|4    | A          | 15314  |
|5    | A          | 451    |
|6    | A          | 513    |
|7    | B          | 15     |
|8    | B          | 13     |
|9    | B          | 1154   |
|10   | B          | 1345   |
|11   | B          | 1221   |
|12   | B          | 234    |
|13   | C          | 2345   |
|14   | C          | 2645   |
|15   | C          | 2645   |
|16   | C          | 2652   |
|17   | C          | 65     |
+-----+------------+--------+
Write a SQL query to find the median salary of each company. Bonus points if you can solve it without using any built-in SQL functions.


+-----+------------+--------+
|Id   | Company    | Salary |
+-----+------------+--------+
|5    | A          | 451    |
|6    | A          | 513    |
|12   | B          | 234    |
|9    | B          | 1154   |
|14   | C          | 2645   |
+-----+------------+--------+
Solution
sql

SELECT t1.Id AS Id, t1.Company, t1.Salary
FROM Employee AS t1 JOIN Employee AS t2
ON t1.Company = t2.Company
GROUP BY t1.Id
HAVING abs(sum(CASE WHEN t2.Salary<t1.Salary THEN 1
                  WHEN t2.Salary>t1.Salary THEN -1
                  WHEN t2.Salary=t1.Salary AND t2.Id<t1.Id THEN 1
                  WHEN t2.Salary=t1.Salary AND t2.Id>t1.Id THEN -1
                  ELSE 0 END)) <= 1
ORDER BY t1.Company, t1.Salary, t1.Id
570. Managers with at Least 5 Direct Reports | Medium | 🔒 LeetCode
The Employee table holds all employees including their managers. Every employee has an Id, and there is also a column for the manager Id.


+------+----------+-----------+----------+
|Id    |Name 	    |Department |ManagerId |
+------+----------+-----------+----------+
|101   |John 	    |A 	        |null      |
|102   |Dan 	    |A 	        |101       |
|103   |James 	  |A 	        |101       |
|104   |Amy 	    |A 	        |101       |
|105   |Anne 	    |A 	        |101       |
|106   |Ron 	    |B 	        |101       |
+------+----------+-----------+----------+
Given the Employee table, write a SQL query that finds out managers with at least 5 direct report. For the above table, your SQL query should return:


+-------+
| Name  |
+-------+
| John  |
+-------+
Note: No one would report to himself.

Solution
sql

SELECT Name
FROM Employee
WHERE id IN
   (SELECT ManagerId
    FROM Employee
    GROUP BY ManagerId
    HAVING COUNT(DISTINCT Id) >= 5)
571. Find Median Given Frequency of Numbers | 🔒 LeetCode
The Numbers table keeps the value of number and its frequency.


+----------+-------------+
|  Number  |  Frequency  |
+----------+-------------|
|  0       |  7          |
|  1       |  1          |
|  2       |  3          |
|  3       |  1          |
+----------+-------------+
In this table, the numbers are 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 3, so the median is (0 + 0) / 2 = 0.


+--------+
| median |
+--------|
| 0.0000 |
+--------+
Write a query to find the median of all numbers and name the result as median.

Solution
sql

SELECT avg(t3.Number) as median
FROM Numbers as t3
JOIN
    (SELECT t1.Number,
        abs(SUM(CASE WHEN t1.Number>t2.Number THEN t2.Frequency ELSE 0 END) -
            SUM(CASE WHEN t1.Number<t2.Number THEN t2.Frequency ELSE 0 END)) AS count_diff
    FROM numbers AS t1, numbers AS t2
    GROUP BY t1.Number) AS t4
ON t3.Number = t4.Number
WHERE t3.Frequency>=t4.count_diff
574. Winning Candidate | Medium | 🔒 LeetCode
Table: Candidate


+-----+---------+
| id  | Name    |
+-----+---------+
| 1   | A       |
| 2   | B       |
| 3   | C       |
| 4   | D       |
| 5   | E       |
+-----+---------+
Table: Vote


+-----+--------------+
| id  | CandidateId  |
+-----+--------------+
| 1   |     2        |
| 2   |     4        |
| 3   |     3        |
| 4   |     2        |
| 5   |     5        |
+-----+--------------+
id is the auto-increment primary key, CandidateId is the id appeared in Candidate table. Write a sql to find the name of the winning candidate, the above example will return the winner B.


+------+
| Name |
+------+
| B    |
+------+
Notes: You may assume there is no tie, in other words there will be at most one winning candidate.

Solution
sql

SELECT Name
FROM Candidate
WHERE id = (SELECT CandidateId
     FROM Vote
     GROUP BY CandidateId
     ORDER BY COUNT(1) desc
     LIMIT 1)

## Assumption: if we have two candidates with the same votes, we choose the one who get the first vote
# SELECT Name
# FROM Candidate JOIN
#     (SELECT CandidateId
#     FROM Vote
#     GROUP BY CandidateId
#     ORDER BY count(1) DESC
#     LIMIT 1) AS t
# ON Candidate.id = t.CandidateId
577. Employee Bonus | Easy | 🔒 LeetCode
Select all employee’s name and bonus whose bonus is < 1000.

Table:Employee


+-------+--------+-----------+--------+
| empId |  name  | supervisor| salary |
+-------+--------+-----------+--------+
|   1   | John   |  3        | 1000   |
|   2   | Dan    |  3        | 2000   |
|   3   | Brad   |  null     | 4000   |
|   4   | Thomas |  3        | 4000   |
+-------+--------+-----------+--------+
empId is the primary key column for this table.
Table: Bonus


+-------+-------+
| empId | bonus |
+-------+-------+
| 2     | 500   |
| 4     | 2000  |
+-------+-------+
empId is the primary key column for this table.
Example ouput:


+-------+-------+
| name  | bonus |
+-------+-------+
| John  | null  |
| Dan   | 500   |
| Brad  | null  |
+-------+-------+
Solution
sql

SELECT name, bonus
FROM Employee LEFT JOIN Bonus
ON Employee.empId = Bonus.empId
WHERE bonus<1000 OR bonus IS NULL;
578. Get Highest Answer Rate Question | Medium | 🔒 LeetCode
Get the highest answer rate question from a table survey_log with these columns: uid, action, question_id, answer_id, q_num, timestamp.

uid means user id; action has these kind of values: “show”, “answer”, “skip”; answer_id is not null when action column is “answer”, while is null for “show” and “skip”; q_num is the numeral order of the question in current session.

Write a sql query to identify the question which has the highest answer rate.

Example: Input:


+------+-----------+--------------+------------+-----------+------------+
| uid  | action    | question_id  | answer_id  | q_num     | timestamp  |
+------+-----------+--------------+------------+-----------+------------+
| 5    | show      | 285          | null       | 1         | 123        |
| 5    | answer    | 285          | 124124     | 1         | 124        |
| 5    | show      | 369          | null       | 2         | 125        |
| 5    | skip      | 369          | null       | 2         | 126        |
+------+-----------+--------------+------------+-----------+------------+
Output:


+-------------+
| survey_log  |
+-------------+
|    285      |
+-------------+
Explanation: question 285 has answer rate 1/1, while question 369 has 0/1 answer rate, so output 285.

Note: The highest answer rate meaning is: answer number’s ratio in show number in the same question.

Solution
sql

#Solution- 1::
SELECT question_id AS survey_log FROM
(SELECT question_id,
        SUM(IF(action='show', 1, 0)) AS num_show,
        SUM(IF(action='answer', 1, 0)) AS num_answer
 FROM survey_log GROUP BY question_id) AS t
ORDER BY (num_answer/num_show) DESC LIMIT 1;

#Solution- 2:
SELECT question_id AS survey_log
FROM (SELECT question_id,
      sum(CASE WHEN  action='show' THEN 1 ELSE 0 END) AS show_count,
      sum(CASE WHEN  action='answer' THEN 1 ELSE 0 END) AS answer_count
    FROM survey_log
    GROUP BY question_id) AS t
ORDER BY answer_count/show_count DESC LIMIT 1;
579. Find Cumulative Salary of an Employee | Hard | 🔒 LeetCode
The Employee table holds the salary information in a year.

Write a SQL to get the cumulative sum of an employee’s salary over a period of 3 months but exclude the most recent month.

The result should be displayed by ‘Id’ ascending, and then by ‘Month’ descending.

Example Input


| Id | Month | Salary |
|----|-------|--------|
| 1  | 1     | 20     |
| 2  | 1     | 20     |
| 1  | 2     | 30     |
| 2  | 2     | 30     |
| 3  | 2     | 40     |
| 1  | 3     | 40     |
| 3  | 3     | 60     |
| 1  | 4     | 60     |
| 3  | 4     | 70     |
Output


| Id | Month | Salary |
|----|-------|--------|
| 1  | 3     | 90     |
| 1  | 2     | 50     |
| 1  | 1     | 20     |
| 2  | 1     | 20     |
| 3  | 3     | 100    |
| 3  | 2     | 40     |
Explanation Employee ‘1’ has 3 salary records for the following 3 months except the most recent month ‘4’: salary 40 for month ‘3’, 30 for month ‘2’ and 20 for month ‘1’ So the cumulative sum of salary of this employee over 3 months is 90(40+30+20), 50(30+20) and 20 respectively.


| Id | Month | Salary |
|----|-------|--------|
| 1  | 3     | 90     |
| 1  | 2     | 50     |
| 1  | 1     | 20     |
Employee ‘2’ only has one salary record (month ‘1’) except its most recent month ‘2’.


| Id | Month | Salary |
|----|-------|--------|
| 2  | 1     | 20     |
Employ ‘3’ has two salary records except its most recent pay month ‘4’: month ‘3’ with 60 and month ‘2’ with 40. So the cumulative salary is as following.


| Id | Month | Salary |
|----|-------|--------|
| 3  | 3     | 100    |
| 3  | 2     | 40     |
Solution
sql

SELECT
    a.id,
    a.month,
    SUM(b.salary) Salary
FROM
    Employee a JOIN Employee b ON
    a.id = b.id AND
    a.month - b.month >= 0 AND
    a.month - b.month < 3
GROUP BY
    a.id, a.month
HAVING
    (a.id, a.month) NOT IN (SELECT id, MAX(month) FROM Employee GROUP BY id)
ORDER BY
    a.id, a.month DESC
580. Count Student Number in Departments | Medium | 🔒 LeetCode
A university uses 2 data tables, student and department, to store data about its students and the departments associated with each major.

Write a query to print the respective department name and number of students majoring in each department for all departments in the department table (even ones with no current students).

Sort your results by descending number of students; if two or more departments have the same number of students, then sort those departments alphabetically by department name.

The student is described as follow:


| Column Name  | Type      |
|--------------|-----------|
| student_id   | Integer   |
| student_name | String    |
| gender       | Character |
| dept_id      | Integer   |
where student_id is the student’s ID number, student_name is the student’s name, gender is their gender, and dept_id is the department ID associated with their declared major.

And the department table is described as below:


| Column Name | Type    |
|-------------|---------|
| dept_id     | Integer |
| dept_name   | String  |
where dept_id is the department’s ID number and dept_name is the department name.

Here is an example input: student table:


| student_id | student_name | gender | dept_id |
|------------|--------------|--------|---------|
| 1          | Jack         | M      | 1       |
| 2          | Jane         | F      | 1       |
| 3          | Mark         | M      | 2       |
department table:


| dept_id | dept_name   |
|---------|-------------|
| 1       | Engineering |
| 2       | Science     |
| 3       | Law         |
The Output should be:


| dept_name   | student_number |
|-------------|----------------|
| Engineering | 2              |
| Science     | 1              |
| Law         | 0              |
Solution
sql

SELECT dept_name,
  SUM(CASE WHEN student_id IS NULL THEN 0 ELSE 1 END) AS student_number
FROM department
LEFT JOIN student
ON department.dept_id = student.dept_id
GROUP BY department.dept_id
ORDER BY student_number DESC, dept_name
584. Find Customer Referee | Easy | 🔒 LeetCode
Given a table customer holding customers information and the referee.


+------+------+-----------+
| id   | name | referee_id|
+------+------+-----------+
|    1 | Will |      NULL |
|    2 | Jane |      NULL |
|    3 | Alex |         2 |
|    4 | Bill |      NULL |
|    5 | Zack |         1 |
|    6 | Mark |         2 |
+------+------+-----------+
Write a query to return the list of customers NOT referred by the person with id ‘2’.

For the sample data above, the result is:


+------+
| name |
+------+
| Will |
| Jane |
| Bill |
| Zack |
+------+
Solution
sql

SELECT name
FROM customer
WHERE referee_id != '2' OR referee_id IS NULL;
585. Investments in 2016 | Medium | 🔒 LeetCode
Write a query to print the sum of all total investment values in 2016 (TIV_2016), to a scale of 2 decimal places, for all policy holders who meet the following criteria:

Have the same TIV_2015 value as one or more other policyholders.
Are not located in the same city as any other policyholder (i.e.: the (latitude, longitude) attribute pairs must be unique). Input Format: The insurance table is described as follows:

| Column Name | Type          |
|-------------|---------------|
| PID         | INTEGER(11)   |
| TIV_2015    | NUMERIC(15,2) |
| TIV_2016    | NUMERIC(15,2) |
| LAT         | NUMERIC(5,2)  |
| LON         | NUMERIC(5,2)  |
where PID is the policyholder’s policy ID, TIV_2015 is the total investment value in 2015, TIV_2016 is the total investment value in 2016, LAT is the latitude of the policy holder’s city, and LON is the longitude of the policy holder’s city.

Sample Input


| PID | TIV_2015 | TIV_2016 | LAT | LON |
|-----|----------|----------|-----|-----|
| 1   | 10       | 5        | 10  | 10  |
| 2   | 20       | 20       | 20  | 20  |
| 3   | 10       | 30       | 20  | 20  |
| 4   | 10       | 40       | 40  | 40  |
Sample Output


| TIV_2016 |
|----------|
| 45.00    |
Explanation


The first record in the table, like the last record, meets both of the two criteria.
The TIV_2015 value '10' is as the same as the third and forth record, and its location unique.

The second record does not meet any of the two criteria. Its TIV_2015 is not like any other policyholders.

And its location is the same with the third record, which makes the third record fail, too.

So, the result is the sum of TIV_2016 of the first and last record, which is 45.
Solution
sql

SELECT SUM(TIV_2016) AS TIV_2016
FROM insurance
WHERE CONCAT(LAT, ',', LON)
    IN (SELECT CONCAT(LAT, ',', LON)
       FROM insurance
       GROUP BY LAT, LON
       HAVING COUNT(1) = 1)
AND TIV_2015 in
    (SELECT TIV_2015
    FROM insurance
    GROUP BY TIV_2015
    HAVING COUNT(1)>1)
586. Customer Placing the Largest Number of Orders | Easy | 🔒 LeetCode
Query the customer_number from the orders table for the customer who has placed the largest number of orders.

It is guaranteed that exactly one customer will have placed more orders than any other customer.

The orders table is defined as follows:


| Column            | Type      |
|-------------------|-----------|
| order_number (PK) | int       |
| customer_number   | int       |
| order_date        | date      |
| required_date     | date      |
| shipped_date      | date      |
| status            | char(15)  |
| comment           | char(200) |
Sample Input


| order_number | customer_number | order_date | required_date | shipped_date | status | comment |
|--------------|-----------------|------------|---------------|--------------|--------|---------|
| 1            | 1               | 2017-04-09 | 2017-04-13    | 2017-04-12   | Closed |         |
| 2            | 2               | 2017-04-15 | 2017-04-20    | 2017-04-18   | Closed |         |
| 3            | 3               | 2017-04-16 | 2017-04-25    | 2017-04-20   | Closed |         |
| 4            | 3               | 2017-04-18 | 2017-04-28    | 2017-04-25   | Closed |         |
Sample Output


| customer_number |
|-----------------|
| 3               |
Explanation


The customer with number '3' has two orders, which is greater than either customer '1' or '2' because each of them  only has one order.
So the result is customer_number '3'.
Solution
sql

# assume: only one match
SELECT customer_number FROM orders
GROUP BY customer_number
ORDER BY COUNT(1) DESC
LIMIT 1

## assume: multiple matches
##  1 1
##  2 1
##  3 1
##
##  1 1 1 1
##  1 1 2 1
##  1 1 3 1
##
##  SELECT t1.customer_number
##  FROM (SELECT customer_number, COUNT(1) AS count
##        FROM orders GROUP BY customer_number) AS t1,
##        (SELECT customer_number, COUNT(1) AS count
##        FROM orders GROUP BY customer_number) AS t2
##  GROUP BY t1.customer_number
##  HAVING max(t1.count) = max(t2.count)
595. Big Countries | Easy | LeetCode
There is a table World


+-----------------+------------+------------+--------------+---------------+
| name            | continent  | area       | population   | gdp           |
+-----------------+------------+------------+--------------+---------------+
| Afghanistan     | Asia       | 652230     | 25500100     | 20343000      |
| Albania         | Europe     | 28748      | 2831741      | 12960000      |
| Algeria         | Africa     | 2381741    | 37100000     | 188681000     |
| Andorra         | Europe     | 468        | 78115        | 3712000       |
| Angola          | Africa     | 1246700    | 20609294     | 100990000     |
+-----------------+------------+------------+--------------+---------------+
A country is big if it has an area of bigger than 3 million square km or a population of more than 25 million.

Write a SQL solution to output big countries' name, population and area.

For example, according to the above table, we should output:


+--------------+-------------+--------------+
| name         | population  | area         |
+--------------+-------------+--------------+
| Afghanistan  | 25500100    | 652230       |
| Algeria      | 37100000    | 2381741      |
+--------------+-------------+--------------+
Solution
sql

SELECT name, population, area
FROM World
WHERE area >= 3000000 OR population > 25000000;
596. Classes More Than 5 Students | Easy | LeetCode
There is a table courses with columns: student and class

Please list out all classes which have more than or equal to 5 students.

For example, the table:


+---------+------------+
| student | class      |
+---------+------------+
| A       | Math       |
| B       | English    |
| C       | Math       |
| D       | Biology    |
| E       | Math       |
| F       | Computer   |
| G       | Math       |
| H       | Math       |
| I       | Math       |
+---------+------------+
Should output:


+---------+
| class   |
+---------+
| Math    |
+---------+
Solution
sql

SELECT class
FROM courses
GROUP BY class
HAVING count(DISTINCT Student)>=5;
597. Friend Requests I: Overall Acceptance Rate | Easy | 🔒 LeetCode
In social network like Facebook or Twitter, people send friend requests and accept others’ requests as well. Now given two tables as below: Table: friend_request


| sender_id | send_to_id |request_date|
|-----------|------------|------------|
| 1         | 2          | 2016_06-01 |
| 1         | 3          | 2016_06-01 |
| 1         | 4          | 2016_06-01 |
| 2         | 3          | 2016_06-02 |
| 3         | 4          | 2016-06-09 |
Table: request_accepted


| requester_id | accepter_id |accept_date |
|--------------|-------------|------------|
| 1            | 2           | 2016_06-03 |
| 1            | 3           | 2016-06-08 |
| 2            | 3           | 2016-06-08 |
| 3            | 4           | 2016-06-09 |
| 3            | 4           | 2016-06-10 |
Write a query to find the overall acceptance rate of requests rounded to 2 decimals, which is the number of acceptance divide the number of requests. For the sample data above, your query should return the following result.


|accept_rate|
|-----------|
|       0.80|
Note:

The accepted requests are not necessarily from the table friend_request. In this case, you just need to simply count the total accepted requests (no matter whether they are in the original requests), and divide it by the number of requests to get the acceptance rate. It is possible that a sender sends multiple requests to the same receiver, and a request could be accepted more than once. In this case, the ‘duplicated’ requests or acceptances are only counted once. If there is no requests at all, you should return 0.00 as the accept_rate. Explanation: There are 4 unique accepted requests, and there are 5 requests in total. So the rate is 0.80.

Follow-up:

Can you write a query to return the accept rate but for every month? How about the cumulative accept rate for every day?

Solution
sql

SELECT IFNULL((round(accepts/requests, 2)), 0.0) AS accept_rate
FROM
    (SELECT count(DISTINCT sender_id, send_to_id) AS requests FROM friend_request) AS t1,
    (SELECT count(DISTINCT requester_id, accepter_id) AS accepts FROM request_accepted) AS t2
601. Human Traffic of Stadium | Hard | LeetCode
Table: Stadium


+---------------+---------+
| Column Name   | Type    |
+---------------+---------+
| id            | int     |
| visit_date    | date    |
| people        | int     |
+---------------+---------+
visit_date is the primary key for this table. Each row of this table contains the visit date and visit id to the stadium with the number of people during the visit. No two rows will have the same visit_date, and as the id increases, the dates increase as well.

Write an SQL query to display the records with three or more rows with consecutive id's, and the number of people is greater than or equal to 100 for each.

Return the result table ordered by visit_date in ascending order.

The query result format is in the following example.


Stadium table:
+------+------------+-----------+
| id   | visit_date | people    |
+------+------------+-----------+
| 1    | 2017-01-01 | 10        |
| 2    | 2017-01-02 | 109       |
| 3    | 2017-01-03 | 150       |
| 4    | 2017-01-04 | 99        |
| 5    | 2017-01-05 | 145       |
| 6    | 2017-01-06 | 1455      |
| 7    | 2017-01-07 | 199       |
| 8    | 2017-01-09 | 188       |
+------+------------+-----------+

Result table:
+------+------------+-----------+
| id   | visit_date | people    |
+------+------------+-----------+
| 5    | 2017-01-05 | 145       |
| 6    | 2017-01-06 | 1455      |
| 7    | 2017-01-07 | 199       |
| 8    | 2017-01-09 | 188       |
+------+------------+-----------+
The four rows with ids 5, 6, 7, and 8 have consecutive ids and each of them has >= 100 people attended. Note that row 8 was included even though the visit_date was not the next day after row 7.
The rows with ids 2 and 3 are not included because we need at least three consecutive ids.
Solution
sql

SELECT DISTINCT s1.*
FROM Stadium s1 JOIN Stadium s2 JOIN Stadium s3
ON (s1.id = s2.id-1 AND s1.id = s3.id-2) OR
 (s1.id = s2.id+1 AND s1.id = s3.id-1) OR
 (s1.id = s2.id+1 AND s1.id = s3.id+2)
    WHERE s1.people >= 100 AND s2.people >= 100 AND s3.people>=100
  ORDER BY visit_date
602. Friend Requests II: Who Has the Most Friends | Medium | 🔒 LeetCode
In social network like Facebook or Twitter, people send friend requests and accept others’ requests as well. Table request_accepted holds the data of friend acceptance, while requester_id and accepter_id both are the id of a person.


| requester_id | accepter_id | accept_date|
|--------------|-------------|------------|
| 1            | 2           | 2016_06-03 |
| 1            | 3           | 2016-06-08 |
| 2            | 3           | 2016-06-08 |
| 3            | 4           | 2016-06-09 |
Write a query to find the the people who has most friends and the most friends number. For the sample data above, the result is:


| id | num |
|----|-----|
| 3  | 3   |
Note:

It is guaranteed there is only 1 people having the most friends. The friend request could only been accepted once, which mean there is no multiple records with the same requester_id and accepter_id value. Explanation: The person with id ‘3’ is a friend of people ‘1’, ‘2’ and ‘4’, so he has 3 friends in total, which is the most number than any others.

Follow-up: In the real world, multiple people could have the same most number of friends, can you find all these people in this case?

sql

SELECT t.id, sum(t.num) AS num
FROM (
      (SELECT requester_id AS id, COUNT(1) AS num
       FROM request_accepted
       GROUP BY requester_id)
      union all
       (SELECT accepter_id AS id, COUNT(1) AS num
        FROM request_accepted
        GROUP BY accepter_id)) AS t
GROUP BY t.id
ORDER BY num DESC
LIMIT 1;
603. Consecutive Available Seats | Easy | 🔒 LeetCode
Several friends at a cinema ticket office would like to reserve consecutive available seats. Can you help to query all the consecutive available seats order by the seat_id using the following cinema table?


| seat_id | free |
|---------|------|
| 1       | 1    |
| 2       | 0    |
| 3       | 1    |
| 4       | 1    |
| 5       | 1    |
Your query should return the following result for the sample case above.


| seat_id |
|---------|
| 3       |
| 4       |
| 5       |
Note:

The seat_id is an auto increment int, and free is bool (‘1’ means free, and ‘0’ means occupied.). Consecutive available seats are more than 2(inclusive) seats consecutively available.

Solution
sql

SELECT DISTINCT t1.seat_id
FROM cinema AS t1 JOIN cinema AS t2
ON abs(t1.seat_id-t2.seat_id)=1
WHERE t1.free='1' AND t2.free='1'
ORDER BY t1.seat_id
607.Sales Person | Easy | 🔒 LeetCode
Description

Given three tables: salesperson, company, orders. Output all the names in the table salesperson, who didn’t have sales to company ‘RED’.

Example Input

Table: salesperson


+----------+------+--------+-----------------+-----------+
| sales_id | name | salary | commission_rate | hire_date |
+----------+------+--------+-----------------+-----------+
|   1      | John | 100000 |     6           | 4/1/2006  |
|   2      | Amy  | 120000 |     5           | 5/1/2010  |
|   3      | Mark | 65000  |     12          | 12/25/2008|
|   4      | Pam  | 25000  |     25          | 1/1/2005  |
|   5      | Alex | 50000  |     10          | 2/3/2007  |
+----------+------+--------+-----------------+-----------+
The table salesperson holds the salesperson information. Every salesperson has a sales_id and a name. Table: company


+---------+--------+------------+
| com_id  |  name  |    city    |
+---------+--------+------------+
|   1     |  RED   |   Boston   |
|   2     | ORANGE |   New York |
|   3     | YELLOW |   Boston   |
|   4     | GREEN  |   Austin   |
+---------+--------+------------+
The table company holds the company information. Every company has a com_id and a name. Table: orders


+----------+----------+---------+----------+--------+
| order_id |  date    | com_id  | sales_id | amount |
+----------+----------+---------+----------+--------+
| 1        | 1/1/2014 |    3    |    4     | 100000 |
| 2        | 2/1/2014 |    4    |    5     | 5000   |
| 3        | 3/1/2014 |    1    |    1     | 50000  |
| 4        | 4/1/2014 |    1    |    4     | 25000  |
+----------+----------+---------+----------+--------+
The table orders holds the sales record information, salesperson and customer company are represented by sales_id and com_id. output


+------+
| name |
+------+
| Amy  |
| Mark |
| Alex |
+------+
Explanation

According to order ‘3’ and ‘4’ in table orders, it is easy to tell only salesperson ‘John’ and ‘Alex’ have sales to company ‘RED’, so we need to output all the other names in table salesperson.

Solution
sql

SELECT name
FROM salesperson
WHERE name NOT IN
    (SELECT DISTINCT salesperson.name
    FROM salesperson, orders, company
    WHERE company.name = 'RED'
    AND salesperson.sales_id = orders.sales_id
    AND orders.com_id = company.com_id)
608. Tree Node | Medium | 🔒 LeetCode
Given a table tree, id is identifier of the tree node and p_id is its parent node’s id.


+----+------+
| id | p_id |
+----+------+
| 1  | null |
| 2  | 1    |
| 3  | 1    |
| 4  | 2    |
| 5  | 2    |
+----+------+
Each node in the tree can be one of three types:

Leaf: if the node is a leaf node. Root: if the node is the root of the tree. Inner: If the node is neither a leaf node nor a root node. Write a query to print the node id and the type of the node. Sort your output by the node id. The result for the above sample is:


+----+------+
| id | Type |
+----+------+
| 1  | Root |
| 2  | Inner|
| 3  | Leaf |
| 4  | Leaf |
| 5  | Leaf |
+----+------+
Explanation

Node ‘1’ is root node, because its parent node is NULL and it has child node ‘2’ and ‘3’. Node ‘2’ is inner node, because it has parent node ‘1’ and child node ‘4’ and ‘5’. Node ‘3’, ‘4’ and ‘5’ is Leaf node, because they have parent node and they don’t have child node. And here is the image of the sample tree as below:


1
      /   \
    2       3
  /   \
4       5
Note

If there is only one node on the tree, you only need to output its root attributes.

Solution
sql

## Basic Ideas: LEFT JOIN
# In tree, each node can only one parent or no parent
## | id | p_id | id (child) |
## |----+------+------------|
## |  1 | null |          1 |
## |  1 | null |          2 |
## |  2 |    1 |          4 |
## |  2 |    1 |          5 |
## |  3 |    1 |       null |
## |  4 |    2 |       null |
## |  5 |    2 |       null |

SELECT t1.id,
    CASE
        WHEN ISNULL(t1.p_id) THEN 'Root'
        WHEN ISNULL(MAX(t2.id)) THEN 'Leaf'
        ELSE 'Inner'
    END AS Type
FROM tree AS t1 LEFT JOIN tree AS t2
ON t1.id = t2.p_id
GROUP BY t1.id, t1.p_id
610. Triangle Judgement | Easy | 🔒 LeetCode
A pupil Tim gets homework to identify whether three line segments could possibly form a triangle. However, this assignment is very heavy because there are hundreds of records to calculate. Could you help Tim by writing a query to judge whether these three sides can form a triangle, assuming table triangle holds the length of the three sides x, y and z.


| x  | y  | z  |
|----|----|----|
| 13 | 15 | 30 |
| 10 | 20 | 15 |
For the sample data above, your query should return the follow result:


| x  | y  | z  | triangle |
|----|----|----|----------|
| 13 | 15 | 30 | No       |
| 10 | 20 | 15 | Yes      |
Solution
sql

SELECT x, y, z,
       CASE
           WHEN x+y>z AND y+z>x AND x+z>y THEN 'Yes'
           ELSE 'No'
       END AS triangle
FROM triangle
612. Shortest Distance in a Plane | Medium | 🔒 LeetCode
Table point_2d holds the coordinates (x,y) of some unique points (more than two) in a plane. Write a query to find the shortest distance between these points rounded to 2 decimals.


| x  | y  |
|----|----|
| -1 | -1 |
| 0  | 0  |
| -1 | -2 |
The shortest distance is 1.00 from point (-1,-1) to (-1,2). So the output should be:


| shortest |
|----------|
| 1.00     |
Note: The longest distance among all the points are less than 10000.

Solution
sql

SELECT ROUND(MIN(SQRT((t1.x-t2.x)*(t1.x-t2.x) + (t1.y-t2.y)*(t1.y-t2.y))), 2) as shortest
FROM point_2d AS t1, point_2d AS t2
WHERE t1.x!=t2.x OR t1.y!=t2.y

# SELECT ROUND(SQRT((t1.x-t2.x)*(t1.x-t2.x) + (t1.y-t2.y)*(t1.y-t2.y)), 2) AS shortest
# FROM point_2d AS t1, point_2d AS t2
# WHERE t1.x!=t2.x OR t1.y!=t2.y
# ORDER BY shortest ASC
# LIMIT 1
613. Shortest Distance in a Line | Easy | 🔒 LeetCode
Table point holds the x coordinate of some points on x-axis in a plane, which are all integers. Write a query to find the shortest distance between two points in these points.


| x   |
|-----|
| -1  |
| 0   |
| 2   |
The shortest distance is ‘1’ obviously, which is from point ‘-1’ to ‘0’. So the output is as below:


| shortest|
|---------|
| 1       |
Note: Every point is unique, which means there is no duplicates in table point.

Follow-up: What if all these points have an id and are arranged from the left most to the right most of x axis?

Solution
sql

SELECT t1.x-t2.x AS shortest
FROM point AS t1 JOIN point AS t2
WHERE t1.x>t2.x
ORDER BY (t1.x-t2.x) ASC
LIMIT 1
614. Second Degree Follower | Medium | 🔒 LeetCode
In facebook, there is a follow table with two columns: followee, follower.

Please write a sql query to get the amount of each follower’s follower if he/she has one.

For example:


+-------------+------------+
| followee    | follower   |
+-------------+------------+
|     A       |     B      |
|     B       |     C      |
|     B       |     D      |
|     D       |     E      |
+-------------+------------+
should output:


+-------------+------------+
| follower    | num        |
+-------------+------------+
|     B       |  2         |
|     D       |  1         |
+-------------+------------+
Explanation: Both B and D exist in the follower list, when as a followee, B’s follower is C and D, and D’s follower is E. A does not exist in follower list.

Note: Followee would not follow himself/herself in all cases. Please display the result in follower’s alphabet order.

Solution
sql

## Explain the business logic
##   A follows B. Then A is follwer, B is followee
## What are second degree followers?
##   A follows B, and B follows C.
##   Then A is the second degree followers of C

SELECT f1.follower, COUNT(DISTINCT f2.follower) AS num
FROM follow AS f1 JOIN follow AS f2
ON f1.follower = f2.followee
GROUP BY f1.follower;
615. Average Salary: Departments VS Company | Hard | 🔒 LeetCode
Given two tables as below, write a query to display the comparison result (higher/lower/same) of the average salary of employees in a department to the company’s average salary. Table: salary


| id | employee_id | amount | pay_date   |
|----|-------------|--------|------------|
| 1  | 1           | 9000   | 2017-03-31 |
| 2  | 2           | 6000   | 2017-03-31 |
| 3  | 3           | 10000  | 2017-03-31 |
| 4  | 1           | 7000   | 2017-02-28 |
| 5  | 2           | 6000   | 2017-02-28 |
| 6  | 3           | 8000   | 2017-02-28 |
The employee_id column refers to the employee_id in the following table employee.


| employee_id | department_id |
|-------------|---------------|
| 1           | 1             |
| 2           | 2             |
| 3           | 2             |
So for the sample data above, the result is:


| pay_month | department_id | comparison  |
|-----------|---------------|-------------|
| 2017-03   | 1             | higher      |
| 2017-03   | 2             | lower       |
| 2017-02   | 1             | same        |
| 2017-02   | 2             | same        |
Explanation In March, the company’s average salary is (9000+6000+10000)/3 = 8333.33… The average salary for department ‘1’ is 9000, which is the salary of employee_id ‘1’ since there is only one employee in this department. So the comparison result is ‘higher’ since 9000 > 8333.33 obviously. The average salary of department ‘2’ is (6000 + 10000)/2 = 8000, which is the average of employee_id ‘2’ and ‘3’. So the comparison result is ‘lower’ since 8000 < 8333.33. With he same formula for the average salary comparison in February, the result is ‘same’ since both the department ‘1’ and ‘2’ have the same average salary with the company, which is 7000.

Solution
sql

SELECT t1.pay_month, t1.department_id,
    (CASE WHEN t1.amount = t2.amount THEN 'same'
          WHEN t1.amount > t2.amount THEN 'higher'
          WHEN t1.amount < t2.amount THEN 'lower' END) AS comparison
FROM
    (SELECT left(pay_date, 7) AS pay_month, department_id, avg(amount) AS amount
    FROM salary JOIN employee
    ON salary.employee_id = employee.employee_id
    GROUP BY pay_month, department_id
    ORDER BY pay_month DESC, department_id) AS t1
    JOIN
    (SELECT left(pay_date, 7) AS pay_month, avg(amount) AS amount
    FROM salary JOIN employee
    ON salary.employee_id = employee.employee_id
    GROUP BY pay_month) AS t2
    ON t1.pay_month = t2.pay_month
618. Students Report By Geography | Hard | 🔒 LeetCode
A U.S graduate school has students from Asia, Europe and America. The students’ location information are stored in table student as below.


| name   | continent |
|--------|-----------|
| Jack   | America   |
| Pascal | Europe    |
| Xi     | Asia      |
| Jane   | America   |
Pivot the continent column in this table so that each name is sorted alphabetically and displayed underneath its corresponding continent. The output headers should be America, Asia and Europe respectively. It is guaranteed that the student number from America is no less than either Asia or Europe. For the sample input, the output is:


| America | Asia | Europe |
|---------|------|--------|
| Jack    | Xi   | Pascal |
| Jane    |      |        |
Follow-up: If it is unknown which continent has the most students, can you write a query to generate the student report?

Solution
sql

SELECT t1.name AS America, t2.name AS Asia, t3.name AS Europe
FROM
    (SELECT (@cnt1 := @cnt1 + 1) AS id, name
    FROM student
    CROSS JOIN (SELECT @cnt1 := 0) AS dummy
    WHERE continent='America'
    ORDER BY name) AS t1
    LEFT JOIN
    (SELECT (@cnt2 := @cnt2 + 1) AS id, name
    FROM student
    CROSS JOIN (SELECT @cnt2 := 0) AS dummy
    WHERE continent='Asia'
    ORDER BY name) AS t2
    ON t1.id = t2.id
    LEFT JOIN
    (SELECT (@cnt3 := @cnt3 + 1) AS id, name
    FROM student
    CROSS JOIN (SELECT @cnt3 := 0) AS dummy
    WHERE continent='Europe'
    ORDER BY name) AS t3
    ON t1.id = t3.id
619. Biggest Single Number | Easy | 🔒 LeetCode
Table number contains many numbers in column num including duplicated ones. Can you write a SQL query to find the biggest number, which only appears once.


+---+
|num|
+---+
| 8 |
| 8 |
| 3 |
| 3 |
| 1 |
| 4 |
| 5 |
| 6 |
For the sample data above, your query should return the following result:


+---+
|num|
+---+
| 6 |
Note: If there is no such number, just output null.

Solution
sql

SELECT IFNULL((
    SELECT num
    FROM number
    GROUP BY num
    HAVING count(1) = 1
    ORDER BY num DESC
    LIMIT 0, 1), NULL) AS num
620. Not Boring Movies | Easy | LeetCode
X city opened a new cinema, many people would like to go to this cinema. The cinema also gives out a poster indicating the movies’ ratings and descriptions. Please write a SQL query to output movies with an odd numbered ID and a description that is not 'boring'. Order the result by rating.

For example, table cinema:


+---------+-----------+--------------+-----------+
|   id    | movie     |  description |  rating   |
+---------+-----------+--------------+-----------+
|   1     | War       |   great 3D   |   8.9     |
|   2     | Science   |   fiction    |   8.5     |
|   3     | irish     |   boring     |   6.2     |
|   4     | Ice song  |   Fantacy    |   8.6     |
|   5     | House card|   Interesting|   9.1     |
+---------+-----------+--------------+-----------+
For the example above, the output should be:


+---------+-----------+--------------+-----------+
|   id    | movie     |  description |  rating   |
+---------+-----------+--------------+-----------+
|   5     | House card|   Interesting|   9.1     |
|   1     | War       |   great 3D   |   8.9     |
+---------+-----------+--------------+-----------+
Solution
sql

SELECT *
FROM Cinema
WHERE description <> 'boring' AND ID % 2 = 1
ORDER BY rating DESC;
626. Exchange Seats | Medium | LeetCode
Mary is a teacher in a middle school and she has a table seat storing students' names and their corresponding seat ids.

The column id is continuous increment.

Mary wants to change seats for the adjacent students.

Can you write a SQL query to output the result for Mary?


+---------+---------+
|    id   | student |
+---------+---------+
|    1    | Abbot   |
|    2    | Doris   |
|    3    | Emerson |
|    4    | Green   |
|    5    | Jeames  |
+---------+---------+
For the sample input, the output is:


+---------+---------+
|    id   | student |
+---------+---------+
|    1    | Doris   |
|    2    | Abbot   |
|    3    | Green   |
|    4    | Emerson |
|    5    | Jeames  |
+---------+---------+
Note:
If the number of students is odd, there is no need to change the last one's seat.

Solution
sql

SELECT
IF(id<(SELECT MAX(id) FROM seat),IF(id%2=0,id-1, id+1),IF(id%2=0, id-1, id)) AS id, student
FROM seat
ORDER BY id;
627. Swap Salary | LeetCode
Table: Salary


+-------------+----------+
| Column Name | Type     |
+-------------+----------+
| id          | int      |
| name        | varchar  |
| sex         | ENUM     |
| salary      | int      |
+-------------+----------+

id is the primary key for this table.
The sex column is ENUM value of type ('m', 'f').
The table contains information about an employee.
Write an SQL query to swap all 'f' and 'm' values (i.e., change all 'f' values to 'm' and vice versa) with a single update statement and no intermediate temp table(s).

Note that you must write a single update statement, DO NOT write any select statement for this problem.

The query result format is in the following example:


Salary table:
+----+------+-----+--------+
| id | name | sex | salary |
+----+------+-----+--------+
| 1  | A    | m   | 2500   |
| 2  | B    | f   | 1500   |
| 3  | C    | m   | 5500   |
| 4  | D    | f   | 500    |
+----+------+-----+--------+

Result table:
+----+------+-----+--------+
| id | name | sex | salary |
+----+------+-----+--------+
| 1  | A    | f   | 2500   |
| 2  | B    | m   | 1500   |
| 3  | C    | f   | 5500   |
| 4  | D    | m   | 500    |
+----+------+-----+--------+
(1, A) and (2, C) were changed from 'm' to 'f'.
(2, B) and (4, D) were changed from 'f' to 'm'.
Solution
sql

# With IF
UPDATE Salary SET sex = IF(sex='m', 'f', 'm')

# With CASE
UPDATE Salary SET sex = CASE WHEN sex='m' THEN 'f' ELSE 'm' END
1045. Customers Who Bought All Products | Medium | 🔒 LeetCode
Table: Customer


+-------------+---------+
| Column Name | Type    |
+-------------+---------+
| customer_id | int     |
| product_key | int     |
+-------------+---------+
product_key is a foreign key to Product table. Table: Product


+-------------+---------+
| Column Name | Type    |
+-------------+---------+
| product_key | int     |
+-------------+---------+
product_key is the primary key column for this table.
Write an SQL query for a report that provides the customer ids from the Customer table that bought all the products in the Product table.

For example:


Customer table:
+-------------+-------------+
| customer_id | product_key |
+-------------+-------------+
| 1           | 5           |
| 2           | 6           |
| 3           | 5           |
| 3           | 6           |
| 1           | 6           |
+-------------+-------------+

Product table:
+-------------+
| product_key |
+-------------+
| 5           |
| 6           |
+-------------+

Result table:
+-------------+
| customer_id |
+-------------+
| 1           |
| 3           |
+-------------+
The customers who bought all the products (5 and 6) are customers with id 1 and 3.
Solution
sql

SELECT customer_id
FROM Customer
GROUP NY customer_id
HAVING count(DISTINCT product_key) = (
    SELECT count(1)
    FROM Product)
1050. Actors and Directors Who Cooperated At Least Three Times | Easy | 🔒 LeetCode
Table: ActorDirector


+-------------+---------+
| Column Name | Type    |
+-------------+---------+
| actor_id    | int     |
| director_id | int     |
| timestamp   | int     |
+-------------+---------+
timestamp is the primary key column for this table.
Write a SQL query for a report that provides the pairs (actor_id, director_id) where the actor have cooperated with the director at least 3 times.

Example:


ActorDirector table:
+-------------+-------------+-------------+
| actor_id    | director_id | timestamp   |
+-------------+-------------+-------------+
| 1           | 1           | 0           |
| 1           | 1           | 1           |
| 1           | 1           | 2           |
| 1           | 2           | 3           |
| 1           | 2           | 4           |
| 2           | 1           | 5           |
| 2           | 1           | 6           |
+-------------+-------------+-------------+

Result table:
+-------------+-------------+
| actor_id    | director_id |
+-------------+-------------+
| 1           | 1           |
+-------------+-------------+
The only pair is (1, 1) where they cooperated exactly 3 times.
Solution
sql

SELECT actor_id, director_id
FROM ActorDirector
GROUP BY actor_id, director_id
HAVING COUNT(1)>=3
1068. Product Sales Analysis I | Easy | 🔒 LeetCode
Table: Sales


+-------------+-------+
| Column Name | Type  |
+-------------+-------+
| sale_id     | int   |
| product_id  | int   |
| year        | int   |
| quantity    | int   |
| price       | int   |
+-------------+-------+
(sale_id, year) is the primary key of this table.
product_id is a foreign key to Product table.
Note that the price is per unit.
Table: Product


+--------------+---------+
| Column Name  | Type    |
+--------------+---------+
| product_id   | int     |
| product_name | varchar |
+--------------+---------+
product_id is the primary key of this table.
Write an SQL query that reports all product names of the products in the Sales table along with their selling year and price.

For example:


Sales table:
+---------+------------+------+----------+-------+
| sale_id | product_id | year | quantity | price |
+---------+------------+------+----------+-------+
| 1       | 100        | 2008 | 10       | 5000  |
| 2       | 100        | 2009 | 12       | 5000  |
| 7       | 200        | 2011 | 15       | 9000  |
+---------+------------+------+----------+-------+

Product table:
+------------+--------------+
| product_id | product_name |
+------------+--------------+
| 100        | Nokia        |
| 200        | Apple        |
| 300        | Samsung      |
+------------+--------------+

Result table:
+--------------+-------+-------+
| product_name | year  | price |
+--------------+-------+-------+
| Nokia        | 2008  | 5000  |
| Nokia        | 2009  | 5000  |
| Apple        | 2011  | 9000  |
+--------------+-------+-------+
Solution
sql

SELECT product_name, year, price
FROM Sales JOIN Product
ON Product.product_id = Sales.product_id
1069. Product Sales Analysis II | Easy | 🔒 LeetCode
Table: Sales


+-------------+-------+
| Column Name | Type  |
+-------------+-------+
| sale_id     | int   |
| product_id  | int   |
| year        | int   |
| quantity    | int   |
| price       | int   |
+-------------+-------+
sale_id is the primary key of this table.
product_id is a foreign key to Product table.
Note that the price is per unit.
Table: Product


+--------------+---------+
| Column Name  | Type    |
+--------------+---------+
| product_id   | int     |
| product_name | varchar |
+--------------+---------+
product_id is the primary key of this table.
Write an SQL query that reports the total quantity sold for every product id.

The query result format is in the following example:


Sales table:
+---------+------------+------+----------+-------+
| sale_id | product_id | year | quantity | price |
+---------+------------+------+----------+-------+
| 1       | 100        | 2008 | 10       | 5000  |
| 2       | 100        | 2009 | 12       | 5000  |
| 7       | 200        | 2011 | 15       | 9000  |
+---------+------------+------+----------+-------+

Product table:
+------------+--------------+
| product_id | product_name |
+------------+--------------+
| 100        | Nokia        |
| 200        | Apple        |
| 300        | Samsung      |
+------------+--------------+

Result table:
+--------------+----------------+
| product_id   | total_quantity |
+--------------+----------------+
| 100          | 22             |
| 200          | 15             |
+--------------+----------------+
Solution
sql

SELECT product_id, sum(quantity) AS total_quantity
FROM Sales
GROUP BY product_id;
1070. Product Sales Analysis III | Medium | 🔒 LeetCode
Table: Sales


+-------------+-------+
| Column Name | Type  |
+-------------+-------+
| sale_id     | int   |
| product_id  | int   |
| year        | int   |
| quantity    | int   |
| price       | int   |
+-------------+-------+
sale_id is the primary key of this table.
product_id is a foreign key to Product table.
Note that the price is per unit.
Table: Product


+--------------+---------+
| Column Name  | Type    |
+--------------+---------+
| product_id   | int     |
| product_name | varchar |
+--------------+---------+
product_id is the primary key of this table.
Write an SQL query that selects the product id, year, quantity, and price for the first year of every product sold.

The query result format is in the following example:


Sales table:
+---------+------------+------+----------+-------+
| sale_id | product_id | year | quantity | price |
+---------+------------+------+----------+-------+
| 1       | 100        | 2008 | 10       | 5000  |
| 2       | 100        | 2009 | 12       | 5000  |
| 7       | 200        | 2011 | 15       | 9000  |
+---------+------------+------+----------+-------+

Product table:
+------------+--------------+
| product_id | product_name |
+------------+--------------+
| 100        | Nokia        |
| 200        | Apple        |
| 300        | Samsung      |
+------------+--------------+

Result table:
+------------+------------+----------+-------+
| product_id | first_year | quantity | price |
+------------+------------+----------+-------+
| 100        | 2008       | 10       | 5000  |
| 200        | 2011       | 15       | 9000  |
+------------+------------+----------+-------+
Solution
sql

SELECT
    product_id,
    year first_year,
    quantity,
    price
FROM Sales
WHERE (product_id, year) IN (SELECT product_id, MIN(year)
                             FROM Sales
                             GROUP BY product_id)
1075. Project Employees I | Easy | 🔒 LeetCode
Table: Project


+-------------+---------+
| Column Name | Type    |
+-------------+---------+
| project_id  | int     |
| employee_id | int     |
+-------------+---------+
(project_id, employee_id) is the primary key of this table.
employee_id is a foreign key to Employee table.
Table: Employee


+------------------+---------+
| Column Name      | Type    |
+------------------+---------+
| employee_id      | int     |
| name             | varchar |
| experience_years | int     |
+------------------+---------+
employee_id is the primary key of this table.
Write an SQL query that reports the average experience years of all the employees for each project, rounded to 2 digits.

The query result format is in the following example:


Project table:
+-------------+-------------+
| project_id  | employee_id |
+-------------+-------------+
| 1           | 1           |
| 1           | 2           |
| 1           | 3           |
| 2           | 1           |
| 2           | 4           |
+-------------+-------------+

Employee table:
+-------------+--------+------------------+
| employee_id | name   | experience_years |
+-------------+--------+------------------+
| 1           | Khaled | 3                |
| 2           | Ali    | 2                |
| 3           | John   | 1                |
| 4           | Doe    | 2                |
+-------------+--------+------------------+

Result table:
+-------------+---------------+
| project_id  | average_years |
+-------------+---------------+
| 1           | 2.00          |
| 2           | 2.50          |
+-------------+---------------+
The average experience years for the first project is (3 + 2 + 1) / 3 = 2.00 and for the second project is (3 + 2) / 2 = 2.50
Solution
sql

SELECT
    p.project_id,
    ROUND(AVG(e.experience_years),2) average_years
FROM
    Project p JOIN Employee e ON
    p.employee_id = e.employee_id
GROUP BY
    p.project_id
1076. Project Employees II | Easy | 🔒 LeetCode
Table: Project


+-------------+---------+
| Column Name | Type    |
+-------------+---------+
| project_id  | int     |
| employee_id | int     |
+-------------+---------+
(project_id, employee_id) is the primary key of this table.
employee_id is a foreign key to Employee table.
Table: Employee


+------------------+---------+
| Column Name      | Type    |
+------------------+---------+
| employee_id      | int     |
| name             | varchar |
| experience_years | int     |
+------------------+---------+
employee_id is the primary key of this table.
Write an SQL query that reports all the projects that have the most employees.

The query result format is in the following example:


Project table:
+-------------+-------------+
| project_id  | employee_id |
+-------------+-------------+
| 1           | 1           |
| 1           | 2           |
| 1           | 3           |
| 2           | 1           |
| 2           | 4           |
+-------------+-------------+

Employee table:
+-------------+--------+------------------+
| employee_id | name   | experience_years |
+-------------+--------+------------------+
| 1           | Khaled | 3                |
| 2           | Ali    | 2                |
| 3           | John   | 1                |
| 4           | Doe    | 2                |
+-------------+--------+------------------+

Result table:
+-------------+
| project_id  |
+-------------+
| 1           |
+-------------+
The first project has 3 employees while the second one has 2.
sql

SELECT project_id
FROM Project
GROUP BY project_id
HAVING COUNT(employee_id) = (SELECT COUNT(employee_id)
                            FROM Project
                            GROUP BY project_id
                            ORDER BY COUNT(employee_id) DESC
                            LIMIT 1)
1077. Project Employees III | Medium | 🔒 LeetCode
Table: Project


+-------------+---------+
| Column Name | Type    |
+-------------+---------+
| project_id  | int     |
| employee_id | int     |
+-------------+---------+
(project_id, employee_id) is the primary key of this table.
employee_id is a foreign key to Employee table.
Table: Employee


+------------------+---------+
| Column Name      | Type    |
+------------------+---------+
| employee_id      | int     |
| name             | varchar |
| experience_years | int     |
+------------------+---------+
employee_id is the primary key of this table.
Write an SQL query that reports the most experienced employees in each project. In case of a tie, report all employees with the maximum number of experience years.

The query result format is in the following example:


Project table:
+-------------+-------------+
| project_id  | employee_id |
+-------------+-------------+
| 1           | 1           |
| 1           | 2           |
| 1           | 3           |
| 2           | 1           |
| 2           | 4           |
+-------------+-------------+

Employee table:
+-------------+--------+------------------+
| employee_id | name   | experience_years |
+-------------+--------+------------------+
| 1           | Khaled | 3                |
| 2           | Ali    | 2                |
| 3           | John   | 3                |
| 4           | Doe    | 2                |
+-------------+--------+------------------+

Result table:
+-------------+---------------+
| project_id  | employee_id   |
+-------------+---------------+
| 1           | 1             |
| 1           | 3             |
| 2           | 1             |
+-------------+---------------+
Both employees with id 1 and 3 have the most experience among the employees of the first project. For the second project, the employee with id 1 has the most experience.
Solution
sql

SELECT
    p.project_id,
    e.employee_id
FROM
    Project p LEFT JOIN Employee e ON
    p.employee_id = e.employee_id
WHERE (p.project_id,
       e.experience_years) IN (SELECT
                                p.project_id,
                                MAX(e.experience_years)
                            FROM
                                Project p JOIN Employee e ON
                                p.employee_id = e.employee_id
                            GROUP BY
                                p.project_id)
1082. Sales Analysis I | Easy | 🔒 LeetCode
Table: Product


+--------------+---------+
| Column Name  | Type    |
+--------------+---------+
| product_id   | int     |
| product_name | varchar |
| unit_price   | int     |
+--------------+---------+
product_id is the primary key of this table.
Table: Sales


+-------------+---------+
| Column Name | Type    |
+-------------+---------+
| seller_id   | int     |
| product_id  | int     |
| buyer_id    | int     |
| sale_date   | date    |
| quantity    | int     |
| price       | int     |
+------ ------+---------+
This table has no primary key, it can have repeated rows.
product_id is a foreign key to Product table.
Write an SQL query that reports the best seller by total sales price, If there is a tie, report them all.

The query result format is in the following example:


Product table:
+------------+--------------+------------+
| product_id | product_name | unit_price |
+------------+--------------+------------+
| 1          | S8           | 1000       |
| 2          | G4           | 800        |
| 3          | iPhone       | 1400       |
+------------+--------------+------------+

Sales table:
+-----------+------------+----------+------------+----------+-------+
| seller_id | product_id | buyer_id | sale_date  | quantity | price |
+-----------+------------+----------+------------+----------+-------+
| 1         | 1          | 1        | 2019-01-21 | 2        | 2000  |
| 1         | 2          | 2        | 2019-02-17 | 1        | 800   |
| 2         | 2          | 3        | 2019-06-02 | 1        | 800   |
| 3         | 3          | 4        | 2019-05-13 | 2        | 2800  |
+-----------+------------+----------+------------+----------+-------+

Result table:
+-------------+
| seller_id   |
+-------------+
| 1           |
| 3           |
+-------------+
Both sellers with id 1 and 3 sold products with the most total price of 2800.
Solution
sql

SELECT seller_id
FROM Sales
GROUP BY seller_id
HAVING SUM(price) = (SELECT SUM(price)
                    FROM Sales
                    GROUP BY seller_id
                    ORDER BY SUM(price) DESC
                    LIMIT 1)
1083. Sales Analysis II | Easy | 🔒 LeetCode
Table: Product


+--------------+---------+
| Column Name  | Type    |
+--------------+---------+
| product_id   | int     |
| product_name | varchar |
| unit_price   | int     |
+--------------+---------+
product_id is the primary key of this table.
Table: Sales


+-------------+---------+
| Column Name | Type    |
+-------------+---------+
| seller_id   | int     |
| product_id  | int     |
| buyer_id    | int     |
| sale_date   | date    |
| quantity    | int     |
| price       | int     |
+------ ------+---------+
This table has no primary key, it can have repeated rows.
product_id is a foreign key to Product table.
Write an SQL query that reports the buyers who have bought S8 but not iPhone. Note that S8 and iPhone are products present in the Product table.

The query result format is in the following example:


Product table:
+------------+--------------+------------+
| product_id | product_name | unit_price |
+------------+--------------+------------+
| 1          | S8           | 1000       |
| 2          | G4           | 800        |
| 3          | iPhone       | 1400       |
+------------+--------------+------------+

Sales table:
+-----------+------------+----------+------------+----------+-------+
| seller_id | product_id | buyer_id | sale_date  | quantity | price |
+-----------+------------+----------+------------+----------+-------+
| 1         | 1          | 1        | 2019-01-21 | 2        | 2000  |
| 1         | 2          | 2        | 2019-02-17 | 1        | 800   |
| 2         | 1          | 3        | 2019-06-02 | 1        | 800   |
| 3         | 3          | 3        | 2019-05-13 | 2        | 2800  |
+-----------+------------+----------+------------+----------+-------+

Result table:
+-------------+
| buyer_id    |
+-------------+
| 1           |
+-------------+
The buyer with id 1 bought an S8 but didn't buy an iPhone. The buyer with id 3 bought both.
Solution
sql

SELECT DISTINCT s.buyer_id
FROM Sales s LEFT JOIN Product p ON
    s.product_id = p.product_id
WHERE p.product_name = 'S8' AND
      s.buyer_id NOT IN (SELECT s.buyer_id
                        FROM Sales s LEFT JOIN Product p ON
                            s.product_id = p.product_id
                        WHERE p.product_name = 'iPhone')
1084. Sales Analysis III | Easy | 🔒 LeetCode
Reports the products that were only sold in spring 2019. That is, between 2019-01-01 and 2019-03-31 inclusive. Select the product that were only sold in spring 2019.


Product table:
+------------+--------------+------------+
| product_id | product_name | unit_price |
+------------+--------------+------------+
| 1          | S8           | 1000       |
| 2          | G4           | 800        |
| 3          | iPhone       | 1400       |
+------------+--------------+------------+

Sales table:
+-----------+------------+----------+------------+----------+-------+
| seller_id | product_id | buyer_id | sale_date  | quantity | price |
+-----------+------------+----------+------------+----------+-------+
| 1         | 1          | 1        | 2019-01-21 | 2        | 2000  |
| 1         | 2          | 2        | 2019-02-17 | 1        | 800   |
| 2         | 2          | 3        | 2019-06-02 | 1        | 800   |
| 3         | 3          | 4        | 2019-05-13 | 2        | 2800  |
+-----------+------------+----------+------------+----------+-------+

Result table:
+-------------+--------------+
| product_id  | product_name |
+-------------+--------------+
| 1           | S8           |
+-------------+--------------+
The product with id 1 was only sold in spring 2019 while the other two were sold after.
Solution
sql

(SELECT DISTINCT s.product_id, p.product_name
FROM Sales s LEFT JOIN Product p ON
    s.product_id = p.product_id
WHERE s.sale_date >= '2019-01-01' AND
      s.sale_date <= '2019-03-31')
EXCEPT -- MINUS if Oracle
(SELECT DISTINCT s.product_id, p.product_name
FROM Sales s LEFT JOIN Product p ON
    s.product_id = p.product_id
WHERE s.sale_date < '2019-01-01' OR
      s.sale_date > '2019-03-31')
1097. Game Play Analysis V | Hard | 🔒 LeetCode
We define the install date of a player to be the first login day of that player. We also define day 1 retention of some date X to be the number of players whose install date is X and they logged back in on the day right after X , divided by the number of players whose install date is X, rounded to 2 decimal places. Write an SQL query that reports for each install date, the number of players that installed the game on that day and the day 1 retention. The query result format is in the following example:


Activity table:
+-----------+-----------+------------+--------------+
| player_id | device_id | event_date | games_played |
+-----------+-----------+------------+--------------+
| 1         | 2         | 2016-03-01 | 5            |
| 1         | 2         | 2016-03-02 | 6            |
| 2         | 3         | 2017-06-25 | 1            |
| 3         | 1         | 2016-03-01 | 0            |
| 3         | 4         | 2016-07-03 | 5            |
+-----------+-----------+------------+--------------+

Result table:
+------------+----------+----------------+
| install_dt | installs | Day1_retention |
+------------+----------+----------------+
| 2016-03-01 | 2        | 0.50           |
| 2017-06-25 | 1        | 0.00           |
+------------+----------+----------------+

Player 1 and 3 installed the game on 2016-03-01 but only player 1 logged back in on 2016-03-02 so the day 1 retention of 2016-03-01 is 1/2 = 0.50
Player 2 installed the game on 2017 -06-25 but didn't log back in on 2017-06-26 so the day 1 retention of 2017-06-25 is 0/1 = 0.00
Solution
sql

SELECT
    install_dt,
    COUNT(player_id) installs,
    ROUND(COUNT(retention)/COUNT(player_id),2) Day1_retention  --the number of record on the next day / the total number of id on the day = retention rate
FROM
    (
    SELECT a.player_id, a.install_dt, b.event_date retention -- id, the record of the first installation day and next day
    FROM
        (SELECT player_id, MIN(event_date) install_dt   --subquery 1 take the first installation of date of each id
        FROM Activity
        GROUP BY player_id) a LEFT JOIN Activity b ON   --sq1 left join the original table, find the login status the next day after the first installation
            a.player_id = b.player_id AND
            a.install_dt + 1=b.event_date
    ) AS tmp
GROUP BY
    install_dt
1098. Unpopular Books | Medium | 🔒 LeetCode
Table: Books


+----------------+---------+
| Column Name    | Type    |
+----------------+---------+
| book_id        | int     |
| name           | varchar |
| available_from | date    |
+----------------+---------+
book_id is the primary key of this table.
Table: Orders


+----------------+---------+
| Column Name    | Type    |
+----------------+---------+
| order_id       | int     |
| book_id        | int     |
| quantity       | int     |
| dispatch_date  | date    |
+----------------+---------+
order_id is the primary key of this table.
book_id is a foreign key to the Books table.
Write an SQL query that reports the books that have sold less than 10 copies in the last year, excluding books that have been available for less than 1 month from today. Assume today is 2019-06-23.

The query result format is in the following example:


Books table:
+---------+--------------------+----------------+
| book_id | name               | available_from |
+---------+--------------------+----------------+
| 1       | "Kalila And Demna" | 2010-01-01     |
| 2       | "28 Letters"       | 2012-05-12     |
| 3       | "The Hobbit"       | 2019-06-10     |
| 4       | "13 Reasons Why"   | 2019-06-01     |
| 5       | "The Hunger Games" | 2008-09-21     |
+---------+--------------------+----------------+

Orders table:
+----------+---------+----------+---------------+
| order_id | book_id | quantity | dispatch_date |
+----------+---------+----------+---------------+
| 1        | 1       | 2        | 2018-07-26    |
| 2        | 1       | 1        | 2018-11-05    |
| 3        | 3       | 8        | 2019-06-11    |
| 4        | 4       | 6        | 2019-06-05    |
| 5        | 4       | 5        | 2019-06-20    |
| 6        | 5       | 9        | 2009-02-02    |
| 7        | 5       | 8        | 2010-04-13    |
+----------+---------+----------+---------------+

Result table:
+-----------+--------------------+
| book_id   | name               |
+-----------+--------------------+
| 1         | "Kalila And Demna" |
| 2         | "28 Letters"       |
| 5         | "The Hunger Games" |
+-----------+--------------------+
Solution
sql

SELECT
    b.book_id, b.name
FROM
    Books b LEFT JOIN (                    -- subquery calculates last year's sales
        SELECT book_id, SUM(quantity) nsold
        FROM Orders
        WHERE dispatch_date BETWEEN '2018-06-23' AND '2019-06-23'
        GROUP BY book_id
    ) o
    ON b.book_id = o.book_id
WHERE
    (o.nsold < 10 OR o.nsold IS NULL) AND           -- Sales less than 10 or no sales
    DATEDIFF('2019-06-23', b.available_from) > 30   -- Not a new book within 1 month
1107. New Users Daily Count | Medium | 🔒 LeetCode
Table: Traffic


+---------------+---------+
| Column Name   | Type    |
+---------------+---------+
| user_id       | int     |
| activity      | enum    |
| activity_date | date    |
+---------------+---------+
There is no primary key for this table, it may have duplicate rows.
The activity column is an ENUM type of ('login', 'logout', 'jobs', 'groups', 'homepage').
Write an SQL query that reports for every date within at most 90 days from today, the number of users that logged in for the first time on that date. Assume today is 2019-06-30.

The query result format is in the following example:


Traffic table:
+---------+----------+---------------+
| user_id | activity | activity_date |
+---------+----------+---------------+
| 1       | login    | 2019-05-01    |
| 1       | homepage | 2019-05-01    |
| 1       | logout   | 2019-05-01    |
| 2       | login    | 2019-06-21    |
| 2       | logout   | 2019-06-21    |
| 3       | login    | 2019-01-01    |
| 3       | jobs     | 2019-01-01    |
| 3       | logout   | 2019-01-01    |
| 4       | login    | 2019-06-21    |
| 4       | groups   | 2019-06-21    |
| 4       | logout   | 2019-06-21    |
| 5       | login    | 2019-03-01    |
| 5       | logout   | 2019-03-01    |
| 5       | login    | 2019-06-21    |
| 5       | logout   | 2019-06-21    |
+---------+----------+---------------+

Result table:
+------------+-------------+
| login_date | user_count  |
+------------+-------------+
| 2019-05-01 | 1           |
| 2019-06-21 | 2           |
+------------+-------------+
Note that we only care about dates with non zero user count.
The user with id 5 first logged in on 2019-03-01 so he's not counted on 2019-06-21.
Solution
sql

#Solution- 1:
SELECT login_date, COUNT(user_id) AS user_count
FROM (SELECT user_id, MIN(activity_date) AS login_date
    FROM Traffic
    WHERE activity = 'login'
    GROUP BY user_id) AS t
WHERE login_date >= DATE_ADD('2019-06-30', INTERVAL -90 DAY) AND login_date <= '2019-06-30'
GROUP BY login_date

#Solution- 2:
SELECT login_date, COUNT(user_id) user_count
FROM
    (SELECT user_id, MIN(activity_date) as login_date
    FROM Traffic
    WHERE activity='login'
    GROUP BY user_id) as t
WHERE DATEDIFF('2019-06-30', login_date) <= 90
GROUP BY login_date
1112. Highest Grade For Each Student | Medium | 🔒 LeetCode
Table: Enrollments


+---------------+---------+
| Column Name   | Type    |
+---------------+---------+
| student_id    | int     |
| course_id     | int     |
| grade         | int     |
+---------------+---------+
(student_id, course_id) is the primary key of this table.
Write a SQL query to find the highest grade with its corresponding course for each student. In case of a tie, you should find the course with the smallest course_id. The output must be sorted by increasing student_id.

The query result format is in the following example:


Enrollments table:
+------------+-------------------+
| student_id | course_id | grade |
+------------+-----------+-------+
| 2          | 2         | 95    |
| 2          | 3         | 95    |
| 1          | 1         | 90    |
| 1          | 2         | 99    |
| 3          | 1         | 80    |
| 3          | 2         | 75    |
| 3          | 3         | 82    |
+------------+-----------+-------+
Result table:
+------------+-------------------+
| student_id | course_id | grade |
+------------+-----------+-------+
| 1          | 2         | 99    |
| 2          | 2         | 95    |
| 3          | 3         | 82    |
+------------+-----------+-------+
Solution
sql

SELECT student_id, MIN(course_id) course_id, grade
FROM Enrollments
WHERE (student_id, grade) IN
        (SELECT student_id, MAX(grade)
        FROM Enrollments
        GROUP BY student_id)
GROUP BY student_id
ORDER BY student_id;
1113.Reported Posts | Easy | 🔒 LeetCode
Table: Actions


+---------------+---------+
| Column Name   | Type    |
+---------------+---------+
| user_id       | int     |
| post_id       | int     |
| action_date   | date    |
| action        | enum    |
| extra         | varchar |
+---------------+---------+
There is no primary key for this table, it may have duplicate rows.
The action column is an ENUM type of ('view', 'like', 'reaction', 'comment', 'report', 'share').
The extra column has optional information about the action such as a reason for report or a type of reaction.
Write an SQL query that reports the number of posts reported yesterday for each report reason. Assume today is 2019-07-05.

The query result format is in the following example:


Actions table:
+---------+---------+-------------+--------+--------+
| user_id | post_id | action_date | action | extra  |
+---------+---------+-------------+--------+--------+
| 1       | 1       | 2019-07-01  | view   | null   |
| 1       | 1       | 2019-07-01  | like   | null   |
| 1       | 1       | 2019-07-01  | share  | null   |
| 2       | 4       | 2019-07-04  | view   | null   |
| 2       | 4       | 2019-07-04  | report | spam   |
| 3       | 4       | 2019-07-04  | view   | null   |
| 3       | 4       | 2019-07-04  | report | spam   |
| 4       | 3       | 2019-07-02  | view   | null   |
| 4       | 3       | 2019-07-02  | report | spam   |
| 5       | 2       | 2019-07-04  | view   | null   |
| 5       | 2       | 2019-07-04  | report | racism |
| 5       | 5       | 2019-07-04  | view   | null   |
| 5       | 5       | 2019-07-04  | report | racism |
+---------+---------+-------------+--------+--------+
Result table:
+---------------+--------------+
| report_reason | report_count |
+---------------+--------------+
| spam          | 1            |
| racism        | 2            |
+---------------+--------------+
Note that we only care about report reasons with non zero number of reports.

Solution
sql

SELECT extra report_reason, COUNT(DISTINCT post_id) report_count
FROM
    (SELECT post_id, extra
    FROM Actions
    WHERE action_date = DATE_SUB('2019-07-05', INTERVAL 1 DAY) AND
          action = 'report') AS tmp
GROUP BY extra
1126. Active Businesses | Medium | 🔒 LeetCode
Table: Events


+---------------+---------+
| Column Name   | Type    |
+---------------+---------+
| business_id   | int     |
| event_type    | varchar |
| occurences    | int     |
+---------------+---------+
(business_id, event_type) is the primary key of this table.
Each row in the table logs the info that an event of some type occured at some business for a number of times.
Write an SQL query to find all active businesses.

An active business is a business that has more than one event type with occurences greater than the average occurences of that event type among all businesses.

The query result format is in the following example:


Events table:
+-------------+------------+------------+
| business_id | event_type | occurences |
+-------------+------------+------------+
| 1           | reviews    | 7          |
| 3           | reviews    | 3          |
| 1           | ads        | 11         |
| 2           | ads        | 7          |
| 3           | ads        | 6          |
| 1           | page views | 3          |
| 2           | page views | 12         |
+-------------+------------+------------+

Result table:
+-------------+
| business_id |
+-------------+
| 1           |
+-------------+
Average for 'reviews', 'ads' and 'page views' are (7+3)/2=5, (11+7+6)/3=8, (3+12)/2=7.5 respectively.
Business with id 1 has 7 'reviews' events (more than 5) and 11 'ads' events (more than 8) so it is an active business.
Solution
sql

SELECT business_id
FROM (SELECT a.business_id, a.event_type, a.occurences, b.event_avg  -- sub 2
      FROM Events a LEFT JOIN
        (SELECT event_type, AVG(occurences) event_avg   -- sub 1
         FROM Events
         GROUP BY event_type) b ON
      a.event_type = b.event_type) tmp
WHERE occurences > event_avg
GROUP BY business_id
HAVING COUNT(event_type) > 1
1127. User Purchase Platform | Hard | 🔒 LeetCode
Table: Spending


+-------------+---------+
| Column Name | Type    |
+-------------+---------+
| user_id     | int     |
| spend_date  | date    |
| platform    | enum    |
| amount      | int     |
+-------------+---------+
The table logs the spendings history of users that make purchases from an online shopping website which has a desktop and a mobile application.
(user_id, spend_date, platform) is the primary key of this table.
The platform column is an ENUM type of ('desktop', 'mobile').
Write an SQL query to find the total number of users and the total amount spent using mobile only, desktop only and both mobile and desktop together for each date.

The query result format is in the following example:


Spending table:
+---------+------------+----------+--------+
| user_id | spend_date | platform | amount |
+---------+------------+----------+--------+
| 1       | 2019-07-01 | mobile   | 100    |
| 1       | 2019-07-01 | desktop  | 100    |
| 2       | 2019-07-01 | mobile   | 100    |
| 2       | 2019-07-02 | mobile   | 100    |
| 3       | 2019-07-01 | desktop  | 100    |
| 3       | 2019-07-02 | desktop  | 100    |
+---------+------------+----------+--------+

Result table:
+------------+----------+--------------+-------------+
| spend_date | platform | total_amount | total_users |
+------------+----------+--------------+-------------+
| 2019-07-01 | desktop  | 100          | 1           |
| 2019-07-01 | mobile   | 100          | 1           |
| 2019-07-01 | both     | 200          | 1           |
| 2019-07-02 | desktop  | 100          | 1           |
| 2019-07-02 | mobile   | 100          | 1           |
| 2019-07-02 | both     | 0            | 0           |
+------------+----------+--------------+-------------+
On 2019-07-01, user 1 purchased using both desktop and mobile, user 2 purchased using mobile only and user 3 purchased using desktop only.
On 2019-07-02, user 2 purchased using mobile only, user 3 purchased using desktop only and no one purchased using both platforms.
Solution
sql

SELECT aa.spend_date,
       aa.platform,
       COALESCE(bb.total_amount, 0) total_amount,
       COALESCE(bb.total_users,0) total_users
FROM
    (SELECT DISTINCT(spend_date), a.platform   -- table aa
    FROM Spending JOIN
        (SELECT 'desktop' AS platform UNION
        SELECT 'mobile' AS platform UNION
        SELECT 'both' AS platform
        ) a
    ) aa
    LEFT JOIN
    (SELECT spend_date,                      -- table bb
            platform,
            SUM(amount) total_amount,
            COUNT(user_id) total_users
    FROM
        (SELECT spend_date,
                user_id,
                (CASE COUNT(DISTINCT platform)
                    WHEN 1 THEN platform
                    WHEN 2 THEN 'both'
                    END) platform,
                SUM(amount) amount
        FROM Spending
        GROUP BY spend_date, user_id
        ) b
    GROUP BY spend_date, platform
    ) bb
    ON aa.platform = bb.platform AND
    aa.spend_date = bb.spend_date
1132. Reported Posts II | Medium | 🔒 LeetCode
Table: Actions


+---------------+---------+
| Column Name   | Type    |
+---------------+---------+
| user_id       | int     |
| post_id       | int     |
| action_date   | date    |
| action        | enum    |
| extra         | varchar |
+---------------+---------+
There is no primary key for this table, it may have duplicate rows.
The action column is an ENUM type of ('view', 'like', 'reaction', 'comment', 'report', 'share').
The extra column has optional information about the action such as a reason for report or a type of reaction.
Table: Removals


+---------------+---------+
| Column Name   | Type    |
+---------------+---------+
| post_id       | int     |
| remove_date   | date    |
+---------------+---------+
post_id is the primary key of this table.
Each row in this table indicates that some post was removed as a result of being reported or as a result of an admin review.
Write an SQL query to find the average for daily percentage of posts that got removed after being reported as spam, rounded to 2 decimal places.

The query result format is in the following example:


Actions table:
+---------+---------+-------------+--------+--------+
| user_id | post_id | action_date | action | extra  |
+---------+---------+-------------+--------+--------+
| 1       | 1       | 2019-07-01  | view   | null   |
| 1       | 1       | 2019-07-01  | like   | null   |
| 1       | 1       | 2019-07-01  | share  | null   |
| 2       | 2       | 2019-07-04  | view   | null   |
| 2       | 2       | 2019-07-04  | report | spam   |
| 3       | 4       | 2019-07-04  | view   | null   |
| 3       | 4       | 2019-07-04  | report | spam   |
| 4       | 3       | 2019-07-02  | view   | null   |
| 4       | 3       | 2019-07-02  | report | spam   |
| 5       | 2       | 2019-07-03  | view   | null   |
| 5       | 2       | 2019-07-03  | report | racism |
| 5       | 5       | 2019-07-03  | view   | null   |
| 5       | 5       | 2019-07-03  | report | racism |
+---------+---------+-------------+--------+--------+

Removals table:
+---------+-------------+
| post_id | remove_date |
+---------+-------------+
| 2       | 2019-07-20  |
| 3       | 2019-07-18  |
+---------+-------------+

Result table:
+-----------------------+
| average_daily_percent |
+-----------------------+
| 75.00                 |
+-----------------------+
The percentage for 2019-07-04 is 50% because only one post of two spam reported posts was removed.
The percentage for 2019-07-02 is 100% because one post was reported as spam and it was removed.
The other days had no spam reports so the average is (50 + 100) / 2 = 75%
Note that the output is only one number and that we do not care about the remove dates.
Solution
sql

WITH t1 AS(
SELECT a.action_date, (COUNT(DISTINCT r.post_id))/(COUNT(DISTINCT a.post_id)) AS result
FROM (SELECT action_date, post_id
FROM actions
WHERE extra = 'spam' AND action = 'report') a
LEFT JOIN
removals r
ON a.post_id = r.post_id
GROUP BY a.action_date)

SELECT ROUND(AVG(t1.result)*100,2) AS  average_daily_percent
FROM t1
1141. User Activity for the Past 30 Days I | Easy | 🔒 LeetCode
Table: Activity


+---------------+---------+
| Column Name   | Type    |
+---------------+---------+
| user_id       | int     |
| session_id    | int     |
| activity_date | date    |
| activity_type | enum    |
+---------------+---------+
There is no primary key for this table, it may have duplicate rows.
The activity_type column is an ENUM of type ('open_session', 'end_session', 'scroll_down', 'send_message').
The table shows the user activities for a social media website.
Note that each session belongs to exactly one user.
Write an SQL query to find the daily active user count for a period of 30 days ending 2019-07-27 inclusively. A user was active on some day if he/she made at least one activity on that day.

The query result format is in the following example:


Activity table:
+---------+------------+---------------+---------------+
| user_id | session_id | activity_date | activity_type |
+---------+------------+---------------+---------------+
| 1       | 1          | 2019-07-20    | open_session  |
| 1       | 1          | 2019-07-20    | scroll_down   |
| 1       | 1          | 2019-07-20    | end_session   |
| 2       | 4          | 2019-07-20    | open_session  |
| 2       | 4          | 2019-07-21    | send_message  |
| 2       | 4          | 2019-07-21    | end_session   |
| 3       | 2          | 2019-07-21    | open_session  |
| 3       | 2          | 2019-07-21    | send_message  |
| 3       | 2          | 2019-07-21    | end_session   |
| 4       | 3          | 2019-06-25    | open_session  |
| 4       | 3          | 2019-06-25    | end_session   |
+---------+------------+---------------+---------------+

Result table:
+------------+--------------+
| day        | active_users |
+------------+--------------+
| 2019-07-20 | 2            |
| 2019-07-21 | 2            |
+------------+--------------+
Note that we do not care about days with zero active users.
Solution
sql

SELECT activity_date AS day, COUNT(DISTINCT user_id) AS active_users
FROM activity
WHERE activity_date > '2019-06-26' AND activity_date < '2019-07-27'
GROUP BY activity_date
1142. User Activity for the Past 30 Days II | Easy | 🔒 LeetCode
Table: Activity


+---------------+---------+
| Column Name   | Type    |
+---------------+---------+
| user_id       | int     |
| session_id    | int     |
| activity_date | date    |
| activity_type | enum    |
+---------------+---------+
There is no primary key for this table, it may have duplicate rows.
The activity_type column is an ENUM of type ('open_session', 'end_session', 'scroll_down', 'send_message').
The table shows the user activities for a social media website.
Note that each session belongs to exactly one user.
Write an SQL query to find the average number of sessions per user for a period of 30 days ending 2019-07-27 inclusively, rounded to 2 decimal places. The sessions we want to count for a user are those with at least one activity in that time period.

The query result format is in the following example:


Activity table:
+---------+------------+---------------+---------------+
| user_id | session_id | activity_date | activity_type |
+---------+------------+---------------+---------------+
| 1       | 1          | 2019-07-20    | open_session  |
| 1       | 1          | 2019-07-20    | scroll_down   |
| 1       | 1          | 2019-07-20    | end_session   |
| 2       | 4          | 2019-07-20    | open_session  |
| 2       | 4          | 2019-07-21    | send_message  |
| 2       | 4          | 2019-07-21    | end_session   |
| 3       | 2          | 2019-07-21    | open_session  |
| 3       | 2          | 2019-07-21    | send_message  |
| 3       | 2          | 2019-07-21    | end_session   |
| 3       | 5          | 2019-07-21    | open_session  |
| 3       | 5          | 2019-07-21    | scroll_down   |
| 3       | 5          | 2019-07-21    | end_session   |
| 4       | 3          | 2019-06-25    | open_session  |
| 4       | 3          | 2019-06-25    | end_session   |
+---------+------------+---------------+---------------+

Result table:
+---------------------------+
| average_sessions_per_user |
+---------------------------+
| 1.33                      |
+---------------------------+
User 1 and 2 each had 1 session in the past 30 days while user 3 had 2 sessions so the average is (1 + 1 + 2) / 3 = 1.33.
Solution
sql

SELECT IFNULL(ROUND(AVG(a.num),2),0) AS average_sessions_per_user
FROM (
SELECT COUNT(DISTINCT session_id) AS num
FROM activity
WHERE activity_date BETWEEN '2019-06-28' AND '2019-07-27'
GROUP BY user_id) a
1148. Article Views I | Easy | 🔒 LeetCode
Table: Views


+---------------+---------+
| Column Name   | Type    |
+---------------+---------+
| article_id    | int     |
| author_id     | int     |
| viewer_id     | int     |
| view_date     | date    |
+---------------+---------+
There is no primary key for this table, it may have duplicate rows.
Each row of this table indicates that some viewer viewed an article (written by some author) on some date.
Note that equal author_id and viewer_id indicate the same person.
Write an SQL query to find all the authors that viewed at least one of their own articles, sorted in ascending order by their id.

The query result format is in the following example:


Views table:
+------------+-----------+-----------+------------+
| article_id | author_id | viewer_id | view_date  |
+------------+-----------+-----------+------------+
| 1          | 3         | 5         | 2019-08-01 |
| 1          | 3         | 6         | 2019-08-02 |
| 2          | 7         | 7         | 2019-08-01 |
| 2          | 7         | 6         | 2019-08-02 |
| 4          | 7         | 1         | 2019-07-22 |
| 3          | 4         | 4         | 2019-07-21 |
| 3          | 4         | 4         | 2019-07-21 |
+------------+-----------+-----------+------------+

Result table:
+------+
| id   |
+------+
| 4    |
| 7    |
+------+
Solution
sql

SELECT DISTINCT author_id AS id
FROM Views
WHERE author_id = viewer_id
ORDER BY author_id
1149. Article Views II | Medium | 🔒 LeetCode
Table: Views


+---------------+---------+
| Column Name   | Type    |
+---------------+---------+
| article_id    | int     |
| author_id     | int     |
| viewer_id     | int     |
| view_date     | date    |
+---------------+---------+
There is no primary key for this table, it may have duplicate rows.
Each row of this table indicates that some viewer viewed an article (written by some author) on some date.
Note that equal author_id and viewer_id indicate the same person.
Write an SQL query to find all the people who viewed more than one article on the same date, sorted in ascending order by their id.

The query result format is in the following example:


Views table:
+------------+-----------+-----------+------------+
| article_id | author_id | viewer_id | view_date  |
+------------+-----------+-----------+------------+
| 1          | 3         | 5         | 2019-08-01 |
| 3          | 4         | 5         | 2019-08-01 |
| 1          | 3         | 6         | 2019-08-02 |
| 2          | 7         | 7         | 2019-08-01 |
| 2          | 7         | 6         | 2019-08-02 |
| 4          | 7         | 1         | 2019-07-22 |
| 3          | 4         | 4         | 2019-07-21 |
| 3          | 4         | 4         | 2019-07-21 |
+------------+-----------+-----------+------------+

Result table:
+------+
| id   |
+------+
| 5    |
| 6    |
+------+
Solution
sql

SELECT DISTINCT viewer_id AS id#, COUNT(DISTINCT article_id) AS total
FROM views
GROUP BY viewer_id, view_date
HAVING count(DISTINCT article_id)>1
ORDER BY 1
1158. Market Analysis I | Medium | 🔒 LeetCode
Table: Users


+----------------+---------+
| Column Name    | Type    |
+----------------+---------+
| user_id        | int     |
| join_date      | date    |
| favorite_brand | varchar |
+----------------+---------+
user_id is the primary key of this table.
This table has the info of the users of an online shopping website where users can sell and buy items.
Table: Orders


+---------------+---------+
| Column Name   | Type    |
+---------------+---------+
| order_id      | int     |
| order_date    | date    |
| item_id       | int     |
| buyer_id      | int     |
| seller_id     | int     |
+---------------+---------+
order_id is the primary key of this table.
item_id is a foreign key to the Items table.
buyer_id and seller_id are foreign keys to the Users table.
Table: Items


+---------------+---------+
| Column Name   | Type    |
+---------------+---------+
| item_id       | int     |
| item_brand    | varchar |
+---------------+---------+
item_id is the primary key of this table.
Write an SQL query to find for each user, the join date and the number of orders they made as a buyer in 2019.

The query result format is in the following example:


Users table:
+---------+------------+----------------+
| user_id | join_date  | favorite_brand |
+---------+------------+----------------+
| 1       | 2018-01-01 | Lenovo         |
| 2       | 2018-02-09 | Samsung        |
| 3       | 2018-01-19 | LG             |
| 4       | 2018-05-21 | HP             |
+---------+------------+----------------+

Orders table:
+----------+------------+---------+----------+-----------+
| order_id | order_date | item_id | buyer_id | seller_id |
+----------+------------+---------+----------+-----------+
| 1        | 2019-08-01 | 4       | 1        | 2         |
| 2        | 2018-08-02 | 2       | 1        | 3         |
| 3        | 2019-08-03 | 3       | 2        | 3         |
| 4        | 2018-08-04 | 1       | 4        | 2         |
| 5        | 2018-08-04 | 1       | 3        | 4         |
| 6        | 2019-08-05 | 2       | 2        | 4         |
+----------+------------+---------+----------+-----------+

Items table:
+---------+------------+
| item_id | item_brand |
+---------+------------+
| 1       | Samsung    |
| 2       | Lenovo     |
| 3       | LG         |
| 4       | HP         |
+---------+------------+

Result table:
+-----------+------------+----------------+
| buyer_id  | join_date  | orders_in_2019 |
+-----------+------------+----------------+
| 1         | 2018-01-01 | 1              |
| 2         | 2018-02-09 | 2              |
| 3         | 2018-01-19 | 0              |
| 4         | 2018-05-21 | 0              |
+-----------+------------+----------------+
Solution
sql

SELECT user_id AS buyer_id, join_date, coalesce(a.orders_in_2019,0)
FROM users
LEFT JOIN
(
SELECT buyer_id, coalesce(count(*), 0) AS orders_in_2019
FROM orders o
JOIN users u
ON u.user_id = o.buyer_id
WHERE extract('year' FROM order_date) = 2019
GROUP BY buyer_id) a
ON users.user_id = a.buyer_id
1159. Market Analysis II | Hard | 🔒 LeetCode
Table: Users


+----------------+---------+
| Column Name    | Type    |
+----------------+---------+
| user_id        | int     |
| join_date      | date    |
| favorite_brand | varchar |
+----------------+---------+
user_id is the primary key of this table.
This table has the info of the users of an online shopping website where users can sell and buy items.
Table: Orders


+---------------+---------+
| Column Name   | Type    |
+---------------+---------+
| order_id      | int     |
| order_date    | date    |
| item_id       | int     |
| buyer_id      | int     |
| seller_id     | int     |
+---------------+---------+
order_id is the primary key of this table.
item_id is a foreign key to the Items table.
buyer_id and seller_id are foreign keys to the Users table.
Table: Items


+---------------+---------+
| Column Name   | Type    |
+---------------+---------+
| item_id       | int     |
| item_brand    | varchar |
+---------------+---------+
item_id is the primary key of this table.
Write an SQL query to find for each user, whether the brand of the second item (by date) they sold is their favorite brand. If a user sold less than two items, report the answer for that user as no.

It is guaranteed that no seller sold more than one item on a day.

The query result format is in the following example:


Users table:
+---------+------------+----------------+
| user_id | join_date  | favorite_brand |
+---------+------------+----------------+
| 1       | 2019-01-01 | Lenovo         |
| 2       | 2019-02-09 | Samsung        |
| 3       | 2019-01-19 | LG             |
| 4       | 2019-05-21 | HP             |
+---------+------------+----------------+

Orders table:
+----------+------------+---------+----------+-----------+
| order_id | order_date | item_id | buyer_id | seller_id |
+----------+------------+---------+----------+-----------+
| 1        | 2019-08-01 | 4       | 1        | 2         |
| 2        | 2019-08-02 | 2       | 1        | 3         |
| 3        | 2019-08-03 | 3       | 2        | 3         |
| 4        | 2019-08-04 | 1       | 4        | 2         |
| 5        | 2019-08-04 | 1       | 3        | 4         |
| 6        | 2019-08-05 | 2       | 2        | 4         |
+----------+------------+---------+----------+-----------+

Items table:
+---------+------------+
| item_id | item_brand |
+---------+------------+
| 1       | Samsung    |
| 2       | Lenovo     |
| 3       | LG         |
| 4       | HP         |
+---------+------------+

Result table:
+-----------+--------------------+
| seller_id | 2nd_item_fav_brand |
+-----------+--------------------+
| 1         | no                 |
| 2         | yes                |
| 3         | yes                |
| 4         | no                 |
+-----------+--------------------+

The answer for the user with id 1 is no because they sold nothing.
The answer for the users with id 2 and 3 is yes because the brands of their second sold items are their favorite brands.
The answer for the user with id 4 is no because the brand of their second sold item is not their favorite brand.
Solution
sql

#Solution- 1:
SELECT user_id AS seller_id,
       IF(ISNULL(item_brand), "no", "yes") AS 2nd_item_fav_brand
FROM Users LEFT JOIN
(SELECT seller_id, item_brand
FROM Orders INNER JOIN Items
ON Orders.item_id = Items.item_id
WHERE (seller_id, order_date) IN
(SELECT seller_id, MIN(order_date) AS order_date
 FROM Orders
 WHERE (seller_id, order_date) NOT IN
 (SELECT seller_id, MIN(order_date) FROM Orders GROUP BY seller_id)
GROUP BY seller_id)
 ) AS t
ON Users.user_id = t.seller_id and favorite_brand = item_brand

#Solution- 2:
WITH t1 AS(
SELECT user_id,
CASE WHEN favorite_brand = item_brand THEN "yes"
ELSE "no"
END AS 2nd_item_fav_brand
FROM users u LEFT JOIN
(SELECT o.item_id, seller_id, item_brand, RANK() OVER(PARTITION BY seller_id ORDER BY order_date) AS rk
FROM orders o join items i
USING (item_id)) a
ON u.user_id = a.seller_id
WHERE a.rk = 2)

SELECT u.user_id AS seller_id, COALESCE(2nd_item_fav_brand,"no") AS 2nd_item_fav_brand
FROM users u LEFT JOIN t1
USING(user_id)
1164. Product Price at a Given Date | Medium | 🔒 LeetCode
Table: Products


+---------------+---------+
| Column Name   | Type    |
+---------------+---------+
| product_id    | int     |
| new_price     | int     |
| change_date   | date    |
+---------------+---------+
(product_id, change_date) is the primary key of this table.
Each row of this table indicates that the price of some product was changed to a new price at some date.
Write an SQL query to find the prices of all products on 2019-08-16. Assume the price of all products before any change is 10.

The query result format is in the following example:


Products table:
+------------+-----------+-------------+
| product_id | new_price | change_date |
+------------+-----------+-------------+
| 1          | 20        | 2019-08-14  |
| 2          | 50        | 2019-08-14  |
| 1          | 30        | 2019-08-15  |
| 1          | 35        | 2019-08-16  |
| 2          | 65        | 2019-08-17  |
| 3          | 20        | 2019-08-18  |
+------------+-----------+-------------+

Result table:
+------------+-------+
| product_id | price |
+------------+-------+
| 2          | 50    |
| 1          | 35    |
| 3          | 10    |
+------------+-------+
Solution
sql

#Solution- 1:
WITH t1 AS (
SELECT a.product_id, new_price
FROM(
SELECT product_id, max(change_date) AS date
FROM products
WHERE change_date<='2019-08-16'
GROUP BY product_id) a
JOIN products p
ON a.product_id = p.product_id AND a.date = p.change_date),

t2 AS (
SELECT distinct product_id
	FROM products)

SELECT t2.product_id, coalesce(new_price,10) AS price
FROM t2 LEFT JOIN t1
ON t2.product_id = t1.product_id
ORDER BY price DESC

#Solution- 2:
SELECT t1.product_id AS product_id, IF(ISNULL(t2.price), 10, t2.price) AS price
FROM
  (SELECT distinct product_id
  FROM Products) AS t1 LEFT JOIN
  (SELECT product_id, new_price AS price
   FROM Products
   WHERE (product_id, change_date) in
      (SELECT product_id, max(change_date)
      FROM Products
      WHERE change_date <='2019-08-16'
      GROUP BY product_id)) AS t2
ON t1.product_id = t2.product_id
1173. Immediate Food Delivery I | Easy | 🔒 LeetCode
Table: Delivery


+-----------------------------+---------+
| Column Name                 | Type    |
+-----------------------------+---------+
| delivery_id                 | int     |
| customer_id                 | int     |
| order_date                  | date    |
| customer_pref_delivery_date | date    |
+-----------------------------+---------+
delivery_id is the primary key of this table.
The table holds information about food delivery to customers that make orders at some date and specify a preferred delivery date (on the same order date or after it).
If the preferred delivery date of the customer is the same as the order date then the order is called immediate otherwise it's called scheduled.

Write an SQL query to find the percentage of immediate orders in the table, rounded to 2 decimal places.

The query result format is in the following example:


Delivery table:
+-------------+-------------+------------+-----------------------------+
| delivery_id | customer_id | order_date | customer_pref_delivery_date |
+-------------+-------------+------------+-----------------------------+
| 1           | 1           | 2019-08-01 | 2019-08-02                  |
| 2           | 5           | 2019-08-02 | 2019-08-02                  |
| 3           | 1           | 2019-08-11 | 2019-08-11                  |
| 4           | 3           | 2019-08-24 | 2019-08-26                  |
| 5           | 4           | 2019-08-21 | 2019-08-22                  |
| 6           | 2           | 2019-08-11 | 2019-08-13                  |
+-------------+-------------+------------+-----------------------------+

Result table:
+----------------------+
| immediate_percentage |
+----------------------+
| 33.33                |
+----------------------+
The orders with delivery id 2 and 3 are immediate while the others are scheduled.
Solution
sql

#Solution- 1:
SELECT
ROUND(SUM(CASE WHEN order_date=customer_pref_delivery_date THEN 1 ELSE 0 END)/count(1)*100, 2) immediate_percentage
FROM Delivery;

#Solution- 2:
SELECT
ROUND(avg(CASE WHEN order_date=customer_pref_delivery_date THEN 1 ELSE 0 END)*100,2) AS immediate_percentage
FROM delivery
1174. Immediate Food Delivery II | Medium | 🔒 LeetCode
Table: Delivery


+-----------------------------+---------+
| Column Name                 | Type    |
+-----------------------------+---------+
| delivery_id                 | int     |
| customer_id                 | int     |
| order_date                  | date    |
| customer_pref_delivery_date | date    |
+-----------------------------+---------+
delivery_id is the primary key of this table.
The table holds information about food delivery to customers that make orders at some date and specify a preferred delivery date (on the same order date or after it).
If the preferred delivery date of the customer is the same as the order date then the order is called immediate otherwise it's called scheduled.

The first order of a customer is the order with the earliest order date that customer made. It is guaranteed that a customer has exactly one first order.

Write an SQL query to find the percentage of immediate orders in the first orders of all customers, rounded to 2 decimal places.

The query result format is in the following example:


Delivery table:
+-------------+-------------+------------+-----------------------------+
| delivery_id | customer_id | order_date | customer_pref_delivery_date |
+-------------+-------------+------------+-----------------------------+
| 1           | 1           | 2019-08-01 | 2019-08-02                  |
| 2           | 2           | 2019-08-02 | 2019-08-02                  |
| 3           | 1           | 2019-08-11 | 2019-08-12                  |
| 4           | 3           | 2019-08-24 | 2019-08-24                  |
| 5           | 3           | 2019-08-21 | 2019-08-22                  |
| 6           | 2           | 2019-08-11 | 2019-08-13                  |
| 7           | 4           | 2019-08-09 | 2019-08-09                  |
+-------------+-------------+------------+-----------------------------+

Result table:
+----------------------+
| immediate_percentage |
+----------------------+
| 50.00                |
+----------------------+
The customer id 1 has a first order with delivery id 1 and it is scheduled.
The customer id 2 has a first order with delivery id 2 and it is immediate.
The customer id 3 has a first order with delivery id 5 and it is scheduled.
The customer id 4 has a first order with delivery id 7 and it is immediate.
Hence, half the customers have immediate first orders.
Solution
sql

#Solution- 1:
SELECT ROUND(SUM(CASE WHEN order_date=customer_pref_delivery_date THEN 1 ELSE 0 END)/count(DISTINCT customer_id)*100, 2) immediate_percentage
FROM Delivery
WHERE (customer_id, order_date) IN
    (SELECT customer_id, MIN(order_date)
    FROM Delivery
    GROUP BY customer_id)
#Solution- 2:
SELECT ROUND(AVG(CASE WHEN order_date = customer_pref_delivery_date THEN 1 ELSE 0 END)*100,2) as
immediate_percentage
FROM
(SELECT *,
 RANK() OVER(PARTITION BY customer_id ORDER BY order_date) AS rk
FROM delivery) a
WHERE a.rk=1
1179. Reformat Department Table | Easy | LeetCode
Table: Department


+---------------+---------+
| Column Name   | Type    |
+---------------+---------+
| id            | int     |
| revenue       | int     |
| month         | varchar |
+---------------+---------+
(id, month) is the primary key of this table.
The table has information about the revenue of each department per month.
The month has values in ["Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"].
Write an SQL query to reformat the table such that there is a department id column and a revenue column for each month.

The query result format is in the following example:


Department table:
+------+---------+-------+
| id   | revenue | month |
+------+---------+-------+
| 1    | 8000    | Jan   |
| 2    | 9000    | Jan   |
| 3    | 10000   | Feb   |
| 1    | 7000    | Feb   |
| 1    | 6000    | Mar   |
+------+---------+-------+

Result table:
+------+-------------+-------------+-------------+-----+-------------+
| id   | Jan_Revenue | Feb_Revenue | Mar_Revenue | ... | Dec_Revenue |
+------+-------------+-------------+-------------+-----+-------------+
| 1    | 8000        | 7000        | 6000        | ... | null        |
| 2    | 9000        | null        | null        | ... | null        |
| 3    | null        | 10000       | null        | ... | null        |
+------+-------------+-------------+-------------+-----+-------------+

Note that the result table has 13 columns (1 for the department id + 12 for the months).
Solution
sql

SELECT id,
SUM(IF(month='Jan', revenue, NULL)) AS Jan_Revenue,
SUM(IF(month='Feb', revenue, NULL)) AS Feb_Revenue,
SUM(IF(month='Mar', revenue, NULL)) AS Mar_Revenue,
SUM(IF(month='Apr', revenue, NULL)) AS Apr_Revenue,
SUM(IF(month='May', revenue, NULL)) AS May_Revenue,
SUM(IF(month='Jun', revenue, NULL)) AS Jun_Revenue,
SUM(IF(month='Jul', revenue, NULL)) AS Jul_Revenue,
SUM(IF(month='Aug', revenue, NULL)) AS Aug_Revenue,
SUM(IF(month='Sep', revenue, NULL)) AS Sep_Revenue,
SUM(IF(month='Oct', revenue, NULL)) AS Oct_Revenue,
SUM(IF(month='Nov', revenue, NULL)) AS Nov_Revenue,
SUM(IF(month='Dec', revenue, NULL)) AS Dec_Revenue
FROM Department
Group BY id;
1193. Monthly Transactions I | Medium | 🔒 LeetCode
Table: Transactions


+---------------+---------+
| Column Name   | Type    |
+---------------+---------+
| id            | int     |
| country       | varchar |
| state         | enum    |
| amount        | int     |
| trans_date    | date    |
+---------------+---------+
id is the primary key of this table.
The table has information about incoming transactions.
The state column is an enum of type ["approved", "declined"].
Write an SQL query to find for each month and country, the number of transactions and their total amount, the number of approved transactions and their total amount.

The query result format is in the following example:


Transactions table:
+------+---------+----------+--------+------------+
| id   | country | state    | amount | trans_date |
+------+---------+----------+--------+------------+
| 121  | US      | approved | 1000   | 2018-12-18 |
| 122  | US      | declined | 2000   | 2018-12-19 |
| 123  | US      | approved | 2000   | 2019-01-01 |
| 124  | DE      | approved | 2000   | 2019-01-07 |
+------+---------+----------+--------+------------+

Result table:
+----------+---------+-------------+----------------+--------------------+-----------------------+
| month    | country | trans_count | approved_count | trans_total_amount | approved_total_amount |
+----------+---------+-------------+----------------+--------------------+-----------------------+
| 2018-12  | US      | 2           | 1              | 3000               | 1000                  |
| 2019-01  | US      | 1           | 1              | 2000               | 2000                  |
| 2019-01  | DE      | 1           | 1              | 2000               | 2000                  |
+----------+---------+-------------+----------------+--------------------+-----------------------+
Solution
sql

WITH t1 AS(
SELECT DATE_FORMAT(trans_date,'%Y-%m') AS month, country, COUNT(state) AS trans_count, sum(amount) AS trans_total_amount
FROM transactions
GROUP BY country, month(trans_date)),

t2 AS (
SELECT DATE_FORMAT(trans_date,'%Y-%m') AS month, country, COUNT(state) AS approved_count, sum(amount) AS approved_total_amount
FROM transactions
WHERE state = 'approved'
GROUP BY country, month(trans_date))

SELECT t1.month, t1.country, COALESCE(t1.trans_count,0) AS trans_count, COALESCE(t2.approved_count,0) AS approved_count, COALESCE(t1.trans_total_amount,0) AS trans_total_amount, COALESCE(t2.approved_total_amount,0) AS approved_total_amount
FROM t1 LEFT JOIN t2
ON t1.country = t2.country and t1.month = t2.month
1194. Tournament Winners | Hard | 🔒 LeetCode
Table: Players


+-------------+-------+
| Column Name | Type  |
+-------------+-------+
| player_id   | int   |
| group_id    | int   |
+-------------+-------+
player_id is the primary key of this table.
Each row of this table indicates the group of each player.
Table: Matches


+---------------+---------+
| Column Name   | Type    |
+---------------+---------+
| match_id      | int     |
| first_player  | int     |
| second_player | int     |
| first_score   | int     |
| second_score  | int     |
+---------------+---------+
match_id is the primary key of this table.
Each row is a record of a match, first_player and second_player contain the player_id of each match.
first_score and second_score contain the number of points of the first_player and second_player respectively.
You may assume that, in each match, players belongs to the same group.
The winner in each group is the player who scored the maximum total points within the group. In the case of a tie, the lowest player_id wins.

Write an SQL query to find the winner in each group.

The query result format is in the following example:


Players table:
+-----------+------------+
| player_id | group_id   |
+-----------+------------+
| 15        | 1          |
| 25        | 1          |
| 30        | 1          |
| 45        | 1          |
| 10        | 2          |
| 35        | 2          |
| 50        | 2          |
| 20        | 3          |
| 40        | 3          |
+-----------+------------+

Matches table:
+------------+--------------+---------------+-------------+--------------+
| match_id   | first_player | second_player | first_score | second_score |
+------------+--------------+---------------+-------------+--------------+
| 1          | 15           | 45            | 3           | 0            |
| 2          | 30           | 25            | 1           | 2            |
| 3          | 30           | 15            | 2           | 0            |
| 4          | 40           | 20            | 5           | 2            |
| 5          | 35           | 50            | 1           | 1            |
+------------+--------------+---------------+-------------+--------------+

Result table:
+-----------+------------+
| group_id  | player_id  |
+-----------+------------+
| 1         | 15         |
| 2         | 35         |
| 3         | 40         |
+-----------+------------+
Solution
sql

WITH t1 AS(
SELECT first_player, SUM(first_score) AS total
FROM
(SELECT first_player, first_score
FROM matches
UNION ALL
SELECT second_player, second_score
FROM matches) a
GROUP BY 1),

t2 AS(
SELECT *, COALESCE(total,0) AS score
FROM players p LEFT JOIN t1
ON p.player_id = t1.first_player)

SELECT group_id, player_id
FROM
(SELECT *, ROW_NUMBER() OVER(PARTITION BY group_id ORDER BY group_id, score DESC) AS rn
FROM t2) b
WHERE b.rn = 1
1204. Last Person to Fit in the Elevator | Medium | 🔒 LeetCode
Table: Queue


+-------------+---------+
| Column Name | Type    |
+-------------+---------+
| person_id   | int     |
| person_name | varchar |
| weight      | int     |
| turn        | int     |
+-------------+---------+
person_id is the primary key column for this table.
This table has the information about all people waiting for an elevator.
The person_id and turn columns will contain all numbers from 1 to n, where n is the number of rows in the table.
The maximum weight the elevator can hold is 1000.

Write an SQL query to find the person_name of the last person who will fit in the elevator without exceeding the weight limit. It is guaranteed that the person who is first in the queue can fit in the elevator.

The query result format is in the following example:


Queue table
+-----------+-------------------+--------+------+
| person_id | person_name       | weight | turn |
+-----------+-------------------+--------+------+
| 5         | George Washington | 250    | 1    |
| 3         | John Adams        | 350    | 2    |
| 6         | Thomas Jefferson  | 400    | 3    |
| 2         | Will Johnliams    | 200    | 4    |
| 4         | Thomas Jefferson  | 175    | 5    |
| 1         | James Elephant    | 500    | 6    |
+-----------+-------------------+--------+------+

Result table
+-------------------+
| person_name       |
+-------------------+
| Thomas Jefferson  |
+-------------------+

Queue table is ordered by turn in the example for simplicity.
In the example George Washington(id 5), John Adams(id 3) and Thomas Jefferson(id 6) will enter the elevator as their weight sum is 250 + 350 + 400 = 1000.
Thomas Jefferson(id 6) is the last person to fit in the elevator because he has the last turn in these three people.
Solution
sql

WITH t1 AS
(
SELECT *,
SUM(weight) OVER(ORDER BY turn) AS cum_weight
FROM queue
ORDER BY turn)

SELECT t1.person_name
FROM t1
WHERE turn = (SELECT MAX(turn) FROM t1 WHERE t1.cum_weight<=1000)
1205. Monthly Transactions II | Medium | 🔒 LeetCode
Table: Transactions


+----------------+---------+
| Column Name    | Type    |
+----------------+---------+
| id             | int     |
| country        | varchar |
| state          | enum    |
| amount         | int     |
| trans_date     | date    |
+----------------+---------+
id is the primary key of this table.
The table has information about incoming transactions.
The state column is an enum of type ["approved", "declined"].
Table: Chargebacks


+----------------+---------+
| Column Name    | Type    |
+----------------+---------+
| trans_id       | int     |
| charge_date    | date    |
+----------------+---------+
Chargebacks contains basic information regarding incoming chargebacks from some transactions placed in Transactions table.
trans_id is a foreign key to the id column of Transactions table.
Each chargeback corresponds to a transaction made previously even if they were not approved.
Write an SQL query to find for each month and country, the number of approved transactions and their total amount, the number of chargebacks and their total amount.

Note: In your query, given the month and country, ignore rows with all zeros.

The query result format is in the following example:


Transactions table:
+------+---------+----------+--------+------------+
| id   | country | state    | amount | trans_date |
+------+---------+----------+--------+------------+
| 101  | US      | approved | 1000   | 2019-05-18 |
| 102  | US      | declined | 2000   | 2019-05-19 |
| 103  | US      | approved | 3000   | 2019-06-10 |
| 104  | US      | approved | 4000   | 2019-06-13 |
| 105  | US      | approved | 5000   | 2019-06-15 |
+------+---------+----------+--------+------------+

Chargebacks table:
+------------+------------+
| trans_id   | trans_date |
+------------+------------+
| 102        | 2019-05-29 |
| 101        | 2019-06-30 |
| 105        | 2019-09-18 |
+------------+------------+

Result table:
+----------+---------+----------------+-----------------+-------------------+--------------------+
| month    | country | approved_count | approved_amount | chargeback_count  | chargeback_amount  |
+----------+`---------+----------------+-----------------+-------------------+--------------------+
| 2019-05  | US      | 1              | 1000            | 1                 | 2000               |
| 2019-06  | US      | 3              | 12000           | 1                 | 1000               |
| 2019-09  | US      | 0              | 0               | 1                 | 5000               |
+----------+---------+----------------+-----------------+-------------------+--------------------+
Solution
sql

#Solution 1:
WITH t1 AS
(SELECT country, extract('month' FROM trans_date), state, COUNT(*) AS approved_count, SUM(amount) AS approved_amount
FROM transactions
WHERE state = 'approved'
GROUP BY 1, 2, 3),
t2 AS(
SELECT t.country, extract('month' FROM c.trans_date), SUM(amount) AS chargeback_amount, COUNT(*) AS chargeback_count
FROM chargebacks c LEFT JOIN transactions t
ON trans_id = id
GROUP BY t.country, extract('month' FROM c.trans_date)),

t3 AS(
SELECT t2.date_part, t2.country, COALESCE(approved_count,0) AS approved_count, COALESCE(approved_amount,0) AS approved_amount, COALESCE(chargeback_count,0) AS chargeback_count, COALESCE(chargeback_amount,0) AS chargeback_amount
FROM t2 LEFT JOIN t1
ON t2.date_part = t1.date_part AND t2.country = t1.country),

t4 AS(
SELECT t1.date_part, t1.country, COALESCE(approved_count,0) AS approved_count, COALESCE(approved_amount,0) AS approved_amount, COALESCE(chargeback_count,0) AS chargeback_count, COALESCE(chargeback_amount,0) AS chargeback_amount
FROM t2 RIGHT JOIN t1
ON t2.date_part = t1.date_part AND t2.country = t1.country)

SELECT *
FROM t3
UNION
SELECT *
FROM t4

#Solution 2:
SELECT month, country,
    SUM(CASE WHEN type='approved' THEN 1 ELSE 0 END) AS approved_count,
    SUM(CASE WHEN type='approved' THEN amount ELSE 0 END) AS approved_amount,
    SUM(CASE WHEN type='chargeback' THEN 1 ELSE 0 END) AS chargeback_count,
    SUM(CASE WHEN type='chargeback' THEN amount ELSE 0 END) AS chargeback_amount
FROM (
    (
    SELECT left(t.trans_date, 7) AS month, t.country, amount,'approved' AS type
    FROM Transactions AS t
    WHERE state='approved'
    )
    UNION ALL (
    SELECT left(c.trans_date, 7) AS month, t.country, amount,'chargeback' AS type
    FROM Transactions AS t JOIN Chargebacks AS c
    ON t.id = c.trans_id
    )
) AS tt
GROUP BY tt.month, tt.country

#Solution 3:
SELECT month, country,
    SUM(CASE WHEN type='approved' THEN count ELSE 0 END) AS approved_count,
    SUM(CASE WHEN type='approved' THEN amount ELSE 0 END) AS approved_amount,
    SUM(CASE WHEN type='chargeback' THEN count ELSE 0 END) AS chargeback_count,
    SUM(CASE WHEN type='chargeback' THEN amount ELSE 0 END) AS chargeback_amount
FROM (
    (
    SELECT LEFT(t.trans_date, 7) AS month, t.country,
    COUNT(1) AS count, SUM(amount) AS amount,'approved' AS type
    FROM Transactions AS t LEFT JOIN Chargebacks AS c
    ON t.id = c.trans_id
    WHERE state='approved'
    GROUP BY LEFT(t.trans_date, 7), t.country
    )
    union (
    SELECT LEFT(c.trans_date, 7) AS month, t.country,
        COUNT(1) AS count, SUM(amount) AS amount,'chargeback' AS type
    FROM Transactions AS t JOIN Chargebacks AS c
    ON t.id = c.trans_id
    GROUP BY LEFT(c.trans_date, 7), t.country
    )
) AS tt
GROUP BY tt.month, tt.country
1211. Queries Quality and Percentage | Easy | 🔒 LeetCode
Table: Queries


+-------------+---------+
| Column Name | Type    |
+-------------+---------+
| query_name  | varchar |
| result      | varchar |
| position    | int     |
| rating      | int     |
+-------------+---------+
There is no primary key for this table, it may have duplicate rows.
This table contains information collected from some queries on a database.
The position column has a value from 1 to 500.
The rating column has a value from 1 to 5. Query with rating less than 3 is a poor query.
We define query quality as:

The average of the ratio between query rating and its position.
We also define poor query percentage as:

The percentage of all queries with rating less than 3.
Write an SQL query to find each query_name, the quality and poor_query_percentage.

Both quality and poor_query_percentage should be rounded to 2 decimal places.

The query result format is in the following example:


Queries table:
+------------+-------------------+----------+--------+
| query_name | result            | position | rating |
+------------+-------------------+----------+--------+
| Dog        | Golden Retriever  | 1        | 5      |
| Dog        | German Shepherd   | 2        | 5      |
| Dog        | Mule              | 200      | 1      |
| Cat        | Shirazi           | 5        | 2      |
| Cat        | Siamese           | 3        | 3      |
| Cat        | Sphynx            | 7        | 4      |
+------------+-------------------+----------+--------+

Result table:
+------------+---------+-----------------------+
| query_name | quality | poor_query_percentage |
+------------+---------+-----------------------+
| Dog        | 2.50    | 33.33                 |
| Cat        | 0.66    | 33.33                 |
+------------+---------+-----------------------+

Dog queries quality is ((5 / 1) + (5 / 2) + (1 / 200)) / 3 = 2.50
Dog queries poor_ query_percentage is (1 / 3) * 100 = 33.33

Cat queries quality equals ((2 / 5) + (3 / 3) + (4 / 7)) / 3 = 0.66
Cat queries poor_ query_percentage is (1 / 3) * 100 = 33.33
Solution
sql

#Solution 1:
SELECT query_name, ROUND(SUM(rating/position)/COUNT(*),2) AS quality,
ROUND(AVG(CASE WHEN rating<3 THEN 1 ELSE 0 END)*100,2) AS poor_query_percentage
FROM queries
GROUP BY query_name

#Solution 2:
SELECT query_name, ROUND(AVG(rating/position), 2) AS quality,
       ROUND(100*SUM(CASE WHEN rating<3 THEN 1 ELSE 0 END)/COUNT(1), 2) AS poor_query_percentage
FROM Queries
GROUP BY query_name
1212. Team Scores in Football Tournament | Medium | 🔒 LeetCode
Table: Teams


+---------------+----------+
| Column Name   | Type     |
+---------------+----------+
| team_id       | int      |
| team_name     | varchar  |
+---------------+----------+
team_id is the primary key of this table.
Each row of this table represents a single football team.
Table: Matches


+---------------+---------+
| Column Name   | Type    |
+---------------+---------+
| match_id      | int     |
| host_team     | int     |
| guest_team    | int     |
| host_goals    | int     |
| guest_goals   | int     |
+---------------+---------+
match_id is the primary key of this table.
Each row is a record of a finished match between two different teams.
Teams host_team and guest_team are represented by their IDs in the teams table (team_id) and they scored host_goals and guest_goals goals respectively.
You would like to compute the scores of all teams after all matches. Points are awarded as follows:

A team receives three points if they win a match (Score strictly more goals than the opponent team).

A team receives one point if they draw a match (Same number of goals as the opponent team).

A team receives no points if they lose a match (Score less goals than the opponent team).

Write an SQL query that selects the team_id, team_name and num_points of each team in the tournament after all described matches. Result table should be ordered by num_points (decreasing order). In case of a tie, order the records by team_id (increasing order).

The query result format is in the following example:


Teams table:
+-----------+--------------+
| team_id   | team_name    |
+-----------+--------------+
| 10        | Leetcode FC  |
| 20        | NewYork FC   |
| 30        | Atlanta FC   |
| 40        | Chicago FC   |
| 50        | Toronto FC   |
+-----------+--------------+

Matches table:
+------------+--------------+---------------+-------------+--------------+
| match_id   | host_team    | guest_team    | host_goals  | guest_goals  |
+------------+--------------+---------------+-------------+--------------+
| 1          | 10           | 20            | 3           | 0            |
| 2          | 30           | 10            | 2           | 2            |
| 3          | 10           | 50            | 5           | 1            |
| 4          | 20           | 30            | 1           | 0            |
| 5          | 50           | 30            | 1           | 0            |
+------------+--------------+---------------+-------------+--------------+

Result table:
+------------+--------------+---------------+
| team_id    | team_name    | num_points    |
+------------+--------------+---------------+
| 10         | Leetcode FC  | 7             |
| 20         | NewYork FC   | 3             |
| 50         | Toronto FC   | 3             |
| 30         | Atlanta FC   | 1             |
| 40         | Chicago FC   | 0             |
+------------+--------------+---------------+
Solution
sql

#Solution 1:
SELECT Teams.team_id, Teams.team_name,
    SUM(CASE WHEN team_id=host_team AND host_goals>guest_goals THEN 3 ELSE 0 END) +
    SUM(CASE WHEN team_id=host_team AND host_goals=guest_goals THEN 1 ELSE 0 END) +
    SUM(CASE WHEN team_id=guest_team AND host_goals<guest_goals THEN 3 ELSE 0 END) +
    SUM(CASE WHEN team_id=guest_team AND host_goals=guest_goals THEN 1 ELSE 0 END) AS num_points
FROM Teams LEFT JOIN Matches
ON Teams.team_id = Matches.host_team OR Teams.team_id = Matches.guest_team
GROUP BY Teams.team_id
ORDER BY num_points DESC, Teams.team_id ASC

#Solution 2:
SELECT Teams.team_id, Teams.team_name, SUM(if(isnull(num_points), 0, num_points)) AS num_points
FROM Teams LEFT JOIN
    (
        SELECT host_team AS team_id,
            SUM(CASE WHEN host_goals>guest_goals THEN 3
                     WHEN host_goals=guest_goals THEN 1
                     ELSE 0 END) AS num_points
        FROM Matches
        GROUP BY host_team
        UNION ALL
        SELECT guest_team AS team_id,
            SUM(CASE WHEN host_goals<guest_goals THEN 3
                     WHEN host_goals=guest_goals THEN 1
                     ELSE 0 END) AS num_points
        FROM Matches
        GROUP BY guest_team
    ) AS tt
ON Teams.team_id = tt.team_id
GROUP BY Teams.team_id
ORDER BY num_points DESC, Teams.team_id ASC

#Solution 3:
SELECT Teams.team_id, Teams.team_name, IFNULL(SUM(num_points), 0) AS num_points
FROM Teams LEFT JOIN
    (
        SELECT host_team AS team_id,
            SUM(CASE WHEN host_goals>guest_goals THEN 3
                     WHEN host_goals=guest_goals THEN 1
                     ELSE 0 END) AS num_points
        FROM Matches
        GROUP BY host_team
        UNION ALL
        SELECT guest_team AS team_id,
            SUM(CASE WHEN host_goals<guest_goals THEN 3
                     WHEN host_goals=guest_goals THEN 1
                     ELSE 0 END) AS num_points
        FROM Matches
        GROUP BY guest_team
    ) AS tt
ON Teams.team_id = tt.team_id
GROUP BY Teams.team_id
ORDER BY num_points DESC, Teams.team_id ASC

#Solution 4:
WITH t1 AS(
SELECT c.host_id, c.host_name, c.host_points
FROM(
SELECT a.match_id, a.team_id AS host_id, a.team_name AS host_name, b.team_id AS guest_id, b.team_name AS guest_name, a.host_goals, a.guest_goals,
CASE WHEN a.host_goals > a.guest_goals THEN 3
     WHEN a.host_goals = a.guest_goals THEN 1
     ELSE 0 END AS host_points,
CASE WHEN a.host_goals < a.guest_goals THEN 3
     WHEN a.host_goals = a.guest_goals THEN 1
     ELSE 0 END AS guest_points
FROM(
SELECT *
FROM matches m
JOIN teams t
ON t.team_id = m.host_team) a
JOIN
(SELECT *
FROM matches m
JOIN teams t
ON t.team_id = m.guest_team) b
ON a.match_id = b.match_id) c
UNION ALL
SELECT d.guest_id, d.guest_name, d.guest_points
FROM(
SELECT a.match_id, a.team_id AS host_id, a.team_name AS host_name, b.team_id AS guest_id, b.team_name AS guest_name, a.host_goals, a.guest_goals,
CASE WHEN a.host_goals > a.guest_goals THEN 3
     WHEN a.host_goals = a.guest_goals THEN 1
     ELSE 0 END AS host_points,
CASE WHEN a.host_goals < a.guest_goals THEN 3
     WHEN a.host_goals = a.guest_goals THEN 1
     ELSE 0 END AS guest_points
FROM(
SELECT *
FROM matches m
JOIN teams t
ON t.team_id = m.host_team) a
JOIN
(SELECT *
FROM matches m
JOIN teams t
ON t.team_id = m.guest_team) b
ON a.match_id = b.match_id) d)

SELECT team_id, team_name, coalesce(total,0) AS num_points
FROM teams t2
LEFT JOIN(
SELECT host_id, host_name, SUM(host_points) AS total
FROM t1
GROUP BY host_id, host_name) e
ON t2.team_id = e.host_id
ORDER BY num_points DESC, team_id
1225. Report Contiguous Dates | Hard | 🔒 LeetCode
Table: Failed


+--------------+---------+
| Column Name  | Type    |
+--------------+---------+
| fail_date    | date    |
+--------------+---------+
Primary key for this table is fail_date.
Failed table contains the days of failed tasks.
Table: Succeeded


+--------------+---------+
| Column Name  | Type    |
+--------------+---------+
| success_date | date    |
+--------------+---------+
Primary key for this table is success_date.
Succeeded table contains the days of succeeded tasks.
A system is running one task every day. Every task is independent of the previous tasks. The tasks can fail or succeed.

Write an SQL query to generate a report of period_state for each continuous interval of days in the period from 2019-01-01 to 2019-12-31.

period_state is 'failed' if tasks in this interval failed or 'succeeded' if tasks in this interval succeeded. Interval of days are retrieved as start_date and end_date.

Order result by start_date.

The query result format is in the following example:


Failed table:
+-------------------+
| fail_date         |
+-------------------+
| 2018-12-28        |
| 2018-12-29        |
| 2019-01-04        |
| 2019-01-05        |
+-------------------+

Succeeded table:
+-------------------+
| success_date      |
+-------------------+
| 2018-12-30        |
| 2018-12-31        |
| 2019-01-01        |
| 2019-01-02        |
| 2019-01-03        |
| 2019-01-06        |
+-------------------+


Result table:
+--------------+--------------+--------------+
| period_state | start_date   | end_date     |
+--------------+--------------+--------------+
| succeeded    | 2019-01-01   | 2019-01-03   |
| failed       | 2019-01-04   | 2019-01-05   |
| succeeded    | 2019-01-06   | 2019-01-06   |
+--------------+--------------+--------------+

The report ignored the system state in 2018 as we care about the system in the period 2019-01-01 to 2019-12-31.
From 2019-01-01 to 2019-01-03 all tasks succeeded and the system state was "succeeded".
From 2019-01-04 to 2019-01-05 all tasks failed and system state was "failed".
From 2019-01-06 to 2019-01-06 all tasks succeeded and system state was "succeeded".
Solution
sql

#Solution 1:
WITH t1 AS(
SELECT MIN(success_date) AS start_date, MAX(success_date) AS end_date, state
FROM(
SELECT *, date_sub(success_date, interval ROW_NUMBER() OVER(ORDER BY success_date) day) AS diff, 1 AS state
FROM succeeded
WHERE success_date BETWEEN "2019-01-01" AND "2019-12-31") a
GROUP BY diff),

t2 AS(
SELECT MIN(fail_date) AS start_date, MAX(fail_date) AS end_date, state
FROM(
SELECT *, date_sub(fail_date, interval ROW_NUMBER() OVER(ORDER BY fail_date) day) AS diff, 0 AS state
FROM failed
WHERE fail_date BETWEEN "2019-01-01" AND "2019-12-31") b
GROUP BY diff)


SELECT
CASE WHEN c.state = 1 THEN "succeeded"
ELSE "failed"
END AS period_state,start_date, end_date
FROM(
SELECT *
FROM t1

UNION ALL

SELECT *
FROM t2) c
ORDER BY start_date

#Solution 2:
## First generate a list of dates
##   succeeded 2019-01-01
##   succeeded 2019-01-02
##   ...
##   failed 2019-01-04
##   ...
## Add group id for contiguous ranges
## Notice: dates themselves are contiguous
##
SELECT period_state, MIN(date) AS start_date, MAX(date) AS end_date
FROM (
    SELECT period_state, date,
         @rank := CASE WHEN @prev = period_state THEN @rank ELSE @rank+1 END AS rank,
         @prev := period_state AS prev
    FROM (
        SELECT 'failed' AS period_state, fail_date AS date
        FROM Failed
        WHERE fail_date BETWEEN '2019-01-01' AND '2019-12-31'
        UNION
        SELECT 'succeeded' AS period_state, success_date AS date
        FROM Succeeded
        WHERE success_date BETWEEN '2019-01-01' AND '2019-12-31') AS t,
        (SELECT @rank:=0, @prev:='') AS rows
    ORDER BY date ASC) AS tt
GROUP BY rank
ORDER BY rank
1241. Number of Comments per Post | Easy | 🔒 LeetCode
Table: Submissions


+---------------+----------+
| Column Name   | Type     |
+---------------+----------+
| sub_id        | int      |
| parent_id     | int      |
+---------------+----------+
There is no primary key for this table, it may have duplicate rows.
Each row can be a post or comment on the post.
parent_id is null for posts.
parent_id for comments is sub_id for another post in the table.
Write an SQL query to find number of comments per each post.

Result table should contain post_id and its corresponding number_of_comments, and must be sorted by post_id in ascending order.

Submissions may contain duplicate comments. You should count the number of unique comments per post.

Submissions may contain duplicate posts. You should treat them as one post.

The query result format is in the following example:


Submissions table:
+---------+------------+
| sub_id  | parent_id  |
+---------+------------+
| 1       | Null       |
| 2       | Null       |
| 1       | Null       |
| 12      | Null       |
| 3       | 1          |
| 5       | 2          |
| 3       | 1          |
| 4       | 1          |
| 9       | 1          |
| 10      | 2          |
| 6       | 7          |
+---------+------------+

Result table:
+---------+--------------------+
| post_id | number_of_comments |
+---------+--------------------+
| 1       | 3                  |
| 2       | 2                  |
| 12      | 0                  |
+---------+--------------------+

The post with id 1 has three comments in the table with id 3, 4 and 9. The comment with id 3 is repeated in the table, we counted it only once.
The post with id 2 has two comments in the table with id 5 and 10.
The post with id 12 has no comments in the table.
The comment with id 6 is a comment on a deleted post with id 7 so we ignored it.
Solution
sql

SELECT a.sub_id AS post_id, coalesce(b.number_of_comments,0) AS number_of_comments
FROM(
SELECT DISTINCT sub_id FROM submissions WHERE parent_id IS NULL) a
LEFT JOIN(
SELECT parent_id, count(DISTINCT(sub_id)) AS number_of_comments
FROM submissions
GROUP BY parent_id
HAVING parent_id = any(SELECT sub_id from submissions WHERE parent_id IS NULL)) b
ON a.sub_id = b.parent_id
ORDER BY post_id
1251. Average Selling Price | Easy | 🔒 LeetCode
Table: Prices


+---------------+---------+
| Column Name   | Type    |
+---------------+---------+
| product_id    | int     |
| start_date    | date    |
| end_date      | date    |
| price         | int     |
+---------------+---------+
(product_id, start_date, end_date) is the primary key for this table.
Each row of this table indicates the price of the product_id in the period from start_date to end_date.
For each product_id there will be no two overlapping periods. That means there will be no two intersecting periods for the same product_id.
Table: UnitsSold


+---------------+---------+
| Column Name   | Type    |
+---------------+---------+
| product_id    | int     |
| purchase_date | date    |
| units         | int     |
+---------------+---------+
There is no primary key for this table, it may contain duplicates.
Each row of this table indicates the date, units and product_id of each product sold.
Write an SQL query to find the average selling price for each product.

average_price should be rounded to 2 decimal places.

The query result format is in the following example:


Prices table:
+------------+------------+------------+--------+
| product_id | start_date | end_date   | price  |
+------------+------------+------------+--------+
| 1          | 2019-02-17 | 2019-02-28 | 5      |
| 1          | 2019-03-01 | 2019-03-22 | 20     |
| 2          | 2019-02-01 | 2019-02-20 | 15     |
| 2          | 2019-02-21 | 2019-03-31 | 30     |
+------------+------------+------------+--------+

UnitsSold table:
+------------+---------------+-------+
| product_id | purchase_date | units |
+------------+---------------+-------+
| 1          | 2019-02-25    | 100   |
| 1          | 2019-03-01    | 15    |
| 2          | 2019-02-10    | 200   |
| 2          | 2019-03-22    | 30    |
+------------+---------------+-------+

Result table:
+------------+---------------+
| product_id | average_price |
+------------+---------------+
| 1          | 6.96          |
| 2          | 16.96         |
+------------+---------------+
Average selling price = Total Price of Product / Number of products sold.
Average selling price for product 1 = ((100 * 5) + (15 * 20)) / 115 = 6.96
Average selling price for product 2 = ((200 * 15) + (30 * 30)) / 230 = 16.96
Solution
sql

SELECT UnitsSold.product_id, ROUND(SUM(units*price)/SUM(units), 2) AS average_price
FROM UnitsSold INNER JOIN Prices
ON UnitsSold.product_id = Prices.product_id
AND UnitsSold.purchase_date BETWEEN Prices.start_date AND Prices.end_date
GROUP BY UnitsSold.product_id
1264. Page Recommendations | Medium | 🔒 LeetCode
Table: Friendship


+---------------+---------+
| Column Name   | Type    |
+---------------+---------+
| user1_id      | int     |
| user2_id      | int     |
+---------------+---------+
(user1_id, user2_id) is the primary key for this table.
Each row of this table indicates that there is a friendship relation between user1_id and user2_id.
Table: Likes


+-------------+---------+
| Column Name | Type    |
+-------------+---------+
| user_id     | int     |
| page_id     | int     |
+-------------+---------+
(user_id, page_id) is the primary key for this table.
Each row of this table indicates that user_id likes page_id.
Write an SQL query to recommend pages to the user with user_id = 1 using the pages that your friends liked. It should not recommend pages you already liked.

Return result table in any order without duplicates.

The query result format is in the following example:


Friendship table:
+----------+----------+
| user1_id | user2_id |
+----------+----------+
| 1        | 2        |
| 1        | 3        |
| 1        | 4        |
| 2        | 3        |
| 2        | 4        |
| 2        | 5        |
| 6        | 1        |
+----------+----------+

Likes table:
+---------+---------+
| user_id | page_id |
+---------+---------+
| 1       | 88      |
| 2       | 23      |
| 3       | 24      |
| 4       | 56      |
| 5       | 11      |
| 6       | 33      |
| 2       | 77      |
| 3       | 77      |
| 6       | 88      |
+---------+---------+

Result table:
+------------------+
| recommended_page |
+------------------+
| 23               |
| 24               |
| 56               |
| 33               |
| 77               |
+------------------+
User one is friend with users 2, 3, 4 and 6.
Suggested pages are 23 from user 2, 24 from user 3, 56 from user 3 and 33 from user 6.
Page 77 is suggested from both user 2 and user 3.
Page 88 is not suggested because user 1 already likes it.
Solution
sql

SELECT DISTINCT page_id AS recommended_page
FROM Likes
WHERE user_id IN (SELECT user2_id
    FROM Friendship
    WHERE user1_id=1
    UNION
    SELECT user1_id
    FROM Friendship
    WHERE user2_id=1)
    AND page_id NOT IN
        (SELECT page_id
        FROM Likes
        WHERE user_id=1)
1270. All People Report to the Given Manager | Medium | 🔒 LeetCode
Table: Employees


+---------------+---------+
| Column Name   | Type    |
+---------------+---------+
| employee_id   | int     |
| employee_name | varchar |
| manager_id    | int     |
+---------------+---------+
employee_id is the primary key for this table.
Each row of this table indicates that the employee with ID employee_id and name employee_name reports his work to his/her direct manager with manager_id
The head of the company is the employee with employee_id = 1.
Write an SQL query to find employee_id of all employees that directly or indirectly report their work to the head of the company.

The indirect relation between managers will not exceed 3 managers as the company is small.

Return result table in any order without duplicates.

The query result format is in the following example:


Employees table:
+-------------+---------------+------------+
| employee_id | employee_name | manager_id |
+-------------+---------------+------------+
| 1           | Boss          | 1          |
| 3           | Alice         | 3          |
| 2           | Bob           | 1          |
| 4           | Daniel        | 2          |
| 7           | Luis          | 4          |
| 8           | Jhon          | 3          |
| 9           | Angela        | 8          |
| 77          | Robert        | 1          |
+-------------+---------------+------------+

Result table:
+-------------+
| employee_id |
+-------------+
| 2           |
| 77          |
| 4           |
| 7           |
+-------------+

The head of the company is the employee with employee_id 1.
The employees with employee_id 2 and 77 report their work directly to the head of the company.
The employee with employee_id 4 report his work indirectly to the head of the company 4 --> 2 --> 1.
The employee with employee_id 7 report his work indirectly to the head of the company 7 --> 4 --> 2 --> 1.
The employees with employee_id 3, 8 and 9 don't report their work to head of company directly or indirectly.
Solution
sql

#Solution 1:
## t3: directly report to employee_id 1
## t2: directly report to t3
## t1: directly report to t2
SELECT t1.employee_id
FROM Employees AS t1 INNER JOIN Employees AS t2
ON t1.manager_id = t2.employee_id
JOIN Employees AS t3
ON t2.manager_id = t3.employee_id
WHERE t3.manager_id = 1 AND t1.employee_id != 1

#Solution 2:
SELECT distinct employee_id
FROM (
SELECT employee_id
FROM Employees
WHERE manager_id IN
(SELECT employee_id
FROM Employees
WHERE manager_id IN
    (SELECT employee_id
    FROM Employees
    WHERE manager_id = 1))
UNION
SELECT employee_id
FROM Employees
WHERE manager_id IN
    (SELECT employee_id
    FROM Employees
    WHERE manager_id = 1)
UNION
SELECT employee_id
    FROM Employees
    WHERE manager_id = 1) AS t
WHERE employee_id != 1

#Solution 3:
SELECT employee_id
FROM employees
WHERE manager_id = 1 AND employee_id != 1
UNION
SELECT employee_id
FROM employees
WHERE manager_id = any (SELECT employee_id
FROM employees
WHERE manager_id = 1 AND employee_id != 1)
UNION
SELECT employee_id
FROM employees
WHERE manager_id = any (SELECT employee_id
FROM employees
WHERE manager_id = any (SELECT employee_id
FROM employees
WHERE manager_id = 1 AND employee_id != 1))
1280. Students and Examinations| Easy | 🔒 LeetCode
Table: Students


+---------------+---------+
| Column Name   | Type    |
+---------------+---------+
| student_id    | int     |
| student_name  | varchar |
+---------------+---------+
student_id is the primary key for this table.
Each row of this table contains the ID and the name of one student in the school.
Table: Subjects


+--------------+---------+
| Column Name  | Type    |
+--------------+---------+
| subject_name | varchar |
+--------------+---------+
subject_name is the primary key for this table.
Each row of this table contains a name of one subject in the school.
Table: Examinations


+--------------+---------+
| Column Name  | Type    |
+--------------+---------+
| student_id   | int     |
| subject_name | varchar |
+--------------+---------+
There is no primary key for this table. It may contain duplicates.
Each student from Students table takes every course from Subjects table.
Each row of this table indicates that a student with ID student_id attended the exam of subject_name.
Write an SQL query to find the number of times each student attended each exam.

Order the result table by student_id and subject_name.

The query result format is in the following example:


Students table:
+------------+--------------+
| student_id | student_name |
+------------+--------------+
| 1          | Alice        |
| 2          | Bob          |
| 13         | John         |
| 6          | Alex         |
+------------+--------------+
Subjects table:
+--------------+
| subject_name |
+--------------+
| Math         |
| Physics      |
| Programming  |
+--------------+
Examinations table:
+------------+--------------+
| student_id | subject_name |
+------------+--------------+
| 1          | Math         |
| 1          | Physics      |
| 1          | Programming  |
| 2          | Programming  |
| 1          | Physics      |
| 1          | Math         |
| 13         | Math         |
| 13         | Programming  |
| 13         | Physics      |
| 2          | Math         |
| 1          | Math         |
+------------+--------------+
Result table:
+------------+--------------+--------------+----------------+
| student_id | student_name | subject_name | attended_exams |
+------------+--------------+--------------+----------------+
| 1          | Alice        | Math         | 3              |
| 1          | Alice        | Physics      | 2              |
| 1          | Alice        | Programming  | 1              |
| 2          | Bob          | Math         | 1              |
| 2          | Bob          | Physics      | 0              |
| 2          | Bob          | Programming  | 1              |
| 6          | Alex         | Math         | 0              |
| 6          | Alex         | Physics      | 0              |
| 6          | Alex         | Programming  | 0              |
| 13         | John         | Math         | 1              |
| 13         | John         | Physics      | 1              |
| 13         | John         | Programming  | 1              |
+------------+--------------+--------------+----------------+
The result table should contain all students and all subjects.
Alice attended Math exam 3 times, Physics exam 2 times and Programming exam 1 time.
Bob attended Math exam 1 time, Programming exam 1 time and didn't attend the Physics exam.
Alex didn't attend any exam.
John attended Math exam 1 time, Physics exam 1 time and Programming exam 1 time.
Solution
sql

#Solution 1: count with null
SELECT Students.student_id, student_name, Subjects.subject_name, COUNT(Examinations.student_id) AS attended_exams
FROM Students JOIN Subjects
LEFT JOIN Examinations
ON Students.student_id = Examinations.student_id AND Subjects.subject_name = Examinations.subject_name
GROUP BY Students.student_id, subject_name

#Solution 2: using ISNULL
SELECT Students.student_id, student_name, Subjects.subject_name, SUM(IF(ISNULL(Examinations.student_id), 0, 1)) AS attended_exams
FROM Students JOIN Subjects
LEFT JOIN Examinations
ON Students.student_id = Examinations.student_id AND Subjects.subject_name = Examinations.subject_name
GROUP BY Students.student_id, subject_name

#Solution 3: coalesce
SELECT a.student_id AS student_id, a.student_name AS student_name, a.subject_name AS subject_name, coalesce(attended_exams,0) AS attended_exams
FROM(
SELECT *
FROM students
CROSS JOIN subjects
GROUP BY student_id, student_name, subject_name) a
LEFT JOIN
(SELECT e.student_id, student_name, subject_name, COUNT(*) AS attended_exams
FROM examinations e JOIN students s
ON e.student_id = s.student_id
GROUP BY e.student_id, student_name, subject_name) b
ON a.student_id = b.student_id AND a.subject_name =b.subject_name
ORDER BY a.student_id ASC, a.subject_name ASC
1285. Find the Start and End Number of Continuous Ranges | Medium | 🔒 LeetCode
Table: Logs


+---------------+---------+
| Column Name   | Type    |
+---------------+---------+
| log_id        | int     |
+---------------+---------+
id is the primary key for this table.
Each row of this table contains the ID in a log Table.
Since some IDs have been removed from Logs. Write an SQL query to find the start and end number of continuous ranges in table Logs.

Order the result table by start_id.

The query result format is in the following example:


Logs table:
+------------+
| log_id     |
+------------+
| 1          |
| 2          |
| 3          |
| 7          |
| 8          |
| 10         |
+------------+

Result table:
+------------+--------------+
| start_id   | end_id       |
+------------+--------------+
| 1          | 3            |
| 7          | 8            |
| 10         | 10           |
+------------+--------------+
The result table should contain all ranges in table Logs.
From 1 to 3 is contained in the table.
From 4 to 6 is missing in the table
From 7 to 8 is contained in the table.
Number 9 is missing in the table.
Number 10 is contained in the table.
Solution
sql

#Solution 1:
SELECT MIN(log_id) AS start_id, MAX(log_id) AS end_id
FROM(
SELECT log_id, log_id-ROW_NUMBER() OVER (ORDER BY log_id) AS rk
FROM logs) a
GROUP BY rk

#Solution 2: Add temporary columns of rank and prev
SELECT MIN(log_id) AS START_ID, MAX(log_id) AS END_ID
FROM (SELECT log_id,
        @rank := CASE WHEN @prev = log_id-1 THEN @rank ELSE @rank+1 END AS rank,
        @prev := log_id AS prev
    FROM Logs,
       (SELECT @rank:=0, @prev:=-1) AS rows) AS tt
GROUP BY rank
ORDER BY START_ID

# Solution 3: Find the starting and ending sequences, then merge two AS one table
## find the starting sequence: 1, 7, 10
## find the ending sequence: 3, 8, 10
## merge them AS one table
SELECT start_id, MIN(end_id) AS end_id
FROM (SELECT t1.log_id AS start_id
    FROM logs AS t1 LEFT JOIN logs AS t2
    ON t1.log_id-1 = t2.log_id
    WHERE t2.log_id IS NULL) tt_start join
    (SELECT t1.log_id AS end_id
    FROM logs AS t1 LEFT JOIN logs AS t2
    ON t1.log_id+1 = t2.log_id
    WHERE t2.log_id IS NULL) tt_end
WHERE start_id<=end_id
GROUP BY start_id
1294. Weather Type in Each Country | Easy | 🔒 LeetCode
Table: Countries


+---------------+---------+
| Column Name   | Type    |
+---------------+---------+
| country_id    | int     |
| country_name  | varchar |
+---------------+---------+
country_id is the primary key for this table.
Each row of this table contains the ID and the name of one country.
Table: Weather


+---------------+---------+
| Column Name   | Type    |
+---------------+---------+
| country_id    | int     |
| weather_state | varchar |
| day           | date    |
+---------------+---------+
(country_id, day) is the primary key for this table.
Each row of this table indicates the weather state in a country for one day.
Write an SQL query to find the type of weather in each country for November 2019.

The type of weather is Cold if the average weather_state is less than or equal 15, Hot if the average weather_state is greater than or equal 25 and Warm otherwise.

Return result table in any order.

The query result format is in the following example:


Countries table:
+------------+--------------+
| country_id | country_name |
+------------+--------------+
| 2          | USA          |
| 3          | Australia    |
| 7          | Peru         |
| 5          | China        |
| 8          | Morocco      |
| 9          | Spain        |
+------------+--------------+
Weather table:
+------------+---------------+------------+
| country_id | weather_state | day        |
+------------+---------------+------------+
| 2          | 15            | 2019-11-01 |
| 2          | 12            | 2019-10-28 |
| 2          | 12            | 2019-10-27 |
| 3          | -2            | 2019-11-10 |
| 3          | 0             | 2019-11-11 |
| 3          | 3             | 2019-11-12 |
| 5          | 16            | 2019-11-07 |
| 5          | 18            | 2019-11-09 |
| 5          | 21            | 2019-11-23 |
| 7          | 25            | 2019-11-28 |
| 7          | 22            | 2019-12-01 |
| 7          | 20            | 2019-12-02 |
| 8          | 25            | 2019-11-05 |
| 8          | 27            | 2019-11-15 |
| 8          | 31            | 2019-11-25 |
| 9          | 7             | 2019-10-23 |
| 9          | 3             | 2019-12-23 |
+------------+---------------+------------+
Result table:
+--------------+--------------+
| country_name | weather_type |
+--------------+--------------+
| USA          | Cold         |
| Austraila    | Cold         |
| Peru         | Hot          |
| China        | Warm         |
| Morocco      | Hot          |
+--------------+--------------+
Average weather_state in USA in November is (15) / 1 = 15 so weather type is Cold.
Average weather_state in Austraila in November is (-2 + 0 + 3) / 3 = 0.333 so weather type is Cold.
Average weather_state in Peru in November is (25) / 1 = 25 so weather type is Hot.
Average weather_state in China in November is (16 + 18 + 21) / 3 = 18.333 so weather type is Warm.
Average weather_state in Morocco in November is (25 + 27 + 31) / 3 = 27.667 so weather type is Hot.
We know nothing about average weather_state in Spain in November so we don't include it in the result table.
Solution
sql

SELECT country_name, CASE WHEN AVG(weather_state) <= 15 THEN "Cold"
                          WHEN AVG(weather_state) >= 25 THEN "Hot"
                          ELSE "Warm" END AS weather_type
FROM Countries INNER JOIN Weather
ON Countries.country_id = Weather.country_id
WHERE MONTH(day) = 11
GORUP BY country_name
1303. Find the Team Size | Easy | 🔒 LeetCode
Table: Employee


+---------------+---------+
| Column Name   | Type    |
+---------------+---------+
| employee_id   | int     |
| team_id       | int     |
+---------------+---------+
employee_id is the primary key for this table.
Each row of this table contains the ID of each employee and their respective team.
Write an SQL query to find the team size of each of the employees.

Return result table in any order.

The query result format is in the following example:


Employee Table:
+-------------+------------+
| employee_id | team_id    |
+-------------+------------+
|     1       |     8      |
|     2       |     8      |
|     3       |     8      |
|     4       |     7      |
|     5       |     9      |
|     6       |     9      |
+-------------+------------+
Result table:
+-------------+------------+
| employee_id | team_size  |
+-------------+------------+
|     1       |     3      |
|     2       |     3      |
|     3       |     3      |
|     4       |     1      |
|     5       |     2      |
|     6       |     2      |
+-------------+------------+
Employees with Id 1,2,3 are part of a team with team_id = 8.
Employees with Id 4 is part of a team with team_id = 7.
Employees with Id 5,6 are part of a team with team_id = 9.
Solution
sql

SELECT employee_id, b.team_size
FROM employee e
JOIN
(
SELECT team_id, count(team_id) AS team_size
FROM employee
GROUP BY team_id) b
ON e.team_id = b.team_id
1308. Running Total for Different Genders | Medium | 🔒 LeetCode
Table: Scores


+---------------+---------+
| Column Name   | Type    |
+---------------+---------+
| player_name   | varchar |
| gender        | varchar |
| day           | date    |
| score_points  | int     |
+---------------+---------+
(gender, day) is the primary key for this table.
A competition is held between females team and males team.
Each row of this table indicates that a player_name and with gender has scored score_point in someday.
Gender is 'F' if the player is in females team and 'M' if the player is in males team.
Write an SQL query to find the total score for each gender at each day.

Order the result table by gender and day

The query result format is in the following example:


Scores table:
+-------------+--------+------------+--------------+
| player_name | gender | day        | score_points |
+-------------+--------+------------+--------------+
| Aron        | F      | 2020-01-01 | 17           |
| Alice       | F      | 2020-01-07 | 23           |
| Bajrang     | M      | 2020-01-07 | 7            |
| Khali       | M      | 2019-12-25 | 11           |
| Slaman      | M      | 2019-12-30 | 13           |
| Joe         | M      | 2019-12-31 | 3            |
| Jose        | M      | 2019-12-18 | 2            |
| Priya       | F      | 2019-12-31 | 23           |
| Priyanka    | F      | 2019-12-30 | 17           |
+-------------+--------+------------+--------------+
Result table:
+--------+------------+-------+
| gender | day        | total |
+--------+------------+-------+
| F      | 2019-12-30 | 17    |
| F      | 2019-12-31 | 40    |
| F      | 2020-01-01 | 57    |
| F      | 2020-01-07 | 80    |
| M      | 2019-12-18 | 2     |
| M      | 2019-12-25 | 13    |
| M      | 2019-12-30 | 26    |
| M      | 2019-12-31 | 29    |
| M      | 2020-01-07 | 36    |
+--------+------------+-------+
For females team:
First day is 2019-12-30, Priyanka scored 17 points and the total score for the team is 17.
Second day is 2019-12-31, Priya scored 23 points and the total score for the team is 40.
Third day is 2020-01-01, Aron scored 17 points and the total score for the team is 57.
Fourth day is 2020-01-07, Alice scored 23 points and the total score for the team is 80.
For males team:
First day is 2019-12-18, Jose scored 2 points and the total score for the team is 2.
Second day is 2019-12-25, Khali scored 11 points and the total score for the team is 13.
Third day is 2019-12-30, Slaman scored 13 points and the total score for the team is 26.
Fourth day is 2019-12-31, Joe scored 3 points and the total score for the team is 29.
Fifth day is 2020-01-07, Bajrang scored 7 points and the total score for the team is 36.
Solution
sql

#Solution 1:
SELECT gender, day,
SUM(score_points) OVER(PARTITION BY gender ORDER BY day) AS total
FROM scores
GROUP BY 1,2
ORDER BY 1,2

#Solution 2:
SELECT t1.gender, t1.day, SUM(t2.score_points) AS total
FROM Scores AS t1 JOIN Scores AS t2
ON t1.gender = t2.gender
AND t1.day>=t2.day
GROUP BY t1.gender, t1.day
1321. Restaurant Growth | Medium | 🔒 LeetCode
Table: Customer


+---------------+---------+
| Column Name   | Type    |
+---------------+---------+
| customer_id   | int     |
| name          | varchar |
| visited_on    | date    |
| amount        | int     |
+---------------+---------+
(customer_id, visited_on) is the primary key for this table.
This table contains data about customer transactions in a restaurant.
visited_on is the date on which the customer with ID (customer_id) have visited the restaurant.
amount is the total paid by a customer.
You are the restaurant owner and you want to analyze a possible expansion (there will be at least one customer every day).

Write an SQL query to compute moving average of how much customer paid in a 7 days window (current day + 6 days before) .

The query result format is in the following example:

Return result table ordered by visited_on.

average_amount should be rounded to 2 decimal places, all dates are in the format ('YYYY-MM-DD').


Customer table:
+-------------+--------------+--------------+-------------+
| customer_id | name         | visited_on   | amount      |
+-------------+--------------+--------------+-------------+
| 1           | Jhon         | 2019-01-01   | 100         |
| 2           | Daniel       | 2019-01-02   | 110         |
| 3           | Jade         | 2019-01-03   | 120         |
| 4           | Khaled       | 2019-01-04   | 130         |
| 5           | Winston      | 2019-01-05   | 110         |
| 6           | Elvis        | 2019-01-06   | 140         |
| 7           | Anna         | 2019-01-07   | 150         |
| 8           | Maria        | 2019-01-08   | 80          |
| 9           | Jaze         | 2019-01-09   | 110         |
| 1           | Jhon         | 2019-01-10   | 130         |
| 3           | Jade         | 2019-01-10   | 150         |
+-------------+--------------+--------------+-------------+

Result table:
+--------------+--------------+----------------+
| visited_on   | amount       | average_amount |
+--------------+--------------+----------------+
| 2019-01-07   | 860          | 122.86         |
| 2019-01-08   | 840          | 120            |
| 2019-01-09   | 840          | 120            |
| 2019-01-10   | 1000         | 142.86         |
+--------------+--------------+----------------+

1st moving average from 2019-01-01 to 2019-01-07 has an average_amount of (100 + 110 + 120 + 130 + 110 + 140 + 150)/7 = 122.86
2nd moving average from 2019-01-02 to 2019-01-08 has an average_amount of (110 + 120 + 130 + 110 + 140 + 150 + 80)/7 = 120
3rd moving average from 2019-01-03 to 2019-01-09 has an average_amount of (120 + 130 + 110 + 140 + 150 + 80 + 110)/7 = 120
4th moving average from 2019-01-04 to 2019-01-10 has an average_amount of (130 + 110 + 140 + 150 + 80 + 110 + 130 + 150)/7 = 142.86
Solution
sql

#Solution 1:
SELECT visited_on, SUM(amount) OVER(ORDER BY visited_on ROWS 6 PRECEDING),
round(avg(amount) OVER(ORDER BY visited_on ROWS 6 PRECEDING),2)
FROM
(
	SELECT visited_on, SUM(amount) AS amount
	FROM customer
	GROUP BY visited_on
	ORDER BY visited_on
) a
ORDER BY visited_on offset 6 ROWS

#Solution 2:
SELECT t1.visited_on,
    SUM(t2.amount) AS amount,
    round(avg(t2.amount), 2) AS average_amount
FROM (
    SELECT visited_on, SUM(amount) AS amount
    FROM Customer
    GROUP BY visited_on) AS t1
    inner join
     (
    SELECT visited_on, SUM(amount) AS amount
    FROM Customer
    GROUP BY visited_on) AS t2
ON t2.visited_on BETWEEN DATE_SUB(t1.visited_on, INTERVAL 6 DAY) and t1.visited_on
GROUP BY t1.visited_on
HAVING COUNT(1)=7
1322. Ads Performance | Easy | 🔒 LeetCode
Table: Ads


+---------------+---------+
| Column Name   | Type    |
+---------------+---------+
| ad_id         | int     |
| user_id       | int     |
| action        | enum    |
+---------------+---------+
(ad_id, user_id) is the primary key for this table.
Each row of this table contains the ID of an Ad, the ID of a user and the action taken by this user regarding this Ad.
The action column is an ENUM type of ('Clicked', 'Viewed', 'Ignored').
A company is running Ads and wants to calculate the performance of each Ad.


Round ctr to 2 decimal points. Order the result table by ctr in descending order and by ad_id in ascending order in case of a tie.

The query result format is in the following example:


Ads table:
+-------+---------+---------+
| ad_id | user_id | action  |
+-------+---------+---------+
| 1     | 1       | Clicked |
| 2     | 2       | Clicked |
| 3     | 3       | Viewed  |
| 5     | 5       | Ignored |
| 1     | 7       | Ignored |
| 2     | 7       | Viewed  |
| 3     | 5       | Clicked |
| 1     | 4       | Viewed  |
| 2     | 11      | Viewed  |
| 1     | 2       | Clicked |
+-------+---------+---------+
Result table:
+-------+-------+
| ad_id | ctr   |
+-------+-------+
| 1     | 66.67 |
| 3     | 50.00 |
| 2     | 33.33 |
| 5     | 0.00  |
+-------+-------+
for ad_id = 1, ctr = (2/(2+1)) * 100 = 66.67
for ad_id = 2, ctr = (1/(1+2)) * 100 = 33.33
for ad_id = 3, ctr = (1/(1+1)) * 100 = 50.00
for ad_id = 5, ctr = 0.00, Note that ad_id = 5 has no clicks or views.
Note that we don't care about Ignored Ads.
Result table is ordered by the ctr. in case of a tie we order them by ad_id
Solution
sql

#Solution 1:
SELECT ad_id,
    (CASE WHEN clicks+views = 0 THEN 0 ELSE ROUND(clicks/(clicks+views)*100, 2) END) AS ctr
FROM
    (SELECT ad_id,
        SUM(CASE WHEN action='Clicked' THEN 1 ELSE 0 END) AS clicks,
        SUM(CASE WHEN action='Viewed' THEN 1 ELSE 0 END) AS views
    FROM Ads
    GROUP BY ad_id) AS t
ORDER BY ctr DESC, ad_id ASC

#Solution 2:
WITH t1 AS(
SELECT ad_id, SUM(CASE WHEN action in ('Clicked') THEN 1 ELSE 0 END) AS clicked
FROM ads
GROUP BY ad_id
)

, t2 AS
(
SELECT ad_id AS ad, SUM(CASE WHEN action in ('Clicked','Viewed') THEN 1 ELSE 0 END) AS total
FROM ads
GROUP BY ad_id
)

SELECT a.ad_id, coalesce(round((clicked +0.0)/nullif((total +0.0),0)*100,2),0) AS ctr
FROM
(
select *
FROM t1 JOIN t2
ON t1.ad_id = t2.ad) a
ORDER BY ctr DESC, ad_id
1327. List the Products Ordered in a Period | Easy | 🔒 LeetCode
Table: Products


+------------------+---------+
| Column Name      | Type    |
+------------------+---------+
| product_id       | int     |
| product_name     | varchar |
| product_category | varchar |
+------------------+---------+
product_id is the primary key for this table.
This table contains data about the company's products.
Table: Orders


+---------------+---------+
| Column Name   | Type    |
+---------------+---------+
| product_id    | int     |
| order_date    | date    |
| unit          | int     |
+---------------+---------+
There is no primary key for this table. It may have duplicate rows.
product_id is a foreign key to Products table.
unit is the number of products ordered in order_date.
Write an SQL query to get the names of products with greater than or equal to 100 units ordered in February 2020 and their amount.

Return result table in any order.

The query result format is in the following example:


Products table:
+-------------+-----------------------+------------------+
| product_id  | product_name          | product_category |
+-------------+-----------------------+------------------+
| 1           | Leetcode Solutions    | Book             |
| 2           | Jewels of Stringology | Book             |
| 3           | HP                    | Laptop           |
| 4           | Lenovo                | Laptop           |
| 5           | Leetcode Kit          | T-shirt          |
+-------------+-----------------------+------------------+

Orders table:
+--------------+--------------+----------+
| product_id   | order_date   | unit     |
+--------------+--------------+----------+
| 1            | 2020-02-05   | 60       |
| 1            | 2020-02-10   | 70       |
| 2            | 2020-01-18   | 30       |
| 2            | 2020-02-11   | 80       |
| 3            | 2020-02-17   | 2        |
| 3            | 2020-02-24   | 3        |
| 4            | 2020-03-01   | 20       |
| 4            | 2020-03-04   | 30       |
| 4            | 2020-03-04   | 60       |
| 5            | 2020-02-25   | 50       |
| 5            | 2020-02-27   | 50       |
| 5            | 2020-03-01   | 50       |
+--------------+--------------+----------+

Result table:
+--------------------+---------+
| product_name       | unit    |
+--------------------+---------+
| Leetcode Solutions | 130     |
| Leetcode Kit       | 100     |
+--------------------+---------+

Products with product_id = 1 is ordered in February a total of (60 + 70) = 130.
Products with product_id = 2 is ordered in February a total of 80.
Products with product_id = 3 is ordered in February a total of (2 + 3) = 5.
Products with product_id = 4 was not ordered in February 2020.
Products with product_id = 5 is ordered in February a total of (50 + 50) = 100.
Solution
sql

#Solution 1:
SELECT a.product_name, a.unit
FROM
(SELECT p.product_name, SUM(unit) AS unit
FROM orders o
JOIN products p
ON o.product_id = p.product_id
WHERE MONTH(order_date)=2 and YEAR(order_date) = 2020
GROUP BY o.product_id) a
WHERE a.unit>=100

#Solution 2:
SELECT product_name, SUM(unit) AS unit
FROM Products JOIN Orders
ON Products.product_id = Orders.product_id
WHERE left(order_date, 7) = "2020-02"
GROUP BY Products.product_id
HAVING SUM(unit)>=100
1336. Number of Transactions per Visit | Hard | 🔒 LeetCode
Table: Visits


+---------------+---------+
| Column Name   | Type    |
+---------------+---------+
| user_id       | int     |
| visit_date    | date    |
+---------------+---------+
(user_id, visit_date) is the primary key for this table.
Each row of this table indicates that user_id has visited the bank in visit_date.
Table: Transactions


+------------------+---------+
| Column Name      | Type    |
+------------------+---------+
| user_id          | int     |
| transaction_date | date    |
| amount           | int     |
+------------------+---------+
There is no primary key for this table, it may contain duplicates.
Each row of this table indicates that user_id has done a transaction of amount in transaction_date.
It is guaranteed that the user has visited the bank in the transaction_date.(i.e The Visits table contains (user_id, transaction_date) in one row)
A bank wants to draw a chart of the number of transactions bank visitors did in one visit to the bank and the corresponding number of visitors who have done this number of transaction in one visit.

Write an SQL query to find how many users visited the bank and didn't do any transactions, how many visited the bank and did one transaction and so on.

The result table will contain two columns:

transactions_count which is the number of transactions done in one visit.
visits_count which is the corresponding number of users who did transactions_count in one visit to the bank.
transactions_count should take all values from 0 to max(transactions_count) done by one or more users.
Order the result table by transactions_count.

The query result format is in the following example:


Visits table:
+---------+------------+
| user_id | visit_date |
+---------+------------+
| 1       | 2020-01-01 |
| 2       | 2020-01-02 |
| 12      | 2020-01-01 |
| 19      | 2020-01-03 |
| 1       | 2020-01-02 |
| 2       | 2020-01-03 |
| 1       | 2020-01-04 |
| 7       | 2020-01-11 |
| 9       | 2020-01-25 |
| 8       | 2020-01-28 |
+---------+------------+
Transactions table:
+---------+------------------+--------+
| user_id | transaction_date | amount |
+---------+------------------+--------+
| 1       | 2020-01-02       | 120    |
| 2       | 2020-01-03       | 22     |
| 7       | 2020-01-11       | 232    |
| 1       | 2020-01-04       | 7      |
| 9       | 2020-01-25       | 33     |
| 9       | 2020-01-25       | 66     |
| 8       | 2020-01-28       | 1      |
| 9       | 2020-01-25       | 99     |
+---------+------------------+--------+
Result table:
+--------------------+--------------+
| transactions_count | visits_count |
+--------------------+--------------+
| 0                  | 4            |
| 1                  | 5            |
| 2                  | 0            |
| 3                  | 1            |
+--------------------+--------------+
* For transactions_count = 0, The visits (1, "2020-01-01"), (2, "2020-01-02"), (12, "2020-01-01") and (19, "2020-01-03") did no transactions so visits_count = 4.
* For transactions_count = 1, The visits (2, "2020-01-03"), (7, "2020-01-11"), (8, "2020-01-28"), (1, "2020-01-02") and (1, "2020-01-04") did one transaction so visits_count = 5.
* For transactions_count = 2, No customers visited the bank and did two transactions so visits_count = 0.
* For transactions_count = 3, The visit (9, "2020-01-25") did three transactions so visits_count = 1.
* For transactions_count >= 4, No customers visited the bank and did more than three transactions so we will stop at transactions_count = 3
The chart drawn for this example is as follows:


Solution
sql

WITH RECURSIVE t1 AS(
                    SELECT visit_date,
                           COALESCE(num_visits,0) as num_visits,
                           COALESCE(num_trans,0) as num_trans
                    FROM ((
                          SELECT visit_date, user_id, COUNT(*) as num_visits
                          FROM visits
                          GROUP BY 1, 2) AS a
                         LEFT JOIN
                          (
                           SELECT transaction_date,
                                 user_id,
                                 count(*) as num_trans
                            FROM transactions
                          GROUP BY 1, 2) AS b
                         ON a.visit_date = b.transaction_date and a.user_id = b.user_id)
                      ),

              t2 AS (
                      SELECT MAX(num_trans) as trans
                        FROM t1
                      UNION ALL
                      SELECT trans-1
                        FROM t2
                      WHERE trans >= 1)

SELECT trans as transactions_count,
       COALESCE(visits_count,0) as visits_count
  FROM t2 LEFT JOIN (
                    SELECT num_trans as transactions_count, COALESCE(COUNT(*),0) as visits_count
                    FROM t1
                    GROUP BY 1
                    ORDER BY 1) AS a
ON a.transactions_count = t2.trans
ORDER BY 1
1341. Movie Rating | Medium | 🔒 LeetCode
Table: Movies


+---------------+---------+
| Column Name   | Type    |
+---------------+---------+
| movie_id      | int     |
| title         | varchar |
+---------------+---------+
movie_id is the primary key for this table.
title is the name of the movie.
Table: Users


+---------------+---------+
| Column Name   | Type    |
+---------------+---------+
| user_id       | int     |
| name          | varchar |
+---------------+---------+
user_id is the primary key for this table.
Table: Movie_Rating


+---------------+---------+
| Column Name   | Type    |
+---------------+---------+
| movie_id      | int     |
| user_id       | int     |
| rating        | int     |
| created_at    | date    |
+---------------+---------+
(movie_id, user_id) is the primary key for this table.
This table contains the rating of a movie by a user in their review.
created_at is the user's review date.
Write the following SQL query:

Find the name of the user who has rated the greatest number of the movies.
In case of a tie, return lexicographically smaller user name.

Find the movie name with the highest average rating in February 2020.
In case of a tie, return lexicographically smaller movie name..

Query is returned in 2 rows, the query result format is in the following example:


Movies table:
+-------------+--------------+
| movie_id    |  title       |
+-------------+--------------+
| 1           | Avengers     |
| 2           | Frozen 2     |
| 3           | Joker        |
+-------------+--------------+

Users table:
+-------------+--------------+
| user_id     |  name        |
+-------------+--------------+
| 1           | Daniel       |
| 2           | Monica       |
| 3           | Maria        |
| 4           | James        |
+-------------+--------------+

Movie_Rating table:
+-------------+--------------+--------------+-------------+
| movie_id    | user_id      | rating       | created_at  |
+-------------+--------------+--------------+-------------+
| 1           | 1            | 3            | 2020-01-12  |
| 1           | 2            | 4            | 2020-02-11  |
| 1           | 3            | 2            | 2020-02-12  |
| 1           | 4            | 1            | 2020-01-01  |
| 2           | 1            | 5            | 2020-02-17  |
| 2           | 2            | 2            | 2020-02-01  |
| 2           | 3            | 2            | 2020-03-01  |
| 3           | 1            | 3            | 2020-02-22  |
| 3           | 2            | 4            | 2020-02-25  |
+-------------+--------------+--------------+-------------+

Result table:
+--------------+
| results      |
+--------------+
| Daniel       |
| Frozen 2     |
+--------------+

Daniel and Maria have rated 3 movies ("Avengers", "Frozen 2" and "Joker") but Daniel is smaller lexicographically.
Frozen 2 and Joker have a rating average of 3.5 in February but Frozen 2 is smaller lexicographically.
Solution
sql

#Solution 1:
(SELECT name AS results
FROM Movie_Rating JOIN Users
ON Movie_Rating.user_id = Users.user_id
GROUP BY Movie_Rating.user_id
ORDER BY count(1) DESC, name
LIMIT 1)
UNION ALL
(SELECT title AS results
FROM Movie_Rating JOIN Movies
ON Movie_Rating.movie_id = Movies.movie_id
WHERE left(created_at, 7) = "2020-02"
GROUP BY Movie_Rating.movie_id
ORDER BY avg(rating) DESC, title
LIMIT 1
)

#Solution 2:
SELECT name AS results
FROM(
(SELECT a.name
FROM(
SELECT name, count(*),
rank() OVER(ORDER BY count(*) DESC) AS rk
FROM movie_rating m
JOIN users u
ON m.user_id = u.user_id
GROUP BY name, m.user_id
ORDER BY rk, name) a
LIMIT 1)
UNION
(SELECT title
FROM(
SELECT title, round(avg(rating),1) AS rnd
FROM movie_rating m
JOIN movies u
on m.movie_id = u.movie_id
WHERE month(created_at) = 2
GROUP BY title
ORDER BY rnd DESC, title) b
LIMIT 1)) AS d
1350. Students With Invalid Departments | Easy | 🔒 LeetCode
Table: Departments


+---------------+---------+
| Column Name   | Type    |
+---------------+---------+
| id            | int     |
| name          | varchar |
+---------------+---------+
id is the primary key of this table.
The table has information about the id of each department of a university.
Table: Students


+---------------+---------+
| Column Name   | Type    |
+---------------+---------+
| id            | int     |
| name          | varchar |
| department_id | int     |
+---------------+---------+
id is the primary key of this table.
The table has information about the id of each student at a university and the id of the department he/she studies at.
Write an SQL query to find the id and the name of all students who are enrolled in departments that no longer exists.

Return the result table in any order.

The query result format is in the following example:


Departments table:
+------+--------------------------+
| id   | name                     |
+------+--------------------------+
| 1    | Electrical Engineering   |
| 7    | Computer Engineering     |
| 13   | Bussiness Administration |
+------+--------------------------+

Students table:
+------+----------+---------------+
| id   | name     | department_id |
+------+----------+---------------+
| 23   | Alice    | 1             |
| 1    | Bob      | 7             |
| 5    | Jennifer | 13            |
| 2    | John     | 14            |
| 4    | Jasmine  | 77            |
| 3    | Steve    | 74            |
| 6    | Luis     | 1             |
| 8    | Jonathan | 7             |
| 7    | Daiana   | 33            |
| 11   | Madelynn | 1             |
+------+----------+---------------+

Result table:
+------+----------+
| id   | name     |
+------+----------+
| 2    | John     |
| 7    | Daiana   |
| 4    | Jasmine  |
| 3    | Steve    |
+------+----------+

John, Daiana, Steve and Jasmine are enrolled in departments 14, 33, 74 and 77 respectively. department 14, 33, 74 and 77 doesn't exist in the Departments table.
Solution
sql

#Solution 1:
SELECT s.id, s.name
FROM students s LEFT JOIN
departments d
ON s.department_id = d.id
WHERE d.name IS NULL;

#Solution 2:
SELECT id, name
FROM Students
WHERE department_id NOT IN
    (SELECT id FROM Departments)
1355. Activity Participants | Medium | 🔒 LeetCode
Table: Friends


+---------------+---------+
| Column Name   | Type    |
+---------------+---------+
| id            | int     |
| name          | varchar |
| activity      | varchar |
+---------------+---------+
id is the id of the friend and primary key for this table.
name is the name of the friend.
activity is the name of the activity which the friend takes part in.
Table: Activities


+---------------+---------+
| Column Name   | Type    |
+---------------+---------+
| id            | int     |
| name          | varchar |
+---------------+---------+
id is the primary key for this table.
name is the name of the activity.
Write an SQL query to find the names of all the activities with neither maximum, nor minimum number of participants.

Return the result table in any order. Each activity in table Activities is performed by any person in the table Friends.

The query result format is in the following example:


Friends table:
+------+--------------+---------------+
| id   | name         | activity      |
+------+--------------+---------------+
| 1    | Jonathan D.  | Eating        |
| 2    | Jade W.      | Singing       |
| 3    | Victor J.    | Singing       |
| 4    | Elvis Q.     | Eating        |
| 5    | Daniel A.    | Eating        |
| 6    | Bob B.       | Horse Riding  |
+------+--------------+---------------+

Activities table:
+------+--------------+
| id   | name         |
+------+--------------+
| 1    | Eating       |
| 2    | Singing      |
| 3    | Horse Riding |
+------+--------------+

Result table:
+--------------+
| results      |
+--------------+
| Singing      |
+--------------+

Eating activity is performed by 3 friends, maximum number of participants, (Jonathan D. , Elvis Q. and Daniel A.)
Horse Riding activity is performed by 1 friend, minimum number of participants, (Bob B.)
Singing is performed by 2 friends (Victor J. and Jade W.)
Solution
sql

#Solution 1:
WITH CTE AS
(SELECT COUNT(*) AS cnt, activity FROM Friends GROUP BY activity)

SELECT activity FROM CTE
WHERE cnt NOT IN
    (SELECT MAX(cnt) FROM CTE
    UNION ALL
    SELECT MIN(cnt) FROM CTE)

#Solution 2:
WITH t1 AS(
SELECT MAX(a.total) AS total
FROM(
    SELECT activity, COUNT(*) AS total
    FROM friends
    GROUP BY activity) a
	UNION ALL
	SELECT MIN(b.total) AS low
    FROM(
    SELECT activity, COUNT(*) AS total
    FROM friends
    GROUP BY activity) b),
t2 AS
(
    SELECT activity, COUNT(*) AS total
    FROM friends
    GROUP BY activity
)

SELECT activity
FROM t1 RIGHT JOIN t2
ON t1.total = t2.total
WHERE t1.total is null
1364. Number of Trusted Contacts of a Customer | Medium | 🔒 LeetCode
Table: Customers


+---------------+---------+
| Column Name   | Type    |
+---------------+---------+
| customer_id   | int     |
| customer_name | varchar |
| email         | varchar |
+---------------+---------+
customer_id is the primary key for this table.
Each row of this table contains the name and the email of a customer of an online shop.
Table: Contacts


+---------------+---------+
| Column Name   | Type    |
+---------------+---------+
| user_id       | id      |
| contact_name  | varchar |
| contact_email | varchar |
+---------------+---------+
(user_id, contact_email) is the primary key for this table.
Each row of this table contains the name and email of one contact of customer with user_id.
This table contains information about people each customer trust. The contact may or may not exist in the Customers table.
Table: Invoices


+--------------+---------+
| Column Name  | Type    |
+--------------+---------+
| invoice_id   | int     |
| price        | int     |
| user_id      | int     |
+--------------+---------+
invoice_id is the primary key for this table.
Each row of this table indicates that user_id has an invoice with invoice_id and a price.
Write an SQL query to find the following for each invoice_id:

customer_name: The name of the customer the invoice is related to.
price: The price of the invoice.
contacts_cnt: The number of contacts related to the customer.
trusted_contacts_cnt: The number of contacts related to the customer and at the same time they are customers to the shop. (i.e His/Her email exists in the Customers table.) Order the result table by invoice_id.
The query result format is in the following example:


Customers table:
+-------------+---------------+--------------------+
| customer_id | customer_name | email              |
+-------------+---------------+--------------------+
| 1           | Alice         | alice@leetcode.com |
| 2           | Bob           | bob@leetcode.com   |
| 13          | John          | john@leetcode.com  |
| 6           | Alex          | alex@leetcode.com  |
+-------------+---------------+--------------------+
Contacts table:
+-------------+--------------+--------------------+
| user_id     | contact_name | contact_email      |
+-------------+--------------+--------------------+
| 1           | Bob          | bob@leetcode.com   |
| 1           | John         | john@leetcode.com  |
| 1           | Jal          | jal@leetcode.com   |
| 2           | Omar         | omar@leetcode.com  |
| 2           | Meir         | meir@leetcode.com  |
| 6           | Alice        | alice@leetcode.com |
+-------------+--------------+--------------------+
Invoices table:
+------------+-------+---------+
| invoice_id | price | user_id |
+------------+-------+---------+
| 77         | 100   | 1       |
| 88         | 200   | 1       |
| 99         | 300   | 2       |
| 66         | 400   | 2       |
| 55         | 500   | 13      |
| 44         | 60    | 6       |
+------------+-------+---------+
Result table:
+------------+---------------+-------+--------------+----------------------+
| invoice_id | customer_name | price | contacts_cnt | trusted_contacts_cnt |
+------------+---------------+-------+--------------+----------------------+
| 44         | Alex          | 60    | 1            | 1                    |
| 55         | John          | 500   | 0            | 0                    |
| 66         | Bob           | 400   | 2            | 0                    |
| 77         | Alice         | 100   | 3            | 2                    |
| 88         | Alice         | 200   | 3            | 2                    |
| 99         | Bob           | 300   | 2            | 0                    |
+------------+---------------+-------+--------------+----------------------+
Alice has three contacts, two of them are trusted contacts (Bob and John).
Bob has two contacts, none of them is a trusted contact.
Alex has one contact and it is a trusted contact (Alice).
John doesn't have any contacts.
Solution
sql

SELECT invoice_id, customer_name, price,
 COUNT(Contacts.user_id) AS contacts_cnt,
 SUM(CASE WHEN Contacts.contact_name IN
    (SELECT customer_name FROM Customers)
    THEN 1 ELSE 0 END) AS trusted_contacts_cnt
FROM Invoices INNER JOIN Customers ON Invoices.user_id = Customers.customer_id
LEFT JOIN Contacts ON Customers.customer_id = Contacts.user_id
GROUP BY Invoices.invoice_id, customer_name
ORDER BY Invoices.invoice_id
1369. Get the Second Most Recent Activity | Hard | 🔒 LeetCode
Table: UserActivity


+---------------+---------+
| Column Name   | Type    |
+---------------+---------+
| username      | varchar |
| activity      | varchar |
| startDate     | Date    |
| endDate       | Date    |
+---------------+---------+
This table does not contain primary key.
This table contain information about the activity performed of each user in a period of time.
A person with username performed a activity from startDate to endDate.
Write an SQL query to show the second most recent activity of each user.

If the user only has one activity, return that one.

A user can't perform more than one activity at the same time. Return the result table in any order.

The query result format is in the following example:


UserActivity table:
+------------+--------------+-------------+-------------+
| username   | activity     | startDate   | endDate     |
+------------+--------------+-------------+-------------+
| Alice      | Travel       | 2020-02-12  | 2020-02-20  |
| Alice      | Dancing      | 2020-02-21  | 2020-02-23  |
| Alice      | Travel       | 2020-02-24  | 2020-02-28  |
| Bob        | Travel       | 2020-02-11  | 2020-02-18  |
+------------+--------------+-------------+-------------+

Result table:
+------------+--------------+-------------+-------------+
| username   | activity     | startDate   | endDate     |
+------------+--------------+-------------+-------------+
| Alice      | Dancing      | 2020-02-21  | 2020-02-23  |
| Bob        | Travel       | 2020-02-11  | 2020-02-18  |
+------------+--------------+-------------+-------------+

The most recent activity of Alice is Travel from 2020-02-24 to 2020-02-28, before that she was dancing from 2020-02-21 to 2020-02-23.
Bob only has one record, we just take that one.
Solution
sql

(SELECT *
FROM UserActivity
GROUP BY username
HAVING count(1) = 1)
UNION
(SELECT a.*
FROM UserActivity AS a LEFT JOIN UserActivity AS b
on a.username = b.username AND a.endDate<b.endDate
GROUP BY a.username, a.endDate
HAVING count(b.endDate) = 1)
1378. Replace Employee ID With The Unique Identifier | Easy | 🔒 LeetCode
Table: Employees


+---------------+---------+
| Column Name   | Type    |
+---------------+---------+
| id            | int     |
| name          | varchar |
+---------------+---------+
id is the primary key for this table.
Each row of this table contains the id and the name of an employee in a company.
Table: EmployeeUNI


+---------------+---------+
| Column Name   | Type    |
+---------------+---------+
| id            | int     |
| unique_id     | int     |
+---------------+---------+
(id, unique_id) is the primary key for this table.
Each row of this table contains the id and the corresponding unique id of an employee in the company.
Write an SQL query to show the unique ID of each user, If a user doesn't have a unique ID replace just show null.

Return the result table in any order.

The query result format is in the following example:


Employees table:
+----+----------+
| id | name     |
+----+----------+
| 1  | Alice    |
| 7  | Bob      |
| 11 | Meir     |
| 90 | Winston  |
| 3  | Jonathan |
+----+----------+

EmployeeUNI table:
+----+-----------+
| id | unique_id |
+----+-----------+
| 3  | 1         |
| 11 | 2         |
| 90 | 3         |
+----+-----------+

EmployeeUNI table:
+-----------+----------+
| unique_id | name     |
+-----------+----------+
| null      | Alice    |
| null      | Bob      |
| 2         | Meir     |
| 3         | Winston  |
| 1         | Jonathan |
+-----------+----------+

Alice and Bob don't have a unique ID, We will show null instead.
The unique ID of Meir is 2.
The unique ID of Winston is 3.
The unique ID of Jonathan is 1.
Solution
sql

SELECT unique_id, name
FROM Employees
LEFT JOIN EmployeeUNI
ON Employees.id = EmployeeUNI.id
1384. Total Sales Amount by Year | Hard | 🔒 LeetCode
Table: Product

+---------------+---------+ | Column Name | Type | +---------------+---------+ | product_id | int | | product_name | varchar | +---------------+---------+ product_id is the primary key for this table. product_name is the name of the product.

Table: Sales


+---------------------+---------+
| Column Name         | Type    |
+---------------------+---------+
| product_id          | int     |
| period_start        | varchar |
| period_end          | date    |
| average_daily_sales | int     |
+---------------------+---------+
product_id is the primary key for this table.
period_start and period_end indicates the start and end date for sales period, both dates are inclusive.
The average_daily_sales column holds the average daily sales amount of the items for the period.
Write an SQL query to report the Total sales amount of each item for each year, with corresponding product name, product_id, product_name and report_year.

Dates of the sales years are between 2018 to 2020. Return the result table ordered by product_id and report_year.

The query result format is in the following example:


Product table:
+------------+--------------+
| product_id | product_name |
+------------+--------------+
| 1          | LC Phone     |
| 2          | LC T-Shirt   |
| 3          | LC Keychain  |
+------------+--------------+

Sales table:
+------------+--------------+-------------+---------------------+
| product_id | period_start | period_end  | average_daily_sales |
+------------+--------------+-------------+---------------------+
| 1          | 2019-01-25   | 2019-02-28  | 100                 |
| 2          | 2018-12-01   | 2020-01-01  | 10                  |
| 3          | 2019-12-01   | 2020-01-31  | 1                   |
+------------+--------------+-------------+---------------------+

Result table:
+------------+--------------+-------------+--------------+
| product_id | product_name | report_year | total_amount |
+------------+--------------+-------------+--------------+
| 1          | LC Phone     |    2019     | 3500         |
| 2          | LC T-Shirt   |    2018     | 310          |
| 2          | LC T-Shirt   |    2019     | 3650         |
| 2          | LC T-Shirt   |    2020     | 10           |
| 3          | LC Keychain  |    2019     | 31           |
| 3          | LC Keychain  |    2020     | 31           |
+------------+--------------+-------------+--------------+
LC Phone was sold for the period of 2019-01-25 to 2019-02-28, and there are 35 days for this period. Total amount 35*100 = 3500.
LC T-shirt was sold for the period of 2018-12-01 to 2020-01-01, and there are 31, 365, 1 days for years 2018, 2019 and 2020 respectively.
LC Keychain was sold for the period of 2019-12-01 to 2020-01-31, and there are 31, 31 days for years 2019 and 2020 respectively.
Solution
sql

SELECT
    b.product_id,
    a.product_name,
    a.yr AS report_year,
    CASE
        WHEN YEAR(b.period_start)=YEAR(b.period_end) AND a.yr=YEAR(b.period_start) THEN DATEDIFF(b.period_end,b.period_start)+1
        WHEN a.yr=YEAR(b.period_start) THEN DATEDIFF(DATE_FORMAT(b.period_start,'%Y-12-31'),b.period_start)+1
        WHEN a.yr=YEAR(b.period_end) THEN DAYOFYEAR(b.period_end)
        WHEN a.yr>YEAR(b.period_start) AND a.yr<YEAR(b.period_end) THEN 365
        ELSE 0
    END * average_daily_sales AS total_amount
FROM
    (SELECT product_id,product_name,'2018' AS yr FROM Product
    UNION
    SELECT product_id,product_name,'2019' AS yr FROM Product
    UNION
    SELECT product_id,product_name,'2020' AS yr FROM Product) a
    JOIN
    Sales b
    ON a.product_id=b.product_id
HAVING total_amount > 0
ORDER BY b.product_id,a.yr
1393. Capital Gain/Loss | Medium | 🔒 LeetCode
Table: Stocks


+---------------+---------+
| Column Name   | Type    |
+---------------+---------+
| stock_name    | varchar |
| operation     | enum    |
| operation_day | int     |
| price         | int     |
+---------------+---------+
(stock_name, day) is the primary key for this table.
The operation column is an ENUM of type ('Sell', 'Buy')
Each row of this table indicates that the stock which has stock_name had an operation on the day operation_day with the price.
It is guaranteed that each 'Sell' operation for a stock has a corresponding 'Buy' operation in a previous day.
Write an SQL query to report the Capital gain/loss for each stock.

The capital gain/loss of a stock is total gain or loss after buying and selling the stock one or many times.

Return the result table in any order.

The query result format is in the following example:


Stocks table:
+---------------+-----------+---------------+--------+
| stock_name    | operation | operation_day | price  |
+---------------+-----------+---------------+--------+
| Leetcode      | Buy       | 1             | 1000   |
| Corona Masks  | Buy       | 2             | 10     |
| Leetcode      | Sell      | 5             | 9000   |
| Handbags      | Buy       | 17            | 30000  |
| Corona Masks  | Sell      | 3             | 1010   |
| Corona Masks  | Buy       | 4             | 1000   |
| Corona Masks  | Sell      | 5             | 500    |
| Corona Masks  | Buy       | 6             | 1000   |
| Handbags      | Sell      | 29            | 7000   |
| Corona Masks  | Sell      | 10            | 10000  |
+---------------+-----------+---------------+--------+

Result table:
+---------------+-------------------+
| stock_name    | capital_gain_loss |
+---------------+-------------------+
| Corona Masks  | 9500              |
| Leetcode      | 8000              |
| Handbags      | -23000            |
+---------------+-------------------+
Leetcode stock was bought at day 1 for 1000$ and was sold at day 5 for 9000$. Capital gain = 9000 - 1000 = 8000$.
Handbags stock was bought at day 17 for 30000$ and was sold at day 29 for 7000$. Capital loss = 7000 - 30000 = -23000$.
Corona Masks stock was bought at day 1 for 10$ and was sold at day 3 for 1010$. It was bought again at day 4 for 1000$ and was sold at day 5 for 500$. At last, it was bought at day 6 for 1000$ and was sold at day 10 for 10000$. Capital gain/loss is the sum of capital gains/losses for each ('Buy' --> 'Sell') operation = (1010 - 10) + (500 - 1000) + (10000 - 1000) = 1000 - 500 + 9000 = 9500$.
Solution
sql

#Solution 1:
SELECT stock_name,
       SUM(CASE WHEN operation = 'Buy' THEN -price ELSE price END) AS capital_gain_loss
FROM Stocks
GROUP BY stock_name;

#Solution 2:
SELECT stock_name, (one-two) AS capital_gain_loss
FROM(
(SELECT stock_name, sum(price) AS one
FROM stocks
WHERE operation = 'Sell'
GROUP BY stock_name) b
LEFT JOIN
(SELECT stock_name AS name, sum(price) AS two
FROM stocks
WHERE operation = 'Buy'
GROUP BY stock_name) c
ON b.stock_name = c.name)
ORDER BY capital_gain_loss DESC;
1398. Customers Who Bought Products A and B but Not C | Medium | 🔒 LeetCode
Table: Customers


+---------------------+---------+
| Column Name         | Type    |
+---------------------+---------+
| customer_id         | int     |
| customer_name       | varchar |
+---------------------+---------+
customer_id is the primary key for this table.
customer_name is the name of the customer.
Table: Orders


+---------------+---------+
| Column Name   | Type    |
+---------------+---------+
| order_id      | int     |
| customer_id   | int     |
| product_name  | varchar |
+---------------+---------+
order_id is the primary key for this table.
customer_id is the id of the customer who bought the product "product_name".
Write an SQL query to report the customer_id and customer_name of customers who bought products "A", "B" but did not buy the product "C" since we want to recommend them buy this product.

Return the result table ordered by customer_id.

The query result format is in the following example.


Customers table:
+-------------+---------------+
| customer_id | customer_name |
+-------------+---------------+
| 1           | Daniel        |
| 2           | Diana         |
| 3           | Elizabeth     |
| 4           | Jhon          |
+-------------+---------------+

Orders table:
+------------+--------------+---------------+
| order_id   | customer_id  | product_name  |
+------------+--------------+---------------+
| 10         |     1        |     A         |
| 20         |     1        |     B         |
| 30         |     1        |     D         |
| 40         |     1        |     C         |
| 50         |     2        |     A         |
| 60         |     3        |     A         |
| 70         |     3        |     B         |
| 80         |     3        |     D         |
| 90         |     4        |     C         |
+------------+--------------+---------------+

Result table:
+-------------+---------------+
| customer_id | customer_name |
+-------------+---------------+
| 3           | Elizabeth     |
+-------------+---------------+
Only the customer_id with id 3 bought the product A and B but not the product C.
Solution
sql

#Solution 1:
WITH t1 AS
(
SELECT customer_id
FROM orders
WHERE product_name = 'B' AND
customer_id IN (SELECT customer_id
FROM orders
WHERE product_name = 'A'))

SELECT t1.customer_id, c.customer_name
FROM t1 JOIN customers c
ON t1.customer_id = c.customer_id
WHERE t1.customer_id != all(SELECT customer_id
FROM orders
WHERE product_name = 'C')

#Solution 2:
SELECT *
FROM Customers
WHERE customer_id IN
    (SELECT DISTINCT customer_id
     FROM Orders
     WHERE product_name = 'A'
    ) AND
    customer_id IN
    (SELECT DISTINCT customer_id
     FROM Orders
     WHERE product_name = 'B'
    ) AND
    customer_id NOT IN
    (SELECT DISTINCT customer_id
     FROM Orders
     WHERE product_name = 'C'
    )
ORDER BY customer_id

#Solution 3:
SELECT Customers.*
FROM (
    SELECT customer_id,
     sum(CASE WHEN product_name = 'A' THEN 1 ELSE 0 END) AS product_a,
     sum(CASE WHEN product_name = 'B' THEN 1 ELSE 0 END) AS product_b
    FROM Orders
    GROUP BY customer_id) AS t JOIN Customers
ON t.customer_id = Customers.customer_id
WHERE t.product_a>0 AND product_b >0 AND Customers.customer_id NOT IN (
    SELECT DISTINCT customer_id
    FROM Orders
    WHERE product_name = 'C')
ORDER BY Customers.customer_id
1407. Top Travellers | Easy | 🔒 LeetCode
Table: Users


+---------------+---------+
| Column Name   | Type    |
+---------------+---------+
| id            | int     |
| name          | varchar |
+---------------+---------+
id is the primary key for this table.
name is the name of the user.
Table: Rides


+---------------+---------+
| Column Name   | Type    |
+---------------+---------+
| id            | int     |
| user_id       | int     |
| distance      | int     |
+---------------+---------+
id is the primary key for this table.
city_id is the id of the city who bought the product "product_name".
Write an SQL query to report the distance travelled by each user.

Return the result table ordered by travelled_distance in descending order, if two or more users travelled the same distance, order them by their name in ascending order.

The query result format is in the following example.


Users table:
+------+-----------+
| id   | name      |
+------+-----------+
| 1    | Alice     |
| 2    | Bob       |
| 3    | Alex      |
| 4    | Donald    |
| 7    | Lee       |
| 13   | Jonathan  |
| 19   | Elvis     |
+------+-----------+

Rides table:
+------+----------+----------+
| id   | user_id  | distance |
+------+----------+----------+
| 1    | 1        | 120      |
| 2    | 2        | 317      |
| 3    | 3        | 222      |
| 4    | 7        | 100      |
| 5    | 13       | 312      |
| 6    | 19       | 50       |
| 7    | 7        | 120      |
| 8    | 19       | 400      |
| 9    | 7        | 230      |
+------+----------+----------+

Result table:
+----------+--------------------+
| name     | travelled_distance |
+----------+--------------------+
| Elvis    | 450                |
| Lee      | 450                |
| Bob      | 317                |
| Jonathan | 312                |
| Alex     | 222                |
| Alice    | 120                |
| Donald   | 0                  |
+----------+--------------------+
Elvis and Lee travelled 450 miles, Elvis is the top traveller as his name is alphabetically smaller than Lee.
Bob, Jonathan, Alex and Alice have only one ride and we just order them by the total distances of the ride.
Donald didn't have any rides, the distance travelled by him is 0.
Solution
sql

#Solution 1:
SELECT U.name AS name, COALESCE(SUM(R.distance),0) AS travelled_distance
FROM Users U LEFT JOIN Rides R
ON R.user_id = U.id
GROUP BY name
ORDER BY travelled_distance DESC, name

#Solution 2:
SELECT name, IFNULL(SUM(distance), 0) AS travelled_distance
FROM Users LEFT JOIN Rides
ON Users.id = Rides.user_id
GROUP BY Users.id
ORDER BY travelled_distance DESC, name

#Solution 3:
SELECT name, SUM(IF(ISNULL(distance), 0, distance)) AS travelled_distance
FROM Users LEFT JOIN Rides
ON Users.id = Rides.user_id
GROUP BY Users.id
ORDER BY travelled_distance DESC, name
1412. Find the Quiet Students in All Exams | Hard | 🔒 LeetCode
Table: Student


+---------------------+---------+
| Column Name         | Type    |
+---------------------+---------+
| student_id          | int     |
| student_name        | varchar |
+---------------------+---------+
student_id is the primary key for this table.
student_name is the name of the student.
Table: Exam


+---------------+---------+
| Column Name   | Type    |
+---------------+---------+
| exam_id       | int     |
| student_id    | int     |
| score         | int     |
+---------------+---------+
(exam_id, student_id) is the primary key for this table.
Student with student_id got score points in exam with id exam_id.
A "quite" student is the one who took at least one exam and didn't score neither the high score nor the low score.

Write an SQL query to report the students (student_id, student_name) being "quiet" in ALL exams.

Don't return the student who has never taken any exam. Return the result table ordered by student_id.

The query result format is in the following example.


Student table:
+-------------+---------------+
| student_id  | student_name  |
+-------------+---------------+
| 1           | Daniel        |
| 2           | Jade          |
| 3           | Stella        |
| 4           | Jonathan      |
| 5           | Will          |
+-------------+---------------+

Exam table:
+------------+--------------+-----------+
| exam_id    | student_id   | score     |
+------------+--------------+-----------+
| 10         |     1        |    70     |
| 10         |     2        |    80     |
| 10         |     3        |    90     |
| 20         |     1        |    80     |
| 30         |     1        |    70     |
| 30         |     3        |    80     |
| 30         |     4        |    90     |
| 40         |     1        |    60     |
| 40         |     2        |    70     |
| 40         |     4        |    80     |
+------------+--------------+-----------+

Result table:
+-------------+---------------+
| student_id  | student_name  |
+-------------+---------------+
| 2           | Jade          |
+-------------+---------------+

For exam 1: Student 1 and 3 hold the lowest and high score respectively.
For exam 2: Student 1 hold both highest and lowest score.
For exam 3 and 4: Studnet 1 and 4 hold the lowest and high score respectively.
Student 2 and 5 have never got the highest or lowest in any of the exam.
Since student 5 is not taking any exam, he is excluded from the result.
So, we only return the information of Student 2.
Solution
sql

#Solution 1:
WITH t1 AS(
SELECT student_id
FROM
(SELECT *,
MIN(score) OVER(PARTITION BY exam_id) AS least,
MAX(score) OVER(PARTITION BY exam_id) AS most
FROM exam) a
WHERE least = score OR most = score)

SELECT DISTINCT student_id, student_name
FROM exam JOIN student
USING (student_id)
WHERE student_id != all(SELECT student_id FROM t1)
order by 1

#Solution 2:
SELECT DISTINCT Student.*
FROM Student INNER JOIN Exam
ON Student.student_id = Exam.student_id
WHERE student.student_id NOT IN
    (SELECT e1.student_id
    FROM Exam AS e1 INNER JOIN
        (SELECT exam_id, MIN(score) AS min_score, MAX(score) AS max_score
        FROM Exam
        GROUP BY exam_id) AS e2
    ON e1.exam_id = e2.exam_id
    WHERE e1.score = e2.min_score OR e1.score = e2.max_score)
ORDER BY student_id
1421. NPV Queries | Medium | 🔒 LeetCode
Table: NPV


+---------------+---------+
| Column Name   | Type    |
+---------------+---------+
| id            | int     |
| year          | int     |
| npv           | int     |
+---------------+---------+
(id, year) is the primary key of this table.
The table has information about the id and the year of each inventory and the corresponding net present value.
Table: Queries


+---------------+---------+
| Column Name   | Type    |
+---------------+---------+
| id            | int     |
| year          | int     |
+---------------+---------+
(id, year) is the primary key of this table.
The table has information about the id and the year of each inventory query.
Write an SQL query to find the npv of all each query of queries table.

Return the result table in any order.

The query result format is in the following example:


NPV table:
+------+--------+--------+
| id   | year   | npv    |
+------+--------+--------+
| 1    | 2018   | 100    |
| 7    | 2020   | 30     |
| 13   | 2019   | 40     |
| 1    | 2019   | 113    |
| 2    | 2008   | 121    |
| 3    | 2009   | 12     |
| 11   | 2020   | 99     |
| 7    | 2019   | 0      |
+------+--------+--------+

Queries table:
+------+--------+
| id   | year   |
+------+--------+
| 1    | 2019   |
| 2    | 2008   |
| 3    | 2009   |
| 7    | 2018   |
| 7    | 2019   |
| 7    | 2020   |
| 13   | 2019   |
+------+--------+

Result table:
+------+--------+--------+
| id   | year   | npv    |
+------+--------+--------+
| 1    | 2019   | 113    |
| 2    | 2008   | 121    |
| 3    | 2009   | 12     |
| 7    | 2018   | 0      |
| 7    | 2019   | 0      |
| 7    | 2020   | 30     |
| 13   | 2019   | 40     |
+------+--------+--------+

The npv value of (7, 2018) is not present in the NPV table, we consider it 0.
The npv values of all other queries can be found in the NPV table.
Solution
sql

#Solution 1:
SELECT q.id, q.year, COALESCE(n.npv,0) AS npv
FROM queries q
LEFT JOIN npv n
ON q.id = n.id AND q.year=n.year

#Solution 2:
SELECT Queries.*, IF(ISNULL(npv), 0, npv) AS npv
FROM Queries LEFT JOIN NPV
ON Queries.id = NPV.id AND Queries.year = NPV.year
1435. Create a Session Bar Chart | Easy | 🔒 LeetCode
Table: Sessions


+---------------------+---------+
| Column Name         | Type    |
+---------------------+---------+
| session_id          | int     |
| duration            | int     |
+---------------------+---------+
session_id is the primary key for this table.
duration is the time in seconds that a user has visited the application.
You want to know how long a user visits your application. You decided to create bins of "[0-5>", "[5-10>", "[10-15>" and "15 minutes or more" and count the number of sessions on it.

Write an SQL query to report the (bin, total) in any order.


The query result format is in the following example.

Sessions table:
+-------------+---------------+
| session_id  | duration      |
+-------------+---------------+
| 1           | 30            |
| 2           | 299           |
| 3           | 340           |
| 4           | 580           |
| 5           | 1000          |
+-------------+---------------+

Result table:
+--------------+--------------+
| bin          | total        |
+--------------+--------------+
| [0-5>        | 3            |
| [5-10>       | 1            |
| [10-15>      | 0            |
| 15 or more   | 1            |
+--------------+--------------+

For session_id 1, 2 and 3 have a duration greater or equal than 0 minutes and less than 5 minutes.
For session_id 4 has a duration greater or equal than 5 minutes and less than 10 minutes.
There are no session with a duration greater or equial than 10 minutes and less than 15 minutes.
For session_id 5 has a duration greater or equal than 15 minutes.
Solution
sql

#Solution 1:
(SELECT '[0-5>' AS bin,
 SUM(CASE WHEN duration/60 < 5 THEN 1 ELSE 0 END) AS total FROM Sessions)
 UNION
(SELECT '[5-10>' AS bin,
 SUM(CASE WHEN ((duration/60 >= 5) AND (duration/60 < 10)) THEN 1 ELSE 0 END) AS total FROM Sessions)
 UNION
(SELECT '[10-15>' AS bin,
 SUM(CASE WHEN ((duration/60 >= 10) AND (duration/60 < 15)) THEN 1 ELSE 0 END) AS total FROM Sessions)
 UNION
(SELECT '15 or more' AS bin,
 SUM(CASE WHEN duration/60 >= 15 THEN 1 ELSE 0 END) AS total FROM Sessions)

#Solution 2:
SELECT '[0-5>' AS bin, count(1) AS total
FROM Sessions
WHERE duration>=0 AND duration < 300
UNION
SELECT '[5-10>' AS bin, count(1) AS total
FROM Sessions
WHERE duration>=300 AND duration < 600
UNION
SELECT '[10-15>' AS bin, count(1) AS total
FROM Sessions
WHERE duration>=600 AND duration < 900
UNION
SELECT '15 or more' AS bin, count(1) AS total
FROM Sessions
WHERE duration >= 900
1440. Evaluate Boolean Expression | Medium | 🔒 LeetCode
Table Variables:


+---------------+---------+
| Column Name   | Type    |
+---------------+---------+
| name          | varchar |
| value         | int     |
+---------------+---------+
name is the primary key for this table.
This table contains the stored variables and their values.
Table Expressions:


+---------------+---------+
| Column Name   | Type    |
+---------------+---------+
| left_operand  | varchar |
| operator      | enum    |
| right_operand | varchar |
+---------------+---------+
(left_operand, operator, right_operand) is the primary key for this table.
This table contains a boolean expression that should be evaluated.
operator is an enum that takes one of the values ('<', '>', '=')
The values of left_operand and right_operand are guaranteed to be in the Variables table.
Write an SQL query to evaluate the boolean expressions in Expressions table.

Return the result table in any order.

The query result format is in the following example.


Variables table:
+------+-------+
| name | value |
+------+-------+
| x    | 66    |
| y    | 77    |
+------+-------+

Expressions table:
+--------------+----------+---------------+
| left_operand | operator | right_operand |
+--------------+----------+---------------+
| x            | >        | y             |
| x            | <        | y             |
| x            | =        | y             |
| y            | >        | x             |
| y            | <        | x             |
| x            | =        | x             |
+--------------+----------+---------------+

Result table:
+--------------+----------+---------------+-------+
| left_operand | operator | right_operand | value |
+--------------+----------+---------------+-------+
| x            | >        | y             | false |
| x            | <        | y             | true  |
| x            | =        | y             | false |
| y            | >        | x             | true  |
| y            | <        | x             | false |
| x            | =        | x             | true  |
+--------------+----------+---------------+-------+
As shown, you need find the value of each boolean exprssion in the table using the variables table.
Solution
sql

#Solution 1:
WITH t1 AS(
SELECT e.left_operand, e.operator, e.right_operand, v.value AS left_val, v_1.value AS right_val
FROM expressions e
JOIN variables v
ON v.name = e.left_operand
JOIN variables v_1
ON v_1.name = e.right_operand)

SELECT t1.left_operand, t1.operator, t1.right_operand,
CASE WHEN t1.operator = '<' THEN (SELECT t1.left_val< t1.right_val)
WHEN t1.operator = '>' THEN (SELECT t1.left_val > t1.right_val)
WHEN t1.operator = '=' THEN (SELECT t1.left_val = t1.right_val)
ELSE FALSE
END AS VALUE
FROM t1

#Solution 2:
# nested INNER JOIN can trim the volume of the intermediate table, which gives us better performance
SELECT t.left_operand, t.operator, t.right_operand,
    (CASE WHEN v1_value>v2.value AND operator = '>' THEN "true"
          WHEN v1_value<v2.value AND operator = '<' THEN "true"
          WHEN v1_value=v2.value AND operator = '=' THEN "true"
          ELSE "false"
          END) AS value
FROM
   (SELECT e.*, v1.value AS v1_value
    FROM Expressions AS e INNER JOIN Variables AS v1
    ON e.left_operand = v1.name) AS t INNER JOIN Variables AS v2
    ON t.right_operand = v2.name

#Solution 3:
SELECT t.left_operand, t.operator, t.right_operand,
    (CASE WHEN operator = '>' THEN IF(v1_value>v2.value, "true", "false")
          WHEN operator = '<' THEN IF(v1_value<v2.value, "true", "false")
          WHEN operator = '=' THEN IF(v1_value=v2.value, "true", "false")
          END) AS value
FROM
   (SELECT e.*, v1.value AS v1_value
    FROM Expressions AS e INNER JOIN Variables AS v1
    ON e.left_operand = v1.name) AS t INNER JOIN Variables AS v2
    ON t.right_operand = v2.name
1445. Apples & Oranges | Medium | 🔒 LeetCode
Table: Sales


+---------------+---------+
| Column Name   | Type    |
+---------------+---------+
| sale_date     | date    |
| fruit         | enum    |
| sold_num      | int     |
+---------------+---------+
(sale_date,fruit) is the primary key for this table.
This table contains the sales of "apples" and "oranges" sold each day.
Write an SQL query to report the difference between number of apples and oranges sold each day.

Return the result table ordered by sale_date in format ('YYYY-MM-DD').

The query result format is in the following example:


Sales table:
+------------+------------+-------------+
| sale_date  | fruit      | sold_num    |
+------------+------------+-------------+
| 2020-05-01 | apples     | 10          |
| 2020-05-01 | oranges    | 8           |
| 2020-05-02 | apples     | 15          |
| 2020-05-02 | oranges    | 15          |
| 2020-05-03 | apples     | 20          |
| 2020-05-03 | oranges    | 0           |
| 2020-05-04 | apples     | 15          |
| 2020-05-04 | oranges    | 16          |
+------------+------------+-------------+

Result table:
+------------+--------------+
| sale_date  | diff         |
+------------+--------------+
| 2020-05-01 | 2            |
| 2020-05-02 | 0            |
| 2020-05-03 | 20           |
| 2020-05-04 | -1           |
+------------+--------------+

Day 2020-05-01, 10 apples and 8 oranges were sold (Difference  10 - 8 = 2).
Day 2020-05-02, 15 apples and 15 oranges were sold (Difference 15 - 15 = 0).
Day 2020-05-03, 20 apples and 0 oranges were sold (Difference 20 - 0 = 20).
Day 2020-05-04, 15 apples and 16 oranges were sold (Difference 15 - 16 = -1).
Solution
sql

#Solution 1:
SELECT sale_date, sum(CASE WHEN fruit='apples' THEN sold_num ELSE -sold_num END) AS diff
FROM Sales
GROUP BY sale_date

#Solution 2:
SELECT sale_date, sold_num-sold AS diff
FROM
((SELECT *
FROM sales
WHERE fruit = 'apples') a
JOIN
(SELECT sale_date AS sale, fruit, sold_num AS sold
FROM sales
WHERE fruit = 'oranges') b
ON a.sale_date = b.sale)
1454. Active Users | Medium | 🔒 LeetCode
Table Accounts:


+---------------+---------+
| Column Name   | Type    |
+---------------+---------+
| id            | int     |
| name          | varchar |
+---------------+---------+
the id is the primary key for this table.
This table contains the account id and the user name of each account.
Table Logins:


+---------------+---------+
| Column Name   | Type    |
+---------------+---------+
| id            | int     |
| login_date    | date    |
+---------------+---------+
There is no primary key for this table, it may contain duplicates.
This table contains the account id of the user who logged in and the login date. A user may log in multiple times in the day.
Write an SQL query to find the id and the name of active users.

Active users are those who logged in to their accounts for 5 or more consecutive days.

Return the result table ordered by the id.

The query result format is in the following example:


Accounts table:
+----+----------+
| id | name     |
+----+----------+
| 1  | Winston  |
| 7  | Jonathan |
+----+----------+

Logins table:
+----+------------+
| id | login_date |
+----+------------+
| 7  | 2020-05-30 |
| 1  | 2020-05-30 |
| 7  | 2020-05-31 |
| 7  | 2020-06-01 |
| 7  | 2020-06-02 |
| 7  | 2020-06-02 |
| 7  | 2020-06-03 |
| 1  | 2020-06-07 |
| 7  | 2020-06-10 |
+----+------------+

Result table:
+----+----------+
| id | name     |
+----+----------+
| 7  | Jonathan |
+----+----------+
User Winston with id = 1 logged in 2 times only in 2 different days, so, Winston is not an active user.
User Jonathan with id = 7 logged in 7 times in 6 different days, five of them were consecutive days, so, Jonathan is an active user.
Follow up question: Can you write a general solution if the active users are those who logged in to their accounts for n or more consecutive days?

Solution
sql

#Solution 1:
WITH t1 AS (
SELECT id,login_date,
lead(login_date,4) OVER(PARTITION BY id ORDER BY login_date) date_5
FROM (SELECT DISTINCT * FROM Logins) b
)

SELECT DISTINCT a.id, a.name FROM t1
INNER JOIN accounts a
ON t1.id = a.id
WHERE DATEDIFF(t1.date_5,login_date) = 4
ORDER BY id

#Soltion 2:
SELECT *
FROM Accounts
WHERE id IN
    (SELECT DISTINCT t1.id
    FROM Logins AS t1 INNER JOIN Logins AS t2
    ON t1.id = t2.id AND datediff(t1.login_date, t2.login_date) BETWEEN 1 AND 4
    GROUP BY t1.id, t1.login_date
    HAVING count(DISTINCT(t2.login_date)) = 4)
ORDER BY id
1459. Rectangles Area | Medium | 🔒 LeetCode
Table: Points


+---------------+---------+
| Column Name   | Type    |
+---------------+---------+
| id            | int     |
| x_value       | int     |
| y_value       | int     |
+---------------+---------+
id is the primary key for this table.
Each point is represented as a 2D Dimensional (x_value, y_value).
Write an SQL query to report of all possible rectangles which can be formed by any two points of the table.

Each row in the result contains three columns (p1, p2, area) where:

p1 and p2 are the id of two opposite corners of a rectangle and p1 < p2.
Area of this rectangle is represented by the column area. Report the query in descending order by area in case of tie in ascending order by p1 and p2.

Points table:
+----------+-------------+-------------+
| id       | x_value     | y_value     |
+----------+-------------+-------------+
| 1        | 2           | 8           |
| 2        | 4           | 7           |
| 3        | 2           | 10          |
+----------+-------------+-------------+

Result table:
+----------+-------------+-------------+
| p1       | p2          | area        |
+----------+-------------+-------------+
| 2        | 3           | 6           |
| 1        | 2           | 2           |
+----------+-------------+-------------+

p1 should be less than p2 and area greater than 0.
p1 = 1 and p2 = 2, has an area equal to |2-4| * |8-7| = 2.
p1 = 2 and p2 = 3, has an area equal to |4-2| * |7-10| = 2.
p1 = 1 and p2 = 3 It's not possible because has an area equal to 0.
Solution
sql

SELECT t1.id AS p1, t2.id AS p2, ABS(t1.x_value-t2.x_value)*ABS(t1.y_value-t2.y_value) AS area
FROM Points AS t1 INNER JOIN Points AS t2
ON t1.id < t2.id
AND t1.x_value != t2.x_value AND t1.y_value != t2.y_value
ORDER BY area DESC, p1, p2
1468. Calculate Salaries | Medium | 🔒 LeetCode
Table Salaries:


+---------------+---------+
| Column Name   | Type    |
+---------------+---------+
| company_id    | int     |
| employee_id   | int     |
| employee_name | varchar |
| salary        | int     |
+---------------+---------+
(company_id, employee_id) is the primary key for this table.
This table contains the company id, the id, the name and the salary for an employee.
Write an SQL query to find the salaries of the employees after applying taxes.

The tax rate is calculated for each company based on the following criteria:

0% If the max salary of any employee in the company is less than 1000$.
24% If the max salary of any employee in the company is in the range [1000, 10000] inclusive.
49% If the max salary of any employee in the company is greater than 10000$.
Return the result table in any order. Round the salary to the nearest integer.
The query result format is in the following example:


Salaries table:
+------------+-------------+---------------+--------+
| company_id | employee_id | employee_name | salary |
+------------+-------------+---------------+--------+
| 1          | 1           | Tony          | 2000   |
| 1          | 2           | Pronub        | 21300  |
| 1          | 3           | Tyrrox        | 10800  |
| 2          | 1           | Pam           | 300    |
| 2          | 7           | Bassem        | 450    |
| 2          | 9           | Hermione      | 700    |
| 3          | 7           | Bocaben       | 100    |
| 3          | 2           | Ognjen        | 2200   |
| 3          | 13          | Nyancat       | 3300   |
| 3          | 15          | Morninngcat   | 1866   |
+------------+-------------+---------------+--------+

Result table:
+------------+-------------+---------------+--------+
| company_id | employee_id | employee_name | salary |
+------------+-------------+---------------+--------+
| 1          | 1           | Tony          | 1020   |
| 1          | 2           | Pronub        | 10863  |
| 1          | 3           | Tyrrox        | 5508   |
| 2          | 1           | Pam           | 300    |
| 2          | 7           | Bassem        | 450    |
| 2          | 9           | Hermione      | 700    |
| 3          | 7           | Bocaben       | 76     |
| 3          | 2           | Ognjen        | 1672   |
| 3          | 13          | Nyancat       | 2508   |
| 3          | 15          | Morninngcat   | 5911   |
+------------+-------------+---------------+--------+
For company 1, Max salary is 21300. Employees in company 1 have taxes = 49%
For company 2, Max salary is 700. Employees in company 2 have taxes = 0%
For company 3, Max salary is 7777. Employees in company 3 have taxes = 24%
The salary after taxes = salary - (taxes percentage / 100) * salary
For example, Salary for Morninngcat (3, 15) after taxes = 7777 - 7777 * (24 / 100) = 7777 - 1866.48 = 5910.52, which is rounded to 5911.
Solution
sql

#Solution 1:
WITH t1 AS (
SELECT company_id, employee_id, employee_name, salary AS sa, MAX(salary) OVER(PARTITION BY company_id) AS maximum
FROM salaries)

SELECT company_id, employee_id, employee_name,
CASE WHEN t1.maximum<1000 THEN t1.sa
WHEN t1.maximum BETWEEN 1000 AND 10000 THEN ROUND(t1.sa*.76,0)
ELSE ROUND(t1.sa*.51,0)
END AS salary
FROM t1

#Soltion 2:
SELECT Salaries.company_id, Salaries.employee_id, Salaries.employee_name,
    ROUND(CASE WHEN salary_max<1000 THEN Salaries.salary
               WHEN salary_max>=1000 AND salary_max<=10000 THEN Salaries.salary * 0.76
               ELSE Salaries.salary * 0.51 END, 0) AS salary
FROM Salaries INNER JOIN (
    SELECT company_id, MAX(salary) AS salary_max
    FROM Salaries
    GROUP BY company_id) AS t
ON Salaries.company_id = t.company_id
1479. Sales by Day of the Week | Hard | 🔒 LeetCode
Table: Orders


+---------------+---------+
| Column Name   | Type    |
+---------------+---------+
| order_id      | int     |
| customer_id   | int     |
| order_date    | date    |
| item_id       | varchar |
| quantity      | int     |
+---------------+---------+
(ordered_id, item_id) is the primary key for this table.
This table contains information of the orders placed.
order_date is the date when item_id was ordered by the customer with id customer_id.
Table: Items


+---------------------+---------+
| Column Name         | Type    |
+---------------------+---------+
| item_id             | varchar |
| item_name           | varchar |
| item_category       | varchar |
+---------------------+---------+
item_id is the primary key for this table.
item_name is the name of the item.
item_category is the category of the item.
You are the business owner and would like to obtain a sales report for category items and day of the week.

Write an SQL query to report how many units in each category have been ordered on each day of the week.

Return the result table ordered by category.

The query result format is in the following example:


Orders table:
+------------+--------------+-------------+--------------+-------------+
| order_id   | customer_id  | order_date  | item_id      | quantity    |
+------------+--------------+-------------+--------------+-------------+
| 1          | 1            | 2020-06-01  | 1            | 10          |
| 2          | 1            | 2020-06-08  | 2            | 10          |
| 3          | 2            | 2020-06-02  | 1            | 5           |
| 4          | 3            | 2020-06-03  | 3            | 5           |
| 5          | 4            | 2020-06-04  | 4            | 1           |
| 6          | 4            | 2020-06-05  | 5            | 5           |
| 7          | 5            | 2020-06-05  | 1            | 10          |
| 8          | 5            | 2020-06-14  | 4            | 5           |
| 9          | 5            | 2020-06-21  | 3            | 5           |
+------------+--------------+-------------+--------------+-------------+

Items table:
+------------+----------------+---------------+
| item_id    | item_name      | item_category |
+------------+----------------+---------------+
| 1          | LC Alg. Book   | Book          |
| 2          | LC DB. Book    | Book          |
| 3          | LC SmarthPhone | Phone         |
| 4          | LC Phone 2020  | Phone         |
| 5          | LC SmartGlass  | Glasses       |
| 6          | LC T-Shirt XL  | T-Shirt       |
+------------+----------------+---------------+

Result table:
+------------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+
| Category   | Monday    | Tuesday   | Wednesday | Thursday  | Friday    | Saturday  | Sunday    |
+------------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+
| Book       | 20        | 5         | 0         | 0         | 10        | 0         | 0         |
| Glasses    | 0         | 0         | 0         | 0         | 5         | 0         | 0         |
| Phone      | 0         | 0         | 5         | 1         | 0         | 0         | 10        |
| T-Shirt    | 0         | 0         | 0         | 0         | 0         | 0         | 0         |
+------------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+
On Monday (2020-06-01, 2020-06-08) were sold a total of 20 units (10 + 10) in the category Book (ids: 1, 2).
On Tuesday (2020-06-02) were sold a total of 5 units  in the category Book (ids: 1, 2).
On Wednesday (2020-06-03) were sold a total of 5 units in the category Phone (ids: 3, 4).
On Thursday (2020-06-04) were sold a total of 1 unit in the category Phone (ids: 3, 4).
On Friday (2020-06-05) were sold 10 units in the category Book (ids: 1, 2) and 5 units in Glasses (ids: 5).
On Saturday there are no items sold.
On Sunday (2020-06-14, 2020-06-21) were sold a total of 10 units (5 +5) in the category Phone (ids: 3, 4).
There are no sales of T-Shirt.
Solution
sql

WITH t1 AS(
SELECT DISTINCT item_category,
CASE WHEN dayname(order_date)='Monday' THEN SUM(quantity) OVER(PARTITION BY item_category,dayname(order_date)) ELSE 0 END AS Monday,
CASE WHEN dayname(order_date)='Tuesday' THEN SUM(quantity) OVER(PARTITION BY item_category,dayname(order_date)) ELSE 0 END AS Tuesday,
CASE WHEN dayname(order_date)='Wednesday' THEN SUM(quantity) OVER(PARTITION BY item_category,dayname(order_date)) ELSE 0 END AS Wednesday,
CASE WHEN dayname(order_date)='Thursday' THEN SUM(quantity) OVER(PARTITION BY item_category,dayname(order_date)) ELSE 0 END AS Thursday,
CASE WHEN dayname(order_date)='Friday' THEN SUM(quantity) OVER(PARTITION BY item_category,dayname(order_date)) ELSE 0 END AS Friday,
CASE WHEN dayname(order_date)='Saturday' THEN SUM(quantity) OVER(PARTITION BY item_category,dayname(order_date)) ELSE 0 END AS Saturday,
CASE WHEN dayname(order_date)='Sunday' THEN SUM(quantity) OVER(PARTITION BY item_category,dayname(order_date)) ELSE 0 END AS Sunday
FROM orders o
RIGHT JOIN items i
USING (item_id))

SELECT item_category AS category, SUM(Monday) AS Monday, SUM(Tuesday) AS Tuesday, SUM(Wednesday) Wednesday, SUM(Thursday) Thursday,
SUM(Friday) Friday, SUM(Saturday) Saturday, SUM(Sunday) Sunday
FROM t1
GROUP BY item_category
1484. Group Sold Products By The Date | Easy | 🔒 LeetCode
Table Activities:


+-------------+---------+
| Column Name | Type    |
+-------------+---------+
| sell_date   | date    |
| product     | varchar |
+-------------+---------+
There is no primary key for this table, it may contains duplicates.
Each row of this table contains the product name and the date it was sold in a market.
Write an SQL query to find for each date, the number of distinct products sold and their names.

The sold-products names for each date should be sorted lexicographically.

Return the result table ordered by sell_date.

The query result format is in the following example.


Activities table:
+------------+-------------+
| sell_date  | product     |
+------------+-------------+
| 2020-05-30 | Headphone   |
| 2020-06-01 | Pencil      |
| 2020-06-02 | Mask        |
| 2020-05-30 | Basketball  |
| 2020-06-01 | Bible       |
| 2020-06-02 | Mask        |
| 2020-05-30 | T-Shirt     |
+------------+-------------+

Result table:
+------------+----------+------------------------------+
| sell_date  | num_sold | products                     |
+------------+----------+------------------------------+
| 2020-05-30 | 3        | Basketball,Headphone,T-shirt |
| 2020-06-01 | 2        | Bible,Pencil                 |
| 2020-06-02 | 1        | Mask                         |
+------------+----------+------------------------------+
For 2020-05-30, Sold items were (Headphone, Basketball, T-shirt), we sort them lexicographically and separate them by comma.
For 2020-06-01, Sold items were (Pencil, Bible), we sort them lexicographically and separate them by comma.
For 2020-06-02, Sold item is (Masks), we just return it.
Solution
sql

SELECT sell_date, COUNT(DISTINCT product) AS num_sold, group_concat(DISTINCT product) AS products
FROM activities
GROUP BY 1
ORDER BY 1
1495. Friendly Movies Streamed Last Month | Easy | 🔒 LeetCode
Table: TVProgram


+---------------+---------+
| Column Name   | Type    |
+---------------+---------+
| program_date  | date    |
| content_id    | int     |
| channel       | varchar |
+---------------+---------+
(program_date, content_id) is the primary key for this table.
This table contains information of the programs on the TV.
content_id is the id of the program in some channel on the TV.
Table: Content


+------------------+---------+
| Column Name      | Type    |
+------------------+---------+
| content_id       | varchar |
| title            | varchar |
| Kids_content     | enum    |
| content_type     | varchar |
+------------------+---------+
content_id is the primary key for this table.
Kids_content is an enum that takes one of the values ('Y', 'N') where:
'Y' means is content for kids otherwise 'N' is not content for kids.
content_type is the category of the content as movies, series, etc.
Write an SQL query to report the distinct titles of the kid-friendly movies streamed in June 2020.

Return the result table in any order.

The query result format is in the following example.


TVProgram table:
+--------------------+--------------+-------------+
| program_date       | content_id   | channel     |
+--------------------+--------------+-------------+
| 2020-06-10 08:00   | 1            | LC-Channel  |
| 2020-05-11 12:00   | 2            | LC-Channel  |
| 2020-05-12 12:00   | 3            | LC-Channel  |
| 2020-05-13 14:00   | 4            | Disney Ch   |
| 2020-06-18 14:00   | 4            | Disney Ch   |
| 2020-07-15 16:00   | 5            | Disney Ch   |
+--------------------+--------------+-------------+

Content table:
+------------+----------------+---------------+---------------+
| content_id | title          | Kids_content  | content_type  |
+------------+----------------+---------------+---------------+
| 1          | Leetcode Movie | N             | Movies        |
| 2          | Alg. for Kids  | Y             | Series        |
| 3          | Database Sols  | N             | Series        |
| 4          | Aladdin        | Y             | Movies        |
| 5          | Cinderella     | Y             | Movies        |
+------------+----------------+---------------+---------------+

Result table:
+--------------+
| title        |
+--------------+
| Aladdin      |
+--------------+
"Leetcode Movie" is not a content for kids.
"Alg. for Kids" is not a movie.
"Database Sols" is not a movie
"Alladin" is a movie, content for kids and was streamed in June 2020.
"Cinderella" was not streamed in June 2020.
Solution
sql

SELCT DISTINCT title
FROM
(SELCT content_id, title
FROM content
WHERE kids_content = 'Y' AND content_type = 'Movies') a
JOIN
tvprogram USING (content_id)
WHERE month(program_date) = 6
1501. Countries You Can Safely Invest In | Medium | 🔒 LeetCode
Table Person:


+----------------+---------+
| Column Name    | Type    |
+----------------+---------+
| id             | int     |
| name           | varchar |
| phone_number   | varchar |
+----------------+---------+
id is the primary key for this table.
Each row of this table contains the name of a person and their phone number.
Phone number will be in the form 'xxx-yyyyyyy' where xxx is the country code (3 characters) and yyyyyyy is the phone number (7 characters) where x and y are digits. Both can contain leading zeros.
Table Country:


+----------------+---------+
| Column Name    | Type    |
+----------------+---------+
| name           | varchar |
| country_code   | varchar |
+----------------+---------+
country_code is the primary key for this table.
Each row of this table contains the country name and its code. country_code will be in the form 'xxx' where x is digits.
Table Calls:


+-------------+------+
| Column Name | Type |
+-------------+------+
| caller_id   | int  |
| callee_id   | int  |
| duration    | int  |
+-------------+------+
There is no primary key for this table, it may contain duplicates.
Each row of this table contains the caller id, callee id and the duration of the call in minutes. caller_id != callee_id
A telecommunications company wants to invest in new countries. The country intends to invest in the countries where the average call duration of the calls in this country is strictly greater than the global average call duration.
Write an SQL query to find the countries where this company can invest.

Return the result table in any order.

The query result format is in the following example.


Person table:
+----+----------+--------------+
| id | name     | phone_number |
+----+----------+--------------+
| 3  | Jonathan | 051-1234567  |
| 12 | Elvis    | 051-7654321  |
| 1  | Moncef   | 212-1234567  |
| 2  | Maroua   | 212-6523651  |
| 7  | Meir     | 972-1234567  |
| 9  | Rachel   | 972-0011100  |
+----+----------+--------------+

Country table:
+----------+--------------+
| name     | country_code |
+----------+--------------+
| Peru     | 051          |
| Israel   | 972          |
| Morocco  | 212          |
| Germany  | 049          |
| Ethiopia | 251          |
+----------+--------------+

Calls table:
+-----------+-----------+----------+
| caller_id | callee_id | duration |
+-----------+-----------+----------+
| 1         | 9         | 33       |
| 2         | 9         | 4        |
| 1         | 2         | 59       |
| 3         | 12        | 102      |
| 3         | 12        | 330      |
| 12        | 3         | 5        |
| 7         | 9         | 13       |
| 7         | 1         | 3        |
| 9         | 7         | 1        |
| 1         | 7         | 7        |
+-----------+-----------+----------+

Result table:
+----------+
| country  |
+----------+
| Peru     |
+----------+
The average call duration for Peru is (102 + 102 + 330 + 330 + 5 + 5) / 6 = 145.666667
The average call duration for Israel is (33 + 4 + 13 + 13 + 3 + 1 + 1 + 7) / 8 = 9.37500
The average call duration for Morocco is (33 + 4 + 59 + 59 + 3 + 7) / 6 = 27.5000
Global call duration average = (2 * (33 + 3 + 59 + 102 + 330 + 5 + 13 + 3 + 1 + 7)) / 20 = 55.70000
Since Peru is the only country where average call duration is greater than the global average, it's the only recommended country.
Solution
sql

WITH t1 AS(
SELECT caller_id AS id, duration AS total
FROM
(SELECT caller_id, duration
FROM calls
UNION ALL
SELECT callee_id, duration
FROM calls) a
)
SELECT name AS country
FROM
(SELECT distinct avg(total) OVER(PARTITION BY code) AS avg_call, avg(total) OVER() AS global_avg, c.name
FROM
((SELECT *, coalesce(total,0) AS duration, SUBSTRING(phone_number FROM 1 for 3) AS code
FROM person RIGHT JOIN t1
USING (id)) b
join country c
ON c.country_code = b.code)) d
WHERE avg_call > global_avg
1511. Customer Order Frequency | Easy | 🔒 LeetCode
Table: Customers


+---------------+---------+
| Column Name   | Type    |
+---------------+---------+
| customer_id   | int     |
| name          | varchar |
| country       | varchar |
+---------------+---------+
customer_id is the primary key for this table.
This table contains information of the customers in the company.
Table: Product


+---------------+---------+
| Column Name   | Type    |
+---------------+---------+
| product_id    | int     |
| description   | varchar |
| price         | int     |
+---------------+---------+
product_id is the primary key for this table.
This table contains information of the products in the company.
price is the product cost.
Table: Orders


+---------------+---------+
| Column Name   | Type    |
+---------------+---------+
| order_id      | int     |
| customer_id   | int     |
| product_id    | int     |
| order_date    | date    |
| quantity      | int     |
+---------------+---------+
order_id is the primary key for this table.
This table contains information on customer orders.
customer_id is the id of the customer who bought "quantity" products with id "product_id".
Order_date is the date in format ('YYYY-MM-DD') when the order was shipped.
Write an SQL query to report the customer_id and customer_name of customers who have spent at least $100 in each month of June and July 2020.

Return the result table in any order.

The query result format is in the following example.


Customers
+--------------+-----------+-------------+
| customer_id  | name      | country     |
+--------------+-----------+-------------+
| 1            | Winston   | USA         |
| 2            | Jonathan  | Peru        |
| 3            | Moustafa  | Egypt       |
+--------------+-----------+-------------+

Product
+--------------+-------------+-------------+
| product_id   | description | price       |
+--------------+-------------+-------------+
| 10           | LC Phone    | 300         |
| 20           | LC T-Shirt  | 10          |
| 30           | LC Book     | 45          |
| 40           | LC Keychain | 2           |
+--------------+-------------+-------------+

Orders
+--------------+-------------+-------------+-------------+-----------+
| order_id     | customer_id | product_id  | order_date  | quantity  |
+--------------+-------------+-------------+-------------+-----------+
| 1            | 1           | 10          | 2020-06-10  | 1         |
| 2            | 1           | 20          | 2020-07-01  | 1         |
| 3            | 1           | 30          | 2020-07-08  | 2         |
| 4            | 2           | 10          | 2020-06-15  | 2         |
| 5            | 2           | 40          | 2020-07-01  | 10        |
| 6            | 3           | 20          | 2020-06-24  | 2         |
| 7            | 3           | 30          | 2020-06-25  | 2         |
| 9            | 3           | 30          | 2020-05-08  | 3         |
+--------------+-------------+-------------+-------------+-----------+

Result table:
+--------------+------------+
| customer_id  | name       |
+--------------+------------+
| 1            | Winston    |
+--------------+------------+
Winston spent $300 (300 * 1) in June and $100 ( 10 * 1 + 45 * 2) in July 2020.
Jonathan spent $600 (300 * 2) in June and $20 ( 2 * 10) in July 2020.
Moustafa spent $110 (10 * 2 + 45 * 2) in June and $0 in July 2020.
Solution
sql

#Solution 1:
SELECT o.customer_id, name
JOIN Product p
ON o.product_id = p.product_id
JOIN Customers c
ON o.customer_id = c.customer_id
GROUP BY 1, 2
HAVING SUM(CASE WHEN date_format(order_date, '%Y-%m')='2020-06'
THEN price*quantity END) >= 100
AND
SUM(CASE WHEN date_format(order_date, '%Y-%m')='2020-07'
THEN price*quantity END) >= 100;

#Solution 2:
SELECT customer_id, name
FROM
(
    SELECT o.customer_id, c.name,
        sum(CASE WHEN left(o.order_date,7) = '2020-06' THEN p.price * o.quantity END) AS JuneSpend,
        sum(CASE WHEN left(o.order_date,7) = '2020-07' THEN p.price * o.quantity END) AS JulySpend
    FROM Orders o
    LEFT JOIN Customers c ON o.customer_id = c.customer_id
    lEFT JOIN Product p ON o.product_id = p.product_id
    GROUP BY o.customer_id
    HAVING JuneSpend >= 100 AND JulySpend >= 100
) AS temp

#Solution 3:
SELECT o.customer_id, c.name
FROM Customers c, Product p, Orders o
WHERE c.customer_id = o.customer_id AND p.product_id = o.product_id
GROUP BY o.customer_id
HAVING
(
    SUM(CASE WHEN o.order_date LIKE '2020-06%' THEN o.quantity*p.price ELSE 0 END) >= 100
    and
    SUM(CASE WHEN o.order_date LIKE '2020-07%' THEN o.quantity*p.price ELSE 0 END) >= 100
);
1517. Find Users With Valid E-Mails | Easy | 🔒 LeetCode
Table: Users


+---------------+---------+
| Column Name   | Type    |
+---------------+---------+
| user_id       | int     |
| name          | varchar |
| mail          | varchar |
+---------------+---------+
user_id is the primary key for this table.
This table contains information of the users signed up in a website. Some e-mails are invalid.
Write an SQL query to find the users who have valid emails.

A valid e-mail has a prefix name and a domain where:

The prefix name is a string that may contain letters (upper or lower case), digits, underscore '_', period '.' and/or dash '-'. The prefix name must start with a letter.
The domain is '@leetcode.com'.
Return the result table in any order.
The query result format is in the following example.


Users
+---------+-----------+-------------------------+
| user_id | name      | mail                    |
+---------+-----------+-------------------------+
| 1       | Winston   | winston@leetcode.com    |
| 2       | Jonathan  | jonathanisgreat         |
| 3       | Annabelle | bella-@leetcode.com     |
| 4       | Sally     | sally.come@leetcode.com |
| 5       | Marwan    | quarz#2020@leetcode.com |
| 6       | David     | david69@gmail.com       |
| 7       | Shapiro   | .shapo@leetcode.com     |
+---------+-----------+-------------------------+

Result table:
+---------+-----------+-------------------------+
| user_id | name      | mail                    |
+---------+-----------+-------------------------+
| 1       | Winston   | winston@leetcode.com    |
| 3       | Annabelle | bella-@leetcode.com     |
| 4       | Sally     | sally.come@leetcode.com |
+---------+-----------+-------------------------+
The mail of user 2 doesn't have a domain.
The mail of user 5 has # sign which is not allowed.
The mail of user 6 doesn't have leetcode domain.
The mail of user 7 starts with a period.
Solution
sql

#Solution 1:
SELECT user_id, name, mail
FROM Users
WHERE mail regexp "^[a-zA-Z]+[a-zA-Z0-9_\\./\\-]{0,}@leetcode\\.com$"
ORDER BY user_id

#Solution 2:
SELECT * FROM Users
WHERE regexp_like(mail, '^[A-Za-z]+[A-Za-z0-9\_\.\-]*@leetcode.com')
1527. Patients With a Condition | Easy | 🔒 LeetCode
Table: Patients


+--------------+---------+
| Column Name  | Type    |
+--------------+---------+
| patient_id   | int     |
| patient_name | varchar |
| conditions   | varchar |
+--------------+---------+
patient_id is the primary key for this table.
'conditions' contains 0 or more code separated by spaces.
This table contains information of the patients in the hospital.
Write an SQL query to report the patient_id, patient_name all conditions of patients who have Type I Diabetes. Type I Diabetes always starts with DIAB1 prefix

Return the result table in any order.

The query result format is in the following example.


Patients
+------------+--------------+--------------+
| patient_id | patient_name | conditions   |
+------------+--------------+--------------+
| 1          | Daniel       | YFEV COUGH   |
| 2          | Alice        |              |
| 3          | Bob          | DIAB100 MYOP |
| 4          | George       | ACNE DIAB100 |
| 5          | Alain        | DIAB201      |
+------------+--------------+--------------+

Result table:
+------------+--------------+--------------+
| patient_id | patient_name | conditions   |
+------------+--------------+--------------+
| 3          | Bob          | DIAB100 MYOP |
| 4          | George       | ACNE DIAB100 |
+------------+--------------+--------------+
Bob and George both have a condition that starts with DIAB1.
Solution
sql

SELECT patient_id, patient_name, conditions
FROM Patients
WHERE conditions LIKE '%DIAB1%'
1532. The Most Recent Three Orders | Medium | 🔒 LeetCode
Table: Customers


+---------------+---------+
| Column Name   | Type    |
+---------------+---------+
| customer_id   | int     |
| name          | varchar |
+---------------+---------+
customer_id is the primary key for this table.
This table contains information about customers.
Table: Orders


+---------------+---------+
| Column Name   | Type    |
+---------------+---------+
| order_id      | int     |
| order_date    | date    |
| customer_id   | int     |
| cost          | int     |
+---------------+---------+
order_id is the primary key for this table.
This table contains information about the orders made customer_id.
Each customer has one order per day.
Write an SQL query to find the most recent 3 orders of each user. If a user ordered less than 3 orders return all of their orders.

Return the result table sorted by customer_name in ascending order and in case of a tie by the customer_id in ascending order. If there still a tie, order them by the order_date in descending order.

The query result format is in the following example:


Customers
+-------------+-----------+
| customer_id | name      |
+-------------+-----------+
| 1           | Winston   |
| 2           | Jonathan  |
| 3           | Annabelle |
| 4           | Marwan    |
| 5           | Khaled    |
+-------------+-----------+

Orders
+----------+------------+-------------+------+
| order_id | order_date | customer_id | cost |
+----------+------------+-------------+------+
| 1        | 2020-07-31 | 1           | 30   |
| 2        | 2020-07-30 | 2           | 40   |
| 3        | 2020-07-31 | 3           | 70   |
| 4        | 2020-07-29 | 4           | 100  |
| 5        | 2020-06-10 | 1           | 1010 |
| 6        | 2020-08-01 | 2           | 102  |
| 7        | 2020-08-01 | 3           | 111  |
| 8        | 2020-08-03 | 1           | 99   |
| 9        | 2020-08-07 | 2           | 32   |
| 10       | 2020-07-15 | 1           | 2    |
+----------+------------+-------------+------+

Result table:
+---------------+-------------+----------+------------+
| customer_name | customer_id | order_id | order_date |
+---------------+-------------+----------+------------+
| Annabelle     | 3           | 7        | 2020-08-01 |
| Annabelle     | 3           | 3        | 2020-07-31 |
| Jonathan      | 2           | 9        | 2020-08-07 |
| Jonathan      | 2           | 6        | 2020-08-01 |
| Jonathan      | 2           | 2        | 2020-07-30 |
| Marwan        | 4           | 4        | 2020-07-29 |
| Winston       | 1           | 8        | 2020-08-03 |
| Winston       | 1           | 1        | 2020-07-31 |
| Winston       | 1           | 10       | 2020-07-15 |
+---------------+-------------+----------+------------+
Winston has 4 orders, we discard the order of "2020-06-10" because it is the oldest order.
Annabelle has only 2 orders, we return them.
Jonathan has exactly 3 orders.
Marwan ordered only one time.
We sort the result table by customer_name in ascending order, by customer_id in ascending order and by order_date in descending order in case of a tie.
Follow-up:
Can you write a general solution for the most recent `n` orders?
Solution
sql

WITH tmp AS (
SELECT a.name, a.customer_id, b.order_id, b.order_date,
ROW_NUMBER() OVER(PARTITION BY a.name, a.customer_id ORDER BY b.order_date DESC) AS rnk
FROM Customers AS a
JOIN Orders AS b
ON a.customer_id = b.customer_id
)

SELECT name AS customer_name, customer_id, order_id, order_date
FROM tmp
WHERE rnk <= 3
ORDER BY customer_name, customer_id, order_date DESC;
1543. Fix Product Name Format | Easy | 🔒 LeetCode
Table: Sales


+--------------+---------+
| Column Name  | Type    |
+--------------+---------+
| sale_id      | int     |
| product_name | varchar |
| sale_date    | date    |
+--------------+---------+
sale_id is the primary key for this table.
Each row of this table contains the product name and the date it was sold.
Since table Sales was filled manually in the year 2000, product_name may contain leading and/or trailing white spaces, also they are case-insensitive.

Write an SQL query to report

product_name in lowercase without leading or trailing white spaces.
sale_date in the format ('YYYY-MM')
total the number of times the product was sold in this month.
Return the result table ordered by product_name in ascending order, in case of a tie order it by sale_date in ascending order.
The query result format is in the following example.


Sales
+------------+------------------+--------------+
| sale_id    | product_name     | sale_date    |
+------------+------------------+--------------+
| 1          |      LCPHONE     | 2000-01-16   |
| 2          |    LCPhone       | 2000-01-17   |
| 3          |     LcPhOnE      | 2000-02-18   |
| 4          |      LCKeyCHAiN  | 2000-02-19   |
| 5          |   LCKeyChain     | 2000-02-28   |
| 6          | Matryoshka       | 2000-03-31   |
+------------+------------------+--------------+

Result table:
+--------------+--------------+----------+
| product_name | sale_date    | total    |
+--------------+--------------+----------+
| lcphone      | 2000-01      | 2        |
| lckeychain   | 2000-02      | 2        |
| lcphone      | 2000-02      | 1        |
| matryoshka   | 2000-03      | 1        |
+--------------+--------------+----------+

In January, 2 LcPhones were sold, please note that the product names are not case sensitive and may contain spaces.
In Februery, 2 LCKeychains and 1 LCPhone were sold.
In March, 1 matryoshka was sold.
Solution
sql

SELECT TRIM(LOWER(product_name)) AS product_name,
       DATE_FORMAT(sale_date, '%Y-%m') AS sale_date,
       COUNT(*) AS total
FROM Sales
GROUP BY 1, DATE_FORMAT(sale_date, '%Y-%m')
ORDER BY 1, 2;
1549. The Most Recent Orders for Each Product | Medium | 🔒 LeetCode
Table: Customers


+---------------+---------+
| Column Name   | Type    |
+---------------+---------+
| customer_id   | int     |
| name          | varchar |
+---------------+---------+
customer_id is the primary key for this table.
This table contains information about the customers.
Table: Orders


+---------------+---------+
| Column Name   | Type    |
+---------------+---------+
| order_id      | int     |
| order_date    | date    |
| customer_id   | int     |
| product_id    | int     |
+---------------+---------+
order_id is the primary key for this table.
This table contains information about the orders made by customer_id.
There will be no product ordered by the same user more than once in one day.
Table: Products


+---------------+---------+
| Column Name   | Type    |
+---------------+---------+
| product_id    | int     |
| product_name  | varchar |
| price         | int     |
+---------------+---------+
product_id is the primary key for this table.
This table contains information about the Products.
Write an SQL query to find the most recent order(s) of each product.

Return the result table sorted by product_name in ascending order and in case of a tie by the product_id in ascending order. If there still a tie, order them by the order_id in ascending order.

The query result format is in the following example:


Customers
+-------------+-----------+
| customer_id | name      |
+-------------+-----------+
| 1           | Winston   |
| 2           | Jonathan  |
| 3           | Annabelle |
| 4           | Marwan    |
| 5           | Khaled    |
+-------------+-----------+

Orders
+----------+------------+-------------+------------+
| order_id | order_date | customer_id | product_id |
+----------+------------+-------------+------------+
| 1        | 2020-07-31 | 1           | 1          |
| 2        | 2020-07-30 | 2           | 2          |
| 3        | 2020-08-29 | 3           | 3          |
| 4        | 2020-07-29 | 4           | 1          |
| 5        | 2020-06-10 | 1           | 2          |
| 6        | 2020-08-01 | 2           | 1          |
| 7        | 2020-08-01 | 3           | 1          |
| 8        | 2020-08-03 | 1           | 2          |
| 9        | 2020-08-07 | 2           | 3          |
| 10       | 2020-07-15 | 1           | 2          |
+----------+------------+-------------+------------+

Products
+------------+--------------+-------+
| product_id | product_name | price |
+------------+--------------+-------+
| 1          | keyboard     | 120   |
| 2          | mouse        | 80    |
| 3          | screen       | 600   |
| 4          | hard disk    | 450   |
+------------+--------------+-------+

Result table:
+--------------+------------+----------+------------+
| product_name | product_id | order_id | order_date |
+--------------+------------+----------+------------+
| keyboard     | 1          | 6        | 2020-08-01 |
| keyboard     | 1          | 7        | 2020-08-01 |
| mouse        | 2          | 8        | 2020-08-03 |
| screen       | 3          | 3        | 2020-08-29 |
+--------------+------------+----------+------------+
keyboard's most recent order is in 2020-08-01, it was ordered two times this day.
mouse's most recent order is in 2020-08-03, it was ordered only once this day.
screen's most recent order is in 2020-08-29, it was ordered only once this day.
The hard disk was never ordered and we don't include it in the result table.
Solution
sql

SELECT p.product_name, o.product_id, o.order_id, o.order_date
FROM(
    SELECT product_id, order_id, order_date,
    RANK() OVER(PARTITION BY product_id ORDER BY order_date DESC) AS seq
    FROM orders
) o
LEFT JOIN products p
    ON o.product_id = p.product_id
WHERE o.seq = 1
ORDER BY 1,2,3
1555. Bank Account Summary | Medium | 🔒 LeetCode
Table: Users


+--------------+---------+
| Column Name  | Type    |
+--------------+---------+
| user_id      | int     |
| user_name    | varchar |
| credit       | int     |
+--------------+---------+
user_id is the primary key for this table.
Each row of this table contains the current credit information for each user.
Table: Transaction


+---------------+---------+
| Column Name   | Type    |
+---------------+---------+
| trans_id      | int     |
| paid_by       | int     |
| paid_to       | int     |
| amount        | int     |
| transacted_on | date    |
+---------------+---------+
trans_id is the primary key for this table.
Each row of this table contains the information about the transaction in the bank.
User with id (paid_by) transfer money to user with id (paid_to).
Leetcode Bank (LCB) helps its coders in making virtual payments. Our bank records all transactions in the table Transaction, we want to find out the current balance of all users and check wheter they have breached their credit limit (If their current credit is less than 0).

Write an SQL query to report.

user_id
user_name
credit, current balance after performing transactions.
credit_limit_breached, check credit_limit ("Yes" or "No") Return the result table in any order.
The query result format is in the following example.


Users table:
+------------+--------------+-------------+
| user_id    | user_name    | credit      |
+------------+--------------+-------------+
| 1          | Moustafa     | 100         |
| 2          | Jonathan     | 200         |
| 3          | Winston      | 10000       |
| 4          | Luis         | 800         |
+------------+--------------+-------------+

Transaction table:
+------------+------------+------------+----------+---------------+
| trans_id   | paid_by    | paid_to    | amount   | transacted_on |
+------------+------------+------------+----------+---------------+
| 1          | 1          | 3          | 400      | 2020-08-01    |
| 2          | 3          | 2          | 500      | 2020-08-02    |
| 3          | 2          | 1          | 200      | 2020-08-03    |
+------------+------------+------------+----------+---------------+

Result table:
+------------+------------+------------+-----------------------+
| user_id    | user_name  | credit     | credit_limit_breached |
+------------+------------+------------+-----------------------+
| 1          | Moustafa   | -100       | Yes                   |
| 2          | Jonathan   | 500        | No                    |
| 3          | Winston    | 9990       | No                    |
| 4          | Luis       | 800        | No                    |
+------------+------------+------------+-----------------------+
Moustafa paid $400 on "2020-08-01" and received $200 on "2020-08-03", credit (100 -400 +200) = -$100
Jonathan received $500 on "2020-08-02" and paid $200 on "2020-08-08", credit (200 +500 -200) = $500
Winston received $400 on "2020-08-01" and paid $500 on "2020-08-03", credit (10000 +400 -500) = $9990
Luis didn't received any transfer, credit = $800
Solution
sql

SELECT Users.user_id AS user_id
    , Users.user_name AS user_name
    , credit+IFNULL(SUM(trans),0) AS credit
    , CASE WHEN credit+IFNULL(SUM(trans),0)>0 THEN 'No' ELSE 'Yes' END AS credit_limit_breached
FROM(
    SELECT paid_by AS user_id, -amount AS trans
    FROM Transaction
    UNION ALL
    SELECT paid_to AS user_id, amount AS trans
    FROM Transaction
) t RIGHT JOIN users ON t.user_id=users.user_id
GROUP BY user_id
1565. Unique Orders and Customers Per Month | Easy | 🔒 LeetCode
Table: Orders


+---------------+---------+
| Column Name   | Type    |
+---------------+---------+
| order_id      | int     |
| order_date    | date    |
| customer_id   | int     |
| invoice       | int     |
+---------------+---------+
order_id is the primary key for this table.
This table contains information about the orders made by customer_id.
Write an SQL query to find the number of unique orders and the number of unique users with invoices > $20 for each different month.

Return the result table sorted in any order.

The query result format is in the following example:


Orders
+----------+------------+-------------+------------+
| order_id | order_date | customer_id | invoice    |
+----------+------------+-------------+------------+
| 1        | 2020-09-15 | 1           | 30         |
| 2        | 2020-09-17 | 2           | 90         |
| 3        | 2020-10-06 | 3           | 20         |
| 4        | 2020-10-20 | 3           | 21         |
| 5        | 2020-11-10 | 1           | 10         |
| 6        | 2020-11-21 | 2           | 15         |
| 7        | 2020-12-01 | 4           | 55         |
| 8        | 2020-12-03 | 4           | 77         |
| 9        | 2021-01-07 | 3           | 31         |
| 10       | 2021-01-15 | 2           | 20         |
+----------+------------+-------------+------------+

Result table:
+---------+-------------+----------------+
| month   | order_count | customer_count |
+---------+-------------+----------------+
| 2020-09 | 2           | 2              |
| 2020-10 | 1           | 1              |
| 2020-12 | 2           | 1              |
| 2021-01 | 1           | 1              |
+---------+-------------+----------------+
In September 2020 we have two orders from 2 different customers with invoices > $20.
In October 2020 we have two orders from 1 customer, and only one of the two orders has invoice > $20.
In November 2020 we have two orders from 2 different customers but invoices < $20, so we don't include that month.
In December 2020 we have two orders from 1 customer both with invoices > $20.
In January 2021 we have two orders from 2 different customers, but only one of them with invoice > $20.
Solution
sql

#Solution 1:
SELECT DATE_FORMAT(order_date, '%Y-%m') AS month, COUNT(DISTINCT order_id) AS order_count, COUNT(DISTINCT customer_id) AS customer_count
    FROM Orders
    WHERE invoice > 20
    GROUP BY YEAR(order_date), MONTH(order_date);

#Solution 2:
SELECT LEFT(order_date, 7) AS month, COUNT(DISTINCT order_id) AS order_count,
	COUNT(DISTINCT customer_id) AS customer_count
FROM orders
WHERE invoice > 20
GROUP BY month
1571. Warehouse Manager | Easy | 🔒 LeetCode
Table: Warehouse


+--------------+---------+
| Column Name  | Type    |
+--------------+---------+
| name         | varchar |
| product_id   | int     |
| units        | int     |
+--------------+---------+
(name, product_id) is the primary key for this table.
Each row of this table contains the information of the products in each warehouse.
Table: Products


+---------------+---------+
| Column Name   | Type    |
+---------------+---------+
| product_id    | int     |
| product_name  | varchar |
| Width         | int     |
| Length        | int     |
| Height        | int     |
+---------------+---------+
product_id is the primary key for this table.
Each row of this table contains the information about the product dimensions (Width, Lenght and Height) in feets of each product.
Write an SQL query to report, How much cubic feet of volume does the inventory occupy in each warehouse.

warehouse_name
volume
Return the result table in any order.
The query result format is in the following example.


Warehouse table:
+------------+--------------+-------------+
| name       | product_id   | units       |
+------------+--------------+-------------+
| LCHouse1   | 1            | 1           |
| LCHouse1   | 2            | 10          |
| LCHouse1   | 3            | 5           |
| LCHouse2   | 1            | 2           |
| LCHouse2   | 2            | 2           |
| LCHouse3   | 4            | 1           |
+------------+--------------+-------------+

Products table:
+------------+--------------+------------+----------+-----------+
| product_id | product_name | Width      | Length   | Height    |
+------------+--------------+------------+----------+-----------+
| 1          | LC-TV        | 5          | 50       | 40        |
| 2          | LC-KeyChain  | 5          | 5        | 5         |
| 3          | LC-Phone     | 2          | 10       | 10        |
| 4          | LC-T-Shirt   | 4          | 10       | 20        |
+------------+--------------+------------+----------+-----------+

Result table:
+----------------+------------+
| warehouse_name | volume     |
+----------------+------------+
| LCHouse1       | 12250      |
| LCHouse2       | 20250      |
| LCHouse3       | 800        |
+----------------+------------+
Volume of product_id = 1 (LC-TV), 5x50x40 = 10000
Volume of product_id = 2 (LC-KeyChain), 5x5x5 = 125
Volume of product_id = 3 (LC-Phone), 2x10x10 = 200
Volume of product_id = 4 (LC-T-Shirt), 4x10x20 = 800
LCHouse1: 1 unit of LC-TV + 10 units of LC-KeyChain + 5 units of LC-Phone.
          Total volume: 1*10000 + 10*125  + 5*200 = 12250 cubic feet
LCHouse2: 2 units of LC-TV + 2 units of LC-KeyChain.
          Total volume: 2*10000 + 2*125 = 20250 cubic feet
LCHouse3: 1 unit of LC-T-Shirt.
          Total volume: 1*800 = 800 cubic feet.
Solution
sql

SELECT a.name AS warehouse_name,
SUM(a.units * b.Width * b.Length * b.Height) AS volume
FROM Warehouse AS a
LEFT JOIN Products AS b
ON a.product_id = b.product_id
GROUP BY a.name;
1581. Customer Who Visited but Did Not Make Any Transactions | Easy | 🔒 LeetCode
Table: Visits


+-------------+---------+
| Column Name | Type    |
+-------------+---------+
| visit_id    | int     |
| customer_id | int     |
+-------------+---------+
visit_id is the primary key for this table.
This table contains information about the customers who visited the mall.
Table: Transactions


+----------------+---------+
| Column Name    | Type    |
+----------------+---------+
| transaction_id | int     |
| visit_id       | int     |
| amount         | int     |
+----------------+---------+
transaction_id is the primary key for this table.
This table contains information about the customers who visited the mall.
Write an SQL query to find the IDs of the users who visited without making any transactions and the number of times they made these types of visits.

Return the result table sorted in any orders.

The query result format is in the following example:


Visits
+----------+-------------+
| visit_id | customer_id |
+----------+-------------+
| 1        | 23          |
| 2        | 9           |
| 4        | 30          |
| 5        | 54          |
| 6        | 96          |
| 7        | 54          |
| 8        | 54          |
+----------+-------------+

Transactions
+----------------+----------+--------+
| transaction_id | visit_id | amount |
+----------------+----------+--------+
| 2              | 5        | 310    |
| 3              | 5        | 300    |
| 9              | 5        | 200    |
| 12             | 1        | 910    |
| 13             | 2        | 970    |
+----------------+----------+--------+

Result table:
+-------------+----------------+
| customer_id | count_no_trans |
+-------------+----------------+
| 54          | 2              |
| 30          | 1              |
| 96          | 1              |
+-------------+----------------+
Customer with id = 23 visited the mall once and made one transaction during the visit with id = 12.
Customer with id = 9 visited the mall once and made one transaction during the visit with id = 13.
Customer with id = 30 visited the mall once and did not make any transactions.
Customer with id = 54 visited the mall three times. During 2 visits they did not make any transactions, and during one visit they made 3 transactions.
Customer with id = 96 visited the mall once and did not make any transactions.
As we can see, users with IDs 30 and 96 visited the mall one time without making any transactions. Also user 54 visited the mall twice and did not make any transactions.
Solution
sql

#Solution 1:
SELECT a.customer_id, COUNT(a.visit_id) AS count_no_trans FROM Visits AS a
LEFT JOIN Transactions AS b
ON a.visit_id = b.visit_id
WHERE b.transaction_id IS NULL
GROUP BY a.customer_id;

#Solution 2:
SELECT customer_id, count(visit_id) AS count_no_trans
FROM Visits
WHERE visit_id NOT IN
    (SELECT visit_id
    FROM Transactions
    GROUP BY visit_id)
GROUP BY customer_id
1587. Bank Account Summary II | Easy | 🔒 LeetCode
Table: Users


+--------------+---------+
| Column Name  | Type    |
+--------------+---------+
| account      | int     |
| name         | varchar |
+--------------+---------+
account is the primary key for this table.
Each row of this table contains the account number of each user in the bank.
Table: Transactions


+---------------+---------+
| Column Name   | Type    |
+---------------+---------+
| trans_id      | int     |
| account       | int     |
| amount        | int     |
| transacted_on | date    |
+---------------+---------+
trans_id is the primary key for this table.
Each row of this table contains all changes made to all accounts.
amount is positive if the user received money and negative if they transferred money.
All accounts start with a balance 0.
Write an SQL query to report the name and balance of users with a balance higher than 10000. The balance of an account is equal to the sum of the amounts of all transactions involving that account.

Return the result table in any order.

The query result format is in the following example.


Users table:
+------------+--------------+
| account    | name         |
+------------+--------------+
| 900001     | Alice        |
| 900002     | Bob          |
| 900003     | Charlie      |
+------------+--------------+

Transactions table:
+------------+------------+------------+---------------+
| trans_id   | account    | amount     | transacted_on |
+------------+------------+------------+---------------+
| 1          | 900001     | 7000       |  2020-08-01   |
| 2          | 900001     | 7000       |  2020-09-01   |
| 3          | 900001     | -3000      |  2020-09-02   |
| 4          | 900002     | 1000       |  2020-09-12   |
| 5          | 900003     | 6000       |  2020-08-07   |
| 6          | 900003     | 6000       |  2020-09-07   |
| 7          | 900003     | -4000      |  2020-09-11   |
+------------+------------+------------+---------------+

Result table:
+------------+------------+
| name       | balance    |
+------------+------------+
| Alice      | 11000      |
+------------+------------+
Alice's balance is (7000 + 7000 - 3000) = 11000.
Bob's balance is 1000.
Charlie's balance is (6000 + 6000 - 4000) = 8000.
Solution
sql

#Solution 1:
SELECT u.name AS NAME,SUM(t.amount) AS BALANCE
FROM Transactions t LEFT JOIN Users u
ON u.account = t.account
GROUP BY u.account
HAVING SUM(t.amount)>10000;


#Solution 2:
WITH tmp AS(
SELECT t.account, u.name, SUM(amount) AS balance
FROM Transactions t
LEFT JOIN Users u ON t.account = u.account
GROUP BY account )

SELECT name, balance
FROM tmp
WHERE balance > 10000
1596. The Most Frequently Ordered Products for Each Customer | Medium | 🔒 LeetCode
Table: Customers


+---------------+---------+
| Column Name   | Type    |
+---------------+---------+
| customer_id   | int     |
| name          | varchar |
+---------------+---------+
customer_id is the primary key for this table.
This table contains information about the customers.
Table: Orders


+---------------+---------+
| Column Name   | Type    |
+---------------+---------+
| order_id      | int     |
| order_date    | date    |
| customer_id   | int     |
| product_id    | int     |
+---------------+---------+
order_id is the primary key for this table.
This table contains information about the orders made by customer_id.
No customer will order the same product more than once in a single day.
Table: Products


+---------------+---------+
| Column Name   | Type    |
+---------------+---------+
| product_id    | int     |
| product_name  | varchar |
| price         | int     |
+---------------+---------+
product_id is the primary key for this table.
This table contains information about the products.
Write an SQL query to find the most frequently ordered product(s) for each customer.

The result table should have the product_id and product_name for each customer_id who ordered at least one order. Return the result table in any order.

The query result format is in the following example:


Customers
+-------------+-------+
| customer_id | name  |
+-------------+-------+
| 1           | Alice |
| 2           | Bob   |
| 3           | Tom   |
| 4           | Jerry |
| 5           | John  |
+-------------+-------+

Orders
+----------+------------+-------------+------------+
| order_id | order_date | customer_id | product_id |
+----------+------------+-------------+------------+
| 1        | 2020-07-31 | 1           | 1          |
| 2        | 2020-07-30 | 2           | 2          |
| 3        | 2020-08-29 | 3           | 3          |
| 4        | 2020-07-29 | 4           | 1          |
| 5        | 2020-06-10 | 1           | 2          |
| 6        | 2020-08-01 | 2           | 1          |
| 7        | 2020-08-01 | 3           | 3          |
| 8        | 2020-08-03 | 1           | 2          |
| 9        | 2020-08-07 | 2           | 3          |
| 10       | 2020-07-15 | 1           | 2          |
+----------+------------+-------------+------------+

Products
+------------+--------------+-------+
| product_id | product_name | price |
+------------+--------------+-------+
| 1          | keyboard     | 120   |
| 2          | mouse        | 80    |
| 3          | screen       | 600   |
| 4          | hard disk    | 450   |
+------------+--------------+-------+
Result table:
+-------------+------------+--------------+
| customer_id | product_id | product_name |
+-------------+------------+--------------+
| 1           | 2          | mouse        |
| 2           | 1          | keyboard     |
| 2           | 2          | mouse        |
| 2           | 3          | screen       |
| 3           | 3          | screen       |
| 4           | 1          | keyboard     |
+-------------+------------+--------------+

Alice (customer 1) ordered the mouse three times and the keyboard one time, so the mouse is the most frquently ordered product for them.
Bob (customer 2) ordered the keyboard, the mouse, and the screen one time, so those are the most frquently ordered products for them.
Tom (customer 3) only ordered the screen (two times), so that is the most frquently ordered product for them.
Jerry (customer 4) only ordered the keyboard (one time), so that is the most frquently ordered product for them.
John (customer 5) did not order anything, so we do not include them in the result table.
Solution
sql

#Solution 1:
SELECT customer_id, Products.product_id, Products.product_name FROM
(SELECT customer_id, product_id, order_count, RANK() OVER(PARTITION BY customer_id ORDER BY order_count DESC) r From
(SELECT customer_id, product_id, COUNT(DISTINCT order_id) AS order_count FROM Orders
GROUP BY customer_id, product_id) order_counts) order_counts_ranked
JOIN Products ON order_counts_ranked.product_id = Products.product_id
WHERE r = 1;

#solution- 2:

SELECT customer_id, T.product_id, product_name
FROM(
    SELECT customer_id, product_id,
    RANK() OVER( PARTITION BY customer_id ORDER BY COUNT(*) DESC ) AS RK
    FROM Orders o
    GROUP BY customer_id, product_id
) T
LEFT JOIN Products p on p.product_id = t.product_id
WHERE RK=1


#Solution-3:

WITH

tmp AS (
  SELECT a.customer_id, b.product_id, c.product_name,
  COUNT(b.order_id) OVER(PARTITION BY a.customer_id, b.product_id) AS freq
  FROM Customers AS a
  JOIN Orders AS b
  ON a.customer_id = b.customer_id
  JOIN Products AS c
  ON b.product_id = c.product_id
),

tmp1 AS (
  SELECT customer_id, product_id, product_name, freq,
  DENSE_RANK() OVER(PARTITION BY customer_id ORDER BY freq DESC) AS rnk
  FROM tmp
)

SELECT DISTINCT customer_id, product_id, product_name FROM tmp1
WHERE rnk = 1;
1607. Sellers With No Sales | Easy | 🔒 LeetCode
Table: Customer


+---------------+---------+
| Column Name   | Type    |
+---------------+---------+
| customer_id   | int     |
| customer_name | varchar |
+---------------+---------+
customer_id is the primary key for this table.
Each row of this table contains the information of each customer in the WebStore.
Table: Orders


+---------------+---------+
| Column Name   | Type    |
+---------------+---------+
| order_id      | int     |
| sale_date     | date    |
| order_cost    | int     |
| customer_id   | int     |
| seller_id     | int     |
+---------------+---------+
order_id is the primary key for this table.
Each row of this table contains all orders made in the webstore.
sale_date is the date when the transaction was made between the customer (customer_id) and the seller (seller_id).
Table: Seller


+---------------+---------+
| Column Name   | Type    |
+---------------+---------+
| seller_id     | int     |
| seller_name   | varchar |
+---------------+---------+
seller_id is the primary key for this table.
Each row of this table contains the information of each seller.
Write an SQL query to report the names of all sellers who did not make any sales in 2020.

Return the result table ordered by seller_name in ascending order.

The query result format is in the following example.


Customer table:
+--------------+---------------+
| customer_id  | customer_name |
+--------------+---------------+
| 101          | Alice         |
| 102          | Bob           |
| 103          | Charlie       |
+--------------+---------------+

Orders table:
+-------------+------------+--------------+-------------+-------------+
| order_id    | sale_date  | order_cost   | customer_id | seller_id   |
+-------------+------------+--------------+-------------+-------------+
| 1           | 2020-03-01 | 1500         | 101         | 1           |
| 2           | 2020-05-25 | 2400         | 102         | 2           |
| 3           | 2019-05-25 | 800          | 101         | 3           |
| 4           | 2020-09-13 | 1000         | 103         | 2           |
| 5           | 2019-02-11 | 700          | 101         | 2           |
+-------------+------------+--------------+-------------+-------------+

Seller table:
+-------------+-------------+
| seller_id   | seller_name |
+-------------+-------------+
| 1           | Daniel      |
| 2           | Elizabeth   |
| 3           | Frank       |
+-------------+-------------+

Result table:
+-------------+
| seller_name |
+-------------+
| Frank       |
+-------------+
Daniel made 1 sale in March 2020.
Elizabeth made 2 sales in 2020 and 1 sale in 2019.
Frank made 1 sale in 2019 but no sales in 2020.
Solution
sql

SELECT seller_name FROM Seller
WHERE seller_id NOT IN (
SELECT DISTINCT seller_id FROM Orders
WHERE YEAR(sale_date)='2020'
)
ORDER BY seller_name;
1613. Find the Missing IDs | Medium | 🔒 LeetCode
Table: Customers


+---------------+---------+
| Column Name   | Type    |
+---------------+---------+
| customer_id   | int     |
| customer_name | varchar |
+---------------+---------+
customer_id is the primary key for this table.
Each row of this table contains the name and the id customer.
Write an SQL query to find the missing customer IDs. The missing IDs are ones that are not in the Customers table but are in the range between 1 and the maximum customer_id present in the table.

Notice that the maximum customer_id will not exceed 100.

Return the result table ordered by ids in ascending order.

The query result format is in the following example.


Customer table:
+-------------+---------------+
| customer_id | customer_name |
+-------------+---------------+
| 1           | Alice         |
| 4           | Bob           |
| 5           | Charlie       |
+-------------+---------------+

Result table:
+-----+
| ids |
+-----+
| 2   |
| 3   |
+-----+
The maximum customer_id present in the table is 5, so in the range [1,5], IDs 2 and 3 are missing from the table.
Solution
sql

WITH RECURSIVE CTE AS(
    SELECT 1 AS 'id', MAX(c.customer_id) AS 'Max_Id'
    FROM Customers c
    UNION ALL
    SELECT id+1, Max_Id
    FROM CTE
    WHERE id < Max_id
)

SELECT id AS 'ids'
FROM CTE c
WHERE c.id NOT IN (SELECT customer_id FROM Customers)
ORDER BY 1 ASC
1623. All Valid Triplets That Can Represent a Country | Easy | 🔒 LeetCode
Table: SchoolA


+---------------+---------+
| Column Name   | Type    |
+---------------+---------+
| student_id    | int     |
| student_name  | varchar |
+---------------+---------+
student_id is the primary key for this table.
Each row of this table contains the name and the id of a student in school A.
All student_name are distinct.
Table: SchoolB


+---------------+---------+
| Column Name   | Type    |
+---------------+---------+
| student_id    | int     |
| student_name  | varchar |
+---------------+---------+
student_id is the primary key for this table.
Each row of this table contains the name and the id of a student in school B.
All student_name are distinct.
Table: SchoolC


+---------------+---------+
| Column Name   | Type    |
+---------------+---------+
| student_id    | int     |
| student_name  | varchar |
+---------------+---------+
student_id is the primary key for this table.
Each row of this table contains the name and the id of a student in school C.
All student_name are distinct.
There is a country with three schools, where each student is enrolled in exactly one school. The country is joining a competition and wants to select one student from each school to represent the country such that:

member_A is selected from SchoolA,
member_B is selected from SchoolB,
member_C is selected from SchoolC, and The selected students' names and IDs are pairwise distinct (i.e. no two students share the same name, and no two students share the same ID). Write an SQL query to find all the possible triplets representing the country under the given constraints.
Return the result table in any order.

The query result format is in the following example.


SchoolA table:
+------------+--------------+
| student_id | student_name |
+------------+--------------+
| 1          | Alice        |
| 2          | Bob          |
+------------+--------------+

SchoolB table:
+------------+--------------+
| student_id | student_name |
+------------+--------------+
| 3          | Tom          |
+------------+--------------+

SchoolC table:
+------------+--------------+
| student_id | student_name |
+------------+--------------+
| 3          | Tom          |
| 2          | Jerry        |
| 10         | Alice        |
+------------+--------------+

Result table:
+----------+----------+----------+
| member_A | member_B | member_C |
+----------+----------+----------+
| Alice    | Tom      | Jerry    |
| Bob      | Tom      | Alice    |
+----------+----------+----------+
Let us see all the possible triplets.
- (Alice, Tom, Tom) --> Rejected because member_B and member_C have the same name and the same ID.
- (Alice, Tom, Jerry) --> Valid triplet.
- (Alice, Tom, Alice) --> Rejected because member_A and member_C have the same name.
- (Bob, Tom, Tom) --> Rejected because member_B and member_C have the same name and the same ID.
- (Bob, Tom, Jerry) --> Rejected because member_A and member_C have the same ID.
- (Bob, Tom, Alice) --> Valid triplet.
Solution
sql

SELECT a.student_name AS 'member_A',
b.student_name AS 'member_B',
c.student_name AS 'member_C'
FROM SchoolA AS a
JOIN SchoolB AS b
ON a.student_id <> b.student_id
AND a.student_name <> b.student_name
JOIN SchoolC AS c
ON a.student_id <> c.student_id
AND b.student_id <> c.student_id
AND a.student_name <> c.student_name
AND b.student_name <> c.student_name;
1633. Percentage of Users Attended a Contest | Easy | 🔒 LeetCode
Table: Users


+-------------+---------+
| Column Name | Type    |
+-------------+---------+
| user_id     | int     |
| user_name   | varchar |
+-------------+---------+
user_id is the primary key for this table.
Each row of this table contains the name and the id of a user.
Table: Register


+-------------+---------+
| Column Name | Type    |
+-------------+---------+
| contest_id  | int     |
| user_id     | int     |
+-------------+---------+
(contest_id, user_id) is the primary key for this table.
Each row of this table contains the id of a user and the contest they registered into.
Write an SQL query to find the percentage of the users registered in each contest rounded to two decimals.

Return the result table ordered by percentage in descending order. In case of a tie, order it by contest_id in ascending order.

The query result format is in the following example.


Users table:
+---------+-----------+
| user_id | user_name |
+---------+-----------+
| 6       | Alice     |
| 2       | Bob       |
| 7       | Alex      |
+---------+-----------+

Register table:
+------------+---------+
| contest_id | user_id |
+------------+---------+
| 215        | 6       |
| 209        | 2       |
| 208        | 2       |
| 210        | 6       |
| 208        | 6       |
| 209        | 7       |
| 209        | 6       |
| 215        | 7       |
| 208        | 7       |
| 210        | 2       |
| 207        | 2       |
| 210        | 7       |
+------------+---------+

Result table:
+------------+------------+
| contest_id | percentage |
+------------+------------+
| 208        | 100.0      |
| 209        | 100.0      |
| 210        | 100.0      |
| 215        | 66.67      |
| 207        | 33.33      |
+------------+------------+
All the users registered in contests 208, 209, and 210. The percentage is 100% and we sort them in the answer table in ascending order.
Alice and Alex registered in contest 215 and the percentage is ((2/3) * 100) = 66.67%
Bob registered in contest 207 and the percentage is ((1/3) * 100) = 33.33%
Solution
sql

SELECT contest_id, ROUND(COUNT(user_id)*100.00/(SELECT COUNT(*) FROM users),2) as percentage
FROM register
GROUP BY contest_id
ORDER BY percentage desc, contest_id
1635. Hopper Company Queries I | Hard | 🔒 LeetCode
Table: Drivers


+-------------+---------+
| Column Name | Type    |
+-------------+---------+
| driver_id   | int     |
| join_date   | date    |
+-------------+---------+
driver_id is the primary key for this table.
Each row of this table contains the driver's ID and the date they joined the Hopper company.
Table: Rides


+--------------+---------+
| Column Name  | Type    |
+--------------+---------+
| ride_id      | int     |
| user_id      | int     |
| requested_at | date    |
+--------------+---------+
ride_id is the primary key for this table.
Each row of this table contains the ID of a ride, the user's ID that requested it, and the day they requested it.
There may be some ride requests in this table that were not accepted.
Table: AcceptedRides


+---------------+---------+
| Column Name   | Type    |
+---------------+---------+
| ride_id       | int     |
| driver_id     | int     |
| ride_distance | int     |
| ride_duration | int     |
+---------------+---------+
ride_id is the primary key for this table.
Each row of this table contains some information about an accepted ride.
It is guaranteed that each accepted ride exists in the Rides table.
Write an SQL query to report the following statistics for each month of 2020:

The number of drivers currently with the Hopper company by the end of the month (active_drivers). The number of accepted rides in that month (accepted_rides). Return the result table ordered by month in ascending order, where month is the month's number (January is 1, February is 2, etc.).

The query result format is in the following example.


Drivers table:
+-----------+------------+
| driver_id | join_date  |
+-----------+------------+
| 10        | 2019-12-10 |
| 8         | 2020-1-13  |
| 5         | 2020-2-16  |
| 7         | 2020-3-8   |
| 4         | 2020-5-17  |
| 1         | 2020-10-24 |
| 6         | 2021-1-5   |
+-----------+------------+

Rides table:
+---------+---------+--------------+
| ride_id | user_id | requested_at |
+---------+---------+--------------+
| 6       | 75      | 2019-12-9    |
| 1       | 54      | 2020-2-9     |
| 10      | 63      | 2020-3-4     |
| 19      | 39      | 2020-4-6     |
| 3       | 41      | 2020-6-3     |
| 13      | 52      | 2020-6-22    |
| 7       | 69      | 2020-7-16    |
| 17      | 70      | 2020-8-25    |
| 20      | 81      | 2020-11-2    |
| 5       | 57      | 2020-11-9    |
| 2       | 42      | 2020-12-9    |
| 11      | 68      | 2021-1-11    |
| 15      | 32      | 2021-1-17    |
| 12      | 11      | 2021-1-19    |
| 14      | 18      | 2021-1-27    |
+---------+---------+--------------+

AcceptedRides table:
+---------+-----------+---------------+---------------+
| ride_id | driver_id | ride_distance | ride_duration |
+---------+-----------+---------------+---------------+
| 10      | 10        | 63            | 38            |
| 13      | 10        | 73            | 96            |
| 7       | 8         | 100           | 28            |
| 17      | 7         | 119           | 68            |
| 20      | 1         | 121           | 92            |
| 5       | 7         | 42            | 101           |
| 2       | 4         | 6             | 38            |
| 11      | 8         | 37            | 43            |
| 15      | 8         | 108           | 82            |
| 12      | 8         | 38            | 34            |
| 14      | 1         | 90            | 74            |
+---------+-----------+---------------+---------------+

Result table:
+-------+----------------+----------------+
| month | active_drivers | accepted_rides |
+-------+----------------+----------------+
| 1     | 2              | 0              |
| 2     | 3              | 0              |
| 3     | 4              | 1              |
| 4     | 4              | 0              |
| 5     | 5              | 0              |
| 6     | 5              | 1              |
| 7     | 5              | 1              |
| 8     | 5              | 1              |
| 9     | 5              | 0              |
| 10    | 6              | 0              |
| 11    | 6              | 2              |
| 12    | 6              | 1              |
+-------+----------------+----------------+

By the end of January --> two active drivers (10, 8) and no accepted rides.
By the end of February --> three active drivers (10, 8, 5) and no accepted rides.
By the end of March --> four active drivers (10, 8, 5, 7) and one accepted ride (10).
By the end of April --> four active drivers (10, 8, 5, 7) and no accepted rides.
By the end of May --> five active drivers (10, 8, 5, 7, 4) and no accepted rides.
By the end of June --> five active drivers (10, 8, 5, 7, 4) and one accepted ride (13).
By the end of July --> five active drivers (10, 8, 5, 7, 4) and one accepted ride (7).
By the end of August --> five active drivers (10, 8, 5, 7, 4) and one accepted ride (17).
By the end of Septemeber --> five active drivers (10, 8, 5, 7, 4) and no accepted rides.
By the end of October --> six active drivers (10, 8, 5, 7, 4, 1) and no accepted rides.
By the end of November --> six active drivers (10, 8, 5, 7, 4, 1) and two accepted rides (20, 5).
By the end of December --> six active drivers (10, 8, 5, 7, 4, 1) and one accepted ride (2).
Solution
sql

SELECT t.month,
    COUNT(DISTINCT driver_id) active_drivers,
    COUNT(DISTINCT rides.ride_id) accepted_rides
FROM
    ((SELECT 1 AS month)
    UNION (SELECT 2 AS month)
    UNION (SELECT 3 AS month)
    UNION (SELECT 4 AS month)
    UNION (SELECT 5 AS month)
    UNION (SELECT 6 AS month)
    UNION (SELECT 7 AS month)
    UNION (SELECT 8 AS month)
    UNION (SELECT 9 AS month)
    UNION (SELECT 10 AS month)
    UNION (SELECT 11 AS month)
    UNION (SELECT 12 AS month)) t
LEFT JOIN
    (SELECT driver_id,
    (CASE WHEN year(join_date) = 2019 THEN '1' ELSE month(join_date) END) `month`
    FROM Drivers
    WHERE year(join_date) <= 2020) d
ON d.month <= t.month
LEFT JOIN
    (SELECT month(requested_at) AS `month`, a.ride_id
    FROM AcceptedRides a
    JOIN Rides r
    ON r.ride_id = a.ride_id
    WHERE year(requested_at) = 2020) rides
ON t.month = rides.month
GROUP BY t.month
ORDER BY t.month
1645. Hopper Company Queries II | Hard | 🔒 LeetCode
Table: Drivers


+-------------+---------+
| Column Name | Type    |
+-------------+---------+
| driver_id   | int     |
| join_date   | date    |
+-------------+---------+
driver_id is the primary key for this table.
Each row of this table contains the driver's ID and the date they joined the Hopper company.
Table: Rides


+--------------+---------+
| Column Name  | Type    |
+--------------+---------+
| ride_id      | int     |
| user_id      | int     |
| requested_at | date    |
+--------------+---------+
ride_id is the primary key for this table.
Each row of this table contains the ID of a ride, the user's ID that requested it, and the day they requested it.
There may be some ride requests in this table that were not accepted.
Table: AcceptedRides


+---------------+---------+
| Column Name   | Type    |
+---------------+---------+
| ride_id       | int     |
| driver_id     | int     |
| ride_distance | int     |
| ride_duration | int     |
+---------------+---------+
ride_id is the primary key for this table.
Each row of this table contains some information about an accepted ride.
It is guaranteed that each accepted ride exists in the Rides table.
Write an SQL query to report the percentage of working drivers (working_percentage) for each month of 2020 where:


Note that if the number of available drivers during a month is zero, we consider the working_percentage to be 0.

Return the result table ordered by month in ascending order, where month is the month's number (January is 1, February is 2, etc.). Round working_percentage to the nearest 2 decimal places.

The query result format is in the following example.


Drivers table:
+-----------+------------+
| driver_id | join_date  |
+-----------+------------+
| 10        | 2019-12-10 |
| 8         | 2020-1-13  |
| 5         | 2020-2-16  |
| 7         | 2020-3-8   |
| 4         | 2020-5-17  |
| 1         | 2020-10-24 |
| 6         | 2021-1-5   |
+-----------+------------+

Rides table:
+---------+---------+--------------+
| ride_id | user_id | requested_at |
+---------+---------+--------------+
| 6       | 75      | 2019-12-9    |
| 1       | 54      | 2020-2-9     |
| 10      | 63      | 2020-3-4     |
| 19      | 39      | 2020-4-6     |
| 3       | 41      | 2020-6-3     |
| 13      | 52      | 2020-6-22    |
| 7       | 69      | 2020-7-16    |
| 17      | 70      | 2020-8-25    |
| 20      | 81      | 2020-11-2    |
| 5       | 57      | 2020-11-9    |
| 2       | 42      | 2020-12-9    |
| 11      | 68      | 2021-1-11    |
| 15      | 32      | 2021-1-17    |
| 12      | 11      | 2021-1-19    |
| 14      | 18      | 2021-1-27    |
+---------+---------+--------------+

AcceptedRides table:
+---------+-----------+---------------+---------------+
| ride_id | driver_id | ride_distance | ride_duration |
+---------+-----------+---------------+---------------+
| 10      | 10        | 63            | 38            |
| 13      | 10        | 73            | 96            |
| 7       | 8         | 100           | 28            |
| 17      | 7         | 119           | 68            |
| 20      | 1         | 121           | 92            |
| 5       | 7         | 42            | 101           |
| 2       | 4         | 6             | 38            |
| 11      | 8         | 37            | 43            |
| 15      | 8         | 108           | 82            |
| 12      | 8         | 38            | 34            |
| 14      | 1         | 90            | 74            |
+---------+-----------+---------------+---------------+

Result table:
+-------+--------------------+
| month | working_percentage |
+-------+--------------------+
| 1     | 0.00               |
| 2     | 0.00               |
| 3     | 25.00              |
| 4     | 0.00               |
| 5     | 0.00               |
| 6     | 20.00              |
| 7     | 20.00              |
| 8     | 20.00              |
| 9     | 0.00               |
| 10    | 0.00               |
| 11    | 33.33              |
| 12    | 16.67              |
+-------+--------------------+

By the end of January --> two active drivers (10, 8) and no accepted rides. The percentage is 0%.
By the end of February --> three active drivers (10, 8, 5) and no accepted rides. The percentage is 0%.
By the end of March --> four active drivers (10, 8, 5, 7) and one accepted ride by driver (10). The percentage is (1 / 4) * 100 = 25%.
By the end of April --> four active drivers (10, 8, 5, 7) and no accepted rides. The percentage is 0%.
By the end of May --> five active drivers (10, 8, 5, 7, 4) and no accepted rides. The percentage is 0%.
By the end of June --> five active drivers (10, 8, 5, 7, 4) and one accepted ride by driver (10). The percentage is (1 / 5) * 100 = 20%.
By the end of July --> five active drivers (10, 8, 5, 7, 4) and one accepted ride by driver (8). The percentage is (1 / 5) * 100 = 20%.
By the end of August --> five active drivers (10, 8, 5, 7, 4) and one accepted ride by driver (7). The percentage is (1 / 5) * 100 = 20%.
By the end of Septemeber --> five active drivers (10, 8, 5, 7, 4) and no accepted rides. The percentage is 0%.
By the end of October --> six active drivers (10, 8, 5, 7, 4, 1) and no accepted rides. The percentage is 0%.
By the end of November --> six active drivers (10, 8, 5, 7, 4, 1) and two accepted rides by two different drivers (1, 7). The percentage is (2 / 6) * 100 = 33.33%.
By the end of December --> six active drivers (10, 8, 5, 7, 4, 1) and one accepted ride by driver (4). The percentage is (1 / 6) * 100 = 16.67%.
Solution
sql

SELECT months_drivers.month, ROUND(COALESCE(100 * COALESCE(total_active_drivers, 0) / total_drivers, 0), 2) AS working_percentage
FROM
(
    SELECT month, COUNT(driver_id) AS total_drivers
    FROM Drivers AS a
    RIGHT JOIN
    (
        SELECT "2020-1-31" AS day, 1 AS month
        UNION SELECT "2020-2-29", 2
        UNION SELECT "2020-3-31", 3
        UNION SELECT "2020-4-30", 4
        UNION SELECT "2020-5-31", 5
        UNION SELECT "2020-6-30", 6
        UNION SELECT "2020-7-31", 7
        UNION SELECT "2020-8-31", 8
        UNION SELECT "2020-9-30", 9
        UNION SELECT "2020-10-31", 10
        UNION SELECT "2020-11-30", 11
        UNION SELECT "2020-12-31", 12
    ) AS months
    ON join_date <= day
    GROUP BY month
) months_drivers
LEFT JOIN
(
    SELECT month, COUNT(DISTINCT b.driver_id) AS total_active_drivers
    FROM
    (
        SELECT ride_id, CAST(substr(requested_at, 6, 2) AS unsigned) AS month
        FROM Rides
        WHERE substr(requested_at, 1, 4) = "2020"
    ) month_rides
    JOIN AcceptedRides AS b
    ON month_rides.ride_id = b.ride_id
    GROUP BY month
) months_active_drivers
ON months_drivers.month = months_active_drivers.month;
1651. Hopper Company Queries III | Hard | 🔒 LeetCode
Table: Drivers


+-------------+---------+
| Column Name | Type    |
+-------------+---------+
| driver_id   | int     |
| join_date   | date    |
+-------------+---------+
driver_id is the primary key for this table.
Each row of this table contains the driver's ID and the date they joined the Hopper company.
Table: Rides


+--------------+---------+
| Column Name  | Type    |
+--------------+---------+
| ride_id      | int     |
| user_id      | int     |
| requested_at | date    |
+--------------+---------+
ride_id is the primary key for this table.
Each row of this table contains the ID of a ride, the user's ID that requested it, and the day they requested it.
There may be some ride requests in this table that were not accepted.
Table: AcceptedRides


+---------------+---------+
| Column Name   | Type    |
+---------------+---------+
| ride_id       | int     |
| driver_id     | int     |
| ride_distance | int     |
| ride_duration | int     |
+---------------+---------+
ride_id is the primary key for this table.
Each row of this table contains some information about an accepted ride.
It is guaranteed that each accepted ride exists in the Rides table.
Write an SQL query to compute the average_ride_distance and average_ride_duration of every 3-month window starting from January - March 2020 to October - December 2020. Round average_ride_distance and average_ride_duration to the nearest two decimal places.

The average_ride_distance is calculated by summing up the total ride_distance values from the three months and dividing it by 3. The average_ride_duration is calculated in a similar way.

Return the result table ordered by month in ascending order, where month is the starting month's number (January is 1, February is 2, etc.).

The query result format is in the following example.


Drivers table:
+-----------+------------+
| driver_id | join_date  |
+-----------+------------+
| 10        | 2019-12-10 |
| 8         | 2020-1-13  |
| 5         | 2020-2-16  |
| 7         | 2020-3-8   |
| 4         | 2020-5-17  |
| 1         | 2020-10-24 |
| 6         | 2021-1-5   |
+-----------+------------+

Rides table:
+---------+---------+--------------+
| ride_id | user_id | requested_at |
+---------+---------+--------------+
| 6       | 75      | 2019-12-9    |
| 1       | 54      | 2020-2-9     |
| 10      | 63      | 2020-3-4     |
| 19      | 39      | 2020-4-6     |
| 3       | 41      | 2020-6-3     |
| 13      | 52      | 2020-6-22    |
| 7       | 69      | 2020-7-16    |
| 17      | 70      | 2020-8-25    |
| 20      | 81      | 2020-11-2    |
| 5       | 57      | 2020-11-9    |
| 2       | 42      | 2020-12-9    |
| 11      | 68      | 2021-1-11    |
| 15      | 32      | 2021-1-17    |
| 12      | 11      | 2021-1-19    |
| 14      | 18      | 2021-1-27    |
+---------+---------+--------------+

AcceptedRides table:
+---------+-----------+---------------+---------------+
| ride_id | driver_id | ride_distance | ride_duration |
+---------+-----------+---------------+---------------+
| 10      | 10        | 63            | 38            |
| 13      | 10        | 73            | 96            |
| 7       | 8         | 100           | 28            |
| 17      | 7         | 119           | 68            |
| 20      | 1         | 121           | 92            |
| 5       | 7         | 42            | 101           |
| 2       | 4         | 6             | 38            |
| 11      | 8         | 37            | 43            |
| 15      | 8         | 108           | 82            |
| 12      | 8         | 38            | 34            |
| 14      | 1         | 90            | 74            |
+---------+-----------+---------------+---------------+

Result table:
+-------+-----------------------+-----------------------+
| month | average_ride_distance | average_ride_duration |
+-------+-----------------------+-----------------------+
| 1     | 21.00                 | 12.67                 |
| 2     | 21.00                 | 12.67                 |
| 3     | 21.00                 | 12.67                 |
| 4     | 24.33                 | 32.00                 |
| 5     | 57.67                 | 41.33                 |
| 6     | 97.33                 | 64.00                 |
| 7     | 73.00                 | 32.00                 |
| 8     | 39.67                 | 22.67                 |
| 9     | 54.33                 | 64.33                 |
| 10    | 56.33                 | 77.00                 |
+-------+-----------------------+-----------------------+

By the end of January --> average_ride_distance = (0+0+63)/3=21, average_ride_duration = (0+0+38)/3=12.67
By the end of February --> average_ride_distance = (0+63+0)/3=21, average_ride_duration = (0+38+0)/3=12.67
By the end of March --> average_ride_distance = (63+0+0)/3=21, average_ride_duration = (38+0+0)/3=12.67
By the end of April --> average_ride_distance = (0+0+73)/3=24.33, average_ride_duration = (0+0+96)/3=32.00
By the end of May --> average_ride_distance = (0+73+100)/3=57.67, average_ride_duration = (0+96+28)/3=41.33
By the end of June --> average_ride_distance = (73+100+119)/3=97.33, average_ride_duration = (96+28+68)/3=64.00
By the end of July --> average_ride_distance = (100+119+0)/3=73.00, average_ride_duration = (28+68+0)/3=32.00
By the end of August --> average_ride_distance = (119+0+0)/3=39.67, average_ride_duration = (68+0+0)/3=22.67
By the end of Septemeber --> average_ride_distance = (0+0+163)/3=54.33, average_ride_duration = (0+0+193)/3=64.33
By the end of October --> average_ride_distance = (0+163+6)/3=56.33, average_ride_duration = (0+193+38)/3=77.00
Solution
sql

SELECT month,
    COALESCE(ROUND(SUM(ride_distance)/3,2),0) AS average_ride_distance,
    COALESCE(ROUND(SUM(ride_duration)/3,2),0) AS average_ride_duration
FROM
(
    SELECT months.month, ride_id
    FROM Rides
    RIGHT JOIN
    (
        SELECT "2020-1-1" AS start, "2020-3-31" AS last, 1 AS month
        UNION SELECT "2020-2-1", "2020-4-30", 2
        UNION SELECT "2020-3-1", "2020-5-31", 3
        UNION SELECT "2020-4-1", "2020-6-30", 4
        UNION SELECT "2020-5-1", "2020-7-31", 5
        UNION SELECT "2020-6-1", "2020-8-31", 6
        UNION SELECT "2020-7-1", "2020-9-30", 7
        UNION SELECT "2020-8-1", "2020-10-31", 8
        UNION SELECT "2020-9-1", "2020-11-30", 9
        UNION SELECT "2020-10-1", "2020-12-31", 10
    ) AS months
    ON months.start <= requested_at AND months.last >= requested_at
) total
LEFT JOIN AcceptedRides AS a
ON total.ride_id=a.ride_id
GROUP BY month
ORDER BY month;
1661. Average Time of Process per Machine | Easy | 🔒 LeetCode
Table: Activity


+----------------+---------+
| Column Name    | Type    |
+----------------+---------+
| machine_id     | int     |
| process_id     | int     |
| activity_type  | enum    |
| timestamp      | float   |
+----------------+---------+
The table shows the user activities for a factory website.
(machine_id, process_id, activity_type) is the primary key of this table.
machine_id is the ID of a machine.
process_id is the ID of a process running on the machine with ID machine_id.
activity_type is an ENUM of type ('start', 'end').
timestamp is a float representing the current time in seconds.
'start' means the machine starts the process at the given timestamp and 'end' means the machine ends the process at the given timestamp.
The 'start' timestamp will always be before the 'end' timestamp for every (machine_id, process_id) pair.
There is a factory website that has several machines each running the same number of processes. Write an SQL query to find the average time each machine takes to complete a process.

The time to complete a process is the 'end' timestamp minus the 'start' timestamp. The average time is calculated by the total time to complete every process on the machine divided by the number of processes that were run.

The resulting table should have the machine_id along with the average time as processing_time, which should be rounded to 3 decimal places.

The query result format is in the following example:


Activity table:
+------------+------------+---------------+-----------+
| machine_id | process_id | activity_type | timestamp |
+------------+------------+---------------+-----------+
| 0          | 0          | start         | 0.712     |
| 0          | 0          | end           | 1.520     |
| 0          | 1          | start         | 3.140     |
| 0          | 1          | end           | 4.120     |
| 1          | 0          | start         | 0.550     |
| 1          | 0          | end           | 1.550     |
| 1          | 1          | start         | 0.430     |
| 1          | 1          | end           | 1.420     |
| 2          | 0          | start         | 4.100     |
| 2          | 0          | end           | 4.512     |
| 2          | 1          | start         | 2.500     |
| 2          | 1          | end           | 5.000     |
+------------+------------+---------------+-----------+

Result table:
+------------+-----------------+
| machine_id | processing_time |
+------------+-----------------+
| 0          | 0.894           |
| 1          | 0.995           |
| 2          | 1.456           |
+------------+-----------------+

There are 3 machines running 2 processes each.
Machine 0's average time is ((1.520 - 0.712) + (4.120 - 3.140)) / 2 = 0.894
Machine 1's average time is ((1.550 - 0.550) + (1.420 - 0.430)) / 2 = 0.995
Machine 2's average time is ((4.512 - 4.100) + (5.000 - 2.500)) / 2 = 1.456
Solution
sql

SELECT machine_id,
	ROUND(SUM(IF(activity_type='start', -timestamp, timestamp)) / COUNT(DISTINCT process_id), 3) AS processing_time
FROM Activity
GROUP BY machine_id
ORDER BY machine_id
1667. Fix Names in a Table | Easy | 🔒 LeetCode
Table: Users


+----------------+---------+
| Column Name    | Type    |
+----------------+---------+
| user_id        | int     |
| name           | varchar |
+----------------+---------+
user_id is the primary key for this table.
This table contains the ID and the name of the user. The name consists of only lowercase and uppercase characters.
Write an SQL query to fix the names so that only the first character is uppercase and the rest are lowercase.

Return the result table ordered by user_id.

The query result format is in the following example:


Users table:
+---------+-------+
| user_id | name  |
+---------+-------+
| 1       | aLice |
| 2       | bOB   |
+---------+-------+

Result table:
+---------+-------+
| user_id | name  |
+---------+-------+
| 1       | Alice |
| 2       | Bob   |
+---------+-------+
Solution
sql

select user_id,
    CONCAT(UPPER(LEFT(name,1)),LOWER(SUBSTRING(name,2))) AS name
FROM Users
ORDER BY user_id
1677. Product's Worth Over Invoices | Easy | 🔒 LeetCode
Table: Product


+-------------+---------+
| Column Name | Type    |
+-------------+---------+
| product_id  | int     |
| name        | varchar |
+-------------+---------+
product_id is the primary key for this table.
This table contains the ID and the name of the product. The name consists of only lowercase English letters. No two products have the same name.
Table: Invoice


+-------------+------+
| Column Name | Type |
+-------------+------+
| invoice_id  | int  |
| product_id  | int  |
| rest        | int  |
| paid        | int  |
| canceled    | int  |
| refunded    | int  |
+-------------+------+
invoice_id is the primary key for this table and the id of this invoice.
product_id is the id of the product for this invoice.
rest is the amount left to pay for this invoice.
paid is the amount paid for this invoice.
canceled is the amount canceled for this invoice.
refunded is the amount refunded for this invoice.
Write an SQL query that will, for all products, return each product name with total amount due, paid, canceled, and refunded across all invoices.

Return the result table ordered by product_name.

The query result format is in the following example:


Product table:
+------------+-------+
| product_id | name  |
+------------+-------+
| 0          | ham   |
| 1          | bacon |
+------------+-------+
Invoice table:
+------------+------------+------+------+----------+----------+
| invoice_id | product_id | rest | paid | canceled | refunded |
+------------+------------+------+------+----------+----------+
| 23         | 0          | 2    | 0    | 5        | 0        |
| 12         | 0          | 0    | 4    | 0        | 3        |
| 1          | 1          | 1    | 1    | 0        | 1        |
| 2          | 1          | 1    | 0    | 1        | 1        |
| 3          | 1          | 0    | 1    | 1        | 1        |
| 4          | 1          | 1    | 1    | 1        | 0        |
+------------+------------+------+------+----------+----------+
Result table:
+-------+------+------+----------+----------+
| name  | rest | paid | canceled | refunded |
+-------+------+------+----------+----------+
| bacon | 3    | 3    | 3        | 3        |
| ham   | 2    | 4    | 5        | 3        |
+-------+------+------+----------+----------+
- The amount of money left to pay for bacon is 1 + 1 + 0 + 1 = 3
- The amount of money paid for bacon is 1 + 0 + 1 + 1 = 3
- The amount of money canceled for bacon is 0 + 1 + 1 + 1 = 3
- The amount of money refunded for bacon is 1 + 1 + 1 + 0 = 3
- The amount of money left to pay for ham is 2 + 0 = 2
- The amount of money paid for ham is 0 + 4 = 4
- The amount of money canceled for ham is 5 + 0 = 5
- The amount of money refunded for ham is 0 + 3 = 3
Solution
sql

SELECT p.name AS name,
    SUM(i.rest) AS rest,
    SUM(i.paid) AS paid,
    SUM(i.canceled) AS canceled,
    SUM(i.refunded) AS refunded
FROM Invoice i
LEFT JOIN Product p ON p.product_id = i.product_id
GROUP BY name
ORDER BY name;
1683. Invalid Tweets | Easy | 🔒 LeetCode
Table: Tweets


+----------------+---------+
| Column Name    | Type    |
+----------------+---------+
| tweet_id       | int     |
| content        | varchar |
+----------------+---------+
tweet_id is the primary key for this table.
This table contains all the tweets in a social media app.
Write an SQL query to find the IDs of the invalid tweets. The tweet is invalid if the number of characters used in the content of the tweet is strictly greater than 15.

Return the result table in any order.

The query result format is in the following example:


Tweets table:
+----------+----------------------------------+
| tweet_id | content                          |
+----------+----------------------------------+
| 1        | Vote for Biden                   |
| 2        | Let us make America great again! |
+----------+----------------------------------+

Result table:
+----------+
| tweet_id |
+----------+
| 2        |
+----------+
Tweet 1 has length = 14. It is a valid tweet.
Tweet 2 has length = 32. It is an invalid tweet.
Solution
sql

SELECT tweet_id
FROM Tweets
WHERE LENGTH(content) > 15;
1693. Daily Leads and Partners | Easy | 🔒 LeetCode
Table: DailySales


+-------------+---------+
| Column Name | Type    |
+-------------+---------+
| date_id     | date    |
| make_name   | varchar |
| lead_id     | int     |
| partner_id  | int     |
+-------------+---------+
This table does not have a primary key.
This table contains the date and the name of the product sold and the IDs of the lead and partner it was sold to.
The name consists of only lowercase English letters.
Write an SQL query that will, for each date_id and make_name, return the number of distinct lead_id's and distinct partner_id's.

Return the result table in any order.

The query result format is in the following example:


DailySales table:
+-----------+-----------+---------+------------+
| date_id   | make_name | lead_id | partner_id |
+-----------+-----------+---------+------------+
| 2020-12-8 | toyota    | 0       | 1          |
| 2020-12-8 | toyota    | 1       | 0          |
| 2020-12-8 | toyota    | 1       | 2          |
| 2020-12-7 | toyota    | 0       | 2          |
| 2020-12-7 | toyota    | 0       | 1          |
| 2020-12-8 | honda     | 1       | 2          |
| 2020-12-8 | honda     | 2       | 1          |
| 2020-12-7 | honda     | 0       | 1          |
| 2020-12-7 | honda     | 1       | 2          |
| 2020-12-7 | honda     | 2       | 1          |
+-----------+-----------+---------+------------+
Result table:
+-----------+-----------+--------------+-----------------+
| date_id   | make_name | unique_leads | unique_partners |
+-----------+-----------+--------------+-----------------+
| 2020-12-8 | toyota    | 2            | 3               |
| 2020-12-7 | toyota    | 1            | 2               |
| 2020-12-8 | honda     | 2            | 2               |
| 2020-12-7 | honda     | 3            | 2               |
+-----------+-----------+--------------+-----------------+
For 2020-12-8, toyota gets leads = [0, 1] and partners = [0, 1, 2] while honda gets leads = [1, 2] and partners = [1, 2].
For 2020-12-7, toyota gets leads = [0] and partners = [1, 2] while honda gets leads = [0, 1, 2] and partners = [1, 2].
Solution
sql

SELECT date_id, make_name,
    COUNT(DISTINCT lead_id) AS unique_leads,
    COUNT(DISTINCT partner_id) AS unique_partners
FROM DailySales
GROUP BY date_id, make_name
1699. Number of Calls Between Two Persons | Medium | 🔒 LeetCode
Table: Calls


+-------------+---------+
| Column Name | Type    |
+-------------+---------+
| from_id     | int     |
| to_id       | int     |
| duration    | int     |
+-------------+---------+
This table does not have a primary key, it may contain duplicates.
This table contains the duration of a phone call between from_id and to_id.
from_id != to_id
Write an SQL query to report the number of calls and the total call duration between each pair of distinct persons (person1, person2) where person1 < person2.

Return the result table in any order.

The query result format is in the following example:


Calls table:
+---------+-------+----------+
| from_id | to_id | duration |
+---------+-------+----------+
| 1       | 2     | 59       |
| 2       | 1     | 11       |
| 1       | 3     | 20       |
| 3       | 4     | 100      |
| 3       | 4     | 200      |
| 3       | 4     | 200      |
| 4       | 3     | 499      |
+---------+-------+----------+

Result table:
+---------+---------+------------+----------------+
| person1 | person2 | call_count | total_duration |
+---------+---------+------------+----------------+
| 1       | 2       | 2          | 70             |
| 1       | 3       | 1          | 20             |
| 3       | 4       | 4          | 999            |
+---------+---------+------------+----------------+
Users 1 and 2 had 2 calls and the total duration is 70 (59 + 11).
Users 1 and 3 had 1 call and the total duration is 20.
Users 3 and 4 had 4 calls and the total duration is 999 (100 + 200 + 200 + 499).
Solution
sql

#Solution 1:
SELECT from_id AS person1,to_id AS person2,
    COUNT(duration) AS call_count, SUM(duration) AS total_duration
FROM (SELECT *
      FROM Calls

      UNION ALL

      SELECT to_id, from_id, duration
      FROM Calls) t1
WHERE from_id < to_id
GROUP BY person1, person2

#Solution 2:
SELECT
    IF(from_id<to_id,from_id,to_id) person1,
    IF(from_id>to_id,from_id,to_id) person2,
    COUNT(*) call_count,
    SUM(duration) total_duration
FROM
    Calls
GROUP BY
    IF(from_id<to_id,from_id,to_id),
    IF(from_id>to_id,from_id,to_id);
1709. Biggest Window Between Visits | Medium | 🔒 LeetCode
Table: UserVisits


+-------------+------+
| Column Name | Type |
+-------------+------+
| user_id     | int  |
| visit_date  | date |
+-------------+------+
This table does not have a primary key.
This table contains logs of the dates that users vistied a certain retailer.
Assume today's date is '2021-1-1'.

Write an SQL query that will, for each user_id, find out the largest window of days between each visit and the one right after it (or today if you are considering the last visit).

Return the result table ordered by user_id.

The query result format is in the following example:


UserVisits table:
+---------+------------+
| user_id | visit_date |
+---------+------------+
| 1       | 2020-11-28 |
| 1       | 2020-10-20 |
| 1       | 2020-12-3  |
| 2       | 2020-10-5  |
| 2       | 2020-12-9  |
| 3       | 2020-11-11 |
+---------+------------+
Result table:
+---------+---------------+
| user_id | biggest_window|
+---------+---------------+
| 1       | 39            |
| 2       | 65            |
| 3       | 51            |
+---------+---------------+
For the first user, the windows in question are between dates:
    - 2020-10-20 and 2020-11-28 with a total of 39 days.
    - 2020-11-28 and 2020-12-3 with a total of 5 days.
    - 2020-12-3 and 2021-1-1 with a total of 29 days.
Making the biggest window the one with 39 days.
For the second user, the windows in question are between dates:
    - 2020-10-5 and 2020-12-9 with a total of 65 days.
    - 2020-12-9 and 2021-1-1 with a total of 23 days.
Making the biggest window the one with 65 days.
For the third user, the only window in question is between dates 2020-11-11 and 2021-1-1 with a total of 51 days.
Solution
sql

SELECT user_id, max(diff) AS biggest_window
FROM
(
    SELECT user_id,
       datediff(coalesce(lead(visit_date) OVER (PARTITION BY user_id ORDER BY visit_date), '2021-01-01'), visit_date) AS diff
    FROM userVisits
) t
GROUP BY user_id
ORDER BY user_id
1715. Count Apples and Oranges | Medium | 🔒 LeetCode
Table: Boxes


+--------------+------+
| Column Name  | Type |
+--------------+------+
| box_id       | int  |
| chest_id     | int  |
| apple_count  | int  |
| orange_count | int  |
+--------------+------+
box_id is the primary key for this table.
chest_id is a foreign key of the chests table.
This table contains information about the boxes and the number of oranges and apples they contain. Each box may contain a chest, which also can contain oranges and apples.
Table: Chests


+--------------+------+
| Column Name  | Type |
+--------------+------+
| chest_id     | int  |
| apple_count  | int  |
| orange_count | int  |
+--------------+------+
chest_id is the primary key for this table.
This table contains information about the chests we have, and the corresponding number if oranges and apples they contain.
Write an SQL query to count the number of apples and oranges in all the boxes. If a box contains a chest, you should also include the number of apples and oranges it has.

Return the result table in any order.

The query result format is in the following example:


Boxes table:
+--------+----------+-------------+--------------+
| box_id | chest_id | apple_count | orange_count |
+--------+----------+-------------+--------------+
| 2      | null     | 6           | 15           |
| 18     | 14       | 4           | 15           |
| 19     | 3        | 8           | 4            |
| 12     | 2        | 19          | 20           |
| 20     | 6        | 12          | 9            |
| 8      | 6        | 9           | 9            |
| 3      | 14       | 16          | 7            |
+--------+----------+-------------+--------------+

Chests table:
+----------+-------------+--------------+
| chest_id | apple_count | orange_count |
+----------+-------------+--------------+
| 6        | 5           | 6            |
| 14       | 20          | 10           |
| 2        | 8           | 8            |
| 3        | 19          | 4            |
| 16       | 19          | 19           |
+----------+-------------+--------------+

Result table:
+-------------+--------------+
| apple_count | orange_count |
+-------------+--------------+
| 151         | 123          |
+-------------+--------------+
box 2 has 6 apples and 15 oranges.
box 18 has 4 + 20 (from the chest) = 24 apples and 15 + 10 (from the chest) = 25 oranges.
box 19 has 8 + 19 (from the chest) = 27 apples and 4 + 4 (from the chest) = 8 oranges.
box 12 has 19 + 8 (from the chest) = 27 apples and 20 + 8 (from the chest) = 28 oranges.
box 20 has 12 + 5 (from the chest) = 17 apples and 9 + 6 (from the chest) = 15 oranges.
box 8 has 9 + 5 (from the chest) = 14 apples and 9 + 6 (from the chest) = 15 oranges.
box 3 has 16 + 20 (from the chest) = 36 apples and 7 + 10 (from the chest) = 17 oranges.
Total number of apples = 6 + 24 + 27 + 27 + 17 + 14 + 36 = 151
Total number of oranges = 15 + 25 + 8 + 28 + 15 + 15 + 17 = 123
Solution
sql

SELECT sum(IFNULL(box.apple_count, 0) + IFNULL(chest.apple_count, 0)) AS apple_count,
    sum(IFNULL(box.orange_count, 0) + IFNULL(chest.orange_count, 0)) AS orange_count
    FROM Boxes AS box
    LEFT JOIN Chests AS chest
    ON box.chest_id = chest.chest_id;
1729. Find Followers Count | Easy | 🔒 LeetCode
Table: Followers


+-------------+------+
| Column Name | Type |
+-------------+------+
| user_id     | int  |
| follower_id | int  |
+-------------+------+
(user_id, follower_id) is the primary key for this table.
This table contains the IDs of a user and a follower in a social media app where the follower follows the user.
Write an SQL query that will, for each user, return the number of followers.

Return the result table ordered by user_id.

The query result format is in the following example:


Followers table:
+---------+-------------+
| user_id | follower_id |
+---------+-------------+
| 0       | 1           |
| 1       | 0           |
| 2       | 0           |
| 2       | 1           |
+---------+-------------+
Result table:
+---------+----------------+
| user_id | followers_count|
+---------+----------------+
| 0       | 1              |
| 1       | 1              |
| 2       | 2              |
+---------+----------------+
The followers of 0 are {1}
The followers of 1 are {0}
The followers of 2 are {0,1}
Solution
sql

SELECT user_id, COUNT(DISTINCT follower_id) followers_count
FROM followers
GROUP BY user_id
ORDER BY user_id
1731. The Number of Employees Which Report to Each Employee | Easy | 🔒 LeetCode
Table: Employees


+-------------+----------+
| Column Name | Type     |
+-------------+----------+
| employee_id | int      |
| name        | varchar  |
| reports_to  | int      |
| age         | int      |
+-------------+----------+
employee_id is the primary key for this table.
This table contains information about the employees and the id of the manager they report to. Some employees do not report to anyone (reports_to is null).
For this problem, we will consider a manager an employee who has at least 1 other employee reporting to them.

Write an SQL query to report the ids and the names of all managers, the number of employees who report directly to them, and the average age of the reports rounded to the nearest integer.

Return the result table ordered by employee_id.

The query result format is in the following example:


Employees table:
+-------------+---------+------------+-----+
| employee_id | name    | reports_to | age |
+-------------+---------+------------+-----+
| 9           | Hercy   | null       | 43  |
| 6           | Alice   | 9          | 41  |
| 4           | Bob     | 9          | 36  |
| 2           | Winston | null       | 37  |
+-------------+---------+------------+-----+

Result table:
+-------------+-------+---------------+-------------+
| employee_id | name  | reports_count | average_age |
+-------------+-------+---------------+-------------+
| 9           | Hercy | 2             | 39          |
+-------------+-------+---------------+-------------+
Hercy has 2 people report directly to him, Alice and Bob. Their average age is (41+36)/2 = 38.5, which is 39 after rounding it to the nearest integer.
Solution
sql

SELECT e1.reports_to AS employee_id,
       e2.name,
       COUNT(e1.reports_to) AS reports_count,
       ROUND(AVG(e1.age),0) AS average_age
FROM employees e1
JOIN employees e2
ON e1.reports_to=e2.employee_id
GROUP BY e1.reports_to
ORDER BY e1.reports_to
1741. Find Total Time Spent by Each Employee | Easy | 🔒 LeetCode
Table: Employees


+-------------+------+
| Column Name | Type |
+-------------+------+
| emp_id      | int  |
| event_day   | date |
| in_time     | int  |
| out_time    | int  |
+-------------+------+
(emp_id, event_day, in_time) is the primary key of this table.
The table shows the employees' entries and exits in an office.
event_day is the day at which this event happened and in_time is the minute at which the employee entered the office and out_time is the time at which he got outnumbered from 1 to 1440.
It's guaranteed that no two events on the same day intersect in time.
Write an SQL query to calculate the total time in minutes spent by each employee on each day at the office. Note that within one day, an employee can enter and leave more than once.

Return the result table in any order.

The query result format is in the following example:


Employees table:
+--------+------------+---------+----------+
| emp_id | event_day  | in_time | out_time |
+--------+------------+---------+----------+
| 1      | 2020-11-28 | 4       | 32       |
| 1      | 2020-11-28 | 55      | 200      |
| 1      | 2020-12-03 | 1       | 42       |
| 2      | 2020-11-28 | 3       | 33       |
| 2      | 2020-12-09 | 47      | 74       |
+--------+------------+---------+----------+
Result table:
+------------+--------+------------+
| day        | emp_id | total_time |
+------------+--------+------------+
| 2020-11-28 | 1      | 173        |
| 2020-11-28 | 2      | 30         |
| 2020-12-03 | 1      | 41         |
| 2020-12-09 | 2      | 27         |
+------------+--------+------------+
Employee 1 has three events two on day 2020-11-28 with a total of (32 - 4) + (200-55) = 173 and one on day 2020-12-03 with a total of (42 - 1) = 41.
Employee 2 has two events one on day 2020-11-28 with a total of (33-3) = 30 and one on day 2020-12-09 with a total of (74 - 47) = 27.
Solution
sql

SELECT event_day AS day, emp_id, SUM(out_time - in_time) AS total_time
FROM Employees
GROUP BY day, emp_id
1747. Leetflex Banned Accounts | Medium | 🔒 LeetCode
Table: LogInfo


+-------------+----------+
| Column Name | Type     |
+-------------+----------+
| account_id  | int      |
| ip_address  | int      |
| login       | datetime |
| logout      | datetime |
+-------------+----------+
There is no primary key for this table, and it may contain duplicates.
The table contains information about the login and logout dates of Leetflex accounts. It also contains the IP address from which the account logged in and out.
It is guaranteed that the logout time is after the login time.
Write an SQL query to find the account_id of the accounts that should be banned from Leetflex. An account should be banned if it was logged in at some moment from two different IP addresses.

Return the result table in any order.

The query result format is in the following example:


LogInfo table:
+------------+------------+---------------------+---------------------+
| account_id | ip_address | login               | logout              |
+------------+------------+---------------------+---------------------+
| 1          | 1          | 2021-02-01 09:00:00 | 2021-02-01 09:30:00 |
| 1          | 2          | 2021-02-01 08:00:00 | 2021-02-01 11:30:00 |
| 2          | 6          | 2021-02-01 20:30:00 | 2021-02-01 22:00:00 |
| 2          | 7          | 2021-02-02 20:30:00 | 2021-02-02 22:00:00 |
| 3          | 9          | 2021-02-01 16:00:00 | 2021-02-01 16:59:59 |
| 3          | 13         | 2021-02-01 17:00:00 | 2021-02-01 17:59:59 |
| 4          | 10         | 2021-02-01 16:00:00 | 2021-02-01 17:00:00 |
| 4          | 11         | 2021-02-01 17:00:00 | 2021-02-01 17:59:59 |
+------------+------------+---------------------+---------------------+

Result table:
+------------+
| account_id |
+------------+
| 1          |
| 4          |
+------------+
Account ID 1 --> The account was active from "2021-02-01 09:00:00" to "2021-02-01 09:30:00" with two different IP addresses (1 and 2). It should be banned.
Account ID 2 --> The account was active from two different addresses (6, 7) but in two different times.
Account ID 3 --> The account was active from two different addresses (9, 13) on the same day but they do not intersect at any moment.
Account ID 4 --> The account was active from "2021-02-01 17:00:00" to "2021-02-01 17:00:00" with two different IP addresses (10 and 11). It should be banned.
Solution
sql

SELECT DISTINCT l1.account_id
FROM LogInfo l1
JOIN LogInfo l2
ON l1.account_id = l2.account_id AND l1.ip_address != l2.ip_address
WHERE NOT (l1.login > l2.logout OR l1.logout < l2.login)
1757. Recyclable and Low Fat Products | Easy | 🔒 LeetCode
Table: Products


+-------------+---------+
| Column Name | Type    |
+-------------+---------+
| product_id  | int     |
| low_fats    | enum    |
| recyclable  | enum    |
+-------------+---------+
product_id is the primary key for this table.
low_fats is an ENUM of type ('Y', 'N') where 'Y' means this product is low fat and 'N' means it is not.
recyclable is an ENUM of types ('Y', 'N') where 'Y' means this product is recyclable and 'N' means it is not.
Write an SQL query to find the ids of products that are both low fat and recyclable.

Return the result table in any order.

The query result format is in the following example:


Products table:
+-------------+----------+------------+
| product_id  | low_fats | recyclable |
+-------------+----------+------------+
| 0           | Y        | N          |
| 1           | Y        | Y          |
| 2           | N        | Y          |
| 3           | Y        | Y          |
| 4           | N        | N          |
+-------------+----------+------------+
Result table:
+-------------+
| product_id  |
+-------------+
| 1           |
| 3           |
+-------------+
Only products 1 and 3 are both low fat and recyclable.
Solution
sql

SELECT product_id
FROM Products
WHERE low_fats = "Y" AND recyclable = "Y"
1767. Find the Subtasks That Did Not Execute | Hard | 🔒 LeetCode
Table: Tasks


+----------------+---------+
| Column Name    | Type    |
+----------------+---------+
| task_id        | int     |
| subtasks_count | int     |
+----------------+---------+
task_id is the primary key for this table.
Each row in this table indicates that task_id was divided into subtasks_count subtasks labelled from 1 to subtasks_count.
It is guaranteed that 2 <= subtasks_count <= 20.
Table: Executed


+---------------+---------+
| Column Name   | Type    |
+---------------+---------+
| task_id       | int     |
| subtask_id    | int     |
+---------------+---------+
(task_id, subtask_id) is the primary key for this table.
Each row in this table indicates that for the task task_id, the subtask with ID subtask_id was executed successfully.
It is guaranteed that subtask_id <= subtasks_count for each task_id.
Write an SQL query to report the IDs of the missing subtasks for each task_id.

Return the result table in any order.

The query result format is in the following example:


Tasks table:
+---------+----------------+
| task_id | subtasks_count |
+---------+----------------+
| 1       | 3              |
| 2       | 2              |
| 3       | 4              |
+---------+----------------+

Executed table:
+---------+------------+
| task_id | subtask_id |
+---------+------------+
| 1       | 2          |
| 3       | 1          |
| 3       | 2          |
| 3       | 3          |
| 3       | 4          |
+---------+------------+

Result table:
+---------+------------+
| task_id | subtask_id |
+---------+------------+
| 1       | 1          |
| 1       | 3          |
| 2       | 1          |
| 2       | 2          |
+---------+------------+
Task 1 was divided into 3 subtasks (1, 2, 3). Only subtask 2 was executed successfully, so we include (1, 1) and (1, 3) in the answer.
Task 2 was divided into 2 subtasks (1, 2). No subtask was executed successfully, so we include (2, 1) and (2, 2) in the answer.
Task 3 was divided into 4 subtasks (1, 2, 3, 4). All of the subtasks were executed successfully.
Solution
sql

WITH RECURSIVE CTE AS
  (SELECT 1 AS subtask_id
   UNION ALL SELECT subtask_id + 1
   FROM CTE
   WHERE subtask_id <
       (SELECT MAX(subtasks_count)
        FROM Tasks) )
SELECT Tasks.task_id,
       CTE.subtask_id
FROM CTE
INNER JOIN Tasks ON CTE.subtask_id <= Tasks.subtasks_count
LEFT JOIN Executed ON Tasks.task_id = Executed.task_id
                      AND CTE.subtask_id = Executed.subtask_id
WHERE Executed.subtask_id IS NULL
ORDER BY NULL
1777. Product's Price for Each Store | Easy | 🔒 LeetCode
Table: Products


+-------------+---------+
| Column Name | Type    |
+-------------+---------+
| product_id  | int     |
| store       | enum    |
| price       | int     |
+-------------+---------+
(product_id,store) is the primary key for this table.
store is an ENUM of type ('store1', 'store2', 'store3') where each represents the store this product is available at.
price is the price of the product at this store.
Write an SQL query to find the price of each product in each store.

Return the result table in any order.

The query result format is in the following example:


Products table:
+-------------+--------+-------+
| product_id  | store  | price |
+-------------+--------+-------+
| 0           | store1 | 95    |
| 0           | store3 | 105   |
| 0           | store2 | 100   |
| 1           | store1 | 70    |
| 1           | store3 | 80    |
+-------------+--------+-------+
Result table:
+-------------+--------+--------+--------+
| product_id  | store1 | store2 | store3 |
+-------------+--------+--------+--------+
| 0           | 95     | 100    | 105    |
| 1           | 70     | null   | 80     |
+-------------+--------+--------+--------+
Product 0 price's are 95 for store1, 100 for store2 and, 105 for store3.
Product 1 price's are 70 for store1, 80 for store3 and, it's not sold in store2.
Solution
sql

SELECT product_id,
       SUM(CASE WHEN store='store1' THEN price END) AS store1,
       SUM(CASE WHEN store='store2' THEN price END) AS store2,
       SUM(CASE WHEN store='store3' THEN price END) AS store3
FROM Products
GROUP BY product_id
1783. Grand Slam Titles | Medium | 🔒 LeetCode
Table: Players


+----------------+---------+
| Column Name    | Type    |
+----------------+---------+
| player_id      | int     |
| player_name    | varchar |
+----------------+---------+
player_id is the primary key for this table.
Each row in this table contains the name and the ID of a tennis player.
Table: Championships


+---------------+---------+
| Column Name   | Type    |
+---------------+---------+
| year          | int     |
| Wimbledon     | int     |
| Fr_open       | int     |
| US_open       | int     |
| Au_open       | int     |
+---------------+---------+
year is the primary key for this table.
Each row of this table containts the IDs of the players who won one each tennis tournament of the grand slam.
Write an SQL query to report the number of grand slam tournaments won by each player. Do not include the players who did not win any tournament.

Return the result table in any order.

The query result format is in the following example:


Players table:
+-----------+-------------+
| player_id | player_name |
+-----------+-------------+
| 1         | Nadal       |
| 2         | Federer     |
| 3         | Novak       |
+-----------+-------------+

Championships table:
+------+-----------+---------+---------+---------+
| year | Wimbledon | Fr_open | US_open | Au_open |
+------+-----------+---------+---------+---------+
| 2018 | 1         | 1       | 1       | 1       |
| 2019 | 1         | 1       | 2       | 2       |
| 2020 | 2         | 1       | 2       | 2       |
+------+-----------+---------+---------+---------+

Result table:
+-----------+-------------+-------------------+
| player_id | player_name | grand_slams_count |
+-----------+-------------+-------------------+
| 2         | Federer     | 5                 |
| 1         | Nadal       | 7                 |
+-----------+-------------+-------------------+

Player 1 (Nadal) won 7 titles: Wimbledon (2018, 2019), Fr_open (2018, 2019, 2020), US_open (2018), and Au_open (2018).
Player 2 (Federer) won 5 titles: Wimbledon (2020), US_open (2019, 2020), and Au_open (2019, 2020).
Player 3 (Novak) did not win anything, we did not include them in the result table.
Solution
sql

#Solution 1:
SELECT player_id, player_name,
	SUM((IF(Wimbledon = player_id,1,0) +
		IF(Fr_open = player_id,1,0) +
		IF(US_open = player_id,1,0) +
		IF(Au_open = player_id,1,0))) as grand_slams_count
FROM Players INNER JOIN Championships
ON Wimbledon = player_id OR Fr_open = player_id OR US_open = player_id OR Au_open = player_id
GROUP BY player_id;

#Solution 2:
WITH cte
     AS (SELECT wimbledon AS id
         FROM   championships
         UNION ALL
         SELECT fr_open AS id
         FROM   championships
         UNION ALL
         SELECT us_open AS id
         FROM   championships
         UNION ALL
         SELECT au_open AS id
         FROM   championships)
SELECT player_id,
       player_name,
       Count(*) AS grand_slams_count
FROM   players
       INNER JOIN cte
               ON players.player_id = cte.id
GROUP  BY 1, 2
ORDER  BY NULL;
1789. Primary Department for Each Employee | Easy | 🔒 LeetCode
Table: Employee


+--------------+---------+
| Column Name  |  Type   |
+--------------+---------+
| employee_id  | int     |
| deprtment_id | int     |
| primary_flag | varchar |
+--------------+---------+
(employee_id, department_id) is the primary key for this table.
employee_id is the id of the employee.
department_id is the id of the department to which the employee belongs.
primary_flag is an ENUM of type ('Y', 'N'). If the flag is 'Y', the department is the primary department for the employee. If the flag is 'N', the department is not the primary.
Employees can belong to multiple departments. When the employee joins other departments, they need to decide which department is their primary department. Note that when an employee belongs to only one department, their primary column is 'N'.

Write an SQL query to report all the employees with their primary department. For employees who belong to one department, report their only department.

Return the result table in any order.

The query result format is in the following example.


Employee table:
+-------------+---------------+--------------+
| employee_id | department_id | primary_flag |
+-------------+---------------+--------------+
| 1           | 1             | N            |
| 2           | 1             | Y            |
| 2           | 2             | N            |
| 3           | 3             | N            |
| 4           | 2             | N            |
| 4           | 3             | Y            |
| 4           | 4             | N            |
+-------------+---------------+--------------+

Result table:
+-------------+---------------+
| employee_id | department_id |
+-------------+---------------+
| 1           | 1             |
| 2           | 1             |
| 3           | 3             |
| 4           | 3             |
+-------------+---------------+
- The Primary department for employee 1 is 1.
- The Primary department for employee 2 is 1.
- The Primary department for employee 3 is 3.
- The Primary department for employee 4 is 3.
Solution
sql

#Solution 1:
SELECT employee_id,department_id
FROM employee
WHERE primary_flag = 'Y' OR employee_id IN
    (SELECT employee_id
     FROM employee
     GROUP BY employee_id
     HAVING COUNT(department_id) = 1)

#Solution 2:
(SELECT employee_id,
      department_id
FROM Employee
WHERE primary_flag = 'Y')
UNION
(SELECT employee_id,
      department_id
FROM Employee
GROUP BY employee_id
HAVING COUNT(employee_id) = 1
ORDER BY NULL);
Advanced SQL

Leveling up

SQL Data Types

SQL Date Format

Data Wrangling with SQL

Using SQL String Functions to Clean Data

Writing Subqueries in SQL

SQL Window Functions

Performance Tuning SQL Queries

Pivoting Data in SQL

Intermediate SQL

Putting it together

SQL Aggregate Functions

SQL COUNT

SQL SUM

SQL MIN/MAX

SQL AVG

SQL GROUP BY

SQL HAVING

SQL CASE

SQL DISTINCT

SQL Joins

SQL INNER JOIN

SQL Outer Joins

SQL LEFT JOIN

SQL RIGHT JOIN

SQL Joins Using WHERE or ON

SQL FULL OUTER JOIN

SQL UNION

SQL Joins with Comparison Operators

SQL Joins on Multiple Keys

SQL Self Joins
Aggregation Patterns

Tally Count
Group Bucket
Rolling Totals


Filtering Patterns

Patch Gaps
Existence Check
Deduplication
Pattern Search


Comparison Patterns

Set Compare
Join Variants
Nested Query
Practice Comparison



Sequencing and Hierarchical Patterns

Rank and Row
Lead-Lag Navigation
Recursive Tree
Gaps and Islands
Practice Sequence and Hierarchy


Transformation Patterns

Pivot Flip
Practice Transformation









Airbnb Business Analyst Interview Guide – Process, Questions & Tips
Next, you’ll tackle a technical screen that combines a 30-minute HackerRank SQL assessment with a short case study or deck critique. You’ll be challenged to write queries, interpret data, and solve real-world business problems—often referencing an Airbnb business analytics tool you’ve used or built. This stage tests your ability to extract insights from complex datasets, apply statistical reasoning, and communicate your findings clearly. Expect to demonstrate proficiency in SQL, Python, and data visualization, as well as your approach to A/B testing and business metric analysis. Your performance here is crucial, as it directly reflects your readiness to drive impact at scale.
SQL / Technical Questions
Expect to showcase your Airbnb advanced analytics chops with SQL questions that assess your ability to manipulate complex datasets, detect patterns, and deliver actionable insights for business decisions:

1. Find the total salary of slacking employees

To solve this, use an INNER JOIN to combine the employees and projects tables, filtering for employees who have at least one project assigned but no completed projects (End_dt IS NULL). Group by employee ID and use HAVING COUNT(p.End_dt) = 0 to identify slacking employees. Finally, sum their salaries using a subquery.

2. Write a query to get the average commute time for each commuter in New York

To solve this, use two subqueries: one to calculate the average commute time for each commuter in New York grouped by commuter_id, and another to calculate the overall average commute time across all commuters in New York. Use the TIMESTAMPDIFF function to calculate the duration of each ride in minutes, and then join the results to display both averages in the output.

3. Write a query to retrieve all user IDs whose transactions have exactly a 10-second gap

To solve this, use the LAG() and LEAD() window functions to calculate the time difference between consecutive transactions. Filter the results to include only those transactions with a 10-second gap and return the distinct user IDs in ascending order.

4. Find the average number of accepted friend requests for each age group

To solve this, use a RIGHT JOIN between the requests_accepted and age_groups tables to associate accepted friend requests with age groups. Calculate the average acceptance by dividing the count of accepted requests by the count of unique users in each age group, grouping by age_group, and ordering the results in descending order.

5. Cumulative Sales Since Last Restocking

To calculate cumulative sales since the last restocking, first identify the latest restocking date for each product using the MAX() function grouped by product_id. Then, use a window function SUM(...) OVER() to compute the running total of sales for each product after its last restocking date. Join the sales, products, and the derived table of last restocking dates, filtering sales that occurred after the last restocking date.

Case Study & Forecasting Questions
These questions test how you translate data into forward-looking strategy, often asking you to forecast metrics or build models that align with Airbnb, Inc. forecast and analysis efforts:

6. How would you build a dynamic pricing system for Airbnb based on demand and availability?

To build a dynamic pricing system, gather data on demand, availability, seasonality, and external factors like local events. Use machine learning models, such as regression or reinforcement learning, to predict optimal prices. Consider factors like user behavior, competitor pricing, and elasticity of demand while ensuring the system adapts to real-time changes.

7. How would you forecast revenue for the next year?

To forecast revenue for the next year, analyze historical revenue data for Facebook’s various revenue streams, considering attributes like seasonality and trends. Depending on the behavior of each stream, use models such as classical time series forecasting, ARMA, or ARIMA to predict future revenue, and sum forecasts for all streams to estimate total revenue.

8. Given a task to predict hotel occupancy rates, how would you design the model?

To design the model, start by collecting relevant data such as historical occupancy rates, booking trends, seasonality, holidays, and external factors like local events or weather. Use this data to train a machine learning model, such as regression or time-series forecasting. Evaluate the model’s performance using metrics like Mean Absolute Error (MAE) or Root Mean Square Error (RMSE) to ensure accuracy.

9. Call Center Resource Management

To address this problem, a predictive model such as a time-series forecasting model or a regression model can be used to predict call volumes and allocate agents accordingly. Metrics to evaluate the model include accuracy in predicting call volumes, agent utilization rates, and customer wait times. Over-allocation ensures better customer satisfaction but may increase costs, while under-allocation risks longer wait times and lower customer satisfaction. Balancing these trade-offs is key to determining the optimal allocation strategy.

10. How would you build a function to return a list of daily forecasted revenue starting from Day 1 to the end of the quarter (Day N)?

To solve this, calculate the daily growth rate by dividing the difference between the total revenue target and Day 1 revenue by the number of days minus one. Then, iteratively compute the revenue for each day by adding the daily growth rate to the previous day’s revenue, storing the results in a list.

Dashboard & Tooling Questions
You’ll be asked to critique and design visualizations, evaluate metrics, and improve user understanding using tools like Minerva and Superset, often within the context of an Airbnb analytics dashboard or other Airbnb analysis software:

11. How would you visualize data with long tail text to effectively convey its characteristics and help extract actionable insights?

To visualize long-tail text data, start with frequency distribution plots like log-log scale histograms or Zipf plots to highlight keyword occurrences. Use semantic analysis techniques such as word clouds or clustering methods like t-SNE to uncover patterns. Integrate these insights with conversion metrics and temporal trends into dashboards for actionable business decisions.

12. Design a dashboard that provides personalized insights, sales forecasts, and inventory recommendations for shop owners

To design a merchant dashboard, prioritize metrics like sales trends (e.g., week-over-week revenue changes), inventory insights (e.g., days remaining for stock), and customer behavior (e.g., repeat purchase rates). Use adaptive visualizations tailored to merchant types, such as cohort charts for customer segmentation or smart banners highlighting actionable insights. Ensure scalability by leveraging metadata to personalize layouts and validate utility through merchant interaction tracking.

13. Interpreting Fraud Detection Trends

To interpret fraud detection graphs, focus on identifying anomalies, spikes, or patterns in fraudulent activities over time. Key insights include understanding the frequency, timing, and types of fraud, which can help refine detection algorithms and implement preventive measures. Use these insights to improve fraud detection processes by enhancing model accuracy, updating rules, and deploying real-time monitoring systems.

14. Critique a Minerva dashboard that shows booking trends by city. What would you improve?

Start by evaluating clarity and usefulness. Are city-level metrics aggregated at appropriate levels? Look for over-cluttered visualizations, inconsistent color schemes, or missing comparison baselines like year-over-year trends. Suggest adding filters for dates, guest demographics, and property types. Finally, recommend aligning KPI definitions across the airbnb analytics dashboard so regional teams interpret the data consistently.

Behavioral & Values Questions
Here, interviewers are looking to understand how you collaborate, communicate, and live out Airbnb’s values—especially in moments of ambiguity, cross-functional tension, or high-stakes decision-making:

15. How would you convey insights and the methods you use to a non-technical audience?

At Airbnb, business analysts frequently collaborate with design, operations, and regional teams that may not have technical expertise. You should describe how you structure your insights using storytelling, visualizations, and real-world analogies. For example, you might walk through how you used clustering to segment guests by travel behavior, then presented clear visuals and trade-offs to help a marketing team prioritize a campaign. Focus on simplifying without oversimplifying.

16. What do you tell an interviewer when they ask you what your strengths and weaknesses are?

Tailor your answer to qualities that align with Airbnb’s values. For strengths, consider areas like strong SQL proficiency, experience with experimentation platforms, or a track record of turning ambiguous problems into structured analysis. For weaknesses, avoid clichés. Instead, be honest and share how you’ve addressed it. For example, you might say you previously over-indexed on perfection in dashboards but learned that speed-to-insight is more critical in fast-paced teams like Airbnb’s regional operations.

17. Why Do You Want to Work With Us

Go beyond generic admiration. In 2025, Airbnb continues to lead in the intersection of travel, trust, and technology. Reference your alignment with Airbnb’s mission and how you’re excited to contribute to key initiatives, such as sustainable travel growth or optimizing the host onboarding experience. Mention interest in working with global teams, using real-time data to drive product or marketplace strategy, and leveraging Airbnb’s strong analytics infrastructure to make meaningful impact.

18. Talk about a time when you had trouble communicating with stakeholders. How were you able to overcome it?

Airbnb analysts often sit between product, operations, and regional leadership. When there is disagreement or confusion, it is your role to bridge data and narrative. Provide a story where you had to identify why your insight wasn’t resonating, then explain how you reframed the data or involved the stakeholders earlier in the analysis process. Focus on active listening, using visual tools, or scenario modeling to create alignment and improve decision-making.

How to Prepare for a Business Analyst Role at Airbnb
To excel in the Airbnb business analyst interview, you’ll want to master both technical and strategic preparation. Start by honing your SQL and Tableau skills, as you’ll be expected to analyze complex datasets and visualize insights using an airbnb business analytics tool. Practice writing advanced queries that extract actionable trends from booking, pricing, and review data—these are core to the technical screens and on-site exercises. Airbnb’s interviewers value candidates who can not only manipulate data but also translate findings into business recommendations using clear, compelling dashboards.

Building your own forecasting projects is a powerful way to stand out. Leverage the open-source Airbnb review dataset, which contains millions of real guest reviews and listing details from global cities. Use this dataset to practice exploratory data analysis, sentiment mining, and predictive modeling—skills that directly mirror the challenges you’ll face at Airbnb. For example, you might forecast review scores or booking trends using Python and Tableau, then present your findings as you would to a cross-functional team. Practice with our AI Interviewer to gain more clarity on the approach to the ideal answers.

Equally important is your ability to tell stories with data and align with Airbnb’s core values. Use the STAR method (Situation, Task, Action, Result) to structure your behavioral answers, and be ready to demonstrate how you “Champion the Mission” by connecting your work to Airbnb’s vision of belonging and community impact.

Finally, simulate the interview environment with mock interviews and peer feedback. Benchmark your SQL speed and accuracy on data-driven platforms, and seek out realistic case studies to sharpen your business sense. This holistic, hands-on approach will help you approach the process with confidence and clarity, ready to make a measurable impact from day one.

SQL / Technical Questions
Expect to showcase your Airbnb advanced analytics chops with SQL questions that assess your ability to manipulate complex datasets, detect patterns, and deliver actionable insights for business decisions:

1. Find the total salary of slacking employees

To solve this, use an INNER JOIN to combine the employees and projects tables, filtering for employees who have at least one project assigned but no completed projects (End_dt IS NULL). Group by employee ID and use HAVING COUNT(p.End_dt) = 0 to identify slacking employees. Finally, sum their salaries using a subquery.

2. Write a query to get the average commute time for each commuter in New York

To solve this, use two subqueries: one to calculate the average commute time for each commuter in New York grouped by commuter_id, and another to calculate the overall average commute time across all commuters in New York. Use the TIMESTAMPDIFF function to calculate the duration of each ride in minutes, and then join the results to display both averages in the output.

3. Write a query to retrieve all user IDs whose transactions have exactly a 10-second gap

To solve this, use the LAG() and LEAD() window functions to calculate the time difference between consecutive transactions. Filter the results to include only those transactions with a 10-second gap and return the distinct user IDs in ascending order.

4. Find the average number of accepted friend requests for each age group

To solve this, use a RIGHT JOIN between the requests_accepted and age_groups tables to associate accepted friend requests with age groups. Calculate the average acceptance by dividing the count of accepted requests by the count of unique users in each age group, grouping by age_group, and ordering the results in descending order.

5. Cumulative Sales Since Last Restocking

To calculate cumulative sales since the last restocking, first identify the latest restocking date for each product using the MAX() function grouped by product_id. Then, use a window function SUM(...) OVER() to compute the running total of sales for each product after its last restocking date. Join the sales, products, and the derived table of last restocking dates, filtering sales that occurred after the last restocking date.

Case Study & Forecasting Questions
These questions test how you translate data into forward-looking strategy, often asking you to forecast metrics or build models that align with Airbnb, Inc. forecast and analysis efforts:

6. How would you build a dynamic pricing system for Airbnb based on demand and availability?

To build a dynamic pricing system, gather data on demand, availability, seasonality, and external factors like local events. Use machine learning models, such as regression or reinforcement learning, to predict optimal prices. Consider factors like user behavior, competitor pricing, and elasticity of demand while ensuring the system adapts to real-time changes.

7. How would you forecast revenue for the next year?

To forecast revenue for the next year, analyze historical revenue data for Facebook’s various revenue streams, considering attributes like seasonality and trends. Depending on the behavior of each stream, use models such as classical time series forecasting, ARMA, or ARIMA to predict future revenue, and sum forecasts for all streams to estimate total revenue.

8. Given a task to predict hotel occupancy rates, how would you design the model?

To design the model, start by collecting relevant data such as historical occupancy rates, booking trends, seasonality, holidays, and external factors like local events or weather. Use this data to train a machine learning model, such as regression or time-series forecasting. Evaluate the model’s performance using metrics like Mean Absolute Error (MAE) or Root Mean Square Error (RMSE) to ensure accuracy.

9. Call Center Resource Management

To address this problem, a predictive model such as a time-series forecasting model or a regression model can be used to predict call volumes and allocate agents accordingly. Metrics to evaluate the model include accuracy in predicting call volumes, agent utilization rates, and customer wait times. Over-allocation ensures better customer satisfaction but may increase costs, while under-allocation risks longer wait times and lower customer satisfaction. Balancing these trade-offs is key to determining the optimal allocation strategy.

10. How would you build a function to return a list of daily forecasted revenue starting from Day 1 to the end of the quarter (Day N)?

To solve this, calculate the daily growth rate by dividing the difference between the total revenue target and Day 1 revenue by the number of days minus one. Then, iteratively compute the revenue for each day by adding the daily growth rate to the previous day’s revenue, storing the results in a list.

Dashboard & Tooling Questions
You’ll be asked to critique and design visualizations, evaluate metrics, and improve user understanding using tools like Minerva and Superset, often within the context of an Airbnb analytics dashboard or other Airbnb analysis software:

11. How would you visualize data with long tail text to effectively convey its characteristics and help extract actionable insights?

To visualize long-tail text data, start with frequency distribution plots like log-log scale histograms or Zipf plots to highlight keyword occurrences. Use semantic analysis techniques such as word clouds or clustering methods like t-SNE to uncover patterns. Integrate these insights with conversion metrics and temporal trends into dashboards for actionable business decisions.

12. Design a dashboard that provides personalized insights, sales forecasts, and inventory recommendations for shop owners

To design a merchant dashboard, prioritize metrics like sales trends (e.g., week-over-week revenue changes), inventory insights (e.g., days remaining for stock), and customer behavior (e.g., repeat purchase rates). Use adaptive visualizations tailored to merchant types, such as cohort charts for customer segmentation or smart banners highlighting actionable insights. Ensure scalability by leveraging metadata to personalize layouts and validate utility through merchant interaction tracking.

13. Interpreting Fraud Detection Trends

To interpret fraud detection graphs, focus on identifying anomalies, spikes, or patterns in fraudulent activities over time. Key insights include understanding the frequency, timing, and types of fraud, which can help refine detection algorithms and implement preventive measures. Use these insights to improve fraud detection processes by enhancing model accuracy, updating rules, and deploying real-time monitoring systems.

14. Critique a Minerva dashboard that shows booking trends by city. What would you improve?

Start by evaluating clarity and usefulness. Are city-level metrics aggregated at appropriate levels? Look for over-cluttered visualizations, inconsistent color schemes, or missing comparison baselines like year-over-year trends. Suggest adding filters for dates, guest demographics, and property types. Finally, recommend aligning KPI definitions across the airbnb analytics dashboard so regional teams interpret the data consistently.

Behavioral & Values Questions
Here, interviewers are looking to understand how you collaborate, communicate, and live out Airbnb’s values—especially in moments of ambiguity, cross-functional tension, or high-stakes decision-making:

15. How would you convey insights and the methods you use to a non-technical audience?

At Airbnb, business analysts frequently collaborate with design, operations, and regional teams that may not have technical expertise. You should describe how you structure your insights using storytelling, visualizations, and real-world analogies. For example, you might walk through how you used clustering to segment guests by travel behavior, then presented clear visuals and trade-offs to help a marketing team prioritize a campaign. Focus on simplifying without oversimplifying.

16. What do you tell an interviewer when they ask you what your strengths and weaknesses are?

Tailor your answer to qualities that align with Airbnb’s values. For strengths, consider areas like strong SQL proficiency, experience with experimentation platforms, or a track record of turning ambiguous problems into structured analysis. For weaknesses, avoid clichés. Instead, be honest and share how you’ve addressed it. For example, you might say you previously over-indexed on perfection in dashboards but learned that speed-to-insight is more critical in fast-paced teams like Airbnb’s regional operations.

17. Why Do You Want to Work With Us

Go beyond generic admiration. In 2025, Airbnb continues to lead in the intersection of travel, trust, and technology. Reference your alignment with Airbnb’s mission and how you’re excited to contribute to key initiatives, such as sustainable travel growth or optimizing the host onboarding experience. Mention interest in working with global teams, using real-time data to drive product or marketplace strategy, and leveraging Airbnb’s strong analytics infrastructure to make meaningful impact.

18. Talk about a time when you had trouble communicating with stakeholders. How were you able to overcome it?

Airbnb analysts often sit between product, operations, and regional leadership. When there is disagreement or confusion, it is your role to bridge data and narrative. Provide a story where you had to identify why your insight wasn’t resonating, then explain how you reframed the data or involved the stakeholders earlier in the analysis process. Focus on active listening, using visual tools, or scenario modeling to create alignment and improve decision-making.
An Exhaustive Guide to Mastering SQL 
for Technical Interviews 
… 
Section 1.2: The Anatomy of a Query 
The SELECT statement is the cornerstone of data retrieval in SQL. 
Understanding its components and the logical order in which they are processed is 
fundamental to writing correct and efficient queries. 
Core Syntax and Logical Order of Operations 
While a query is written in a specific order (SELECT, FROM, WHERE, etc.), the database engine 
processes it in a different logical order. 
A solid mental model of this processing order helps in understanding how clauses interact. 
1. FROM / JOIN: Identifies the source tables and combines them. 
2. WHERE: Filters individual rows based on conditions. 
3. GROUP BY: Aggregates the filtered rows into groups. 
4. HAVING: Filters the aggregated groups. 
5. SELECT: Selects the final columns and performs calculations. 
6. DISTINCT: Removes duplicate rows from the result. 
7. ORDER BY: Sorts the final result set. 
8. LIMIT / OFFSET: Restricts the number of rows returned. 
Clauses & Operators 
● SELECT: Specifies the columns to be returned. Using * selects all columns, but it is a best 
practice in production code to explicitly name the required columns to improve clarity 
and performance.8 
● DISTINCT: Used within the SELECT clause to return only unique values, eliminating 
duplicate rows from the result set.13 
● WHERE: Filters the result set based on one or more conditions applied to individual rows 
before any grouping occurs.12 
● ORDER BY: Sorts the final result set in ascending (ASC, default) or descending (DESC) 
order based on one or more columns.11 
● LIMIT / OFFSET: LIMIT restricts the number of rows returned, while OFFSET skips a 
specified number of rows before starting to return results. These are crucial for 
implementing pagination.13 
● Operators: 
○ Comparison: =, <>, !=, >, <, >=, <= are used for comparing values.13 
○ Logical: AND, OR, NOT are used to combine multiple conditions in a WHERE clause.13 
○ Pattern Matching & Lists: 
■ LIKE: Used with wildcards (% for zero or more characters, _ for a single 
character) for pattern matching in strings.11 
■ IN: Checks if a value exists within a specified list of values.12 
■ BETWEEN: Selects values within a given range (inclusive).13 
■ IS NULL / IS NOT NULL: Specifically used to check for the presence or absence of 
a value, as NULL cannot be compared using standard operators like = or <>.14 
Practice Questions 
● Question: Write a query to find employees whose names start with ‘Int’. 
○ Answer: SELECT * FROM employees WHERE employee_name LIKE 'Int%';.13 
● Question: Write a query to find orders where the order amount exists between 1000 and 
5000. 
○ Answer: SELECT * FROM orders WHERE order_amount BETWEEN 1000 AND 5000;.12 
● Question: Explain the difference between BETWEEN and IN. 
○ Answer: BETWEEN is used to select values within a continuous range (e.g., numbers, 
dates), and it is inclusive of the start and end values. IN is used to check if a value 
matches any value in a discrete list of specified values.12 
● Question: Are NULL values equal to zero or a blank space? Explain. 
○ Answer: No. A NULL value represents the absence of data or an unknown value. It is 
distinct from zero, which is a specific number, and a blank space or empty string, 
which is a character value of length zero. NULL cannot be compared with any other 
value using standard comparison operators; one must use IS NULL or IS NOT NULL.12 
Section 1.3: Data Definition and Manipulation (DDL, DML, DCL, TCL) 
SQL commands are categorized into functional subsets. 
A clear understanding of these categories is often tested in foundational interview questions. 
● Data Definition Language (DDL): Defines and manages the structure of database 
objects. 
○ Commands: CREATE, ALTER, DROP, TRUNCATE.1 
● Data Manipulation Language (DML): Used to manage the data within the schema 
objects. 
○ Commands: INSERT, UPDATE, DELETE, SELECT.1 
● Data Control Language (DCL): Manages access rights and permissions to the 
database. 
○ Commands: GRANT, REVOKE.1 
● Transaction Control Language (TCL): Manages transactions within the database. 
○ Commands: COMMIT, ROLLBACK, SAVEPOINT.13 
● Data Query Language (DQL): Primarily consists of the SELECT statement for retrieving 
data.1 
The DELETE vs. TRUNCATE vs. DROP Debate 
This is a classic interview question that tests a candidate's understanding of how the 
database handles data removal at different levels. 
● DELETE: A DML command that removes rows from a table one by one. It can be used with 
a WHERE clause to remove specific rows. Because it is a logged operation, it is generally 
slower and can be rolled back within a transaction. It also fires any DELETE triggers 
associated with the table.11 
● TRUNCATE: A DDL command that removes all rows from a table by deallocating the data 
pages. It is much faster than DELETE for large tables as it is minimally logged. It cannot 
be used with a WHERE clause and typically cannot be rolled back (though behavior can 
vary by RDBMS within an explicit transaction). It does not fire DELETE triggers.11 
● DROP: A DDL command that completely removes the table itself, including its structure, 
data, indexes, constraints, and permissions. The action is irreversible without a backup.11 
Practice Questions 
● Question: What is the difference between DELETE and TRUNCATE? Can you roll back a 
TRUNCATE statement? 
○ Answer: DELETE is a DML command that removes rows individually and is logged, 
making it slower but allowing for rollback and firing triggers. TRUNCATE is a DDL 
command that deallocates all data pages at once, making it faster but generally not 
rollbackable (with some exceptions like SQL Server within a BEGIN TRAN...ROLLBACK 
block) and it does not fire triggers.11 
● Question: How would you add a column to an existing table? 
○ Answer: Using the ALTER TABLE command with the ADD COLUMN clause. For 
example: ALTER TABLE employees ADD email VARCHAR(100);.14 
● Question: How can you copy data from one table to another? 
○ Answer: Using the INSERT INTO... SELECT... statement. For example, to copy all 
records: INSERT INTO new_table SELECT * FROM old_table;.13 To create a new table 
with the same structure and data, one can use 
SELECT * INTO new_table FROM old_table; in some SQL dialects.11 
Section 1.4: Database Normalization 
Normalization is a systematic approach to designing a database schema to minimize data 
redundancy and improve data integrity. 
It involves dividing larger tables into smaller, well-structured tables and defining relationships 
between them.11 
Normal Forms (1NF, 2NF, 3NF, BCNF) 
● First Normal Form (1NF): A table is in 1NF if all its columns contain atomic (indivisible) 
values, and each row is unique. This means no repeating groups or multi-valued 
columns.11 
● Second Normal Form (2NF): A table must be in 1NF and all of its non-key attributes must 
be fully functionally dependent on the entire primary key. This rule applies to tables with 
composite primary keys and aims to eliminate partial dependencies, where a non-key 
attribute depends on only part of the composite primary key.11 
● Third Normal Form (3NF): A table must be in 2NF and all its attributes must be 
dependent only on the primary key. This eliminates transitive dependencies, where a 
non-key attribute is dependent on another non-key attribute.11 
● Boyce-Codd Normal Form (BCNF): A stricter version of 3NF. A table is in BCNF if for 
every non-trivial functional dependency X→Y, X is a superkey. In simpler terms, every 
determinant must be a candidate key.13 
Denormalization 
Denormalization is the process of intentionally introducing redundancy into a normalized 
database design to improve query performance. 
By combining tables and reducing the number of required JOINs, read operations can be 
made significantly faster. 
This is a common strategy in data warehousing and reporting databases (OLAP systems) 
where read performance is more critical than write efficiency.11 
Practice Questions 
● Question: Explain the different normal forms (1NF, 2NF, 3NF). 
○ Answer: 1NF ensures atomic values in columns. 2NF builds on 1NF and removes 
partial dependencies on composite keys. 3NF builds on 2NF and removes transitive 
dependencies, where non-key attributes depend on other non-key attributes.13 
● Question: What is denormalization and when would you use it? 
○ Answer: Denormalization is the strategic introduction of redundancy to a database 
to improve read performance by reducing the need for complex joins. It is often used 
in analytical systems (OLAP) or high-traffic applications where query speed is a 
primary concern, and the trade-off of increased storage and more complex updates 
is acceptable.11 
Part II: Intermediate SQL - Aggregating and 
Combining Data 
This section transitions from single-table operations to the core analytical tasks of 
summarizing and merging data from multiple sources. 
The progression from simple JOINs to subqueries and then to Common Table Expressions 
(CTEs) reflects a growing sophistication in a developer's approach to problem-solving. 
Interviews often test this maturity curve. 
A candidate who can solve a problem with a JOIN is competent; one who recognizes when a 
CTE offers superior clarity and efficiency demonstrates a higher level of expertise. 
Section 2.1: Mastering Aggregation 
Aggregation is the process of transforming detailed, row-level data into summarized, 
meaningful information. 
Aggregate Functions 
These functions operate on a set of values to return a single, summary value. 
They are essential for calculating metrics. 
● COUNT(): Returns the number of rows. COUNT(*) counts all rows, while 
COUNT(column_name) counts non-NULL values in that column.13 
● SUM(): Calculates the total sum of a numeric column.13 
● AVG(): Calculates the average value of a numeric column.13 
● MIN(): Returns the minimum value in a column.13 
● MAX(): Returns the maximum value in a column.13 
Grouping Data with GROUP BY 
The GROUP BY clause is used with aggregate functions to group rows that have the same 
values in specified columns into summary rows. 
For each group, the aggregate function calculates a summary value.13 
Filtering Groups with WHERE vs. HAVING 
This distinction is a fundamental and frequently asked interview question. 
● WHERE clause filters individual rows before they are passed to the GROUP BY clause and 
aggregate functions. It operates on row-level data.12 
● HAVING clause filters groups after the GROUP BY clause has been applied and the 
aggregations have been calculated. It operates on the aggregated results.12 
Practice Questions 
● Question: Find the average salary for each department. 
○ Answer: SELECT department, AVG(salary) AS avg_salary FROM employees GROUP 
BY department;.13 
● Question: List departments with more than 10 employees. 
○ Answer: SELECT department, COUNT(*) FROM employees GROUP BY department 
HAVING COUNT(*) > 10;.14 
● Question: Explain, with an example, the difference between the WHERE and HAVING 
clauses. 
○ Answer: WHERE filters rows before aggregation, while HAVING filters groups after 
aggregation. For example, to find the total sales for products with a price over $10, 
but only for categories with total sales exceeding $1000, you would use WHERE price 
> 10 to filter products first, then GROUP BY category, and finally HAVING SUM(sales) 
> 1000 to filter the resulting categories.12 
Section 2.2: The Art of the JOIN 
JOIN clauses are the primary mechanism for combining data from two or more tables based 
on a related column. 
Core JOIN Types 
● INNER JOIN: Returns only the rows that have matching values in both tables. It is the 
most common type of join.11 
● LEFT JOIN (or LEFT OUTER JOIN): Returns all rows from the left table and the matched 
rows from the right table. If there is no match, NULL is returned for the columns from the 
right table. This is useful for finding entities that may not have a corresponding entry in 
another table (e.g., customers who have never placed an order).11 
● RIGHT JOIN (or RIGHT OUTER JOIN): The inverse of a LEFT JOIN. It returns all rows from 
the right table and the matched rows from the left table. NULL is returned for left table 
columns where there is no match.11 
● FULL OUTER JOIN: Returns all rows when there is a match in either the left or the right 
table. It effectively combines the results of both LEFT and RIGHT joins. If there is no 
match for a given row, the columns from the other table will contain NULL.11 
Advanced JOIN Types 
● CROSS JOIN: Returns the Cartesian product of the two tables, meaning every row from 
the first table is combined with every row from the second table. It is used less frequently 
but can be useful for generating all possible combinations of data.11 
● SELF JOIN: This is a regular join, but the table is joined with itself. It is used to query 
hierarchical data or to compare rows within the same table. Table aliases are required to 
distinguish between the two instances of the table in the query.11 
Practice Questions 
● Question: Write a query to join 3 tables. 
○ Answer: SELECT * FROM table1 t1 JOIN table2 t2 ON t1.id = t2.t1_id JOIN table3 t3 
ON t2.id = t3.t2_id;.13 
● Question: Get all employees and their project names, showing NULL if an employee is 
not assigned a project. 
○ Answer: This requires a LEFT JOIN from the employees table to the projects table. 
SELECT e.name, p.project_name FROM employees e LEFT JOIN projects p ON e.id = 
p.employee_id;.13 
● Question: Write a query to find pairs of employees who have the same salary. 
○ Answer: This is a classic SELF JOIN problem. SELECT e1.name, e2.name, e1.salary 
FROM employees e1 JOIN employees e2 ON e1.salary = e2.salary AND e1.id < e2.id; 
The e1.id < e2.id condition is crucial to avoid listing the same pair twice (e.g., A-B and 
B-A) and an employee with themselves.13 
● Question: Find all salespeople and customers who live in the same city. 
○ Answer: This requires an INNER JOIN between the two tables on the city column. 
SELECT s.name, c.name, s.city FROM salespeople s INNER JOIN customers c ON 
s.city = c.city;.12 
Section 2.3: Advanced Data Combination 
Beyond JOINs, SQL provides other powerful tools for combining and filtering datasets. 
Set Operators 
Set operators combine the results of two or more SELECT statements. 
● UNION vs. UNION ALL: This is a very common interview topic. 
○ UNION: Combines the result sets and removes duplicate rows. This deduplication 
step requires extra processing, making it slower.11 
○ UNION ALL: Combines the result sets but includes all rows, including duplicates. It is 
significantly more performant because it does not need to check for duplicates.11 
● INTERSECT: Returns only the rows that appear in both result sets.12 
● EXCEPT (or MINUS in Oracle): Returns the rows from the first result set that do not 
appear in the second result set.13 
Subqueries (Nested Queries) 
A subquery is a SELECT statement nested inside another statement. 
They allow for complex, multi-step logic where the result of an inner query is used to guide 
the outer query. 
● Placement: Subqueries can be used in the SELECT list, the FROM clause (where they are 
often called derived tables), and most commonly, the WHERE clause.11 
● Correlated vs. Non-Correlated Subqueries: 
○ Non-Correlated: The inner query can be run independently of the outer query. Its 
result is calculated once and then used by the outer query.11 
○ Correlated: The inner query depends on the outer query for its values. It is evaluated 
once for each row processed by the outer query. Correlated subqueries can be less 
efficient than other methods like JOINs or CTEs.11 
Practice Questions 
● Question: What is the difference between UNION and UNION ALL? Which is more 
performant and why? 
○ Answer: UNION combines results and removes duplicates, while UNION ALL 
combines results and keeps duplicates. UNION ALL is more performant because it 
avoids the computationally expensive sort or hash operation required to identify and 
remove duplicates.11 
● Question: Write a query to fetch employees who earn more than the average salary in 
the entire company. 
○ Answer: This is a classic use case for a subquery in the WHERE clause. SELECT 
name, salary FROM employees WHERE salary > (SELECT AVG(salary) FROM 
employees);.13 
● Question: Explain the difference between a correlated and a non-correlated subquery. 
○ Answer: A non-correlated subquery is self-contained and executes once, passing its 
result to the outer query. A correlated subquery references columns from the outer 
query and thus must be re-executed for each row processed by the outer query, 
which can lead to performance issues.11 
Part III: Advanced SQL for Modern Data Analysis 
This section delves into the modern SQL features that are now standard expectations in 
data-centric interviews. 
Mastery of window functions and Common Table Expressions (CTEs) is what separates 
top-tier candidates from the rest. 
These features are not merely individual topics to learn; they form a synergistic toolset. 
The combination of CTEs and window functions is a modern SQL "power combo" capable of 
elegantly solving most complex analytical questions. 
CTEs provide the framework for readable, modular logic, while window functions provide the 
engine for sophisticated calculations. 
Section 3.1: Unleashing the Power of Window Functions 
Window functions perform a calculation across a set of table rows that are related to the 
current row. 
Unlike GROUP BY aggregations, they do not collapse the result set; they return a value for 
each row, preserving the original row's identity.9 
This capability is essential for tasks like ranking, calculating running totals, and comparing 
values between rows. 
The core of a window function is the OVER() clause, which defines the "window" or set of rows 
the function operates on. 
It has two main components: 
● PARTITION BY: Divides the rows into groups (partitions). The window function is applied 
independently to each partition. This is conceptually similar to GROUP BY but does not 
collapse the rows. 
● ORDER BY: Orders the rows within each partition. This is crucial for functions that depend 
on sequence, such as RANK() or LAG(). 
Key Categories of Window Functions 
1. Ranking Functions: Used to assign a rank to each row within a partition based on a 
specified order. The differences in how they handle tied values are a critical interview 
topic. 
○ ROW_NUMBER(): Assigns a unique, sequential integer to each row, regardless of 
ties.15 
○ RANK(): Assigns the same rank to tied values but leaves gaps in the subsequent 
ranks (e.g., 1, 2, 2, 4).14 
○ DENSE_RANK(): Assigns the same rank to tied values but does not leave gaps (e.g., 1, 
2, 2, 3). This is often the most useful ranking function in interviews.12 
○ NTILE(n): Distributes the rows in an ordered partition into a specified number of 
ranked groups (buckets).23 
2. Navigation/Offset Functions: Used to access data from a different row relative to the 
current row within the same result set. 
○ LAG(column, offset, default): Accesses data from a previous row in the partition.9 
○ LEAD(column, offset, default): Accesses data from a subsequent row in the 
partition.9 
3. Aggregate Window Functions: Apply standard aggregate functions (SUM, AVG, COUNT, 
MIN, MAX) over a defined window. 
○ Frame Clause (ROWS BETWEEN...): This clause provides fine-grained control over 
the window frame. For example, ROWS BETWEEN UNBOUNDED PRECEDING AND 
CURRENT ROW defines a window that includes all rows from the start of the partition 
up to the current row, which is essential for calculating running totals.13 
The following table provides a quick reference for these essential functions. 
Function Syntax Example Purpose Common Interview 
Use Case 
ROW_NUMBER() ROW_NUMBER() 
OVER (ORDER BY 
salary DESC) 
Assigns a unique 
sequential integer 
to each row. 
Arbitrarily breaking 
ties; assigning a 
unique ID to a 
result set. 
RANK() RANK() OVER 
(PARTITION BY 
dept ORDER BY 
salary DESC) 
Ranks rows, with 
gaps after ties. 
General ranking 
scenarios where 
gaps are 
acceptable. 
DENSE_RANK() DENSE_RANK() 
OVER (PARTITION 
BY dept ORDER BY 
salary DESC) 
Ranks rows, with no 
gaps after ties. 
Finding "Top N per 
group" (e.g., top 3 
salaries in each 
department). 
NTILE(n) NTILE(4) OVER 
(ORDER BY sales 
DESC) 
Divides rows into n 
ranked groups 
(e.g., quartiles). 
Segmenting 
customers into 
sales quartiles or 
performance 
buckets. 
LAG() LAG(sales, 1) OVER 
(PARTITION BY 
product ORDER BY 
month) 
Accesses a value 
from a previous 
row. 
Calculating 
period-over-period 
growth (e.g., 
month-over-month 
sales change). 
LEAD() LEAD(event_time, 1) 
OVER (PARTITION 
BY user_id ORDER 
BY event_time) 
Accesses a value 
from a subsequent 
row. 
Calculating the 
duration until the 
next event for 
sessionization. 
SUM() OVER() SUM(sales) OVER 
(PARTITION BY year 
ORDER BY month 
ROWS 
UNBOUNDED 
PRECEDING) 
Calculates a sum 
over a window. 
Calculating a 
running total or 
cumulative sum 
(e.g., year-to-date 
sales). 
AVG() OVER() AVG(price) OVER 
(ORDER BY date 
ROWS BETWEEN 6 
PRECEDING AND 
CURRENT ROW) 
Calculates an 
average over a 
window. 
Calculating a 
moving average 
(e.g., 7-day rolling 
average price). 
 
Section 3.2: Simplifying Complexity with Common Table Expressions 
(CTEs) 
 
A Common Table Expression, defined using the WITH clause, creates a temporary, named 
result set that can be referenced within a single SELECT, INSERT, UPDATE, or DELETE 
statement. 
CTEs are indispensable for breaking down complex queries into logical, readable steps, 
making them far superior to deeply nested subqueries.1 
 
Recursive CTEs 
A powerful feature of CTEs is their ability to be recursive, meaning a CTE can reference itself. 
This is the standard SQL method for querying hierarchical data, such as organizational charts, 
bill of materials, or network graphs.13 
A recursive CTE has two parts: an anchor member that returns the base result, and a 
recursive member that references the CTE itself, joined with the anchor. 
The recursion stops when the recursive member returns no more rows. 
Practice Questions 
● Question: Rewrite a complex query containing multiple nested subqueries to use CTEs 
for improved readability. 
○ Answer: This involves identifying each subquery's purpose, converting it into a 
named CTE using the WITH clause, and then joining these CTEs in the final SELECT 
statement. 
● Question: Given an employees table with employee_id and manager_id, write a query to 
f
 ind the entire reporting hierarchy for a specific employee. 
○ Answer: This is a classic recursive CTE problem. The anchor member selects the 
starting employee. The recursive member repeatedly joins the employees table to the 
CTE to find the next level of direct reports, continuing until all levels of the hierarchy 
are traversed.13 
● Question: Use a CTE to rank customers by total purchase amount and return the top 10. 
○ Answer: A first CTE would calculate the total purchase amount for each customer 
using SUM() and GROUP BY. A second CTE would then use DENSE_RANK() on the 
result of the first CTE. The final query would select from the second CTE where the 
rank is less than or equal to 10.13 
Section 3.3: Data Transformation Techniques 
Beyond selecting and joining data, SQL provides a rich set of functions for transforming data 
within a query. 
● Conditional Logic with CASE: The CASE statement provides if-then-else logic, allowing 
for the creation of new columns or conditional aggregations based on specified rules. It 
is incredibly versatile for tasks like bucketing data, creating flags, or pivoting.6 
● Handling NULLs: 
○ COALESCE(val1, val2,...): Returns the first non-NULL value from a list of expressions. 
It is commonly used to substitute a default value for NULLs.12 
○ NULLIF(expr1, expr2): Returns NULL if the two expressions are equal; otherwise, it 
returns the first expression. Useful for preventing division-by-zero errors by 
converting a zero denominator to NULL.25 
● Date/Time Manipulation: Nearly every analytical query involves dates. Common 
functions include EXTRACT() or DATE_PART() to get components like year or month, 
DATEDIFF() to find the interval between two dates, and various formatting functions.9 
● String Manipulation: Functions like CONCAT() (or ||), SUBSTRING(), UPPER(), LOWER(), 
and TRIM() are essential for cleaning and formatting text data.12 
● Pivoting Data: This involves transforming data from a row-based format to a 
column-based format. While some databases have a native PIVOT function, the universal 
method involves using an aggregate function with a CASE statement for each desired 
new column.5 
Practice Questions 
● Question: Write a query to swap gender values ('M' to 'F' and 'F' to 'M') in a table. 
○ Answer: UPDATE employees SET gender = CASE WHEN gender = 'M' THEN 'F' ELSE 
'M' END;.13 
● Question: Calculate the number of days an employee has been with the company. 
○ Answer: SELECT name, DATEDIFF(CURDATE(), joining_date) AS days_with_company 
FROM employees; (Syntax may vary by SQL dialect).13 
● Question: Pivot a table of sales data to show total sales for each product category 
('Electronics', 'Clothing', 'Home Goods') as separate columns for each year. 
○ Answer: SELECT year, SUM(CASE WHEN category = 'Electronics' THEN sales ELSE 0 
END) AS electronics_sales, SUM(CASE WHEN category = 'Clothing' THEN sales ELSE 
0 END) AS clothing_sales, SUM(CASE WHEN category = 'Home Goods' THEN sales 
ELSE 0 END) AS home_goods_sales FROM sales_table GROUP BY year; 
Part IV: Cracking the Code - SQL Interview Patterns 
and Puzzles 
 
Moving from syntax to strategy is the most critical step in interview preparation. 
Technical interviews are not just about knowing commands; they are about recognizing a 
problem's underlying structure and applying the most efficient solution pattern. 
Candidates who struggle often do so because they treat each problem as unique, attempting 
to invent a solution from scratch under pressure. 
Successful candidates, however, have developed a mental library of these patterns, allowing 
them to quickly classify a problem and deploy a proven technique. 
The most challenging problems on platforms like LeetCode are frequently combinations of 
two or more fundamental patterns. 
For instance, a "Hard" problem might first require sessionizing user activity (a LAG-based 
pattern) and then finding the top N longest sessions (a DENSE_RANK pattern). 
This section deconstructs these complex problems into their core building blocks, teaching 
not just the patterns themselves but also how to combine them. 
The following matrix provides a high-level map, linking common question types to their 
primary solution techniques. 
This framework should guide the approach to any new problem encountered. 
 
Problem Pattern / 
Question Type 
Primary SQL 
Technique(s) 
Key Functions Example 
LeetCode/DataLem
 ur Problem 
Nth Highest Salary Window Function 
or Subquery/LIMIT 
DENSE_RANK(), 
LIMIT, OFFSET 
LeetCode 176, 177 
24 
Top N per Group Window Function 
with Partitioning 
DENSE_RANK(), 
PARTITION BY 
LeetCode 185 24 
Finding Duplicates Aggregation or 
Window Function 
GROUP BY, HAVING 
COUNT(*) > 1, 
ROW_NUMBER() 
LeetCode 182, 196 
24 
Rolling 
Average/Sum 
Aggregate Window 
Function with 
Frame 
AVG(), SUM(), 
ROWS BETWEEN 
Tweets' Rolling 
Averages 29 
Period-over-Period Navigation Window 
Function 
LAG() Y-on-Y Growth Rate 
29 
Consecutive Events 
/ Streaks 
Window Functions 
to create groups 
ROW_NUMBER(), 
Date Arithmetic 
LeetCode 180 24 
Gaps and Islands Window Functions 
to identify groups 
ROW_NUMBER(), 
LAG() 
Human Traffic of 
Stadium 24 
Analyzing Pairs Self-Join JOIN table AS a 
JOIN table AS b 
Analyze Pairs of 
Things 7 
Hierarchy Traversal Recursive Common 
Table Expression 
WITH RECURSIVE Tree Node 24 
Sessionization Navigation Window 
Function 
LAG(), LEAD() Calculating User 
Activity 23 
 
Section 4.1: Foundational Patterns 
 
These patterns represent the most common tasks and are frequently asked in interviews for 
all data roles. 
 
Top-N / Nth-Highest 
 
This pattern involves finding a specific rank or the top N records, either across an entire table 
or within distinct groups. 
● Technique: The most robust and modern solution is to use window functions like 
DENSE_RANK() or RANK() partitioned by the grouping column. An outer query or CTE is 
then used to filter on the calculated rank. For simpler cases (Nth highest overall), a 
subquery or LIMIT with OFFSET can also be used.7 
● Example Problem: Find the top 3 salaries in each department. 
● Logic: 
1. Use DENSE_RANK() to assign a salary rank for each employee, restarting the rank for 
each department. 
2. The OVER clause will be (PARTITION BY department_id ORDER BY salary DESC). 
3. Use a CTE to store this ranked result. 
4. Select from the CTE where the rank is less than or equal to 3.24 
Finding Duplicates 
This involves identifying rows that share the same values across one or more specified 
columns. 
● Technique 1 (GROUP BY): Group the data by the columns that define a duplicate and 
use HAVING COUNT(*) > 1 to filter for the groups with more than one entry.9 
● Technique 2 (Window Function): Use ROW_NUMBER() OVER (PARTITION BY col1, col2... 
ORDER BY some_column) to assign a sequence number to each row within a group of 
duplicates. Any row with a ROW_NUMBER > 1 is a duplicate that can be filtered or 
deleted.11 This method is particularly useful for deleting duplicates while keeping one 
instance. 
● Example Problem: Find all duplicate emails in the Person table. 
● Logic (GROUP BY): 
1. GROUP BY Email. 
2. Filter the groups with HAVING COUNT(Email) > 1.24 
Existence Checks 
This pattern addresses questions about finding records in one table that do or do not have a 
corresponding record in another table. 
● Technique 1 (LEFT JOIN): LEFT JOIN from the primary table to the secondary table. 
Where a match is not found, the columns from the secondary table will be NULL. Filtering 
WHERE secondary_table.key IS NULL will find all records from the primary table that have 
no match.13 
● Technique 2 (NOT IN): Use a subquery to select all keys from the secondary table and 
f
 ilter the primary table WHERE primary_table.key NOT IN (...). Caution: This method can 
produce unexpected empty results if the subquery's result set contains any NULL 
values.20 
● Technique 3 (NOT EXISTS): Use a correlated subquery with NOT EXISTS. This is often 
more performant and safer with NULLs than NOT IN.9 
● Example Problem: Find all customers who have never placed an order. 
● Logic (LEFT JOIN): 
1. LEFT JOIN the Customers table to the Orders table on customer_id. 
2. Filter the results WHERE Orders.order_id IS NULL.24 
Section 4.2: Analytical Patterns 
These patterns are central to business intelligence and data science, focusing on time-series 
analysis and trend calculation. 
Rolling Metrics (Moving Averages) 
This pattern involves calculating an aggregate (like an average or sum) over a moving window 
of time (e.g., a 7-day rolling average). 
● Technique: Use an aggregate window function (AVG(), SUM()) with an ORDER BY clause 
to define the sequence and a frame clause (ROWS BETWEEN N PRECEDING AND 
CURRENT ROW) to define the size of the window.6 
● Example Problem: Calculate the 7-day rolling average of daily sales. 
● Logic: 
1. First, ensure you have a table with daily total sales. 
2. If not, create one using a CTE with SUM() and GROUP BY date. 
3. On this daily sales table, apply the window function AVG(daily_sales) OVER (ORDER 
BY date ROWS BETWEEN 6 PRECEDING AND CURRENT ROW). 
Cumulative Metrics (Running Totals) 
This pattern involves calculating the cumulative sum or count of a metric over time. 
● Technique: Use an aggregate window function (SUM(), COUNT()) with an ORDER BY 
clause and the frame clause ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT 
ROW.13 
● Example Problem: For each player, report the cumulative games played so far by date. 
● Logic: 
1. Use the window function SUM(games_played) OVER (PARTITION BY player_id ORDER 
BY event_date). 
2. The default frame for ORDER BY is RANGE BETWEEN UNBOUNDED PRECEDING AND 
CURRENT ROW, which achieves the running total.24 
Period-over-Period Analysis 
This involves comparing a metric from the current period to the same metric from a previous 
period (e.g., month-over-month, year-over-year). 
● Technique: The LAG() window function is the perfect tool. It allows you to pull a value 
from a previous row into the current row, making direct comparison and calculation 
possible.23 
● Example Problem: Calculate the month-over-month percentage growth in sales. 
● Logic: 
1. Create a CTE that aggregates sales by month. 
2. In a second query on the CTE, use LAG(monthly_sales, 1) OVER (ORDER BY month) to 
get the previous month's sales. 
3. Calculate the growth using the formula: 
(current_sales−previous_sales)/previous_sales. 
Section 4.3: Advanced Patterns & Puzzles 
These patterns often appear in interviews for senior roles or at companies with complex data 
challenges. 
They test a candidate's ability to solve non-obvious problems with creative applications of 
SQL features. 
Consecutive Events / Streaks (Gaps and Islands) 
This pattern involves identifying continuous sequences of events (islands) separated by 
breaks (gaps). 
A common variant is finding N consecutive events. 
● Technique: The core trick is to create a grouping identifier for each "island." This is done 
by subtracting a sequence number generated by ROW_NUMBER() from the actual date or 
ID. For consecutive items, this difference will be constant. 
● Example Problem: Find all numbers that appear at least three times consecutively in a 
Logs table. 
● Logic: 
1. A simple solution uses self-joins: SELECT l1.Num FROM Logs l1 JOIN Logs l2 ON l1.Id 
= l2.Id - 1 AND l1.Num = l2.Num JOIN Logs l3 ON l1.Id = l3.Id - 2 AND l1.Num = 
l3.Num.24 
2. A more robust "Gaps and Islands" solution: 
■ Use a CTE to calculate ROW_NUMBER() OVER (ORDER BY id) and 
ROW_NUMBER() OVER (PARTITION BY Num ORDER BY id). 
■ The difference between these two row numbers will be constant for any 
consecutive block of the same number. 
■ Group by this difference and the number itself, and use HAVING COUNT(*) >= 3. 
Sessionization 
This involves grouping a stream of user events into distinct sessions, typically defined by a 
period of inactivity. 
● Technique: Use the LAG() function to get the timestamp of the previous event for a user. 
Calculate the time difference between the current event and the previous one. A CASE 
statement can then create a flag (e.g., set to 1) whenever this difference exceeds the 
inactivity threshold (e.g., 30 minutes). A cumulative sum of this flag then serves as a 
unique session_id for each group of events.23 
● Example Problem: Group user clicks into sessions, where a new session starts after 30 
minutes of inactivity. 
● Logic: 
1. Use a CTE to calculate LAG(event_timestamp, 1) OVER (PARTITION BY user_id ORDER 
BY event_timestamp) to get the previous_event_time. 
2. Use a second CTE to calculate the time difference and set a new_session_flag using 
a CASE statement. 
3. Use a third CTE to create a session_id by calculating SUM(new_session_flag) OVER 
(PARTITION BY user_id ORDER BY event_timestamp). 
4. Finally, group by user_id and session_id to analyze each session. 
Section 4.4: A Curated Gauntlet of Practice Problems 
This section provides a comprehensive set of practice problems, categorized by pattern and 
difficulty, to solidify the concepts learned. 
Each problem includes a detailed breakdown of the logic and the final query. 
(Note: Due to the exhaustive nature of a full problem set, a representative selection is 
provided below. A complete interview preparation would involve working through dozens of 
such problems from the provided resources.) 
Easy Difficulty 
● Problem: LeetCode 182: Duplicate Emails 24 
○ Pattern: Finding Duplicates 
○ Statement: Write a SQL query to find all duplicate emails in a table named Person. 
○ Schema: Person (Id INT, Email VARCHAR) 
○ Logic: This is a direct application of the GROUP BY and HAVING pattern. Group the 
rows by the Email column and then use the HAVING clause to filter for groups where 
the count of rows is greater than 1. 
○ Query: 
SQL 
SELECT Email 
FROM Person 
GROUP BY Email 
HAVING COUNT(Email) > 1; 
● Problem: LeetCode 181: Employees Earning More Than Their Managers 24 
○ Pattern: Analyzing Pairs (Self-Join) 
○ Statement: Given an Employee table with Id, Name, Salary, and ManagerId, find 
employees who earn more than their managers. 
○ Schema: Employee (Id INT, Name VARCHAR, Salary INT, ManagerId INT) 
○ Logic: This requires comparing rows within the same table. A SELF JOIN is the ideal 
pattern. Join the Employee table to itself, aliasing one instance as E (for employee) 
and the other as M (for manager). The join condition will be E.ManagerId = M.Id. The 
WHERE clause then filters for rows where E.Salary > M.Salary. 
○ Query: 
SQL 
SELECT E.Name AS Employee 
FROM Employee E 
JOIN Employee M ON E.ManagerId = M.Id 
WHERE E.Salary > M.Salary; 
Medium Difficulty 
● Problem: LeetCode 176: Second Highest Salary 24 
○ Pattern: Nth-Highest 
○ Statement: Write a SQL query to get the second highest salary from the Employee 
table. If there is no second highest salary, the query should return null. 
○ Schema: Employee (Id INT, Salary INT) 
○ Logic (Subquery): Find the maximum salary in the table. Then, find the maximum 
salary that is less than the overall maximum salary. Using MAX() on a filtered set 
handles cases with ties for the highest salary correctly. 
○ Query (Subquery): 
SQL 
SELECT MAX(Salary) AS SecondHighestSalary 
FROM Employee 
WHERE Salary < (SELECT MAX(Salary) FROM Employee); 
○ Logic (Window Function): Use DENSE_RANK() to rank salaries in descending order. 
Then select the salary where the rank is 2. This approach is more generalizable for 
f
 inding the Nth highest salary. 
○ Query (Window Function): 
SQL 
WITH RankedSalaries AS ( 
SELECT Salary, DENSE_RANK() OVER (ORDER BY Salary DESC) as rnk 
FROM Employee 
) 
SELECT Salary 
FROM RankedSalaries 
WHERE rnk = 2; 
 
● Problem: LeetCode 184: Department Highest Salary 24 
○ Pattern: Top-N per Group (where N=1) 
○ Statement: Find employees who have the highest salary in each department. 
○ Schema: Employee (Id, Name, Salary, DepartmentId), Department (Id, Name) 
○ Logic: This requires finding the maximum salary for each department and then 
finding the employee(s) who match that salary in that department. A window function 
is the cleanest approach. 
○ Query (Window Function): 
SQL 
WITH RankedEmployees AS ( 
    SELECT 
        D.Name AS Department, 
        E.Name AS Employee, 
        E.Salary, 
        RANK() OVER (PARTITION BY E.DepartmentId ORDER BY E.Salary DESC) as rnk 
    FROM Employee E 
    JOIN Department D ON E.DepartmentId = D.Id 
) 
SELECT Department, Employee, Salary 
FROM RankedEmployees 
WHERE rnk = 1; 
 
 
Hard Difficulty 
 
● Problem: LeetCode 185: Department Top Three Salaries 24 
○ Pattern: Top-N per Group 
○ Statement: Find employees who earn the top three salaries in each department. 
○ Schema: Employee (Id, Name, Salary, DepartmentId), Department (Id, Name) 
○ Logic: This is a direct extension of the previous problem. The key is to use 
DENSE_RANK() instead of RANK() to handle ties correctly (e.g., if two employees 
share the 2nd highest salary, the next salary is still the 3rd highest). 
○ Query: 
SQL 
WITH RankedSalaries AS ( 
    SELECT 
        E.*, 
        D.Name as DepartmentName, 
        DENSE_RANK() OVER (PARTITION BY E.DepartmentId ORDER BY E.Salary DESC) as 
rnk 
    FROM Employee E 
    JOIN Department D ON E.DepartmentId = D.Id 
) 
SELECT 
    DepartmentName AS Department, 
    Name AS Employee, 
    Salary 
FROM RankedSalaries 
WHERE rnk <= 3; 
 
● Problem: LeetCode 262: Trips and Users 24 
○ Pattern: Conditional Aggregation 
○ Statement: Find the cancellation rate of requests with unbanned users for each day 
between '2013-10-01' and '2013-10-03'. 
○ Schema: Trips (Id, Client_Id, Driver_Id, Status, Request_at), Users (Users_Id, Banned, 
Role) 
○ Logic: This complex problem requires several steps: 
1. Filter the Trips table for the specified date range. 
2. Filter out trips involving banned clients or banned drivers. This can be done with 
a WHERE clause and subqueries on the Users table. 
3. Group the results by Request_at date. 
4. For each day, calculate the total number of valid requests (COUNT(*)). 
5. For each day, calculate the number of canceled requests using conditional 
aggregation: SUM(CASE WHEN Status LIKE 'cancelled%' THEN 1 ELSE 0 END). 
6. Divide the canceled count by the total count and round to two decimal places. 
○ Query: 
SQL 
SELECT 
    T.Request_at AS Day, 
    ROUND( 
        SUM(CASE WHEN T.Status IN ('cancelled_by_driver', 'cancelled_by_client') THEN 1.0 
ELSE 0.0 END) / COUNT(T.Id), 2 
    ) AS "Cancellation Rate" 
FROM Trips T 
JOIN Users C ON T.Client_Id = C.Users_Id 
JOIN Users D ON T.Driver_Id = D.Users_Id 
WHERE C.Banned = 'No' AND D.Banned = 'No' 
AND T.Request_at BETWEEN '2013-10-01' AND '2013-10-03' 
GROUP BY T.Request_at; 
Part V: Beyond the Query - Performance, 
Optimization, and Architecture 
In interviews for senior roles, writing a correct query is merely the first step. 
The follow-up questions often transition into performance, scalability, and system 
architecture. 
These questions are designed to gauge a candidate's real-world experience and engineering 
mindset. 
A junior candidate can write a query that works; a senior candidate understands why it works, 
how it will perform at scale, and its impact on the broader system. 
Demonstrating this deeper level of thinking can significantly influence the outcome of an 
interview and the level of the subsequent offer.8 
Section 5.1: Writing Performant SQL 
Optimizing a slow-running query is a common task and a frequent interview scenario. 
A structured approach to diagnosis is key. 
Indexes: The Key to Fast Lookups 
An index is a database object that provides a fast lookup path to data in a table, much like an 
index in a book. 
Without an index, the database must perform a full table scan, reading every row to find the 
ones that match the query's conditions. 
With an index, it can directly seek the relevant data pages.12 
● Clustered vs. Non-Clustered Index: 
○ Clustered Index: Determines the physical order of data in a table. Because the data 
can only be physically sorted in one way, a table can have only one clustered index. It 
is highly efficient for range queries.11 
○ Non-Clustered Index: Has a structure separate from the data rows. It contains the 
indexed values and pointers to the actual data rows. A table can have multiple 
non-clustered indexes. They are efficient for specific lookups.11 
● Trade-offs: While indexes dramatically speed up SELECT queries, they slow down data 
modification operations (INSERT, UPDATE, DELETE) because the index itself must also be 
updated.14 
Execution Plans 
An execution plan (or query plan) is the sequence of steps the database query optimizer 
generates to execute a SQL statement. 
Analyzing the execution plan using a command like EXPLAIN is the primary method for 
diagnosing performance bottlenecks. 
It reveals whether indexes are being used effectively, identifies costly operations like full table 
scans or inefficient join methods, and provides cost estimates for each step of the query.13 
Common Optimization Techniques 
● SELECT Specific Columns: Avoid using SELECT *. Specifying only the necessary 
columns reduces the amount of data that needs to be processed and transferred.7 
● Filter Early: Apply WHERE clauses as early as possible to reduce the size of the dataset 
that subsequent operations (like joins and aggregations) have to work with.7 
● Use JOINs Efficiently: Ensure that join conditions are on indexed columns. Understand 
the different join algorithms (e.g., Nested Loop, Hash Join, Merge Join) and how the 
optimizer chooses them. 
● Prefer UNION ALL: If duplicate rows are acceptable or known not to exist, UNION ALL is 
always more performant than UNION because it skips the deduplication step.11 
● Avoid Unnecessary Subqueries: While useful, subqueries (especially correlated ones) 
can sometimes be rewritten as more efficient JOINs or CTEs.7 
Practice Questions 
● Question: Your query is running slow. What are the first steps you would take to 
diagnose the problem? 
○ Answer: 1) Analyze the query execution plan (EXPLAIN) to identify bottlenecks like 
full table scans or expensive joins. 2) Verify that appropriate indexes exist on columns 
used in WHERE clauses and JOIN conditions. 3) Check table statistics to ensure the 
query optimizer has accurate information. 4) Evaluate the query logic for potential 
improvements, such as rewriting subqueries or simplifying complex conditions.8 
● Question: Explain the difference between a clustered and a non-clustered index. 
○ Answer: A clustered index dictates the physical storage order of rows in a table; 
there can be only one. A non-clustered index is a separate structure with pointers to 
the data rows; a table can have many. Clustered indexes are generally faster for 
range scans, while non-clustered indexes are better for point lookups.11 
Section 5.2: Database Architecture and System Design Concepts 
For senior roles, questions may touch upon higher-level database architecture and design 
principles. 
● OLTP vs. OLAP: 
○ Online Transaction Processing (OLTP): These systems are designed to handle a 
large number of short, atomic transactions (e.g., e-commerce order entry, banking 
transactions). They are optimized for fast writes and updates, with highly normalized 
schemas to ensure data integrity.11 
○ Online Analytical Processing (OLAP): These systems are designed for complex 
queries and analysis on large volumes of data (e.g., data warehouses). They are 
optimized for fast reads, often using denormalized schemas (like star or snowflake 
schemas) to minimize joins and speed up aggregations.11 
● ACID Properties: These are a set of four properties that guarantee the reliability of 
database transactions. 
○ Atomicity: Ensures that a transaction is an "all or nothing" operation. Either all of its 
operations complete successfully, or none of them do.11 
○ Consistency: Ensures that a transaction brings the database from one valid state to 
another, upholding all integrity constraints.11 
○ Isolation: Ensures that concurrent transactions do not interfere with each other, 
producing the same result as if they were executed sequentially.11 
○ Durability: Guarantees that once a transaction has been committed, it will remain so, 
even in the event of a system failure.11 
● Database Partitioning: The process of dividing a very large table into smaller, more 
manageable pieces (partitions) while still treating it as a single table logically. This can 
dramatically improve query performance and manageability. 
○ Horizontal Partitioning: Divides a table by rows (e.g., partitioning sales data by 
month or year).13 
○ Vertical Partitioning: Divides a table by columns (e.g., separating frequently 
accessed columns from rarely accessed large text columns).13 
Practice Questions 
● Question: Explain the ACID properties. 
○ Answer: ACID stands for Atomicity (all or nothing), Consistency (database remains in 
a valid state), Isolation (concurrent transactions don't interfere), and Durability 
(committed changes are permanent). Together, they ensure transactions are 
processed reliably.11 
● Question: What is the difference between OLTP and OLAP databases? 
○ Answer: OLTP systems are for managing day-to-day transactions, optimized for 
writes, and highly normalized (e.g., an e-commerce site's live database). OLAP 
systems are for analysis and business intelligence, optimized for reads, and often 
denormalized (e.g., a data warehouse for reporting).11 
Conclusion and Final Recommendations 
This guide has synthesized a vast array of resources into a structured, comprehensive 
curriculum for SQL interview preparation. 
The journey from foundational syntax to advanced analytical patterns and architectural 
considerations reflects the evolving demands of the modern data industry. 
Success in a SQL interview is not merely a function of memorizing answers but of developing 
a deep, intuitive understanding of how to model problems and apply the most effective tools 
to solve them. 
The key takeaways are twofold: 
1. Master the Fundamentals, but Differentiate with the Advanced: A flawless command 
of basic JOINs, aggregations, and filtering is the price of entry. The ability to fluently 
deploy window functions and CTEs to solve complex analytical puzzles is what 
distinguishes a top-tier candidate. A significant portion of preparation time should be 
dedicated to mastering the concepts in Part III and recognizing the patterns in Part IV. 
2. Think in Patterns, Not Just Problems: The most effective preparation strategy is to 
move beyond solving individual, disconnected problems and instead focus on identifying 
and mastering the underlying patterns. Recognizing a question as a "Top-N-per-Group" 
problem or a "Gaps-and-Islands" puzzle immediately narrows the solution space and 
provides a clear path forward, which is a critical advantage under the pressure of an 
interview. 
Actionable Recommendations for the Candidate: 
● Structured Practice: Work through the practice questions in this guide sequentially, 
ensuring a solid grasp of one section before moving to the next. Supplement this with 
active problem-solving on platforms like LeetCode and DataLemur, but always with the 
goal of categorizing each new problem into one of the patterns discussed. 
● Verbalize Your Logic: For every practice problem, articulate the step-by-step logic 
before writing the code. Explain the chosen pattern, the function of each clause, and any 
assumptions made. This practice is invaluable for the live interview setting, where 
communicating the thought process is as important as the final query. 
● Focus on Performance: After solving a problem correctly, always ask the follow-up 
question: "How can I make this more efficient?" Consider indexing strategies, alternative 
query structures (e.g., JOIN vs. EXISTS), and the potential impact of large data volumes. 
Being prepared to discuss optimization demonstrates seniority and real-world 
experience. 
By adopting this structured, pattern-oriented approach, the candidate will be well-equipped 
to not only answer the questions asked but also to demonstrate the analytical rigor and deep 
technical understanding that top companies seek in their data professionals. 
SQL Interview Questions: Complete Study 
Guide 
Based on my research of current SQL interview practices, I've compiled a comprehensive guide 
covering the most common SQL interview questions by syntax and question types. The field of 
SQL interviewing has evolved significantly, with companies now focusing on practical 
problem-solving skills rather than just theoretical knowledge. 
Most Common SQL Interview Question Categories 
1. Basic SQL Fundamentals (Entry Level) 
These questions test your understanding of core database concepts:ccslearningacademy+2 
Database Concepts: 
● What is SQL and its purpose? 
● Difference between SQL and NoSQL databases 
● Understanding of tables, columns, rows, and relationships 
● Primary keys vs foreign keys vs unique keys 
Basic Query Operations: 
● SELECT statements with filtering (WHERE clause) 
● Sorting data (ORDER BY) 
● Basic aggregate functions (COUNT, SUM, AVG, MIN, MAX) 
● Data insertion, updating, and deletion 
2. SQL Joins (Most Critical Topic) 
Joins are considered the most important SQL interview topic across all experience 
levels:datainterview+2 
Join Types You Must Master: 
● INNER JOIN - Returns only matching records 
● LEFT JOIN - Returns all left table records plus matches 
● RIGHT JOIN - Returns all right table records plus matches 
● FULL OUTER JOIN - Returns all records from both tables 
● CROSS JOIN - Cartesian product 
● SELF JOIN - Joining table with itself 
Common Join Interview Questions: 
● Write queries joining 2-3 tables 
● Explain when to use each join type 
● Handle NULL values in joins 
● Optimize join performancedatainterview 
3. Aggregate Functions and GROUP BY 
These questions test your ability to summarize and analyze data:simplilearn+1 
Key Concepts: 
● Using COUNT, SUM, AVG, MIN, MAX effectively 
● GROUP BY for data aggregation 
● HAVING vs WHERE clause differences 
● Combining multiple aggregate functions 
Typical Questions: 
● Find department-wise employee counts 
● Calculate running totals and moving averages 
● Group data by multiple columns 
● Filter grouped results with HAVING 
4. Window Functions (Advanced Level) 
Window functions are increasingly common in technical interviews, especially for senior 
roles:datalemur+2 
Essential Window Functions: 
● ROW_NUMBER() - Assigns sequential numbers 
● RANK() and DENSE_RANK() - Ranking with/without gaps 
● LAG() and LEAD() - Access previous/next row values 
● PARTITION BY - Create data subsets for calculations 
Common Window Function Problems: 
● Find Nth highest salary 
● Calculate year-over-year growth 
● Identify top N records per group 
● Create running totals and moving averagesdatalemur 
5. Subqueries and CTEs 
These test your ability to write complex, nested queries:sqlpad+1 
Subquery Types: 
● Scalar subqueries - Return single values 
● Correlated subqueries - Reference outer query 
● EXISTS and NOT EXISTS clauses 
● Common Table Expressions (CTEs) for readable complex queries 
Typical Applications: 
● Find records above/below averages 
● Complex filtering conditions 
● Multi-step data transformations 
6. Data Types and Constraints 
Understanding data integrity and table design:reddit+2 
Key Constraints: 
● NOT NULL - Prevent empty values 
● UNIQUE - Ensure distinctness 
● PRIMARY KEY - Table identifiers 
● FOREIGN KEY - Referential integrity 
● CHECK - Custom validation rules 
● DEFAULT - Automatic value assignment 
7. Performance Optimization 
Critical for senior-level positions:interviewbit+1youtube 
Optimization Techniques: 
● Index usage and types (clustered vs non-clustered) 
● Query execution plans analysis 
● Avoiding SELECT * statements 
● Proper WHERE clause ordering 
● JOIN optimization strategies 
Common Performance Questions: 
● How to identify slow queries? 
● When and how to create indexes? 
● Query optimization best practices 
● Database design impact on performance 
Classic SQL Problem Types 
1. Ranking Problems 
● Find 2nd/3rd highest salary 
● Top N customers by revenue 
● Rank employees within departments 
2. Duplicate Handling 
● Identify duplicate records 
● Remove duplicates efficiently 
● Find unique vs distinct values 
3. Date and Time Analysis 
● Extract date parts (year, month, day) 
● Calculate date differences 
● Time-based aggregations 
● Sequence and gap analysis 
4. String Manipulation 
● Text processing functions 
● Pattern matching with LIKE 
● String concatenation and extraction 
Interview Preparation Strategy 
By Experience Level: 
Entry Level (0-2 years):codesignal+1 
● Master basic SELECT, WHERE, ORDER BY 
● Understand all join types thoroughly 
● Practice aggregate functions 
● Learn constraint basics 
Mid-Level (2-5 years):dataquest+1 
● Complex multi-table queries 
● Window functions proficiency 
● Subquery optimization 
● Basic performance concepts 
Senior Level (5+ years):stratascratch+1 
● Advanced query optimization 
● Database design principles 
● Complex analytical queries 
● Scalability considerations 
Practice Approach: 
1. Start with fundamentals - Ensure solid grasp of basic concepts 
2. Progress systematically - Build complexity gradually 
3. Practice real scenarios - Use business-like problems 
4. Focus on problem-solving - Explain your thinking process 
5. Optimize solutions - Consider performance implications 
The key to SQL interview success is consistent practice with increasingly complex problems, 
understanding the reasoning behind different approaches, and being able to explain your 
solutions clearly to interviewers. Focus on mastering joins first, as they appear in virtually 
every SQL interview, then build proficiency in window functions and performance optimization 
for more advanced positions.dataquest+1 
PAGES
https://docs.aws.amazon.com/wellarchitected/latest/analytics-lens/modern-data-architecture.html
https://www.pdfdrive.com/data-science-and-big-data-analytics-e58447171.html
https://amplitude.com/explore/analytics/marketing-analytics
https://analytic-edge.com/blog/a-complete-guide-to-marketing-analytics/
https://analytics.google.com/analytics/web/?utm_source=demoaccount&utm_medium=demoaccount&utm_campaign=demoaccount#/p213025502/reports/intelligenthome?params=_u..nav%3Dmaui&collectionId=6266965030
https://analytics.google.com/analytics/web/#/report/visitors-benchmarking-channels/a58522639w92480850p96277455/_u.dateOption=last30days&_.verticalId=7&_.verticalGeoId=US&_.businessClassSize=SIZE_100000/
https://app.dataforseo.com/api-explorer/domain-analytics
https://aws.amazon.com/big-data/datalakes-and-analytics/modern-data-architecture/
https://blog.coupler.io/facebook-ads-analytics/
https://blog.coupler.io/marketing-data-analytics/
https://blog.panoply.io/how-to-become-an-analytics-manager-a-guide
https://chakrvyuh.medium.com/the-product-managers-analytics-strategy-c6cf384a2752
https://channelmix.com/about/our-approach/marketing-analytics-maturity-curve/
https://data-storyteller.medium.com/frameworks-for-answering-business-case-questions-during-analytics-and-data-science-interviews-373a67b78378
https://dataanalytics.buffalostate.edu/sites/dataanalytics.buffalostate.edu/files/uploads/Student%20resources/pandas_cheatsheet.jpg
https://dataanalytics.buffalostate.edu/sites/dataanalytics.buffalostate.edu/files/uploads/Student%20resources/what%20chart%20should%20I%20use%3F.jpeg
https://datalemur.com/blog/meta-data-scientist-interview-guide#Analytics-Execution-Questions
https://deepsync.com/marketing-analytics/
https://deloitte.wsj.com/cio/tag/data_and_analytics
https://dribbble.com/shots/16255161-Stock-Investment-Analytics
https://engage.marketo.com/rs/460-TDH-945/images/definitive-guide-to-marketing-metrics-marketing-analytics.pdf
https://engage.marketo.com/rs/460-TDH-945/images/Marketing-Metrics-and-Analytics-Cheat-Sheet.pdf
https://heroify.co/job-descriptions/marketing-analytics-manager-job-description/
https://ironfocus.com/blog/the-8-best-marketing-analytics-techniques/
https://jobs.apple.com/en-us/details/200574280/marketing-analytics-manager?team=CORSV
https://madgicx.com/blog/facebook-ads-analytics
https://marketbridge.com/resource/a-roadmap-for-modern-marketing-analytics/?utm_source=redirect
https://medium.com/@pmannuel/meta-data-science-analytics-screening-interview-cheat-sheet-735abb17fc8e
https://medium.com/analytics-and-data/you-work-in-analytics-what-do-you-do-c00dc48e7ce3
https://medium.com/analytics-vidhya/how-to-easily-install-hadoop-with-docker-ad094d556f11
https://medium.owox.com/marketing-analytics-checklist-7a3bc2739d2c
https://medium.owox.com/marketing-analytics-empowering-businesses-to-make-data-driven-decisions-1fee59f47412
https://netflixtechblog.com/analytics-at-netflix-who-we-are-and-what-we-do-7d9c08fe6965
https://news.mit.edu/topic/analytics
https://prepfully.com/interview-guides/facebook-data-scientist-product-analytics
https://salespanel.io/resources/marketing-analytics-examples/
https://shepherd.com/best-books/for-data-science-and-analytics-leaders
https://studio.youtube.com/channel/UCil4Tiu9YqE0fpJaV6M8LRA/analytics/tab-content/period-default/explore?entity_type=CHANNEL&entity_id=UCil4Tiu9YqE0fpJaV6M8LRA&time_period=4_weeks&explore_type=TABLE_AND_CHART&metric=VIEWS&granularity=DAY&t_metrics=VIEWS&t_metrics=WATCH_TIME&t_metrics=SUBSCRIBERS_NET_CHANGE&t_metrics=TOTAL_ESTIMATED_EARNINGS&t_metrics=VIDEO_THUMBNAIL_IMPRESSIONS&t_metrics=VIDEO_THUMBNAIL_IMPRESSIONS_VTR&dimension=VIDEO&o_column=VIEWS&o_direction=ANALYTICS_ORDER_DIRECTION_DESC
https://thecmo.com/digital-marketing/marketing-analytics-books/#customer-analytics-for-dummies
https://thecmo.com/digital-marketing/marketing-analytics-books/#data-driven-harnessing-data-and-ai-to-reinvent-customer-engagement
https://thecmo.com/digital-marketing/marketing-analytics-books/#driving-digital-strategy
https://thecmo.com/digital-marketing/marketing-analytics-books/#introduction-to-algorithmic-marketing
https://thecmo.com/digital-marketing/marketing-analytics-books/#marketing-analytics-roadmap
https://thecmo.com/digital-marketing/marketing-analytics-books/#optimal-database-marketing
https://thecmo.com/digital-marketing/marketing-analytics-books/#quantitative-marketing-and-marketing-management
https://towardsdatascience.com/5-books-to-grow-as-a-leader-in-data-analytics-e16b31a5925a
https://visionedgemarketing.com/drive-growth-with-analytics-and-marketing-operations/
https://walmart.wd5.myworkdayjobs.com/WalmartExternal/job/San-Bruno-CA/Senior-Product-Manager--US----Marketing-Analytics_R-1580313-2?source=Job_Board_LinkedIn&_ccid=169047105394001rade8hz
https://www.behance.net/gallery/115930467/Game-store-UXUI-Design?tracking_source=search_projects%7Canalytics+dashboard
https://www.behance.net/gallery/121693289/SEO-Dashboard-Vart?tracking_source=search_projects%7Canalytics+dashboard
https://www.behance.net/gallery/147283791/NFT-analytics-tool-UXUI-design?tracking_source=search_projects%7Canalytics+dashboard
https://www.behance.net/gallery/163117397/Ecommerce-Analytics-Admin-Dashboard-UI-Concept?tracking_source=search_projects%7Cgame+dashboard
https://www.discoverdatascience.org/career-information/data-analytics-manager/
https://www.factors.ai/blog/the-ultimate-guide-to-advanced-marketing-analytics-techniques
https://www.forbes.com/councils/forbestechcouncil/2024/06/20/creating-a-marketing-analytics-strategy-17-challenges-to-overcome/
https://www.fullstory.com/blog/marketing-analytics/
https://www.google.com/imgres?imgurl=https%3A%2F%2Fstorage.googleapis.com%2Fgweb-cloudblog-publish%2Fimages%2F4_Data_Pipeline_Architecture.max-1100x1100.jpg&imgrefurl=https%3A%2F%2Fcloud.google.com%2Fblog%2Fproducts%2Fdata-analytics%2Fsolve-your-data-analytics-etl-or-machine-learning-challenges&tbnid=AQb8BfyAIz8jvM&vet=12ahUKEwiIvJ6i1KT9AhUHPUQIHU2HDMcQMyhRegUIARChAQ..i&docid=4AP2IhaLX5VePM&w=1100&h=612&q=data%20architecture%20examples&ved=2ahUKEwiIvJ6i1KT9AhUHPUQIHU2HDMcQMyhRegUIARChAQ
https://www.google.com/imgres?imgurl=https%3A%2F%2Fwww.marketingcharts.com%2Fwp-content%2Fuploads%2F2020%2F10%2FActivateMC-B2B-Demand-Gen-Marketing-Attribution-Oct2020.png&tbnid=lOo5TWI9gQ9PlM&vet=12ahUKEwickvXWusD_AhVkh-4BHalIAJoQMyguegUIARDGAg..i&imgrefurl=https%3A%2F%2Fwww.marketingcharts.com%2Fcustomer-centric%2Fanalytics-automated-and-martech-115075&docid=Qi7KizblPs2BjM&w=1640&h=912&q=b2b%20demand%20generation%20model&ved=2ahUKEwickvXWusD_AhVkh-4BHalIAJoQMyguegUIARDGAg
https://www.google.com/search?q=facebook+ads+analytics+framework&rlz=1CDGOYI_enUS1046US1046&oq=facebook+ads+analytics+framework&gs_lcrp=EgZjaHJvbWUyBggAEEUYOdIBCTEwMzA3ajBqN6gCFLACAeIDBBgCIF8&hl=en-US&sourceid=chrome-mobile&ie=UTF-8
https://www.google.com/search?q=marketing+analytics+manager%C2%A0&sca_esv=c40a88bc622d6705&rlz=1CDGOYI_enUS1046US1046&hl=en-US&sxsrf=ADLYWII_twBUl1m4q3iwIB4Yix2E2MY8Cg%3A1736453153050&ei=ISyAZ9TgApeDm9cPhu3e0A8&oq=marketing+analytics+manager%C2%A0&gs_lp=EhNtb2JpbGUtZ3dzLXdpei1zZXJwIh1tYXJrZXRpbmcgYW5hbHl0aWNzIG1hbmFnZXLCoDIEECMYJzIEECMYJzIEECMYJzIOEAAYgAQYkQIYyQMYigUyCxAAGIAEGJECGIoFMgsQABiABBiRAhiKBTIFEAAYgAQyBRAAGIAESOCDAVCvEljJMHAEeAGQAQCYAZUBoAHlFqoBBTE2LjEyuAEDyAEA-AEBmAIgoAKiF6gCD8ICChAAGLADGNYEGEfCAgcQIxgnGOoCwgIKEAAYgAQYQxiKBcICCxAuGIAEGLEDGIMBwgILEAAYgAQYsQMYgwHCAhEQABiABBiRAhixAxiDARiKBcICEBAAGIAEGLEDGEMYgwEYigWYAwPxBQBVrgPYaL8AiAYBkAYIkgcFMTguMTSgB9SCAg&sclient=mobile-gws-wiz-serp&jbr=sep:0
https://www.google.com/search?q=marketing+analytics+plan&sca_esv=eb239daa366442ce&rlz=1CDGOYI_enUS1046US1046&hl=en-US&sxsrf=ADLYWILx6gCr-SnVu4-1YX0fvLr-WP8wkQ%3A1730406143435&ei=_-YjZ_6kGsGqur8PuZ3uEA&oq=marketing+analytics+plan&gs_lp=EhNtb2JpbGUtZ3dzLXdpei1zZXJwIhhtYXJrZXRpbmcgYW5hbHl0aWNzIHBsYW4yCBAAGBYYHhgPMgYQABgWGB4yBhAAGBYYHjIGEAAYFhgeMgYQABgWGB4yBhAAGBYYHjIGEAAYFhgeMgYQABgWGB5Ihx9QkAxYqxxwAHgAkAEBmAGAAaAB9AqqAQQzLjEwuAEDyAEA-AEBmAILoAKhCcICCBAAGIAEGLADwgIOEAAYgAQYsAMYhgMYigXCAgsQABiABBiwAxiiBMICCxAAGLADGKIEGIkFwgIKECMYgAQYJxiKBcICBRAhGKABwgILEAAYgAQYhgMYigXCAgUQABiABMICBBAjGCfCAgsQABiABBiRAhiKBZgDAIgGAZAGCZIHAzMuOKAHj1g&sclient=mobile-gws-wiz-serp
https://www.google.com/search?q=operational+marketing+analytics&rlz=1CDGOYI_enUS1046US1046&oq=operational+marketing+analytics&gs_lcrp=EgZjaHJvbWUyBggAEEUYOdIBCDUyMjdqMGo5qAIAsAIB4gMEGAIgXw&hl=en-US&sourceid=chrome-mobile&ie=UTF-8
https://www.google.com/search?q=phd+topics+to+research+for+analytics&rlz=1C1GCTT_enUS1149US1149&oq=phd+topics+to+research+for+analytics&gs_lcrp=EgZjaHJvbWUyBggAEEUYOdIBCTExNjM4ajBqNKgCALACAQ&sourceid=chrome&ie=UTF-8
https://www.google.com/search?q=who+are+thought+leaders+in+analytics&oq=who+are+thought+leaders+in+analytics&gs_lcrp=EgRlZGdlKgYIABBFGDkyBggAEEUYOTIHCAEQIRifBTIHCAIQIRifBdIBCDYwNTJqMGoxqAIAsAIA&sourceid=chrome&ie=UTF-8
https://www.google.com/search?sca_esv=ae63dcd7dad308b0&rlz=1CDGOYI_enUS1046US1046&hl=en-US&sxsrf=ADLYWILderKpSosXU28r3ki-FvysKyya0A:1735714489290&q=marketing+analytics+framework&udm=2&fbs=AEQNm0Aa4sjWe7Rqy32pFwRj0UkWd8nbOJfsBGGB5IQQO6L3JyJJclJuzBPl12qJyPx7ESLhUrJjUTNoEvHUqShJdFAludxGgUOUWV4iPCRNxFVNBgSKgKJLz7zAJjWkLmMrw3WG1ME8wZ9FhFsiZtLyzIywgwb1Tc8N-_Lms_2PFNtmQpfSPqCneBDXJBWHai9RZ8wgqOZ0hAA2SgO9kgeoHmGLP3Hy8L8HF44HXUrQmw0TI7IxfJE&sa=X&ved=2ahUKEwi07sKs-NOKAxXMHjQIHQm5JV4QtKgLegQIFRAB&biw=430&bih=744&dpr=3#imgrc=rgpJmLdftAPlzM&imgdii=M5dsPIRukLRKcM
https://www.linkedin.com/posts/biscorecard_the-top-10-books-every-data-and-analytics-activity-6836990648671858688-YICJ
https://www.linkedin.com/pulse/mastering-data-roadmap-effective-marketing-analytics-qnjkc
https://www.metacareers.com/areas-of-work/data/?p[teams][0]=Data%20%26%20Analytics&teams[0]=Data%20%26%20Analytics
https://www.packtpub.com/en-us/product/building-analytics-teams-9781800203167?utm_source=google&utm_medium=cpc&utm_campaign=20955391193
https://www.packtpub.com/en-us/product/data-science-for-marketing-analytics-9781789959413?type=print&gad_source=1&gclid=Cj0KCQjwsoe5BhDiARIsAOXVoUvLhP-8UgshK77ERGgvNlIkVH5Ej2tpbb0ETF0ihAEqUuR-_IWvbUsaAouaEALw_wcB
https://www.pdfdrive.com/building-a-digital-analytics-organization-create-value-by-integrating-analytical-processes-technology-and-people-into-business-operations-e168116936.html
https://www.pdfdrive.com/business-analytics-for-managers-taking-business-intelligence-beyond-reporting-e167628994.html
https://www.pdfdrive.com/business-intelligence-and-analytics-e56416503.html
https://www.pdfdrive.com/creating-value-with-big-data-analytics-making-smarter-marketing-decisions-e37597216.html
https://www.pdfdrive.com/cutting-edge-marketing-analytics-real-world-cases-and-data-sets-for-hands-on-learning-e158524087.html
https://www.pdfdrive.com/disruptive-analytics-charting-your-strategy-for-next-generation-business-analytics-e183824033.html
https://www.pdfdrive.com/handbook-of-marketing-analytics-methods-and-applications-in-marketing-management-public-policy-and-litigation-support-e187522700.html
https://www.pdfdrive.com/key-business-analytics-the-60-business-analysis-tools-every-manager-needs-to-know-e158219377.html
https://www.pdfdrive.com/marketing-analytics-a-practical-guide-to-real-marketing-science-e185802275.html
https://www.pdfdrive.com/marketing-analytics-data-driven-techniques-with-microsoft-excel-e175201699.html
https://www.pdfdrive.com/predictive-marketing-easy-ways-every-marketer-can-use-customer-analytics-and-big-data-e158028018.html
https://www.pdfdrive.com/social-media-analytics-strategy-using-data-to-optimize-business-performance-e183147182.html
https://www.pdfdrive.com/statistical-data-analytics-foundations-for-data-mining-informatics-and-knowledge-discovery-e184770503.html
https://www.pecan.ai/blog/data-analytics-managers-a-closer-look/
https://www.perlego.com/book/1589509/marketing-analytics-a-practical-guide-to-improving-consumer-insights-using-data-techniques-pdf?utm_source=google&utm_medium=cpc&campaignid=19700019274&adgroupid=146555143616&gad_source=1&gbraid=0AAAAADN2nNLAoyaVhRWt1AyIqXSpjZHhj&gclid=Cj0KCQjw1Yy5BhD-ARIsAI0RbXb_-yPHYCDboePDmFSkgNAC06zyGlWbcI2QVy42MMMVC25jO6BOX4EaAncgEALw_wcB
https://www.semrush.com/blog/facebook-ads-analytics/
https://www.shutterstock.com/search/data-analytics-abstract?image_type=vector
https://www.snowflake.com/guides/marketing-analytics/
https://www.thinkful.com/blog/data-analytics-career-path/
https://www.thoughtspot.com/blog/the-top-10-books-every-data-and-analytics-leader-must-read
https://www.toptal.com/external-blogs/growth-collective/facebook-ads-analytics
https://www.udemy.com/course/analytics-engineering-bootcamp/
https://www.velvetjobs.com/job-descriptions/marketing-analytics-manager
https://www.youtube.com/results?search_query=analytics
https://www.youtube.com/watch?v=c_VEoQLm_1A&ab_channel=NewcompAnalytics
https://zoommetrix.com/content/images/wordpress/digital-analytics-roadmap-1024x578.jpg










Interview Themes & Questions



Strategic Thinking & Business Impact
STRATEGY
Describe a time when your analytics insights significantly changed a marketing strategy.
…
How do you approach long-term planning for analytics capabilities?
…
How do you quantify the ROI of your analytics investments or initiatives?
…
Team Leadership & Stakeholder Management
STRATEGY
How do you build and mentor a high-performing analytics team?
…
Describe a cross-functional initiative where you had to collaborate with Product, Engineering, and Marketing.
…
How do you manage executive expectations around analytics deliverables?
…
Technical Aptitude in Analytics & Data Engineering
STRATEGY
…
…
…
…
…
…
Channel Optimization & Growth Marketing
STRATEGY
…
…
…
…
…
…
Statistical & Modeling Techniques
STRATEGY
…
…
…
…
…
…
…
STRATEGY
…
…


Marketing Analytics Q&A (MAQA)



Role Overview & Key Focus Areas



Lead Analytics Strategy
STRATEGY
Building and scaling marketing analytics functions
Define KPIs and create overarching analytics frameworks (e.g., attribution models, performance dashboards, marketing mix models, etc.).
Drive Optimization & Growth (@ROI)
STRATEGY
Demonstrating tangible ROI improvements
Partner with marketing teams to allocate budgets effectively, optimize CAC, and boost top-line revenue through insights-driven decisions.
Manage Cross-Functional Teams
STRATEGY
Managing complex cross-functional initiatives and teams
Collaborate with Data Engineering, Product, and Marketing to ensure data pipelines are robust, reporting is consistent, and the insights are actionable.
Data Management & Advancements
STRATEGY
Technical expertise in data engineering, attribution, and channel optimization
Implement advanced analytics methodologies (predictive models, ML pipelines, big data infrastructure) and mentor analysts/data scientists.







Technical Skills and Knowledge:
Can you explain how you would measure the ROI of a marketing campaign? 
This tests your understanding of key performance indicators (KPIs), attribution models, and financial metrics.
What analytics tools are you proficient in? 
Expect to discuss software like Google Analytics, Adobe Analytics, Tableau, SQL, Python, or R for data manipulation and visualization.
How do you ensure data quality in your analytics work?
This can involve discussing data cleaning processes, validation checks, and maintaining data integrity.
Describe an instance where you used predictive analytics in marketing.
They'll want to see your ability to use data to forecast trends or consumer behavior.

Strategic Application:
How have you used data to drive a marketing strategy?
Look to share examples where data insights led to strategic decisions or campaign adjustments.
Can you walk us through how you've segmented a market for a previous campaign?
This shows your ability to analyze customer data to tailor marketing efforts.
What methods do you use to assess customer lifetime value (CLV)?
Understanding CLV is crucial for long-term marketing strategy and resource allocation.

Leadership and Communication:
How do you communicate complex data insights to non-technical stakeholders?
Demonstrating your ability to translate data into actionable insights for marketing teams or executives.
Tell us about a time when you led a team to solve a significant analytics challenge.
Leadership in this context involves team management, project oversight, and problem-solving.
What's your approach to integrating analytics with broader marketing goals?
This question checks alignment between analytics and overall business objectives.

Scenario-Based Questions:
If you noticed a decline in engagement on our social media platforms, what steps would you take?
Here, they're looking for your problem-solving skills, use of analytics to diagnose issues, and your strategy to improve performance.
How would you handle a situation where the data contradicts the marketing team's intuition?
This tests your ability to advocate for data-driven decisions while managing team dynamics.

Behavioral Questions:
Describe a time when you had to persuade a team to change their marketing strategy based on your data analysis.
Looks at your influence and negotiation skills.
What was the most challenging data analysis project you've worked on, and how did you overcome it?
This assesses your problem-solving approach, resilience, and the ability to learn from challenges.




Strategic Vision
How do you align marketing analytics strategies with overall business goals?
I believe in aligning analytics strategy with business goals through a three-tiered approach
First, I establish foundational metrics that directly tie to business objectives - for example, in USAA's context, this would mean tracking not just acquisition metrics but also member lifetime value and cross-product adoption rates
Second, I implement predictive analytics to anticipate member needs across different life stages and service branches
Third, I ensure we're measuring the effectiveness of our personalization efforts to deliver relevant products to members at the right time
Decision-Making
How do you prioritize between short-term performance metrics and long-term brand value?
I use a balanced scorecard approach that weighs both immediate performance and long-term brand equity
For short-term metrics, I focus on campaign performance, conversion rates, and member acquisition costs
For long-term brand value, I track member satisfaction scores, retention rates, and brand sentiment, particularly among military families
I've found success in creating composite metrics that combine both - for instance, in my current role, I developed a 'Member Trust Index' that balances immediate response rates with long-term engagement metrics
This helped us optimize our marketing spend while maintaining high trust scores
Team Management
How do you build and manage a high-performing analytics team?
My approach to building high-performing analytics teams centers on three pillars: skills diversity, clear career paths, and continuous learning
I ensure the team has a mix of technical skills (data science, statistical analysis) and business acumen
I establish clear progression paths for team members, from junior analysts to senior strategic roles
I also implement regular training programs - for instance, I created a 'Data Democracy' initiative where analysts rotate through different business units to gain comprehensive understanding of member needs
Stakeholder Communication
How do you communicate complex analytical insights to non-technical stakeholders, such as senior leadership?
I follow a 'pyramid principle' in communicating analytics insights: start with the key finding, support with evidence, then provide detailed analysis as needed
For example, when presenting to senior leadership, I might lead with 'Our veteran-focused campaign increased product adoption by 35%,' then support this with key metrics, and finally detail the analysis methodology
I also create different versions of reports for different audiences - executive summaries for leadership, detailed analysis for marketing teams, and technical documentation for analytics teams
This approach has helped secure buy-in for major initiatives and increased data-driven decision making across departments
Cross-Functional Collaboration
Can you describe a time when you worked closely with marketing, sales, or product teams to influence a major decision?
A significant example was when I led a cross-functional initiative to optimize our military retirement services marketing strategy
Working with marketing, product teams, and customer service:
Situation: We noticed lower than expected engagement with retirement products among transitioning service members
Task: Develop a data-driven approach to improve product relevance and timing
Action: Led a collaborative team to analyze member journey data, conducted predictive modeling of transition patterns, and designed targeted engagement strategies
Result: Achieved 45% increase in retirement product adoption among transitioning service members, 30% improvement in response rates, and 25% increase in member satisfaction scores"
Grok
How have you used data to drive a marketing strategy?
Look to share examples where data insights led to strategic decisions or campaign adjustments.
Can you walk us through how you've segmented a market for a previous campaign?
This shows your ability to analyze customer data to tailor marketing efforts.
What methods do you use to assess customer lifetime value (CLV)?
Understanding CLV is crucial for long-term marketing strategy and resource allocation.




Behavioral Questions
Conflict Resolution: Describe a time when you faced resistance to implementing a data-driven strategy. How did you overcome it?
Mentorship: How do you mentor junior analysts and ensure their growth aligns with the company's objectives?
Adaptability: Share an example of a time when you had to pivot quickly based on unexpected data insights.
Describe a time when you had to persuade a team to change their marketing strategy based on your data analysis.
Scenario-Based Questions
If you noticed a decline in engagement on our social media platforms, what steps would you take?
Here, they're looking for your problem-solving skills, use of analytics to diagnose issues, and your strategy to improve performance.
How would you handle a situation where the data contradicts the marketing team's intuition?
This tests your ability to advocate for data-driven decisions while managing team dynamics.
Cultural Fit and Soft Skills
Company Fit: Why do you want to work for our company, and how do you see your skills contributing to our success?
Team Collaboration: How do you ensure alignment between the analytics team and creative teams in the marketing department?
Decision Under Uncertainty: Can you provide an example of a time you made a decision with incomplete or uncertain data?




Vision & Strategy
What's your vision for the future of marketing analytics?


How do you see AI/ML changing the marketing analytics landscape?

What's your approach to building a three-year analytics roadmap?

Describe how you've built and executed a long-term analytics strategy.
Situation: Company lacked clear analytics direction and governance
Task: Develop comprehensive three-year analytics transformation strategy
Action: Created capability assessment, built business case, developed phased approach
Result: Achieved 200% ROI on analytics investments, established company as industry leader
Data & Analytics Foundations
How do you ensure your team's analytics initiatives align with broader business objectives?


Describe how you've transformed raw data insights into actionable business strategies.


How do you balance the need for quick insights versus comprehensive analysis?
Tell me about a time when you had to overhaul your organization's analytics infrastructure.
Situation: Inherited outdated analytics stack with siloed data sources
Task: Needed to modernize infrastructure while maintaining business continuity
Action: Led cross-functional initiative to evaluate vendors, developed phased implementation plan, secured executive buy-in, managed $2M budget
Result: Reduced reporting time by 60%, increased data accuracy by 85%, enabled real-time decision making

Marketing Strategy & Performance
Campaign Analysis • Multi-channel attribution modeling • Customer journey mapping and funnel analysis • ROI measurement methodologies • Budget allocation optimization
Customer Analytics • Segmentation strategies • Lifetime value calculation • Churn prediction and retention analysis • Behavioral analytics
Leadership & Management
Team Development • Building and managing analytics teams • Cross-functional collaboration • Project prioritization • Mentoring and skill development
Stakeholder Management • Executive communication • Data storytelling • Managing expectations • Change management
Strategic Questions:
"How do you develop data literacy across non-technical teams?"
"Describe your approach to managing up and across in matrix organizations."
"How do you handle resistance to data-driven decision making?"
"Tell me about a time you had to influence a major strategic decision without direct authority."
Situation: CEO was committed to expanding into new market based on gut feeling
Task: Need to validate decision with data while maintaining relationships
Action: Conducted comprehensive market analysis, built coalition of supporters, presented compelling narrative
Result: Redirected expansion to more promising market, saved $5M in potential losses
Leadership Challenges:
 "How do you maintain team morale during major technological transitions?"
"Describe your approach to innovation within a structured corporate environment."
"How do you balance maintaining existing systems versus investing in new capabilities?"
"Tell me about a time you had to lead your team through a major organizational change."
Situation: Company merger required integrating two distinct analytics teams
Task: Unite teams, standardize processes, maintain productivity
Action: Created integration roadmap, established clear roles, implemented mentoring program
Result: Retained 90% of key talent, achieved integration 2 months ahead of schedule
Business Impact
Strategic Planning • Goal setting and KPI development • Resource allocation • Long-term roadmap development • Business case development
Problem-Solving • Real-world scenario analysis • Past experience with challenging projects • Decision-making process • Risk assessment
Complex Scenarios:
"How do you measure the impact of brand marketing initiatives?"
"Describe how you've helped shape product strategy through analytics."
"What's your framework for evaluating new marketing technology investments?"
"Share an example of how you've driven significant business transformation through analytics."
Situation: Company was losing market share due to poor customer targeting
Task: Develop data-driven customer strategy to reverse trend
Action: Led development of predictive customer models, created segmentation strategy, implemented personalization
Result: Increased customer retention by 25%, grew market share by 15%, reduced acquisition costs by 30%
Industry Knowledge
Market Understanding • Current marketing technology landscape • Industry trends and best practices • Competitive analysis • Privacy and compliance considerations



Behavioral Interview
The purpose of a behavioral interview is to assess a candidate's potential future performance by asking them to provide specific examples of past behaviors and situations that demonstrate relevant skills and qualities required for the job.
Essentially, using past actions as a predictor of future behavior; this is achieved by asking questions about past experiences and how they handled specific challenges.
Key points about behavioral interviews:
Focus on past experiences - Unlike traditional interview questions, behavioral questions ask candidates to describe real situations they've encountered in previous roles, not hypothetical scenarios.
STAR method - Candidates are often encouraged to use the STAR method (Situation, Task, Action, Result) to structure their answers, providing detailed context about their actions and the outcomes.
Better prediction of future performance - By evaluating past behaviors, interviewers can gain a more reliable understanding of how a candidate might respond to similar situations in the future role.

Recruiter Screening




General & Behavioral Questions
Can you describe a time when analytics significantly influenced a marketing strategy? (Grok)

How have you handled a situation where stakeholders disagreed with your data-driven recommendations? (Grok)

Question 1
Tell me about your experience in developing and implementing marketing analytics strategies. (Gemini)
Response (Gemini):
Throughout my career, I've focused on building and scaling data-driven marketing strategies.
At Shopify, I led an initiative to identify high-converting customer segments using [[advanced attribution modeling]].
This involved collaborating with data engineers in building various data pipelines and a custom attribution model that accurately reflected the customer journey.
By doubling down on these <<high-value segments>>, we achieved a 6.5% lift in ROAD for our paid acquisition channels (Facebook Ads, Google Ads).
At Adobe, I led a team that drove significant organic acquisition growth, increasing its revenue share from 6% to 14%.
This involved a deep dive into SEO, optimizing landing pages, and collaborating with product teams to improve conversion rates.
We saw a 52% year-over-year increase in organic traffic and a 16% lift in organic revenue.
These experiences have solidified my belief in a comprehensive approach to marketing analytics, encompassing data collection, analysis, visualization, and strategic implementation.
Question 2
How do you stay updated on the latest trends and challenges in the marketing analytics industry? (Gemini)
Response (Gemini):
Regularly read industry publications like MarTech and Search Engine Journal, follow thought leaders on platforms like LinkedIn, and attend industry conferences.
Engage in online communities and forums to learn from other professionals and discuss emerging challenges.
Recently, I've been diving deep into AI/ML applications in marketing analytics and data democratization trends.
It's crucial to understand how these advancements can be leveraged to improve efficiency and drive better decision-making.


Describe a time when you had to overcome a significant challenge in your analytics work. (Gemini)
Response (Gemini):
At Course Hero, I was tasked with building a marketing dashboard to track and analyze SEO performance. The challenge was integrating data from various sources with different formats and levels of granularity. To overcome this, I collaborated with engineers to develop a data pipeline that standardized and consolidated the data. I also implemented data quality checks to ensure accuracy and consistency. Ultimately, the dashboard provided valuable insights into keyword performance, search rankings, and landing page optimization, leading to a 60% increase in landing page CTR and a 36.8% year-over-year improvement in conversion rates.

How do you communicate complex data insights to non-technical stakeholders? (Gemini)
Why are you interested in working for USAA? (Gemini)
How would you describe your leadership style? (Gemini)
Tell me about a time you had to make a difficult decision based on data. (Gemini)
How do you handle conflicting priorities? (Gemini)
What are your salary expectations? (Gemini)

General/Behavioral Questions:
Tell me about your experience in developing and implementing marketing analytics strategies.
How do you stay updated on the latest trends and challenges in the marketing analytics industry?
Describe a time when you had to overcome a significant challenge in your analytics work.
How do you communicate complex data insights to non-technical stakeholders?
Why are you interested in working for USAA?
How would you describe your leadership style?
Tell me about a time you had to make a difficult decision based on data.
How do you handle conflicting priorities?
What are your salary expectations?
Technical Questions:
What experience do you have with different marketing attribution models?
How would you approach measuring the ROI of a specific marketing campaign?
Describe your experience with A/B testing and statistical analysis.
What are your preferred data visualization tools and why?
How would you approach building a customer segmentation model?
Explain your experience with data warehousing and ETL processes.
What are your preferred methods for data cleaning and preprocessing?
How would you handle missing or incomplete data?
Describe your experience with predictive modeling and machine learning in marketing.





Interview Process


Round 1: Recruiter Screening
The first step in the interview process is the recruiter screen:
Format: Phone Call
Duration: 30-45 minutes
Interviewer: Technical Recruiter or Talent Acquisition Specialist
Questions: Culture fit, Understanding your Experience, Logistics

Insider Tip
Have a convincing answer ready to go for the inevitable question "Why do you want to be a Product Data Scientist at Meta?".
Product Analytics Data Scientists at Meta DO NOT build Machine Learning models.


Say why you’re interested in a career at Meta 🌟: Meta interviewers like to see people who know about the company culture, products, and challenges.



Use the keywords from the job description: experimentation key product metrics drive roadmaps cross-functional teams

Job Description Usage
“Why do you want to work here?”




Drive roadmaps
How you’ve worked on projects and experimentations with multiple other teams in collaboration




Key product metrics
Story about how you defined key product metrics to better understand and track the performance of a product or business line




Experiments
Time you ran an A/B test, and the impact that experiment had on the future product roadmap





Round 2: Technical Screening
The next step after the phone screen is a virtual technical screen:
Format: Virtual video call
Duration: 45 - 60 minutes
Interviewer: Hiring Manager/Senior Data Scientist
Questions: Technical Skills (SQL), Product case
The Meta SQL test is typically conducted on Coderpad, where the interviewer can watch you code live.
Insider Tip: Meta needs you to be very fast & accurate with writing SQL.

SQL Questions



SQL Question 1: Average Post Hiatus
SQL Question 2: Facebook Power Users
SQL Question 3: Can you explain the difference between WHERE and HAVING?
SQL Question 4: Active User Retention
SQL Question 5: What's the difference between a left and right join?

Product Sense Questions


You'll also be asked a light "Product Sense" question as part of your technical phone screen. This question is usually related to the SQL coding question.

Example - If your SQL coding question is about analyzing churn of Facebook Marketplace users, you might first be asked an open-ended metrics question like 
"What are some metrics you'd track to measure the health of Facebook Marketplace?"


Round 3: Interviews On-Site (4 or 5)
The next step after the phone screen is a virtual technical screen:
Format: Virtual video call
Duration: 45 minutes each
Interviewer: Hiring Manager/Senior Data Scientist
Topics: Analytical Execution, Analytical Reasoning, DS Technical Skills, & Behavioral Questions


The Meta Data Science onsite interview covers:

Analytical Execution
Tests your probability skills, statistical foundations, and raw math brain power; Probability, Statistics, Hypothesis Testing
Elements of descriptive statistics (mean, median, mode, percentiles)
Common probability distributions (binomial, normal, poisson)
Combinations, Permutations, Conditional Probability, and Bayes' Theorem
Issues analyzing real-world data (outliers, missing values, etc.)
Key statistics concepts (Law of Large Numbers, Central Limit Theorem, etc.)
Conditional probabilities




Analytical Reasoning
Usually have a long back-and-forth discussion around some specific new product or feature; focusing on Product Metrics, evaluating trade-offs, and A/B testing
What data would you analyze to see if building this new product/feature is worth it?
How would you design an A/B test for the new feature?
What A/B testing pitfalls might you encounter?




DS Technical Skills
SQL




Behavioral Questions




: probability, statistics, hypothesis testing
Analytical Reasoning: product metrics definition, evaluating tradeoffs, A/B testing
DS Technical Skills: SQL
Behavioral Interview Questions







Stage 2: Analytical Reasoning
General Knowledge Required





Category
Data Science Skills
Statistics Skills
Programming
Python, R, SQL
Basic programming (mainly R), focused on statistical software
Mathematical Foundations
Basic math, probability, and linear algebra
Strong foundation in calculus, probability, and linear algebra
Statistical Analysis
Applied in machine learning models
Core skill, including hypothesis testing and inference
Machine Learning & AI
Key focus area, involving algorithms for automation
Not typically a focus
Big Data Tools
Hadoop, Spark, NoSQL databases
Rarely used
Data Engineering
ETL processes, data warehousing, and cloud technologies
Not emphasized
Data Collection
Not a core focus, but used in building datasets
Strong focus on experimental design and data collection
Predictive Modeling
Built through machine learning algorithms
Core skill using statistical models




Questions Set (Generalized)
@ Analytical Reasoning


On Instagram, the probability of a user watching a story to completion is 0.8. If a user posts a sequence of 4 stories, what is the probability that a viewer will watch all 4 stories? What about at least 2 stories?
What is the difference between Type I and Type II errors in hypothesis testing?
Say you roll a die three times. What is the probability of getting two sixes in a row?
Can you explain what a p-value and confidence interval are, but in layman's terms?
Explain the concept of covariance and correlation. How are they different, and what do they measure?
A Facebook Ads analyst is investigating the effectiveness of a new ad targeting algorithm. As a general baseline, they know that 1% of all users who see an ad convert (make a purchase). The new algorithm correctly identifies 80% of users who will convert for an ad. The algorithm also incorrectly flags 10% of non-converting users as likely to convert. Given that the algorithm has flagged a user as likely to convert, what is the probability that this user will actually convert?








Stage 2: Analytical Reasoning
General Knowledge Requirements

General Requirements
Question Types
Experiment Setup & Process
Step 1:
Define Metrics and KPIs
Step 2:
Design the Experiment
Design Summary Overview
Design Summary Overview
Sample Size
1. Calculation & Requirements
2. Process to Calculate Sample Size
3. Implementing Sample Size Calculation in Python
Notes @ Sample Sizing
Step 3:
Execute Experiment
Questions Set (Generalized)
@ Analytical Reasoning
Questions Set (Product)
@ Analytical Reasoning
Questions Set 1 (A/B Testing)
@ Analytical Reasoning
Questions Set 2 (A/B Testing)
@ Analytical Reasoning
Section 1: Experimental Design Interview Questions
Section 2: Metric Selection Interview Questions
Section 3: Interpretation of A/B Test Results Interview Questions



What data would you analyze to see if building this new product/feature is worth it?
How would you design an A/B test for the new feature?
What A/B testing pitfalls might you encounter?
What success metrics would you track, to see if this new feature is good?
What guardrail or counter-metrics would you track?
If some key metric went up, but a different metric got worse, how would you determine whether to ship the feature?
If there suddenly was a drop in some key metric, how would you troubleshoot the root-cause of the metric change?


General Requirements


Defining a Product Metric
Diagnosing a Metric Change
Brainstorming Product Features
Designing A/B Tests









Question Types






Experimental Design Questions
Metric Selection Questions
Interpretation of A/B Test Results Questions
Statistical Power Calculation Questions
Multiple Testing Questions













Experiment Setup & Process
Goal: Improve the conversion rate by optimizing the messaging copy near the CTA (e.g., “Get Started,” “Sign Up”).
Hypothesis: Clearer, value-driven messaging near the CTA button will increase the percentage of visitors who click on it, leading to a higher conversion rate.


Step 1:
Define Metrics and KPIs




Primary Metric: Conversion Rate (CR) — Percentage of users who click on the CTA.
Secondary Metrics: Bounce Rate, Engagement Time (to check for potential negative impacts on overall engagement).
KPIs: Target a 5% lift in conversion rate over the control version.


Step 2:
Design the Experiment





Design Summary Overview

Control Group (A): Current version of the CTA with the original copy (e.g., “Start Free Trial”).
Variant Group (B): New version with revised messaging focused on value proposition (e.g., “Start Your Journey – Free Trial Inside”).
Traffic Split: Distribute website visitors equally (50% Control, 50% Variant).
Sample Size Calculation: Calculate the minimum sample size required to detect a 5% lift in CR with 95% confidence and 80% power.
Python Calculation
from statsmodels.stats.power import GofChisquarePower
alpha = 0.05  # significance level
power = 0.80  # desired power
effect_size = 0.05  # expected lift in conversion rate

sample_size_per_group = GofChisquarePower().solve_power(
    effect_size=effect_size, nobs=None, alpha=alpha, power=power
)
print("Sample size per group:", sa

Design Summary Overview



Sample Size
1. Calculation & Requirements
Base Conversion Rate - This is the current conversion rate (or success rate) of the control group (e.g., percentage of visitors currently clicking on the CTA).
Minimum Detectable Effect (MDE) - The minimum effect size you want to detect, often expressed as a percentage increase or decrease in conversion rate. For example, if the baseline conversion rate is 10% and you want to detect an increase to 11%, the MDE is 1 percentage point.
Statistical Significance Level (Alpha) - This is the probability of rejecting the null hypothesis when it is true, typically set to 0.05 (or 5%). It determines the likelihood of a Type I error (false positive).
Statistical Power (1 - Beta) - The probability of correctly detecting a true effect, often set to 0.8 (or 80%). Power reduces the chance of a Type II error (false negative).
2. Process to Calculate Sample Size
The formula is derived from the proportions test for two independent samples.
Sample Size Calculation Formula

Where;

3. Implementing Sample Size Calculation in Python
Python code that leverages statsmodels.
Python Calculation
from statsmodels.stats.power import NormalIndPower
from scipy.stats import norm

baseline_rate = 0.10  # Baseline conversion rate (e.g., 10%)
mde = 0.01            # Minimum detectable effect (e.g., 1% increase to 11%)
alpha = 0.05          # Significance level
power = 0.8           # Statistical power

# Calculate effect size
effect_size = mde / baseline_rate

# Calculate sample size per group
power_analysis = NormalIndPower()
sample_size_per_group = power_analysis.solve_power(
    effect_size=effect_size,
    alpha=alpha,
    power=power,
    alternative='two-sided'
)
print("Sample size per group:", round(sample_size_per_group))

Notes @ Sample Sizing 


Effect Size and Sample Size Relationship:
The smaller the MDE you want to detect, the larger the required sample size.
Traffic Constraints:
If your website has limited traffic, it may take longer to reach the required sample size, which can affect how long your test runs.
Sample Size in Multi-Variant or Multi-Arm Tests:
If you’re testing more than two variants, the sample size calculation per group generally increases, depending on the test design.




Step 3:
Execute Experiment




Randomization: Ensure visitors are randomly assigned to Control or Variant to avoid selection bias.
Track Events:
Track views, clicks on CTA, and successful conversions in each group.
Using event tracking @Amplitude


















Source @Medium



Source @AnalyticsToolkit



Questions Set (Generalized)
@ Analytical Reasoning
















Questions Set (Product)
@ Analytical Reasoning
Once you read the above resources, you're ready to tackle real Meta analytical reasoning interview questions.


Meta's mobile app is suddenly experiencing high bounce rates and low session durations. How would you troubleshoot this issue?

























A user advocacy group raises concerns about the accessibility of Meta's platform for individuals with hearing disabilities. What are some product improvements that could be made with Facebook Live and Facebook videos? What metrics would you define, to see if your features had a positive impact?
Imagine you launched a feature to grow engagement of Facebook Groups. The Daily-Active-Users of groups goes up by 2%, but the average time-spent on Facebook Groups goes down by 3%. How would you determine if you should ship this feature?
Meta is trying to launch social shopping, similar to TikTok Shop. Without building a beta-test of the feature, how would you opportunity size the revenue impact from the feature?
Imagine Meta is planning to launch a new video feature aimed at young adults. How would you assess the product-market fit and define success metrics to ensure resonance with the target demographic?
Meta's data science team is analyzing user engagement metrics for a new close-friends Reels tab. However, the data shows a significant drop in engagement rates shortly after the feature launch. How would you investigate the cause of the drop in user engagement, prioritize potential factors contributing to the decline, and propose data-driven strategies to address the issue?
Meta's advertising team is exploring ways to optimize ad targeting to increase revenue and improve ad relevance for users. However, ad click-through rates are lower than expected, indicating potential issues with targeting accuracy. How would you analyze user demographic and behavioral data to assess the effectiveness of ad targeting algorithms, and what strategies would you propose to improve targeting accuracy and ad performance?
Meta's product team is considering introducing a new feature that allows users to customize their profile settings. However, there are concerns about potential privacy implications and data security risks associated with the feature. How would you conduct a privacy impact assessment to evaluate the potential risks and benefits of implementing the new feature, and what analytical methods would you use to assess user privacy preferences and mitigate privacy concerns?
Meta's data science team is exploring ways to improve search relevance for users navigating its marketplace platform. However, search queries are returning irrelevant or inaccurate results, leading to frustration among users. How would you analyze user search queries and click-through behavior to identify issues with search relevance?
Meta's data science team is investigating the impact of algorithmic bias on content recommendations in its news feed. Users have reported instances of bias in recommended content, leading to concerns about fairness and diversity. How would you quantify and measure algorithmic bias in content recommendations, and what analytical techniques would you use to identify biased patterns and mitigate the impact of bias on user experience and content diversity?
The PM responsible for Facebook events has a new idea to drive engagement – when your friends mark that they'll attend an event, you will get a notification. How would you measure the success of this notification? What counter-metrics would you look at?
The Instagram Monetization team would love to double the amount of ads shown on Instagram – it's the quickest way to nearly double revenue over-night. What do you think about this idea? How would you determine the optimal ad-load for Instagram?



Questions Set 1 (A/B Testing)
@ Analytical Reasoning












Explain how you would set up a randomized controlled trial (RCT) to evaluate the effectiveness of a new privacy feature on Meta's messaging platform.
Describe a methodological approach you would use to assess the usability of a new user interface design for Meta's virtual reality applications.
We try a new ML algorithm which improves ad targeting for e-commerce companies, who run a special type of ad known as the "shoppable feed ad". We want to test if this new ML algorithm is better. How do we test it? How many ads, or ad viewers, or advertisers, do we need to collect data from before we can reach a statistically significant result?
How would you recruit participants for interviews or focus groups, and what strategies would you use to ensure diverse perspectives are represented?
If you have an experiment, but multiple hypotheses, what could go wrong? How do you control/correct for the potential pitfalls of multiple hypothesis testing?
What's the novelty effect in A/B testing? How can it be identified and accounted for?





Questions Set 2 (A/B Testing)
@ Analytical Reasoning


Section 1: Experimental Design Interview Questions
1. How do you determine the duration of an A/B test? To determine the duration of an A/B test, consider the following factors:

Sample size and statistical significance: The primary factor in determining test duration is reaching a statistically significant result. You need a large enough sample size in each variation to confidently conclude that the observed differences are not due to chance.
Business cycle and seasonality: Consider your business cycle and seasonality when determining test duration. For example, if you're an e-commerce site like Amazon, you may need to run tests for at least a full week to capture behavior across weekdays and weekends.
User behavior and purchasing cycle: Think about your typical user behavior and purchasing cycle. If you're testing a change related to a high-consideration purchase with a long decision cycle, you may need to run the test for several weeks to fully capture the impact on conversions.
Minimum detectable effect: The smaller the minimum improvement you want to be able to detect, the larger the sample size needed and thus the longer the test duration. If you only care about detecting large effects, you can reach significance faster.




2. What are some common pitfalls to avoid when designing an A/B test? Common pitfalls in A/B test design include:

inadequate sample sizes
biased sampling methods
insufficient randomization
running too many experiments at once
In an interview, you usually want to contextualize your answer about A/B testing pitfalls to the business & team at-hand. For example, if you were interviewing at Uber on the Driver Growth division, here are some specific A/B testing issues you might encounter:

Difficulty isolating variables: Driver behavior is influenced by many external factors like local market conditions, seasonality, competitor activity, etc. This can make it challenging to isolate the impact of a specific A/B test variable.
Long time to reach statistical significance: Given the long-term nature of driver acquisition and retention, it may take months for a test to reach statistically-significant results on metrics like driver retention and lifetime value
Potential interference between simultaneous tests: With multiple teams likely running A/B tests concurrently on different aspects of the driver experience (e.g. signup flow, incentives, app features), there is risk of tests interfering with each other and confounding results.
Ethical considerations with underserved segments: If an A/B test inadvertently provides a worse experience to certain underserved driver segments, even if unintentional, it could have outsized negative impact on those groups.
























3. How would you ensure randomization in an A/B test? Randomization in an A/B test can be ensured by randomly assigning participants to treatment and control groups, thereby minimizing the risk of bias and ensuring that the groups are comparable.

4. Can you explain the concept of bucketing in the context of A/B testing? Bucketing refers to the process of assigning participants to treatment and control groups based on predetermined criteria, such as geographic location, device type, or user segment.

5. What considerations should be made when selecting the sample size for an A/B test? Sample size for an A/B test should be determined based on considerations such as the desired level of statistical power, expected effect size, baseline conversion rate, and significance level.

6. What is a control group, and why is it important in A/B testing? The control group serves as a baseline for comparison, allowing researchers to assess the impact of the treatment by comparing outcomes between the treatment and control groups.

7. How would you handle variations in user behavior over time during an A/B test? Variations in user behavior over time can be addressed by conducting the test over a sufficient duration, ensuring that the test period covers different days of the week, times of day, and user segments.

8. Describe the process of creating treatment groups for an A/B test. Treatment groups can be created by randomly assigning participants to different experimental conditions or by using stratified sampling methods to ensure that each group is representative of the population. Usually the in-house A/B testing framework at a company like Facebook or Uber is able to do this for you, automatically!

9. What measures can be taken to minimize the impact of external factors on the results of an A/B test? External factors can be minimized by conducting the test in a controlled environment, implementing safeguards to prevent interference, and monitoring external events that may impact the results.

10. How would you determine the statistical significance level for an A/B test? The statistical significance level, often denoted as alpha (α), is typically set at 0.05 or 0.01, indicating the acceptable probability of falsely rejecting the null hypothesis.


Section 2: Metric Selection Interview Questions
























11. What criteria would you use to choose appropriate metrics for an A/B test? Appropriate metrics for an A/B test should be relevant to the business objectives, sensitive to changes in the treatment, reliable, and actionable.

12. Can you differentiate between primary and secondary metrics in A/B testing? Primary metrics are directly related to the primary goal of the experiment, while secondary metrics provide additional insights or context but are not the primary focus.

13. How would you prioritize metrics when they conflict with each other in an A/B test? Prioritization of metrics should consider their alignment with the primary goals, sensitivity to changes, reliability, and practical relevance to the business.

Vanity Metrics

14. What are vanity metrics, and why should they be avoided in A/B testing? Vanity metrics are superficial metrics that may be misleading or irrelevant to the business objectives and should be avoided in A/B testing.

For example, imagine you were interviewing for a Product Data Science role at Meta, and had a question about key metrics to track for Facebook Groups. Here's some potential vanity metrics to avoid mentioning to your interviewer:

Total number of Groups: Tracking the total number of Groups on the platform might seem important, but it doesn't necessarily reflect the health or engagement of those Groups. Many could be inactive or low-quality.
Total number of Group members: Similar to total number of Groups, tracking total Group membership doesn't account for member activity or engagement. A Group could have many members but low participation. Focusing on this could lead to tactics that drive superficial member growth without improving the Group experience.
Number of Group posts: Measuring the raw number of posts in Groups doesn't consider the quality, relevance, or value of those posts. This metric could be gamed by encouraging low-effort, spammy posting just to drive up the numbers, rather than facilitating meaningful conversations.
15. How do you ensure that the selected metrics are relevant to the business goals? Selected metrics should directly reflect the impact of the treatment on the desired outcomes, such as conversion rate, retention rate, revenue, or user satisfaction.

16. Explain the difference between leading and lagging indicators in the context of A/B testing. Leading indicators are predictive metrics that signal future outcomes, while lagging indicators are retrospective metrics that reflect past performance

For example, imagine you were interviewing to be a Data Scientist on Airbnb's Pricing Team. Some leading indicators you could bring up:

Number of hosts viewing the new pricing recommendations: This measures initial engagement with the new pricing feature and predicts future adoption.
Percentage of hosts accepting the pricing suggestions: This indicates the perceived relevance and trustworthiness of the recommendations, predicting future usage.
Change in average listing price: This immediate shift can predict the eventual impact on bookings and revenue.
Lagging Indicators to bring up for the Airbnb Data Scientist Interview:

Host retention and lifetime value: The long-term impact on host satisfaction and retention on the platform is crucial, but will significantly lag the initial pricing changes.
Guest reviews mentioning price: An eventual lagging indicator of guest price perception and satisfaction, which could impact rebookings and word of mouth.
17. How would you handle situations where the chosen metrics may be influenced by external factors? External factors influencing the metrics should be identified and controlled for, or alternative metrics should be selected that are less susceptible to external influences.

18. What role does statistical power play in metric selection for A/B testing? Statistical power considerations should be taken into account when selecting metrics to ensure that they are sensitive enough to detect meaningful differences.

19. Can you provide examples of quantitative and qualitative metrics used in A/B testing? Examples of quantitative metrics include conversion rate, revenue per user, and average session duration, while qualitative metrics include user satisfaction ratings and feedback.

20. How would you measure user engagement in an A/B test? User engagement can be measured using metrics such as session duration, number of page views, click-through rate, or interaction frequency.




Section 3: Interpretation of A/B Test Results Interview Questions




















21. What steps would you take to validate the results of an A/B test Validation of A/B test results involves cross-checking with other data sources, conducting sensitivity analyses, and ensuring that the observed effects are consistent and robust.

22. How do you differentiate between statistically significant results and practical significance in A/B testing? Statistical significance alone does not guarantee practical significance; it is essential to consider the magnitude of the effect and its potential impact on the business objectives.

23. What factors could lead to false positives or false negatives in the results of an A/B test? False positives may occur due to random chance or multiple testing, while false negatives may result from inadequate sample sizes or insufficient statistical power.

24. Can you explain the concept of effect size and its relevance in interpreting A/B test results? Effect size quantifies the magnitude of the difference between treatment groups and provides context for interpreting the practical significance of the results.

25. How would you communicate the findings of an A/B test to stakeholders? Communication of A/B test findings should be clear, concise, and tailored to the audience, highlighting key insights, implications, and next steps.

26. What considerations should be made when comparing the performance of multiple variants in an A/B test? Comparison of multiple variants should consider both statistical significance and practical significance, as well as potential trade-offs between different performance metrics.

27. How do you assess the robustness of A/B test results against variations in data distribution? The robustness of A/B test results can be assessed by conducting sensitivity analyses, testing alternative hypotheses, and examining the consistency of results across subgroups.

28. What role does confidence interval play in interpreting the uncertainty of A/B test results? Confidence intervals provide a range of plausible values for the true effect size, accounting for uncertainty in the estimate.

29. How would you handle situations where the results of an A/B test are inconclusive? Inconclusive results may occur due to insufficient sample sizes, unexpected variations in user behavior, or limitations in the experimental design.

30. Can you discuss the importance of considering practical constraints and ethical implications in interpreting A/B test results? Consideration of practical constraints and ethical implications is crucial for interpreting A/B test results responsibly and making informed decisions.




Section 4: Statistical Power Calculation Interview Questions










31. What factors influence the statistical power of an A/B test? Factors influencing the statistical power include sample size, effect size, significance level, and variability in the data.

32. How would you calculate the statistical power for a given A/B test scenario? Statistical power can be calculated using statistical software or online calculators based on the desired level of significance, effect size, and sample size.

33. Can you explain the relationship between sample size, effect size, and statistical power? Sample size, effect size, and statistical power are interrelated, with larger sample sizes and effect sizes leading to higher statistical power.

34. How does the significance level affect the statistical power of an A/B test? The significance level, typically set at 0.05 or 0.01, determines the threshold for rejecting the null hypothesis and affects the statistical power.

35. What measures can be taken to increase the statistical power of an A/B test? Increasing the sample size, choosing more sensitive metrics, or reducing variability in the data can help increase the statistical power of an A/B test.

36. Can you discuss the trade-offs between statistical power and Type I error rate in A/B testing? Trade-offs between statistical power and Type I error rate involve balancing the risk of false positives with the risk of false negatives.

37. How would you determine the appropriate effect size for calculating the statistical power? The appropriate effect size for calculating statistical power depends on the context of the experiment and the magnitude of the expected difference between groups.

38. What role does variability in the data play in estimating the statistical power? Variability in the data, measured by standard deviation or variance, influences the precision of estimates and, consequently, the statistical power.

39. Can you provide examples of scenarios where a low statistical power could lead to misleading conclusions? Low statistical power increases the risk of Type II errors, where true effects may go undetected due to insufficient sample sizes.

40. How do you interpret the results of a power analysis in the context of A/B testing? Interpretation of power analysis results involves assessing whether the chosen sample size provides adequate sensitivity to detect meaningful differences with a desired level of confidence.

For more on Power Calculations read this publication by the Boston University School of Public Health.




Section 5: Handling Multiple Testing Interview Questions






41. What is multiple testing, and why is it a concern in A/B testing? Multiple testing refers to the practice of conducting multiple statistical comparisons simultaneously, leading to an increased risk of false positives..

42. How do you control the family-wise error rate in multiple testing scenarios? Family-wise error rate control methods, such as Bonferroni correction or Holm-Bonferroni method, adjust the significance threshold to account for multiple comparisons.

43. Can you explain the Bonferroni correction and its application in A/B testing? The Bonferroni correction divides the significance level by the number of comparisons to maintain the overall Type I error rate at the desired level.

44. What are some alternative methods for controlling the Type I error rate in multiple testing? Alternative methods for controlling Type I error rate include false discovery rate (FDR) control and sequential testing procedures.

45. How would you adjust the p-values for multiple comparisons in an A/B test? P-values can be adjusted using methods such as the Benjamini-Hochberg procedure or the Šidák correction to account for multiple comparisons.

46. Can you discuss the trade-offs between different approaches to multiple testing correction? Trade-offs in multiple testing correction involve balancing the risk of false positives with the potential loss of statistical power due to stringent correction methods.

47. What considerations should be made when interpreting results after multiple testing corrections? Interpretation of results after multiple testing corrections should consider both statistical significance and practical significance, as well as potential biases or confounding factors.

48. How do you determine the appropriate correction method based on the specific A/B test scenario? The appropriate correction method depends on factors such as the number of comparisons, the correlation structure of the data, and the desired balance between Type I and Type II error rates.

49. Can you provide examples of situations where failing to correct for multiple testing could lead to erroneous conclusions? Failure to correct for multiple testing can lead to an inflated Type I error rate and erroneous conclusions about the significance of the results.

50. How do you communicate the implications of multiple testing corrections to stakeholders? Communication of the implications of multiple testing corrections to stakeholders involves explaining the rationale behind the correction methods and the impact on the interpretation of the results.















MARKETING ANALYTICS MANAGER
INTERVIEW TOPICS OVERVIEW


RESPONSIBILITIES





DATA ANALYSIS & INSIGHTS
@CREDITSESAME

COLLECTIONS
Collect, analyze, and interpret large datasets from various marketing channels (e.g., paid search, email, social media, SEO, and CRM).


INSIGHTS
Provide actionable insights into marketing performance metrics, such as ROI, CAC, LTV, churn, and conversion rates.


OPPORTUNITIES
Identify trends, patterns, and opportunities to improve campaign performance and overall marketing effectiveness.







REPORTING & DASHBOARDS
@ADOBE

TRACKING
Design and maintain dashboards for real-time performance tracking using tools like Tableau, Looker, or Power BI.


AUTOMATION
Automate reporting processes to reduce manual effort and increase efficiency.


UTILIZATION
Provide weekly, monthly, and quarterly reports to key stakeholders, including the C-suite, marketing, and sales teams.











3. Marketing Attribution
Develop and refine attribution models (e.g., first-touch, last-touch, multi-touch, or algorithmic models).
Ensure accurate tracking of customer journeys and campaign impact across multiple channels.
Collaborate with marketing teams to integrate attribution findings into campaign planning.
4. Campaign Performance Measurement
Collaborate with the marketing team to define KPIs and success metrics for campaigns.
Conduct post-campaign analysis to determine effectiveness and suggest optimization strategies.
Monitor A/B tests and experiments to evaluate creative, messaging, and targeting strategies.
5. Strategic Collaboration
Work cross-functionally with marketing, sales, finance, and product teams to align goals and analytics efforts.
Provide strategic input to marketing leaders on budget allocation and channel prioritization.
Translate complex data into business insights that influence marketing strategies.
6. Data Integrity and Quality Assurance
Ensure data accuracy, consistency, and cleanliness across marketing tools and databases.
Audit data collection processes and troubleshoot discrepancies in tracking systems like Google Analytics, Adobe Analytics, or Mixpanel.
Implement processes to maintain high-quality data and compliance with regulations (e.g., GDPR, CCPA).
7. Advanced Analytics and Modeling
Build predictive models for customer segmentation, churn analysis, and CLV prediction.
Use statistical and machine learning techniques to inform decision-making and marketing strategies.
Drive the adoption of advanced analytics tools and methodologies within the marketing team.

HOMEAGLOW
Creating and maintaining reporting of Marketing KPIs and lead analyses of trends in marketing platform performance through on-site conversion;
Continuously improving the accuracy of marketing measurement through approaches, including attribution, media mix modeling and incrementality studies;
Partnering with the VP, Marketing to set growth targets, including quantifying supply & demand at a market level;
Increasing the testing velocity of performance and lifecycle marketers by designing experiments and creating tools to empower marketers to run and analyze experiments effectively;
Partnering with Finance to translate marketing operational metrics to financial metrics, set marketing budgets and growth targets.



QUALIFICATIONS




DATA ANALYSIS & INSIGHTS
@CREDITSESAME

COLLECTIONS
Collect, analyze, and interpret large datasets from various marketing channels (e.g., paid search, email, social media, SEO, and CRM).


INSIGHTS
Provide actionable insights into marketing performance metrics, such as ROI, CAC, LTV, churn, and conversion rates.


OPPORTUNITIES
Identify trends, patterns, and opportunities to improve campaign performance and overall marketing effectiveness.







1. Education and Background
Bachelor’s degree in Marketing, Business, Statistics, Computer Science, Economics, or a related field. (Advanced degrees like an MBA or a Master’s in Analytics are often preferred.)
Experience in marketing analytics, performance marketing, or business intelligence (typically 3–5+ years).
2. Technical Skills
Proficiency in analytics tools like Google Analytics, Adobe Analytics, or Mixpanel.
Experience with data visualization platforms like Tableau, Looker, Power BI, or Data Studio.
Advanced skills in Excel and SQL for data manipulation and analysis.
Familiarity with programming languages such as Python or R for advanced analytics and modeling.
Knowledge of marketing automation platforms (e.g., HubSpot, Marketo) and CRM systems (e.g., Salesforce).
Experience with A/B testing platforms like Optimizely or Google Optimize.
3. Analytical and Business Acumen
Strong ability to translate complex data into clear, actionable insights.
Understanding of digital marketing metrics and the relationship between them (e.g., impressions, CTR, conversions, retention).
Familiarity with statistical methods and predictive modeling techniques.
4. Communication and Stakeholder Management
Excellent written and verbal communication skills for presenting insights to technical and non-technical audiences.
Ability to collaborate with cross-functional teams and manage multiple stakeholders.
Strong storytelling skills with data, aligning analytics with business goals.
5. Leadership and Strategic Thinking
Experience managing teams or mentoring junior analysts (if applicable).
Strategic mindset with a focus on long-term growth and innovation in marketing analytics.
Proven ability to prioritize tasks and manage deadlines in a fast-paced environment.
6. Industry Knowledge
Familiarity with marketing trends, tools, and technologies (e.g., multi-touch attribution, AI-driven insights).
Understanding of customer journey mapping and funnel optimization.
Awareness of privacy and data regulations (e.g., GDPR, CCPA).

5+ years of experience in business and/or marketing analytics, including experience working directly with media buyers & financial analysts;
A deep understanding of various measurement techniques and how to deploy them effectively (attribution, media mix modeling, incrementality studies, etc…);
Experience supporting website conversion rate optimization & analyzing website funnel metrics;
Track record of increasing the velocity and quality of experiments run by media buyers;
Track record demonstrably improving marketing measurement that led to an improvement in the efficiency of the marketing program;
Experience working cross-functionally with Finance & Engineering.




Analytics Management 
& Leadership (ALP)




Key Functions & Responsibilities
Strategy & Leadership
Beyond the technical toolkit, the "Manager" title carries a distinct set of strategic and leadership responsibilities
Team Leadership, Business Influence, & Results
Job descriptions make it clear that success in this role is measured not by personal analytical output, but by the ability to lead a team, influence the organization, and drive tangible business results
Role Types: Individual Contributor & Management
Mentoring and coaching to foster skill development and career growth and conducting performance management.
The role requires a "player-coach" who can both guide the team's work and develop its members into trusted thought partners.  
Primary owner of the team's output and workflow. This involves robust project management skills to prioritize initiatives, manage resources, and ensure the timely delivery of high-quality analytics solutions. They are responsible for defining the team's roadmap, aligning it with broader business objectives, and communicating progress to leadership.  
Cross-Functional Context, Data Translation, & Solutions
An Analytics Manager does not operate in a silo. They are a central node in the organization's information network, acting as a critical liaison between the technical data team and various business units. The phrases cross-functional collaboration and partner with stakeholders are among the most frequent in the entire data corpus, highlighting the relational nature of the role.
The manager must translate business needs into actionable data solutions and, conversely, translate complex analytical findings into clear, compelling narratives for non-technical audiences. This requires deep partnership with a wide array of departments. The analysis shows managers are expected to work closely with: 
Marketing: To optimize channel performance, measure campaign ROI, and understand customer acquisition funnels.  
Product: To inform product strategy through user behavior analysis, A/B testing, and feature adoption metrics.  
Sales: To analyze sales performance, inform territory planning, and identify growth opportunities.  
Finance: To support financial planning and analysis (FP&A), forecasting, and budgeting.  
Operations: To identify process inefficiencies and improve operational effectiveness.  
Value Proposition: Driving Business Impact
The final and most important function of the Analytics Manager is to drive business value. The role is explicitly designed to move the organization beyond descriptive reporting ("what happened") toward prescriptive and predictive insights ("what will happen" and "what should we do"). The ultimate goal is to deliver actionable insights that inform strategic commercial decisions and enable a culture of data-driven decision-making.  
This impact is measured by the ability to influence Key Performance Indicators (KPIs). The manager is responsible for defining, tracking, and challenging these metrics, ensuring they are aligned with strategic goals. Whether it's increasing customer lifetime value, reducing fraud losses, or improving marketing efficiency, the Analytics Manager's success is tied directly to the performance of the business.  
This reveals that the manager's primary value is not in their own analytical output, but in their capacity to act as a force multiplier. Their job is to amplify the analytical impact of their team and the entire organization. They achieve this by mentoring analysts, building scalable self-service tools, and effectively translating data into strategy for business leaders. The bottleneck in modern organizations is rarely data access; it is interpretation and application. The Analytics Manager is hired to solve this "last mile" problem, making them one of the most strategically vital middle-management roles in any company.
Role Terms & Phrases
Lexical Analysis
Dominant Terminology
The specific words and phrases used in job descriptions provide a quantitative lens through which to view the role's core priorities and underlying tensions. A lexical analysis of the 15,000-word corpus reveals a dual focus on technical execution and strategic influence.
After removing common stop words and lemmatizing the text, a clear hierarchy of concepts emerges. The most frequent terms cluster around three distinct themes: the Data/Technical Core, the Business Mandate, and the People/Process Layer.
TERMINOLOGY






data
analytics
business
team
155
121
108
91
experience
manager
insights
strategy
87
73
65
59
skills
management
marketing
product
51
48
45
41
senior
financial
lead
analysis
39
38
37
35
development
support
performance
role
33
32
30
29
work
sql
leadership
tools
28
27
26
25
growth
solutions
drive
reporting
24
23
22
21
stakeholders
tableau




20
19






Key Phrases and Strategic Imperatives
Analyzing common multi-word phrases (bi-grams and tri-grams) provides deeper insight into the operational realities and strategic goals of the position. These recurring phrases function as a shorthand for the core imperatives of the Analytics Manager.
TERMINOLOGY






cross-functional teams
data-driven decision
business intelligence
key performance indicators
32
28
22
19
machine learning
data analysis
data visualization
financial services
17
16
15
14
senior manager
data science
modern data stack
project management
13
12
11
11
stakeholder management
product management
marketing analytics
business analytics
10
9
9
9
bachelor's degree
data-driven insights
power bi
sql python
8
8
8
7

The phrase cross-functional teams is the most frequent, reinforcing that collaboration is the primary mode of operation. The prevalence of data-driven decision making and data-driven insights clearly articulates the role's purpose. Phrases like business intelligence, data visualization, and key performance indicators (KPIs) describe the primary outputs and metrics of the job. The appearance of machine learning and modern data stack high on this list confirms their growing importance.
The language used in these job descriptions reveals a fundamental duality. The consistent co-occurrence of technical terms (sql, machine learning, power bi) and strategic business terms (business intelligence, stakeholder management, key performance indicators) within the same postings highlights the hybrid nature of the role. An Analytics Manager must be "bilingual," fluent in the language of technology and the language of business strategy. This duality makes the role uniquely challenging to hire for, as it requires finding a rare individual who can comfortably and credibly operate in both worlds.
Role Archetypes
Specializations within Analytics Leadership
While "Analytics Manager" is a common title, the analysis reveals it is not a monolithic role. Instead, it serves as an umbrella term for several distinct archetypes, each with a specialized focus, unique stakeholder set, and tailored technology stack. The most significant predictor of an Analytics Manager's required skills is the business function they are hired to support.
The Marketing Analytics Manager
Focus
This archetype is centered on the customer journey and marketing effectiveness. Key responsibilities include analyzing customer acquisition funnels, measuring channel performance (both paid and owned), developing Marketing Mix Models (MMM) and Multi-Touch Attribution (MTA) models, and calculating customer lifetime value (LTV).  
Stakeholders: Chief Marketing Officer (CMO), Marketing Directors, and digital marketing channel managers.
Tech Stack: Characterized by marketing-specific tools like Google Analytics and Google Tag Manager, alongside the foundational SQL, Python, and BI tools. Experience with advertising platform data (e.g., Meta, Google Ads) is critical.  
The Platform / Analytics Engineering Manager
Focus: This is the most technically oriented archetype, focused on building and maintaining the data infrastructure that empowers all other analysts. Their work involves data modeling in dbt, building and managing ETL/ELT pipelines with tools like Airflow, ensuring data governance, and developing a robust, scalable self-service analytics platform.  
Stakeholders: Head of Data, Data Engineering leads, and other analytics teams who are their internal customers.
Tech Stack: Dominated by the Modern Data Stack: Snowflake/BigQuery/Redshift, dbt, Airflow/Dagster, Python, and often Infrastructure as Code tools like Terraform.
Examples: The "Cloud Data & Analytics Manager" at PwC , the "Manager, Analytics Engineering" at GitLab, and the "Senior Manager, Analytics Engineering" at Coinbase.
The Domain-Specific Analytics Manager (e.g., Healthcare/Pharma)
Focus: This archetype leverages deep industry-specific knowledge and data sources to drive commercial strategy. In healthcare and pharma, this means analyzing medical claims data, prescription data, and clinical trial results to understand the patient journey, measure brand performance, and support sales and marketing targeting.  
Stakeholders: Brand Leads, Commercial Operations, and Medical Affairs teams.
Tech Stack: Includes the standard SQL, Python/R, and BI tools, but is differentiated by required expertise in specialized third-party data sources like IQVIA, Komodo Health, and Symphony Health.  
Examples: The "Senior Manager, Data Science & Analytics" at Pfizer and the "Senior Manager, Business Analytics and Forecasting" at Tempus AI.  
The existence of these distinct archetypes demonstrates that domain expertise is often a more critical hiring factor than proficiency in a niche tool. While the foundational technical skills are largely portable, the ability to apply them effectively within a specific business context is what truly defines a successful Analytics Manager.
Strategic Professional Development
Career Growth & Management
The comprehensive analysis of the Analytics Manager role provides a clear roadmap for professionals, hiring managers, and organizations seeking to leverage data for strategic advantage. The following recommendations are designed to translate these findings into actionable strategies.
Analytics Managers Skills & Utilities
Building a T-Shaped Profile
The modern Analytics Manager must be a "T-shaped" professional, possessing both broad foundational skills and deep specialized expertise.
Build the Horizontal Bar: Master the Foundational Trinity: achieve advanced proficiency in SQL, become an expert in at least one major BI platform (Tableau or Power BI), and gain functional competency in Python for data analysis. These are the table stakes.
Dig the Vertical Bar: Choose and commit to one of the archetypes detailed in Section 5. Develop deep domain expertise in that area, whether it's understanding marketing funnels, product experimentation, financial modeling, or the intricacies of the Modern Data Stack. This specialization is what will differentiate you and command a premium in the job market.









CORE TOPICS





















…




BROAD TOPIC
SUB-TOPICS
DETAILED TOPICS


























UNPROCESSED


PRESTART
Pillar
Actions
Artefacts
People
Ask recruiter for the org-chart, recent performance reviews (redacted), and open headcount.• Book 1-on-1 intros with direct reports and “critical friends” (product, eng, finance).
30-min intro slots on calendar
Domain
Review quarterly business review (QBR), OKRs, product roadmap, and data architecture diagrams.
Personal briefing deck
Process & Tooling
Request Git, BI, warehouse, experimentation-platform access; skim run-books & dbt docs.
Checklist in Notion
Delivery
Ask for “Top 5 painful unanswered questions” from each exec.
Backlog seed
Governance & Culture
Read data policy, SLAs, model-risk framework, security training.
Compliance sign-off


DAYS 1-30 “ABSORB”
Pillar
Objectives
Success Signals
People
• Conduct 1-on-1s with every team member (strengths, pain points, goals).• Shadow analysts in live stakeholder meetings.• Identify skill gaps & morale issues.
Trust baseline; anonymized SWOT
Domain
• Map the metric hierarchy (north-star → sub-metrics).• Catalog critical dashboards & their owners (“system archeology”).
Metric map Miro board
Process & Tooling
• Audit ELT → marts → BI lineage; note broken tests & flaky jobs.• Observe sprint rituals; assess ticket hygiene.
One-page “current-state”
Delivery
• Ship at least one quick-win (e.g., fix a wrong KPI, automate a weekly CSV).
Exec kudos; visible win (locallyoptimistic.com)
Governance & Culture
• Join incident post-mortems; learn the escalation ladder.• Re-iterate data ethics & privacy norms in team slack.
Zero SLA breaches



DAYS 61-90 “ACCELERATE”
Pillar
Objectives
Success Signals
People
• Draft individual development plans (IDPs) with each report.• Negotiate scope with adjacent teams to reduce ticket back-and-forth.
Signed IDPs; RACI matrix
Domain
• Build a “metric contract” template (owner, formula, business logic).• Validate top 10 metrics with Finance.
Contracts published in dbt docs
Process & Tooling
• Stand up CI/CD for dbt + airflow (if missing).• Propose a request-intake Kanban to replace ad-hoc pings.
PR merged; Jira board live
Delivery
• Prioritize the backlog against OKRs; commit to Q-end impact stories.• Kick off one A/B test or forecasting model with high business value.
Road-mapped backlog approved
Governance & Culture
• Institute weekly data-quality stand-ups (<15 min).• Publish first monthly analytics newsletter.
Reduction in data-incidents



MONTH 4-6 “OPTIMIZE”
People – formal mentorship pairing, succession plans, performance criteria tied to OKRs.


Domain – quarterly roadmap refresh; socialize a KPI deprecation framework.


Process & Tooling – migrate legacy ETLs to dbt; implement automated cost monitoring in BigQuery.


Delivery – institutionalize an “experiment review board”; publish reusable notebook templates.


Governance & Culture – run a data-literacy workshop for non-technical teams (HBR highlights this as a key unlock for org-wide data fluency) hbr.org.

MONTH 7-12 “STRATEGIZE & SCALE”
Goal
Key Milestones
Org & Talent
• Complete team skills matrix → training budget.
• Split team into Analytics Engineering vs Insights & Science pods with clear charters.
Platform Evolution
• Evaluate buy-vs-build for observability or experimentation platforms.
Portfolio Management
• Introduce a quarterly “analytics OKR demo day.”
• Score all projects on impact/effort → cull low-ROI work.
Cross-Functional Influence
• Co-lead annual planning offsite; set company-level Data Maturity targets.
Thought Leadership
• Speak at internal town-hall; optionally publish a blog or conference talk to aid hiring brand.


Executive Metrics Dashboard (what your VP will watch)
Area
Sample KPI
Target by Month 12
Data Quality
% Freshness SLA met
≥ 98 %
Delivery
Avg. cycle-time, idea → insight
–40 %
Adoption
Monthly active Looker users
+50 %
Experimentation
Tests / quarter with stat. power ≥ 80 %
≥ 15
ROI
Incremental ARR attributed to analytics projects
> $2 M



Templates & Artifacts to Prepare
30-60-90 PDF (one page)


IDP Google Sheets per report


KPI Catalog in dbt docs/


Analytics Intake Form (Google Form + Slack alert)


Incident Post-mortem Template (Markdown)


Quarterly Business Review slide deck



Final Advice
Sequence “Listen → Learn → Lead.” Jumping to re-architecture on Day 1 alienates teams; quick-wins earn capital first.


Anchor everything in business outcomes. Stakeholders remember revenue saved or growth unlocked, not model F1-scores.


Invest in culture early. High trust multiplies every engineering or statistical improvement you later introduce locallyoptimistic.com.




Competency
Key Concepts & Behaviors
Application in a Project (Use Case)
Critical Thinking
Deconstructing vague business questions into specific, testable hypotheses. Identifying potential biases in data and analysis. Prioritizing tasks based on impact.
Transforming the vague request "Why are sales down?" into a structured analytical plan to investigate specific channels, regions, and customer segments.
Communication
Translating technical findings into clear business language. Practicing active listening with stakeholders. Structuring a compelling data story (What? So What? Now What?).
Presenting the results of a churn analysis to non-technical marketing and product leaders, focusing on the business impact and actionable recommendations.
Business Acumen
Understanding company KPIs (e.g., LTV, CAC, Churn). Knowing the industry, market trends, and how the business generates revenue.
Aligning an analysis on website traffic with the company's primary goal of increasing qualified leads, not just page views.
Collaboration
Working with data engineers to define data requirements. Partnering with product managers to measure feature success. Using version control (Git) to share code.
Partnering with the marketing team to provide them with a customer segmentation analysis that they can then use to build a targeted email campaign.




Soft Skills
Communication
Written Communication
- Report writing
- Documentation of findings
- Email summaries for stakeholders




Verbal Communication
- Presenting insights to teams
- Simplifying technical concepts
- Handling Q&A sessions


Problem-Solving
Analytical Thinking
- Breaking down complex problems
- Identifying key metrics
- Root cause analysis




Solution Design
- Proposing actionable recommendations
- Prioritizing solutions by impact
- Iterative testing and feedback


Attention to Detail
Data Accuracy
- Cross-checking calculations
- Validating data sources
- Spotting anomalies




Process Rigor
- Following standard operating procedures
- Documenting workflows
- Ensuring reproducibility




Management & Leadership
Communication
Translate complex findings into clear, actionable insights
Simplifying technical jargon, storytelling with data, creating compelling reports


Problem-Solving
Critical thinking, asking the right questions, methodical approaches
Attention to Detail
Errors in data or analysis can lead to flawed conclusions
Curiosity and Continuous Learning
The field evolves rapidly with new tools and methods


7. Management Domain

Core Skills and Competencies

Strategic Management Competencies

- Strategic Thinking [Skill] - Ability to envision long-term goals and develop comprehensive strategies
- Decision Making [Skill] - Data-driven analysis and judgment under uncertainty
- Systems Thinking [Skill] - Understanding interconnected organizational elements
- Business Acumen [Skill] - Financial literacy, market understanding, competitive analysis

People Management Competencies

- Emotional Intelligence [Skill] - Self-awareness and interpersonal relationship management
- Communication [Skill] - Clear, transparent, and contextual information sharing
- Delegation and Empowerment [Skill] - Effective task assignment and team development
- Coaching and Mentoring [Skill] - Developing team members' capabilities and potential

Key Methodologies and Frameworks

Strategic Planning Frameworks

- McKinsey 7-S Framework [Framework] - Strategy, Structure, Systems, Shared Values, Style, Staff, Skills
- Porter's Five Forces [Framework] - Industry competition analysis model
- BCG Growth-Share Matrix [Framework] - Portfolio management and resource allocation
- Blue Ocean Strategy [Framework] - Creating uncontested market spaces

Project Management Methodologies

- PMBOK Guide [Methodology] - Project Management Body of Knowledge (PMI standard)
- Agile/Scrum [Methodology] - Iterative development with continuous improvement
- Lean Project Management [Methodology] - Waste elimination and value optimization
- PRINCE2 [Methodology] - Structured project management approach

Change Management Frameworks

- Kotter's 8-Step Process [Framework] - Systematic organizational change approach
- ADKAR Model [Framework] - Awareness, Desire, Knowledge, Ability, Reinforcement

Essential Tools and Software Platforms

- Monday.com [Tool] - Visual project management with automation capabilities
- Asana [Tool] - Task management with strategic goal alignment
- Slack [Tool] - Team communication and integration platform
- Microsoft Teams [Tool] - Unified communication and collaboration suite
- Tableau [Tool] - Data visualization and business intelligence

Sub-disciplines and Specializations

Project Management Specializations

- IT Project Management [Specialization] - Technology implementation and software development
- Healthcare Project Management [Specialization] - Medical and pharmaceutical projects
- Marketing Project Management [Specialization] - Campaign and brand management projects

Functional Management Areas

- Operations Management [Specialization] - Process optimization and resource allocation
- Human Resources Management [Specialization] - Talent acquisition, development, and retention
- Financial Management [Specialization] - Budget planning, cost control, financial analysis
- Supply Chain Management [Specialization] - End-to-end logistics and vendor management

Emerging Trends and Technologies

- Virtual Team Leadership [Trend] - Managing distributed and remote teams effectively
- AI-Driven Management [Trend] - Artificial intelligence for decision support and automation
- Agile Management Beyond IT [Trend] - Applying agile principles across all business functions
- ESG Management [Trend] - Environmental, Social, and Governance considerations

Certifications and Qualifications

- PMP (Project Management Professional) [Certification] - Global standard for project management
- AMA Certified Professional in Management (CPM) [Certification] - Comprehensive management competencies
- Six Sigma Black Belt [Certification] - Process improvement and quality management
- Scrum Master Certified [Certification] - Agile team facilitation and coaching



2. Soft Skills and Leadership Excellence
Strategic Leadership
Vision Development
Analytics Strategy: Developing 3-5 year analytics roadmaps aligned with business objectives phDataApplexus
Change Management: Leading organizational transformation toward data-driven decision making
Cross-functional Influence: Building analytics adoption across departments without direct authority
Team Leadership
Talent Development: Creating structured mentorship programs and career progression paths CCL
Performance Management: Setting measurable goals, conducting evaluations, and managing underperformance
Cultural Transformation: Fostering collaborative environments that encourage experimentation and learning
Organizational Impact
Executive Communication: Presenting analytics strategy to board members and C-suite executives
Resource Allocation: Securing budgets and prioritizing analytics investments for maximum ROI
Governance Frameworks: Establishing data governance, ethics policies, and quality standards
Communication and Influence
Data Storytelling Mastery
Narrative Construction: Building compelling stories that connect data insights to business strategy HBS Online +3
Audience Adaptation: Tailoring communication style for technical teams, executives, and board members HBS Online +3
Visualization Excellence: Creating impactful visualizations that drive action and decision-making HBS OnlineTechTarget
Stakeholder Management
Expectation Management: Aligning diverse stakeholder expectations with analytics capabilities BrainhubFloat
Conflict Resolution: Managing disagreements between technical and business teams
Partnership Building: Developing trust-based relationships with key business leaders
Knowledge Transfer
Training Design: Creating educational programs to improve analytical literacy across the organization
Documentation Standards: Establishing frameworks for consistent analytical reporting
Mentorship Excellence: Developing junior analysts through structured coaching programs CCL
Business Acumen Development
Financial Literacy
ROI Analysis: Quantifying business value of analytics investments with clear financial projections TestGorilla
Budget Management: Managing analytics budgets, resource allocation, and vendor relationships
Commercial Understanding: Translating analytical insights into revenue opportunities and cost savings
Strategic Thinking
Industry Analysis: Understanding competitive landscapes, market dynamics, and industry trends
Business Model Comprehension: Connecting analytics work to value creation and capture mechanisms
Innovation Management: Identifying new business opportunities enabled by analytics capabilities
Market Intelligence
Competitive Analysis: Benchmarking analytics capabilities against industry leaders
Trend Identification: Anticipating market shifts and technological disruptions
Customer Insight: Understanding customer needs and translating them into analytics requirements




7. Organizational Context and Integration
Analytics Function Models
Centralized Excellence
Structure: Single analytics team serving entire organization dbt LabsHeap
Benefits: Consistent standards, knowledge sharing, economies of scale dbt Labs
Challenges: Potential disconnect from business needs, slower response times dbt Labs
Best Fit: Early-stage analytics maturity, smaller organizations LatentView Analytics
Embedded Distribution
Structure: Analytics professionals within business units dbt LabsHeap
Benefits: Domain expertise, faster response, business alignment dbt Labs
Challenges: Inconsistent standards, duplicated efforts, skill gaps dbt Labs
Best Fit: Large organizations, mature business units dbt Labs
Hybrid Federation (Recommended)
Structure: Central CoE with embedded analysts dbt Labsdbt Labs
Benefits: Balanced governance and agility, standardized yet responsive dbt Labs
Implementation: 70% of organizations use hybrid models dbt LabsTechTarget
Success Factors: Clear roles, regular rotation, strong communication dbt Labs
Cross-functional Collaboration
Marketing Partnership
Shared Objectives: Customer acquisition, retention, and lifetime value optimization 365 Data Science
Collaboration Patterns: Campaign analysis, attribution modeling, customer segmentation 365 Data Science
Success Metrics: Marketing ROI, customer acquisition cost, retention rates Intent Tale365 Data Science
Product Team Integration
Shared Objectives: Product adoption, feature usage, and user experience optimization
Collaboration Patterns: A/B testing, usage analytics, feature impact analysis
Success Metrics: Product adoption rates, feature utilization, user satisfaction
Finance Alignment
Shared Objectives: Financial planning, cost optimization, and revenue growth
Collaboration Patterns: Budget forecasting, variance analysis, profitability modeling
Success Metrics: Forecast accuracy, budget compliance, revenue growth
Organizational Maturity
Stage 1: Reactive Analytics
Characteristics: Manual reporting, basic dashboards, limited data governance Graphable
Timeline: 0-12 months
Investment: $10,000-$50,000
Key Actions: Data quality improvement, basic tool implementation Graphable
Stage 2: Proactive Analytics
Characteristics: Automated reporting, self-service tools, established governance Graphable
Timeline: 12-24 months
Investment: $50,000-$200,000
Key Actions: Advanced analytics implementation, team expansion Graphable
Stage 3: Predictive Analytics
Characteristics: Machine learning models, predictive insights, strategic integration Graphable
Timeline: 24-36 months
Investment: $200,000-$1,000,000
Key Actions: ML platform deployment, advanced skill development Graphable
Stage 4: Prescriptive Analytics
Characteristics: Automated decision-making, optimization algorithms, competitive advantage GraphableAWS
Timeline: 36+ months
Investment: $1,000,000+
Key Actions: AI integration, organizational transformation Graphable


Analytics Manager in Marketing: Comprehensive Role Analysis
Core Responsibilities
Data Strategy Development
The Analytics Manager designs the overall analytics framework for marketing initiatives. They identify which metrics matter most for business goals, establish KPIs, and create measurement plans that align with marketing objectives. They determine what data needs to be collected, how it should be structured, and which analytics tools will provide the most valuable insights.
Cross-Channel Performance Analysis
Marketing analytics managers analyze performance across multiple channels (paid search, social media, email, content marketing, etc.) to evaluate ROI and efficiency. They build comprehensive views of the customer journey by connecting touch points across channels, helping to understand attribution models that accurately reflect how different marketing efforts contribute to conversions.
Predictive Modeling & Forecasting
Using historical data, these managers develop predictive models to forecast marketing outcomes. They might create models for customer lifetime value (CLV), churn prediction, or lead scoring, and use these insights to optimize marketing spend allocation or personalization efforts. These models help the marketing team anticipate trends and make proactive adjustments rather than reactive changes.
Testing Program Management
Analytics Managers design and oversee experimental frameworks like A/B or multivariate testing programs. They establish methodologies for test design, implementation, and analysis while ensuring statistical validity. This helps marketing teams systematically improve campaigns based on empirical evidence rather than assumptions.
Key Functions
Reporting & Dashboard Creation
Analytics Managers build automated reporting systems and interactive dashboards that provide real-time visibility into marketing performance. These visualizations translate complex data into accessible formats for stakeholders at different levels of the organization, from executives to campaign managers. They focus not just on what happened, but why it happened and what actions should follow.
Technical Infrastructure Management
These professionals often oversee the technical infrastructure supporting marketing analytics, including:
Data warehouse architecture and management
ETL (Extract, Transform, Load) processes
Integration between marketing platforms and data systems
Implementation of tracking codes and pixels
Data quality assurance processes
Insight Generation & Communication
Beyond just reporting numbers, Analytics Managers extract meaningful insights and communicate them effectively to stakeholders. They craft data narratives that connect analytical findings to business strategy, helping marketing teams understand the "so what" behind the metrics and guiding tactical decisions based on data.
Team Leadership & Development
For senior roles, Analytics Managers typically lead a team of analysts, data scientists, or data engineers. They provide mentorship, technical guidance, and professional development opportunities while fostering a data-driven culture within the marketing organization.
Specialized Duties
Customer Segmentation & Targeting
Analytics Managers develop sophisticated segmentation models that divide the customer base into meaningful groups based on behaviors, preferences, or value. These segmentation frameworks guide personalization strategies and campaign targeting to improve marketing efficiency.
Marketing Mix Modeling
They conduct complex analyses to determine optimal budget allocation across marketing channels. Marketing mix models help understand the incremental impact of each channel while accounting for external factors like seasonality or competitive activity.
Advanced Analytics Implementation
Analytics Managers typically implement and oversee specialized analytics approaches:

Cohort analysis to understand customer behavior over time
Retention analysis to measure customer loyalty
Path analysis to optimize conversion funnels
Anomaly detection to quickly identify issues or opportunities
Sentiment analysis to gauge brand perception

Vendor Relationship Management
These professionals often manage relationships with analytics vendors, data providers, and technology partners. They evaluate new tools, negotiate contracts, and ensure the organization has access to the most effective analytics capabilities within budget constraints.
Skills and Knowledge Areas
For context, successful Marketing Analytics Managers typically possess:

Technical skills in data analysis tools (SQL, Python, R) and visualization platforms (Tableau, Power BI, Looker)
Deep understanding of marketing channels, metrics, and best practices
Statistical knowledge for experimental design and significance testing
Project management capabilities to coordinate cross-functional analytics initiatives
Communication skills to translate technical findings into business recommendations
Strategic thinking to connect analytics to broader business objectives

Evolving Responsibilities
The role continues to evolve with new technologies and approaches:

Privacy-First Analytics: Adapting measurement strategies to work within increasing privacy restrictions and regulations like GDPR and CCPA
Machine Learning Integration: Applying AI and machine learning to automate insights generation and predictive capabilities
Customer Data Platform Management: Centralizing customer data for unified analysis across touchpoints
Marketing Automation Enhancement: Using analytics to optimize automated marketing journeys and trigger-based campaigns





Strategic Data Leadership & Decision Making
Setting the analytics vision and roadmap for the marketing organization
Translating complex data into actionable business insights for executive stakeholders
Making high-impact recommendations based on data analysis to drive marketing strategy
Defining and monitoring key performance indicators (KPIs) across all marketing channels
Developing measurement frameworks to evaluate marketing effectiveness
Team & Stakeholder Management
Building and leading a team of analysts and data scientists
Managing relationships with key stakeholders across marketing, sales, and product teams
Ensuring alignment between analytics initiatives and broader business objectives
Coaching team members and developing their analytical capabilities
Facilitating data-driven decision making across the organization
Analytics Infrastructure & Governance
Overseeing the marketing analytics tech stack and data architecture
Establishing data quality standards and governance protocols
Implementing attribution models and measurement frameworks
Ensuring compliance with data privacy regulations
Managing analytics budget and resource allocation
Standardizing reporting and analytics processes across teams






1. Strategic Vision & Business Impact
Focus: The ability to align marketing analytics with overarching business objectives and drive measurable outcomes.
Example Topics:
Aligning analytics strategy with company goals.
Measuring ROI and long-term value creation.
Using analytics to inform marketing and business strategy.
Prioritizing between brand-building and performance marketing metrics.

2. Leadership & Team Management
Focus: Building, managing, and mentoring analytics teams while fostering cross-functional collaboration.
Example Topics:
Managing and scaling high-performing teams.
Driving cross-functional initiatives with marketing, product, and sales teams.
Mentoring and upskilling team members.
Handling conflict and managing stakeholder expectations.

3. Technical & Tooling Expertise
Focus: Proficiency in data collection, analysis, and reporting tools as well as statistical techniques.
Example Topics:
Tools: Google Analytics, Tableau, Power BI, SQL, Python, R.
Data integration and warehousing solutions (e.g., BigQuery, Snowflake).
Advanced analytics techniques: regression analysis, clustering, machine learning.
Data quality assurance and governance.

4. Attribution & Tracking
Focus: Designing and implementing systems to track marketing performance and attribute results to channels.
Example Topics:
Types of attribution models (e.g., first-click, last-click, multi-touch).
Setting up and validating tracking systems (e.g., Google Tag Manager).
Addressing challenges in cross-channel or multi-device attribution.
Optimizing campaigns based on attribution insights.

5. Campaign Performance & Optimization
Focus: Analyzing, measuring, and optimizing marketing campaigns for better outcomes.
Example Topics:
Setting and evaluating campaign KPIs.
A/B testing and experimentation frameworks.
Budget allocation and optimization.
Identifying underperforming campaigns and providing recommendations.

6. Data Integration & Reporting
Focus: Combining data from multiple sources to provide actionable insights.
Example Topics:
Merging data from CRM, ad platforms, social media, and other marketing tools.
Creating dashboards and visualizations for stakeholders.
Automating reports to track key metrics.
Ensuring a single source of truth for marketing data.

7. Advanced Analytics & Predictive Modeling
Focus: Using statistical and machine learning methods to forecast trends and improve decision-making.
Example Topics:
Predictive models for CLV, churn, and lead scoring.
Segmentation and clustering techniques.
Time-series forecasting for campaign planning.
Utilizing AI/ML to enhance personalization and targeting.

8. Stakeholder Communication & Storytelling
Focus: Effectively communicating insights to technical and non-technical audiences.
Example Topics:
Simplifying complex data for senior executives.
Creating compelling narratives with data visualizations.
Driving decisions with data-backed recommendations.
Collaborating with creative teams to ensure data-driven content strategies.

9. Marketing Technology (MarTech) & Innovation
Focus: Leveraging MarTech tools and staying updated on industry trends.
Example Topics:
Integrating marketing automation platforms (e.g., HubSpot, Marketo).
Evaluating new tools to enhance analytics capabilities.
Understanding trends like cookie-less tracking or generative AI in marketing.
Implementing CDPs (Customer Data Platforms) for unified profiles.

10. Problem-Solving & Case Studies
Focus: Handling real-world challenges and providing practical solutions.
Example Topics:
Diagnosing issues in underperforming campaigns.
Resolving conflicts between data insights and stakeholder intuition.
Scaling analytics in high-growth environments.
Handling incomplete or messy data sets.

11. Emerging Trends & Industry Knowledge
Focus: Staying updated with evolving trends in marketing and analytics.
Example Topics:
Privacy and data regulation (e.g., GDPR, CCPA).
Trends in zero-party data and customer consent management.
AI and machine learning applications in marketing.
Cross-channel and omnichannel marketing analytics.






First 90 Days: Focus on Learning and Building Relationships
Understand the Business: Immerse yourself in USAA's culture, values, and business model.
Get familiar with their product lines (insurance, banking, investments) and target audience (military members and their families).
Analyze past marketing campaigns and their performance.
Review existing analytics tools, data infrastructure, and reporting processes.
Build Relationships:
Meet with key stakeholders across different departments (marketing, product, IT, finance) to understand their needs and challenges.
Establish strong relationships with your team members. Understand their skills, strengths, and areas for development.
Identify Quick Wins: Look for opportunities to make an immediate impact.
Can you improve an existing reporting dashboard?
Can you optimize a current campaign based on initial data analysis?
Can you identify a key area where better data analysis could drive faster decision-making?
Develop a 90-Day Plan: Present your findings and a prioritized plan for your first 90 days to your manager and key stakeholders.
First Year: Demonstrate Value and Build Momentum
Implement Your Plan: Execute on the priorities you identified in your 90-day plan.
Data Infrastructure & Tools:
Assess the current marketing analytics technology stack and identify any gaps or areas for improvement.
Explore and implement new tools or technologies to enhance data collection, analysis, and reporting (e.g., customer segmentation tools, marketing automation platforms, data visualization software).
Measurement & Reporting:
Establish clear KPIs and reporting frameworks to track marketing performance across all channels.
Develop automated dashboards and reports to provide stakeholders with timely and actionable insights.
Team Development:
Coach and mentor your team members, providing opportunities for growth and skill development.
Foster a culture of collaboration, innovation, and data-driven decision-making.
Two Years: Strategic Leadership and Innovation
Thought Leadership: Position yourself and your team as thought leaders in marketing analytics within USAA.
Share your expertise and insights with senior management.
Present at internal conferences or workshops.
Stay abreast of industry trends and best practices.
Advanced Analytics: Explore and implement advanced analytics techniques (e.g., predictive modeling, machine learning) to drive more sophisticated marketing strategies.
Customer lifetime value prediction
Churn prevention
Personalized marketing recommendations
Cross-functional Collaboration: Lead cross-functional initiatives to leverage data and analytics for broader business impact.
Work with product teams to inform product development.
Partner with finance to optimize marketing ROI.
Collaborate with customer service to improve customer experience.



Responsibilities and Functions
A Marketing Analytics Director is responsible for leading and managing a team of analysts to collect, analyze, and interpret marketing data, transforming it into actionable insights that drive marketing effectiveness and business outcomes. In the digital age, marketers have access to an abundance of data from various sources, and the Marketing Analytics Director plays a critical role in extracting meaningful patterns and trends from this data. Their key responsibilities and functions typically include:  
Developing and Implementing Marketing Analytics Strategies: This involves defining key performance indicators (KPIs), establishing measurement frameworks, and outlining the analytical approaches to be used across various marketing channels.  
Leading and Managing the Analytics Team: This includes setting team goals, providing guidance and mentorship, fostering a data-driven culture, and ensuring the team has the necessary resources and skills to succeed.  
Analyzing Marketing Campaign Performance: This involves collecting and analyzing data from various sources, such as website traffic, social media engagement, email campaigns, and advertising platforms, to assess campaign effectiveness, identify trends, and uncover optimization opportunities.  
Conducting Deep-Dive Data Analysis: This includes performing in-depth analysis to understand customer behavior, identify market trends, and uncover insights that can inform marketing strategies and product development.  
Managing Marketing Performance Tracking Systems: This involves overseeing the implementation and maintenance of marketing analytics tools and technologies, ensuring data accuracy and consistent reporting.  
Collaborating with Cross-Functional Teams: This includes working closely with marketing, sales, product, and IT teams to align on key business metrics, share insights, and ensure data-driven decision-making across the organization.  
Communicating Insights and Recommendations: This involves presenting findings and recommendations to senior management and stakeholders in a clear and concise manner, translating complex data into actionable strategies.  
Staying Up-to-Date with Industry Trends: This includes keeping abreast of the latest marketing analytics technologies, methodologies, and best practices to ensure the team is using the most effective tools and approaches.  
Key Objectives and Performance Indicators
The key objectives of a Marketing Analytics Director are aligned with driving business growth and improving marketing effectiveness. Marketing analytics helps predict customer needs, optimize marketing spending, and ensure that every marketing dollar invested yields the best possible results. Some common objectives and associated performance indicators include:  
Increase Marketing ROI: Measured by metrics such as return on ad spend (ROAS), customer acquisition cost (CAC), and customer lifetime value (CLTV).  
Improve Customer Engagement: Measured by metrics such as website traffic, social media engagement, email open rates, and conversion rates.  
Optimize Marketing Campaigns: Measured by metrics such as click-through rates (CTR), conversion rates, and cost per acquisition (CPA).  
Enhance Customer Segmentation and Targeting: Measured by the effectiveness of targeted marketing campaigns and personalized messaging.
Drive Data-Driven Decision Making: Measured by the adoption of data-driven insights in marketing strategies and business decisions.




Core Areas of Responsibility for a Marketing Analytics Manager
1. Data Analysis and Insights
Collect, analyze, and interpret large datasets from various marketing channels (e.g., paid search, email, social media, SEO, and CRM).
Provide actionable insights into marketing performance metrics, such as ROI, CAC, LTV, churn, and conversion rates.
Identify trends, patterns, and opportunities to improve campaign performance and overall marketing effectiveness.
2. Reporting and Dashboard Creation
Design and maintain dashboards for real-time performance tracking using tools like Tableau, Looker, or Power BI.
Automate reporting processes to reduce manual effort and increase efficiency.
Provide weekly, monthly, and quarterly reports to key stakeholders, including the C-suite, marketing, and sales teams.
3. Marketing Attribution
Develop and refine attribution models (e.g., first-touch, last-touch, multi-touch, or algorithmic models).
Ensure accurate tracking of customer journeys and campaign impact across multiple channels.
Collaborate with marketing teams to integrate attribution findings into campaign planning.
4. Campaign Performance Measurement
Collaborate with the marketing team to define KPIs and success metrics for campaigns.
Conduct post-campaign analysis to determine effectiveness and suggest optimization strategies.
Monitor A/B tests and experiments to evaluate creative, messaging, and targeting strategies.
5. Strategic Collaboration
Work cross-functionally with marketing, sales, finance, and product teams to align goals and analytics efforts.
Provide strategic input to marketing leaders on budget allocation and channel prioritization.
Translate complex data into business insights that influence marketing strategies.
6. Data Integrity and Quality Assurance
Ensure data accuracy, consistency, and cleanliness across marketing tools and databases.
Audit data collection processes and troubleshoot discrepancies in tracking systems like Google Analytics, Adobe Analytics, or Mixpanel.
Implement processes to maintain high-quality data and compliance with regulations (e.g., GDPR, CCPA).
7. Advanced Analytics and Modeling
Build predictive models for customer segmentation, churn analysis, and CLV prediction.
Use statistical and machine learning techniques to inform decision-making and marketing strategies.
Drive the adoption of advanced analytics tools and methodologies within the marketing team.
Core Qualifications for a Marketing Analytics Manager
1. Education and Background
Bachelor’s degree in Marketing, Business, Statistics, Computer Science, Economics, or a related field. (Advanced degrees like an MBA or a Master’s in Analytics are often preferred.)
Experience in marketing analytics, performance marketing, or business intelligence (typically 3–5+ years).
2. Technical Skills
Proficiency in analytics tools like Google Analytics, Adobe Analytics, or Mixpanel.
Experience with data visualization platforms like Tableau, Looker, Power BI, or Data Studio.
Advanced skills in Excel and SQL for data manipulation and analysis.
Familiarity with programming languages such as Python or R for advanced analytics and modeling.
Knowledge of marketing automation platforms (e.g., HubSpot, Marketo) and CRM systems (e.g., Salesforce).
Experience with A/B testing platforms like Optimizely or Google Optimize.
3. Analytical and Business Acumen
Strong ability to translate complex data into clear, actionable insights.
Understanding of digital marketing metrics and the relationship between them (e.g., impressions, CTR, conversions, retention).
Familiarity with statistical methods and predictive modeling techniques.
4. Communication and Stakeholder Management
Excellent written and verbal communication skills for presenting insights to technical and non-technical audiences.
Ability to collaborate with cross-functional teams and manage multiple stakeholders.
Strong storytelling skills with data, aligning analytics with business goals.
5. Leadership and Strategic Thinking
Experience managing teams or mentoring junior analysts (if applicable).
Strategic mindset with a focus on long-term growth and innovation in marketing analytics.
Proven ability to prioritize tasks and manage deadlines in a fast-paced environment.
6. Industry Knowledge
Familiarity with marketing trends, tools, and technologies (e.g., multi-touch attribution, AI-driven insights).
Understanding of customer journey mapping and funnel optimization.
Awareness of privacy and data regulations (e.g., GDPR, CCPA).







Roadmap for a Marketing Analytics Manager
First 90 Days
Establish Foundations
Day 1–30
Learning and Understanding
Orientation and Team Integration
Meet with key stakeholders across marketing, sales, finance, and product teams
Understand the organizational goals, marketing strategies, and KPIs
Familiarize yourself with tools and platforms in use (e.g., Google Analytics, Tableau, SQL databases, or marketing automation platforms).
Audit Existing Analytics Infrastructure
Evaluate current marketing analytics workflows, dashboards, and reporting systems.
Assess data sources for quality, completeness, and accessibility.
Identify gaps in metrics, attribution models, and campaign tracking processes.
Understand Campaign and Performance Metrics

Deep dive into recent marketing campaigns to understand performance drivers.
Review historical data for key marketing KPIs like CAC, LTV, conversion rates, and channel performance.
Document findings to benchmark current performance.
Quick Wins

Address low-hanging fruit, such as fixing broken dashboards or creating new reports for immediate stakeholder needs.
Build trust by solving small but visible issues, e.g., updating metrics definitions or cleaning key datasets.
Day 31–60: Building Credibility and Early Impact

Develop a Strategic Analytics Plan

Collaborate with leadership to define key marketing objectives for the next quarter and year.
Establish analytics priorities aligned with these objectives, such as improving attribution models or campaign ROI analysis.
Data Democratization

Standardize reporting templates and build dashboards that provide actionable insights.
Train stakeholders on accessing and interpreting analytics tools and dashboards.
Cross-Functional Collaboration

Set up regular check-ins with marketing, sales, and product teams to align on goals and analytics needs.
Understand sales and customer feedback to integrate qualitative insights into marketing performance metrics.
Day 61–90: Delivering Value

Implement Process Improvements

Create frameworks for ongoing campaign analysis, forecasting, and funnel performance tracking.
Introduce automated reporting for key metrics, reducing manual effort.
Pilot New Analytical Models

Launch pilot tests for predictive models, e.g., churn prediction, lead scoring, or customer segmentation.
Implement a basic multi-touch attribution model, if applicable.
Deliver Strategic Insights

Present actionable insights to marketing leadership on campaign performance, customer behavior trends, or growth opportunities.
Recommend data-driven optimizations to increase marketing ROI.
Lay the Foundation for Future Growth

Draft a roadmap for advanced analytics projects (e.g., marketing mix modeling, customer journey analysis).
First Year: Scaling Analytics Impact
Quarter 2 (Month 4–6): Refining and Expanding

Deepen Attribution Models

Enhance multi-touch attribution to evaluate the contribution of each marketing channel.
Assess the impact of non-digital channels (e.g., offline campaigns, events) on overall performance.
Segment-Based Insights

Implement audience segmentation to inform personalized campaigns.
Provide insights on high-value segments and their preferred channels.
Measure Marketing ROI

Develop a robust framework for calculating and optimizing campaign ROI.
Introduce budget allocation models based on performance data.
Quarter 3 (Month 7–9): Advanced Analytics Initiatives

Customer Lifetime Value (CLV)

Develop models to predict CLV and inform acquisition and retention strategies.
Optimize spend allocation based on CLV insights.
Experimentation Frameworks

Establish a formalized A/B testing program for campaigns and landing pages.
Train marketing teams on best practices for experimentation.
Forecasting and Predictive Analytics

Build predictive models to forecast campaign outcomes, seasonality trends, and revenue growth.
Use these insights to refine marketing plans proactively.
Quarter 4 (Month 10–12): Leadership and Long-Term Planning

Embed Analytics in Decision-Making

Position analytics as a core component of strategic planning by delivering executive-level insights.
Ensure dashboards and reports are accessible, easy to use, and integral to day-to-day decision-making.
Team Development

Identify areas for skill development among your team and stakeholders.
Mentor team members on advanced analytics techniques and storytelling with data.
Year-End Review and Future Planning

Conduct a comprehensive review of marketing performance, analytics achievements, and challenges.
Propose an advanced analytics roadmap for the next year, focusing on scaling efforts and integrating AI/ML where applicable.
Key Success Metrics
Time-to-insight: Reduction in the time required to gather and interpret data.
Stakeholder adoption: Increased usage of analytics tools and dashboards across teams.
Campaign optimization: Improvement in ROI, CAC, and conversion rates over time.
Attribution accuracy: Enhanced ability to track the impact of marketing spend.
Strategic impact: Influence on long-term planning and resource allocation decisions.
This roadmap ensures short-term wins while setting the foundation for long-term success and innovation in marketing analytics.







Onboarding & Immediate Assessment
Data Landscape & Key Metrics
Audit Existing Data: Review current data sources (CRM, web analytics, ad platforms, transactional data).
Identify Primary KPIs: Clarify the organization’s top-level success metrics (revenue, CAC, ROI/ROAS, retention rates).
Check Data Quality: Assess data accuracy, consistency, and completeness (e.g., are UTMs tracked correctly? Is data siloed or integrated?).
Stakeholder Alignment
Meet Cross-Functional Teams: Marketing, Product, Finance, and Data Engineering to understand their needs.
Clarify Responsibilities: Understand who owns which data sources and how frequently reports/insights are expected.
Set Expectations: Communicate your focus areas (attribution, dashboards, modeling) and timeline for deliverables.

Data Infrastructure & Governance
a) Data Architecture Review
Data Warehouse: Evaluate existing solutions (BigQuery, Redshift, etc.) for scalability and performance.
ETL/ELT Processes: Review pipelines (e.g., Airflow, Fivetran, dbt) to ensure consistent data ingestion and transformations.
BI & Dashboarding Tools: Confirm the tool stack (Tableau, Power BI, Looker) aligns with data infrastructure.
b) Governance & Data Dictionary
Data Ownership & Access: Establish who can access which data (e.g., role-based permissions).
Standard Definitions: Create a centralized data dictionary (e.g., LTV, churn, CAC), ensuring consistent reporting across departments.
Privacy & Compliance: Ensure alignment with regulations (GDPR, CCPA) and manage user consent frameworks.

3. Establishing Core Reporting & Quick Wins
a) Baseline Dashboards & Reporting

Executive Dashboard: Build or refine top-level dashboards for leadership (e.g., weekly or monthly business reviews).
Channel-Specific Reports: Provide granular insights on SEO, paid search, display, social, and email performance.
Conversion Funnel Tracking: Set up funnel analytics to identify drop-off points and opportunities for optimization.
b) Quick Wins & Low-Hanging Fruit

Data Cleanup: Fix inconsistent naming conventions, unify campaign tags, ensure funnel events are properly tracked.
Attribution Fixes: Address any immediate gaps in attribution (e.g., double counting, missing last-touch data).
Automated Alerts: Implement triggers or alerts for anomalies (sudden CTR drops, unexpected CPA spikes).
4. Scaling & Advanced Analytics
a) Attribution Modeling & Experimentation

Multi-Touch Attribution: Evaluate if the company is ready for advanced models (e.g., data-driven or algorithmic).
Incremental & Lift Tests: Implement holdout groups or geo-based experiments to measure true channel lift.
A/B & Multivariate Testing: Systematically test channel strategies, landing pages, creative, and audience targeting.
b) Predictive & Prescriptive Analytics

CLV & Churn Models: Use regression or machine learning methods (e.g., gradient boosting, random forests) to predict lifetime value, churn propensity, and optimal re-engagement.
Forecasting & Budget Allocation: Build forecasting models for revenue or lead volumes, informing budget and resource planning (e.g., marketing mix modeling).
Segmentation & Personalization: Use RFM (recency, frequency, monetary) or custom cluster analyses to tailor marketing strategies and messaging.
5. Team Building & Cross-Functional Collaboration
a) Team Structure & Skills

Hiring & Mentoring: Add specialists (data scientists, analysts, data engineers) or upskill existing team members.
Collaboration Models: Establish how analytics integrates with marketing squads, product pods, or ops teams (e.g., agile sprints, cross-functional projects).
Knowledge Sharing: Set up internal training on analytics best practices, data quality, and performance measurement.
b) Stakeholder Engagement

Regular Updates: Present insights to executives and other stakeholders.
Data-Driven Culture: Advocate for experimentation and evidence-based decision-making across the organization.
Feedback Loops: Gather feedback on reporting and insights, iterating to meet evolving business needs.
6. Ongoing Optimization & Innovation
a) Continuous Improvement

Performance Optimization: Monitor channel performance, iterate on experiments, and refine attribution frameworks.
Process Automation: Reduce manual reporting overhead with automated pipelines, self-service dashboards, and real-time analytics.
Quality Management: Conduct periodic audits of data flows, ensuring minimal drift and maximum reliability.
b) Future-Looking Initiatives

Advanced Modeling: Explore ML-based personalization, recommendation engines, or real-time bidding strategies.
Emerging Channels & Technologies: Stay informed on emerging ad platforms, AI-driven analytics, and privacy-first solutions.
Scaling to Global Markets: Adapt analytics strategies for new geographies (currency conversion, local regulations, different channel usage patterns).






Data & Analytics Foundations
Statistical Analysis Methods • Experience with A/B testing and experimental design • Regression analysis and predictive modeling • Statistical significance and confidence intervals • Time series analysis for trend identification
Technical Skills • SQL proficiency and database management • Python/R for data analysis • BI tools (Tableau, Power BI, Looker) • Marketing automation platforms • Google Analytics and similar tools
Marketing Strategy & Performance
Campaign Analysis • Multi-channel attribution modeling • Customer journey mapping and funnel analysis • ROI measurement methodologies • Budget allocation optimization
Customer Analytics • Segmentation strategies • Lifetime value calculation • Churn prediction and retention analysis • Behavioral analytics
Leadership & Management
Team Development • Building and managing analytics teams • Cross-functional collaboration • Project prioritization • Mentoring and skill development
Stakeholder Management • Executive communication • Data storytelling • Managing expectations • Change management
Business Impact
Strategic Planning • Goal setting and KPI development • Resource allocation • Long-term roadmap development • Business case development
Problem-Solving • Real-world scenario analysis • Past experience with challenging projects • Decision-making process • Risk assessment
Industry Knowledge
Market Understanding • Current marketing technology landscape • Industry trends and best practices • Competitive analysis • Privacy and compliance considerations





Roadmap for a Marketing Analytics Manager
First 90 Days: Establish Foundations
Day 1–30: Learning and Understanding
Orientation and Team Integration
Meet with key stakeholders across marketing, sales, finance, and product teams.
Understand the organizational goals, marketing strategies, and KPIs.
Familiarize yourself with tools and platforms in use (e.g., Google Analytics, Tableau, SQL databases, or marketing automation platforms).
Audit Existing Analytics Infrastructure
Evaluate current marketing analytics workflows, dashboards, and reporting systems.
Assess data sources for quality, completeness, and accessibility.
Identify gaps in metrics, attribution models, and campaign tracking processes.
Understand Campaign and Performance Metrics
Deep dive into recent marketing campaigns to understand performance drivers.
Review historical data for key marketing KPIs like CAC, LTV, conversion rates, and channel performance.
Document findings to benchmark current performance.
Quick Wins
Address low-hanging fruit, such as fixing broken dashboards or creating new reports for immediate stakeholder needs.
Build trust by solving small but visible issues, e.g., updating metrics definitions or cleaning key datasets.
Day 31–60: Building Credibility and Early Impact
Develop a Strategic Analytics Plan
Collaborate with leadership to define key marketing objectives for the next quarter and year.
Establish analytics priorities aligned with these objectives, such as improving attribution models or campaign ROI analysis.
Data Democratization
Standardize reporting templates and build dashboards that provide actionable insights.
Train stakeholders on accessing and interpreting analytics tools and dashboards.
Cross-Functional Collaboration
Set up regular check-ins with marketing, sales, and product teams to align on goals and analytics needs.
Understand sales and customer feedback to integrate qualitative insights into marketing performance metrics.
Day 61–90: Delivering Value
Implement Process Improvements
Create frameworks for ongoing campaign analysis, forecasting, and funnel performance tracking.
Introduce automated reporting for key metrics, reducing manual effort.
Pilot New Analytical Models
Launch pilot tests for predictive models, e.g., churn prediction, lead scoring, or customer segmentation.
Implement a basic multi-touch attribution model, if applicable.
Deliver Strategic Insights
Present actionable insights to marketing leadership on campaign performance, customer behavior trends, or growth opportunities.
Recommend data-driven optimizations to increase marketing ROI.
Lay the Foundation for Future Growth
Draft a roadmap for advanced analytics projects (e.g., marketing mix modeling, customer journey analysis).
First Year: Scaling Analytics Impact
Quarter 2 (Month 4–6): Refining and Expanding
Deepen Attribution Models
Enhance multi-touch attribution to evaluate the contribution of each marketing channel.
Assess the impact of non-digital channels (e.g., offline campaigns, events) on overall performance.
Segment-Based Insights
Implement audience segmentation to inform personalized campaigns.
Provide insights on high-value segments and their preferred channels.
Measure Marketing ROI
Develop a robust framework for calculating and optimizing campaign ROI.
Introduce budget allocation models based on performance data.
Quarter 3 (Month 7–9): Advanced Analytics Initiatives
Customer Lifetime Value (CLV)
Develop models to predict CLV and inform acquisition and retention strategies.
Optimize spend allocation based on CLV insights.
Experimentation Frameworks
Establish a formalized A/B testing program for campaigns and landing pages.
Train marketing teams on best practices for experimentation.
Forecasting and Predictive Analytics
Build predictive models to forecast campaign outcomes, seasonality trends, and revenue growth.
Use these insights to refine marketing plans proactively.
Quarter 4 (Month 10–12): Leadership and Long-Term Planning
Embed Analytics in Decision-Making
Position analytics as a core component of strategic planning by delivering executive-level insights.
Ensure dashboards and reports are accessible, easy to use, and integral to day-to-day decision-making.
Team Development
Identify areas for skill development among your team and stakeholders.
Mentor team members on advanced analytics techniques and storytelling with data.
Year-End Review and Future Planning
Conduct a comprehensive review of marketing performance, analytics achievements, and challenges.
Propose an advanced analytics roadmap for the next year, focusing on scaling efforts and integrating AI/ML where applicable.
Key Success Metrics
Time-to-insight: Reduction in the time required to gather and interpret data.
Stakeholder adoption: Increased usage of analytics tools and dashboards across teams.
Campaign optimization: Improvement in ROI, CAC, and conversion rates over time.
Attribution accuracy: Enhanced ability to track the impact of marketing spend.
Strategic impact: Influence on long-term planning and resource allocation decisions.










Key Methodologies and Frameworks

- McKinsey 7-S Framework [Framework] - Strategy, Structure, Systems, Shared Values, Style, Staff, Skills
- Porter's Five Forces [Framework] - Industry analysis for competitive advantage
- SWOT Analysis [Framework] - Strengths, Weaknesses, Opportunities, Threats assessment
- OKRs (Objectives and Key Results) [Framework] - Goal-setting methodology popularized by Google
- Design Thinking [Methodology] - Human-centered approach to innovation and problem-solving
- Lean Six Sigma [Methodology] - Process improvement and waste reduction









Analytics Knowledge & Utilities (AKU)
ANALYTICS




Analytics, the systematic computational analysis of data or statistics, has become the bedrock of modern decision-making across all sectors.


It encompasses a wide range of job functions, responsibilities, and utilities, from interpreting historical data to predicting future outcomes.




Analytics Summary:
Core Knowledge & Applied Responsibilities


This section defines analytics and outlines its core purpose: to discover and communicate data patterns, transforming raw data into actionable insights that drive business decisions, optimization, and growth.




Data Patterns Discovery, Interpretation, & Communication


Analytics is the process of discovering, interpreting, and communicating significant patterns in data


Data Transformation, Insights, Decisions, Optimization, & Growth Goals


The primary goal is to transform raw data into actionable insights that can drive business decisions, optimize processes, and foster growth


Data Collection, Cleaning, Processing, & Modeling


Job functions within analytics are diverse, ranging from data collection and cleaning to the development of complex predictive models




Analytics Core Concepts:
Key Analytics Terminology & Definitions


This section breaks down the essential vocabulary of the analytics field, defining foundational terms such as the different types of analytics (descriptive, predictive, prescriptive), business intelligence, machine learning, and big data.



Descriptive Analytics
Summarizes historical data to understand what has happened.



































Predictive Analytics
Uses statistical models and forecasting techniques to understand the future and answer "what could happen?".
Prescriptive Analytics
Suggests actions to take based on predictions, helping to answer "what should we do?".
Business Intelligence (BI)
Focuses on using data to understand business performance, often through dashboards and reports tracking Key Performance Indicators (KPIs).
Machine Learning (ML)
A subset of Artificial Intelligence (AI) that uses algorithms to learn from data and make predictions or decisions without being explicitly programmed.
Big Data
The large volume of data, both structured and unstructured, that inundates a business daily.
Data Governance
The overall management of the availability, usability, integrity, and security of data within an organization.

Frameworks & Methodologies: Structuring the Analytical Process
This section details the structured approaches and workflows used in analytics projects, covering foundational requirements, standard processes like CRISP-DM, and key statistical theories that guide effective data analysis.
Foundational Requirements
Core knowledge includes a strong understanding of statistical concepts, data handling techniques, and the ability to translate business problems into analytical questions.
Standard Analytics Workflow
A typical, iterative workflow includes Problem Definition, Data Collection, Data Preparation, Exploratory Data Analysis (EDA), Modeling, Evaluation, and Deployment.
Key Statistical Theories & Applications
Methods include descriptive statistics (summarizing data) and inferential statistics (drawing conclusions). Key applications include Hypothesis Testing, Regression Analysis, and A/B Testing.
Implementation Models & Frameworks
Frameworks like CRISP-DM (Cross-Industry Standard Process for Data Mining) provide a structured, six-phase approach to guide analytics projects from business understanding to deployment.

Skills, Tools & Technologies: The Analyst's Toolkit
This section provides an overview of the essential software and platforms in an analyst's toolkit, covering programming languages, data visualization tools, and the cloud and big data technologies used for data storage and processing.
Programming & Data Management
Languages: Proficiency in Python and R is essential for data analysis and ML, while SQL remains the standard for querying and managing relational databases.
Data Management & Engineering
Understanding of database architecture, ETL pipelines, and data warehousing concepts - including data storage, processing & pipelines; utilizing cloud platforms such as AWS, Google Cloud, and Azure offer scalable services for data storage, processing, and analysis.
Big Data & Transformation: Technologies like Apache Spark process massive datasets, while tools like dbt (data build tool) are used for transforming data directly within a data warehouse.
Statistics & Probability
Solid foundation in correlation, regression analysis, hypothesis testing, and root cause analysis.
Data Analysis & Science
Ability to perform segmentation, cohort analysis, and A/B testing, with advanced skills in predictive modeling and machine learning.
Visualization & Business Intelligence (BI)
Tools: Tableau, Power BI, and Looker are market-leading tools for creating interactive dashboards, charts, and reports that allow for visual data exploration.
Leadership & Strategic Business (Soft) Skills
Critical Thinking & Collaboration: The ability to critically evaluate problems, work effectively in teams, and translate analytical outcomes into business solutions.
Stakeholder Communication & Storytelling: Skill in communicating complex findings to non-technical audiences and using data storytelling techniques to persuade and drive action.

Responsibilities, Functions, & Application: Job Related Segments
This section illustrates the practical application of analytics across various business domains, showcasing real-world examples such as customer segmentation, operational optimization, fraud detection, and personalized recommendations.
Customer-Centric Analytics
Customer Segmentation: Grouping customers based on shared behaviors or characteristics to personalize marketing efforts.
Churn Prediction: Building models to identify customers at high risk of leaving, enabling proactive retention strategies.
Business Operations & Optimization
Operations Optimization: Using data to improve efficiency and reduce costs in areas like supply chain management and manufacturing.
Fraud Detection: Applying analytical models to identify and prevent fraudulent financial transactions in real-time.
Product & Marketing Analytics
Personalized Recommendations: Powering recommendation engines on platforms like Netflix and Amazon to enhance user experience.
Marketing Mix Modeling: Determining the effectiveness of various marketing channels to optimize advertising spend.
Community: Professional Networks & Resources
This section highlights the importance of professional networking and continuous learning by listing key industry associations, online forums, and other community resources that help analytics professionals stay connected and current.
Key Associations & Conferences
Organizations like INFORMS (Institute for Operations Research and the Management Sciences) and DAMA International offer professional development, standards, and networking opportunities.
Forums & Social Communities
Online communities are vital for problem-solving and knowledge sharing, including Stack Overflow for technical questions, Kaggle for competitions and datasets, and Reddit forums like r/datascience and r/analytics.
Content & Thought Leaders
Staying current requires following industry blogs, publications, and newsletters from prominent thought leaders and organizations.
Governance: Ethics, Regulations & Compliance
This section addresses the critical legal and ethical considerations in analytics, focusing on essential topics like data privacy regulations (GDPR, CCPA), algorithmic bias, and the need for transparent and accountable data governance.
Data Privacy & Compliance
Protecting personal and sensitive information is paramount. This involves strict adherence to data protection regulations like Europe's GDPR (General Data Protection Regulation) and the CCPA (California Consumer Privacy Act).
Algorithmic Bias & Fairness
A critical responsibility is to be aware of and mitigate biases inherent in data and algorithms to ensure fair and equitable outcomes.
Transparency & Accountability
Organizations must be transparent with stakeholders about how data is used and remain accountable for the decisions and impacts of their analytical models.
Outlook: Current Trends & Directions
This section explores the emerging technologies and methodologies shaping the analytics landscape, including the growing integration of AI, the demand for real-time processing, and the move toward data democratization and explainable AI.
AI Integration & Automation
Augmented Analytics: The use of AI/ML to augment human intelligence and automate data preparation, insight discovery, and analysis for all user levels.
Real-Time & Edge Processing
Real-Time Analytics: A growing demand for immediate insights that enables businesses to make faster, in-the-moment decisions.
Edge Analytics: The practice of processing data near its source (e.g., on IoT devices) to reduce latency and enable rapid response.
Data Democratization & Architecture
Data Democratization: A movement toward empowering employees at all levels with access to data and user-friendly, self-service analytics tools.
Data Fabric: An emerging architectural approach that simplifies and integrates data management across complex cloud and on-premises environments.
Advanced Methodologies
Explainable AI (XAI): A rising field focused on developing models and techniques that can explain how complex AI systems arrive at their decisions, building trust and transparency.



























Keyword & Phrase Index (KPI)
Most common keywords, terms, and phrases for analytics management roles.
Keyword Utilization Index (KUI)




Keyword
Count
data
240
analytics
144
marketing
134
experience
131
business
108
team
75
insights
67
product
63






Professional Development Roadmaps (PDR)
Skill Acquisition Sequences
Foundation to Intermediate (Years 1-3)
Statistical Fundamentals: Descriptive statistics, hypothesis testing, and basic regression
Programming Basics: SQL proficiency, Python/R fundamentals, and data manipulation
Visualization Skills: Chart selection, dashboard design, and storytelling techniques
Business Acumen: Industry knowledge, financial literacy, and stakeholder management
Intermediate to Advanced (Years 4-6)
Advanced Analytics: Machine learning, predictive modeling, and experimental design
Technical Leadership: Code review, methodology development, and junior mentoring
Strategic Thinking: Business strategy alignment, ROI measurement, and innovation management
Cross-functional Collaboration: Project management, stakeholder influence, and change management
Advanced to Expert (Years 7+)
Thought Leadership: Industry expertise, research publication, and conference speaking
Organizational Impact: Strategy development, budget management, and executive communication
Innovation Management: Emerging technology adoption, methodology development, and competitive advantage
Talent Development: Team building, succession planning, and knowledge transfer






Responsibilities and Functions
A Marketing Analytics Director is responsible for leading and managing a team of analysts to collect, analyze, and interpret marketing data, transforming it into actionable insights that drive marketing effectiveness and business outcomes. In the digital age, marketers have access to an abundance of data from various sources, and the Marketing Analytics Director plays a critical role in extracting meaningful patterns and trends from this data. Their key responsibilities and functions typically include:  
Developing and Implementing Marketing Analytics Strategies: This involves defining key performance indicators (KPIs), establishing measurement frameworks, and outlining the analytical approaches to be used across various marketing channels.  
Leading and Managing the Analytics Team: This includes setting team goals, providing guidance and mentorship, fostering a data-driven culture, and ensuring the team has the necessary resources and skills to succeed.  
Analyzing Marketing Campaign Performance: This involves collecting and analyzing data from various sources, such as website traffic, social media engagement, email campaigns, and advertising platforms, to assess campaign effectiveness, identify trends, and uncover optimization opportunities.  
Conducting Deep-Dive Data Analysis: This includes performing in-depth analysis to understand customer behavior, identify market trends, and uncover insights that can inform marketing strategies and product development.  
Managing Marketing Performance Tracking Systems: This involves overseeing the implementation and maintenance of marketing analytics tools and technologies, ensuring data accuracy and consistent reporting.  
Collaborating with Cross-Functional Teams: This includes working closely with marketing, sales, product, and IT teams to align on key business metrics, share insights, and ensure data-driven decision-making across the organization.  
Communicating Insights and Recommendations: This involves presenting findings and recommendations to senior management and stakeholders in a clear and concise manner, translating complex data into actionable strategies.  
Staying Up-to-Date with Industry Trends: This includes keeping abreast of the latest marketing analytics technologies, methodologies, and best practices to ensure the team is using the most effective tools and approaches.  
Key Objectives and Performance Indicators
The key objectives of a Marketing Analytics Director are aligned with driving business growth and improving marketing effectiveness. Marketing analytics helps predict customer needs, optimize marketing spending, and ensure that every marketing dollar invested yields the best possible results. Some common objectives and associated performance indicators include:  
Increase Marketing ROI: Measured by metrics such as return on ad spend (ROAS), customer acquisition cost (CAC), and customer lifetime value (CLTV).  
Improve Customer Engagement: Measured by metrics such as website traffic, social media engagement, email open rates, and conversion rates.  
Optimize Marketing Campaigns: Measured by metrics such as click-through rates (CTR), conversion rates, and cost per acquisition (CPA).  
Enhance Customer Segmentation and Targeting: Measured by the effectiveness of targeted marketing campaigns and personalized messaging.
Drive Data-Driven Decision Making: Measured by the adoption of data-driven insights in marketing strategies and business decisions.

Basic SQL interview questions for beginners (0 to 2 years of experience) 
Basic SQL data types and simple SELECT query
Question: Write a SQL query that retrieves the `first_name`, `last_name`, and `email` columns from a table named `users`, where the `email` domain is “example.com”. Assume that `email` is a `VARCHAR` type.

Example Answer:

SELECT first_name, last_name, email

FROM users

WHERE email LIKE '%@example.com';
Explanation: This query selects the `first_name`, `last_name`, and `email` columns from the `users` table and filters the rows to include only those with an email domain of “example.com”. The `LIKE` operator is used with a wildcard (`%`) to match any characters before “@example.com”.

SQL joins and relationships
Question: Write a SQL query to retrieve the `order_id` and `order_date` from an `orders` table and the `product_name` from a `products` table for all orders. Assume that the `orders` table has a `product_id` foreign key that references the `product_id` in the `products` table.

Example Answer:

SELECT o.order_id, o.order_date, p.product_name

FROM orders o

JOIN products p ON o.product_id = p.product_id;
Explanation: This query retrieves data from both the `orders` and `products` tables using an `INNER JOIN`. The `JOIN` is performed on the `product_id` column, which is common between the two tables, allowing the query to combine rows from each table where there is a matching `product_id`.

Basic data manipulation
Question: Write a SQL query to update the `salary` column in the `employees` table, increasing it by 10% for all employees who work in the “Sales” department. Assume the `department` column is of type `VARCHAR`.

Example Answer:

UPDATE employees

SET salary = salary * 1.10

WHERE department = 'Sales';
Explanation: This query updates the `salary` field in the `employees` table by multiplying the current salary by 1.10 (a 10% increase) for all employees in the “Sales” department. The `WHERE` clause ensures that only rows where the `department` is “Sales” are affected.

Learning tip: Want to review SQL basics before your next interview? Journey into SQL with Taylor Swift is a fun and accessible learning path in CodeSignal Learn where you’ll practice key querying skills using Taylor Swift’s discography as your database.

Intermediate SQL interview questions (2 to 5 years of experience) 
Complex SQL queries and subqueries
Question: Write a SQL query to find the top 3 customers with the highest total `order_amount` from the `orders` table. Assume that each order is linked to a customer via a `customer_id` column, and the `order_amount` is a numeric column.

Example Answer:

SELECT customer_id, SUM(order_amount) AS total_spent

FROM orders

GROUP BY customer_id

ORDER BY total_spent DESC

LIMIT 3;
Explanation: This query calculates the total `order_amount` spent by each customer using the `SUM()` function and groups the results by `customer_id`. The `ORDER BY` clause sorts the results in descending order of total spent, and the `LIMIT` clause restricts the output to the top 3 customers. This type of query is essential for analyzing customer behavior and identifying high-value customers.

Subqueries and data integrity
Question: Write a SQL query to find all employees in the `employees` table whose `salary` is greater than the average salary in their department. Assume that the table has `employee_id`, `department_id`, and `salary` columns.

Example Answer:

SELECT employee_id, department_id, salary

FROM employees e

WHERE salary > (

    SELECT AVG(salary)

    FROM employees

    WHERE department_id = e.department_id

);
Explanation: This query uses a subquery to calculate the average salary within each department. The main query then selects employees whose salary exceeds the average salary of their respective department. The use of correlated subqueries (where the subquery references a column from the outer query) is a powerful technique for comparing data within grouped contexts.

Indexes, performance, and transaction control
Question: Suppose you need to delete a large number of records from the `transactions` table where the `transaction_date` is older than one year. Write a SQL script that includes steps to ensure the deletion is efficient and doesn’t affect the performance of the database during the operation. Assume an index exists on the `transaction_date` column.

Example Answer:

BEGIN;

SET TRANSACTION ISOLATION LEVEL READ COMMITTED;

DELETE FROM transactions

WHERE transaction_date < NOW() - INTERVAL '1 year';

COMMIT;
Explanation: This script begins with a `BEGIN` statement to start a transaction. The `SET TRANSACTION ISOLATION LEVEL` command ensures that the operation uses the appropriate isolation level to prevent reading data that has been modified but not committed by other transactions (dirty reads), improving performance during the deletion. The `DELETE` operation then removes records older than one year, leveraging the existing index on `transaction_date` for faster execution. Finally, the `COMMIT` statement ensures that all changes are saved permanently, maintaining data integrity and consistency.

Learning tip: Refresh your SQL scripting skills before your next interview or assessment with the Learning SQL Scripting with Leo Messi learning path in CodeSignal Learn. Practice joins, functions, conditional logic, and more using stats from soccer star Lionel Messi’s career as your database. 

Advanced SQL interview questions (5 years experience or more)
SQL optimization techniques and handling large datasets
Question: You have a table `large_sales` with millions of rows and a composite index on `(customer_id, sale_date) named `idx_customer_date`. Write an optimized SQL query to retrieve the total sales amount for each `customer_id` in the year 2023, considering the potential performance impact due to the dataset size.

Example Answer:

SELECT customer_id, SUM(sale_amount) AS total_sales

FROM large_sales

WHERE sale_date BETWEEN '2023-01-01' AND '2023-12-31'

GROUP BY customer_id

USE INDEX (idx_customer_date);
Explanation: This query retrieves the total sales amount for each `customer_id` for the year 2023 from a very large dataset. By specifying the `USE INDEX` hint, the query explicitly directs the database to utilize the composite index on `(customer_id, sale_date)` to optimize the filtering and grouping operations instead of an index on just `sale_date`. This is crucial for maintaining performance when dealing with large datasets, as it minimizes the amount of data scanned.

Advanced data modeling and stored procedures
Question: Design a stored procedure named `UpdateEmployeeDepartment` that transfers an employee to a new department while ensuring that the new department’s `budget` is not exceeded. Assume that `employees` and `departments` tables exist, with `employees` containing `employee_id`, `department_id`, and `salary`, and `departments` containing `department_id`, `budget`, and `current_expenditure`.

Example Answer:

DELIMITER //

CREATE PROCEDURE UpdateEmployeeDepartment(IN emp_id INT, IN new_dept_id INT)

BEGIN

    DECLARE emp_salary DECIMAL(10,2);

    DECLARE current_expenditure DECIMAL(10,2);

    DECLARE dept_budget DECIMAL(10,2);

    SELECT salary INTO emp_salary FROM employees WHERE employee_id = emp_id;

    SELECT current_expenditure, budget INTO current_expenditure, dept_budget 

    FROM departments WHERE department_id = new_dept_id;

    IF current_expenditure + emp_salary <= dept_budget THEN

        UPDATE employees SET department_id = new_dept_id WHERE employee_id = emp_id;

        UPDATE departments SET current_expenditure = current_expenditure + emp_salary 

        WHERE department_id = new_dept_id;

    ELSE

        SIGNAL SQLSTATE '45000' SET MESSAGE_TEXT = 'Budget exceeded for the new department';

    END IF;

END //

DELIMITER ;
Explanation: This stored procedure first retrieves the salary of the employee being transferred and the budget and current expenditure of the target department. It then checks if adding the employee’s salary to the department’s current expenditure would exceed the department’s budget. If not, the employee is transferred, and the department’s expenditure is updated. If the budget would be exceeded, the procedure raises an error, ensuring budget constraints are respected. This approach demonstrates advanced data modeling by handling complex relationships between entities in the database.

Database architecture considerations and triggers
Question: Write a trigger named `CheckInventoryBeforeInsert` that prevents the insertion of a new order in the `orders` table if the total quantity of items ordered exceeds the available stock in the `inventory` table. Assume the `orders` table has `product_id` and `quantity` columns, and the `inventory` table has `product_id` and `stock_quantity` columns.

Example Answer:

DELIMITER //

CREATE TRIGGER CheckInventoryBeforeInsert

BEFORE INSERT ON orders

FOR EACH ROW

BEGIN

    DECLARE available_stock INT;

    SELECT stock_quantity INTO available_stock 

    FROM inventory 

    WHERE product_id = NEW.product_id;

    IF NEW.quantity > available_stock THEN

        SIGNAL SQLSTATE '45000' SET MESSAGE_TEXT = 'Insufficient stock for the product';

    END IF;

END //

DELIMITER ;
Explanation: This trigger executes before a new order is inserted into the `orders` table. It checks if the quantity being ordered exceeds the available stock in the `inventory` table. If the order quantity is greater than the available stock, the trigger prevents the insert operation by raising an error. This ensures that the database maintains data integrity and consistency, crucial for systems where inventory management is critical. It also reflects an understanding of how triggers can enforce business rules at the database level, which is a key consideration in robust database architecture.

Hard SQL server interview questions for senior developers (10+ years of experience)
High-availability solutions and disaster recovery strategies
Question: Can you describe a high-availability solution for an SQL Server environment, and how you would implement a disaster recovery plan to minimize downtime and data loss?

Example Answer: I would use Always On Availability Groups for high availability, setting up primary and secondary replicas across different servers, ideally in separate geographic locations. The primary replica handles transactions, while secondary replicas are kept in sync.

For disaster recovery, I’d configure a secondary replica in a remote data center with automatic failover. This setup ensures minimal downtime and no data loss if the primary server fails. I’d also establish regular backups and test the failover process to ensure reliability.

Performance tuning complex systems
Question: Can you walk me through your approach to diagnosing and resolving performance issues in a complex SQL Server system with multiple large databases?

Example Answer: I start by analyzing wait statistics to find bottlenecks like CPU or I/O issues. Then, I examine query execution plans to spot inefficiencies, such as unnecessary table scans.

For optimization, I may tune indexes, rewrite queries, or partition large tables. I also check system configurations, such as memory and I/O settings, and ensure regular maintenance tasks like index rebuilding are in place to keep performance stable.

Security best practices in SQL server management
Question: What are some of the security best practices you follow when setting up and managing SQL Server databases?

Example Answer: I follow the principle of least privilege, assigning minimal permissions needed for tasks. I integrate SQL Server with Active Directory for secure authentication and use encryption for sensitive data with tools like Transparent Data Encryption (TDE).

I also ensure SQL Server is regularly patched and perform security audits to monitor for unauthorized access. Regular reviews of activity logs help me quickly detect and respond to any security issues.

SQL performance tuning interview questions
Query optimization and execution plans analysis
Question: How do you approach optimizing a slow-running query in SQL Server, and what role do execution plans play in this process?

Example Answer: When optimizing a slow query, I start by analyzing its execution plan to identify bottlenecks like full table scans or expensive joins. The execution plan shows how SQL Server processes the query, helping me spot inefficiencies.

Based on the plan, I might rewrite the query, add or modify indexes, or adjust the query structure to reduce processing time. I continually review the updated execution plan to ensure the changes improve performance.

Index management and query optimization
Question: Can you explain your process for managing indexes to ensure efficient query performance in SQL Server?

Example Answer: I regularly monitor index usage to identify underutilized or missing indexes. If a query is slow, I check the execution plan to see if an index could improve performance.

I also evaluate existing indexes to ensure they are not redundant or overlapping, which could cause unnecessary overhead. Periodically, I perform index maintenance, such as rebuilding or reorganizing fragmented indexes, to keep the database performing optimally.
Basic SQL interview questions for beginners (0 to 2 years of experience) 
Basic SQL data types and simple SELECT query
Question: Write a SQL query that retrieves the `first_name`, `last_name`, and `email` columns from a table named `users`, where the `email` domain is “example.com”. Assume that `email` is a `VARCHAR` type.

Example Answer:

SELECT first_name, last_name, email

FROM users

WHERE email LIKE '%@example.com';
Explanation: This query selects the `first_name`, `last_name`, and `email` columns from the `users` table and filters the rows to include only those with an email domain of “example.com”. The `LIKE` operator is used with a wildcard (`%`) to match any characters before “@example.com”.

SQL joins and relationships
Question: Write a SQL query to retrieve the `order_id` and `order_date` from an `orders` table and the `product_name` from a `products` table for all orders. Assume that the `orders` table has a `product_id` foreign key that references the `product_id` in the `products` table.

Example Answer:

SELECT o.order_id, o.order_date, p.product_name

FROM orders o

JOIN products p ON o.product_id = p.product_id;
Explanation: This query retrieves data from both the `orders` and `products` tables using an `INNER JOIN`. The `JOIN` is performed on the `product_id` column, which is common between the two tables, allowing the query to combine rows from each table where there is a matching `product_id`.

Basic data manipulation
Question: Write a SQL query to update the `salary` column in the `employees` table, increasing it by 10% for all employees who work in the “Sales” department. Assume the `department` column is of type `VARCHAR`.

Example Answer:

UPDATE employees

SET salary = salary * 1.10

WHERE department = 'Sales';
Explanation: This query updates the `salary` field in the `employees` table by multiplying the current salary by 1.10 (a 10% increase) for all employees in the “Sales” department. The `WHERE` clause ensures that only rows where the `department` is “Sales” are affected.

Learning tip: Want to review SQL basics before your next interview? Journey into SQL with Taylor Swift is a fun and accessible learning path in CodeSignal Learn where you’ll practice key querying skills using Taylor Swift’s discography as your database.

Intermediate SQL interview questions (2 to 5 years of experience) 
Complex SQL queries and subqueries
Question: Write a SQL query to find the top 3 customers with the highest total `order_amount` from the `orders` table. Assume that each order is linked to a customer via a `customer_id` column, and the `order_amount` is a numeric column.

Example Answer:

SELECT customer_id, SUM(order_amount) AS total_spent

FROM orders

GROUP BY customer_id

ORDER BY total_spent DESC

LIMIT 3;
Explanation: This query calculates the total `order_amount` spent by each customer using the `SUM()` function and groups the results by `customer_id`. The `ORDER BY` clause sorts the results in descending order of total spent, and the `LIMIT` clause restricts the output to the top 3 customers. This type of query is essential for analyzing customer behavior and identifying high-value customers.

Subqueries and data integrity
Question: Write a SQL query to find all employees in the `employees` table whose `salary` is greater than the average salary in their department. Assume that the table has `employee_id`, `department_id`, and `salary` columns.

Example Answer:

SELECT employee_id, department_id, salary

FROM employees e

WHERE salary > (

    SELECT AVG(salary)

    FROM employees

    WHERE department_id = e.department_id

);
Explanation: This query uses a subquery to calculate the average salary within each department. The main query then selects employees whose salary exceeds the average salary of their respective department. The use of correlated subqueries (where the subquery references a column from the outer query) is a powerful technique for comparing data within grouped contexts.

Indexes, performance, and transaction control
Question: Suppose you need to delete a large number of records from the `transactions` table where the `transaction_date` is older than one year. Write a SQL script that includes steps to ensure the deletion is efficient and doesn’t affect the performance of the database during the operation. Assume an index exists on the `transaction_date` column.

Example Answer:

BEGIN;

SET TRANSACTION ISOLATION LEVEL READ COMMITTED;

DELETE FROM transactions

WHERE transaction_date < NOW() - INTERVAL '1 year';

COMMIT;
Explanation: This script begins with a `BEGIN` statement to start a transaction. The `SET TRANSACTION ISOLATION LEVEL` command ensures that the operation uses the appropriate isolation level to prevent reading data that has been modified but not committed by other transactions (dirty reads), improving performance during the deletion. The `DELETE` operation then removes records older than one year, leveraging the existing index on `transaction_date` for faster execution. Finally, the `COMMIT` statement ensures that all changes are saved permanently, maintaining data integrity and consistency.

Learning tip: Refresh your SQL scripting skills before your next interview or assessment with the Learning SQL Scripting with Leo Messi learning path in CodeSignal Learn. Practice joins, functions, conditional logic, and more using stats from soccer star Lionel Messi’s career as your database. 

Advanced SQL interview questions (5 years experience or more)
SQL optimization techniques and handling large datasets
Question: You have a table `large_sales` with millions of rows and a composite index on `(customer_id, sale_date) named `idx_customer_date`. Write an optimized SQL query to retrieve the total sales amount for each `customer_id` in the year 2023, considering the potential performance impact due to the dataset size.

Example Answer:

SELECT customer_id, SUM(sale_amount) AS total_sales

FROM large_sales

WHERE sale_date BETWEEN '2023-01-01' AND '2023-12-31'

GROUP BY customer_id

USE INDEX (idx_customer_date);
Explanation: This query retrieves the total sales amount for each `customer_id` for the year 2023 from a very large dataset. By specifying the `USE INDEX` hint, the query explicitly directs the database to utilize the composite index on `(customer_id, sale_date)` to optimize the filtering and grouping operations instead of an index on just `sale_date`. This is crucial for maintaining performance when dealing with large datasets, as it minimizes the amount of data scanned.

Advanced data modeling and stored procedures
Question: Design a stored procedure named `UpdateEmployeeDepartment` that transfers an employee to a new department while ensuring that the new department’s `budget` is not exceeded. Assume that `employees` and `departments` tables exist, with `employees` containing `employee_id`, `department_id`, and `salary`, and `departments` containing `department_id`, `budget`, and `current_expenditure`.

Example Answer:

DELIMITER //

CREATE PROCEDURE UpdateEmployeeDepartment(IN emp_id INT, IN new_dept_id INT)

BEGIN

    DECLARE emp_salary DECIMAL(10,2);

    DECLARE current_expenditure DECIMAL(10,2);

    DECLARE dept_budget DECIMAL(10,2);

    SELECT salary INTO emp_salary FROM employees WHERE employee_id = emp_id;

    SELECT current_expenditure, budget INTO current_expenditure, dept_budget 

    FROM departments WHERE department_id = new_dept_id;

    IF current_expenditure + emp_salary <= dept_budget THEN

        UPDATE employees SET department_id = new_dept_id WHERE employee_id = emp_id;

        UPDATE departments SET current_expenditure = current_expenditure + emp_salary 

        WHERE department_id = new_dept_id;

    ELSE

        SIGNAL SQLSTATE '45000' SET MESSAGE_TEXT = 'Budget exceeded for the new department';

    END IF;

END //

DELIMITER ;
Explanation: This stored procedure first retrieves the salary of the employee being transferred and the budget and current expenditure of the target department. It then checks if adding the employee’s salary to the department’s current expenditure would exceed the department’s budget. If not, the employee is transferred, and the department’s expenditure is updated. If the budget would be exceeded, the procedure raises an error, ensuring budget constraints are respected. This approach demonstrates advanced data modeling by handling complex relationships between entities in the database.

Database architecture considerations and triggers
Question: Write a trigger named `CheckInventoryBeforeInsert` that prevents the insertion of a new order in the `orders` table if the total quantity of items ordered exceeds the available stock in the `inventory` table. Assume the `orders` table has `product_id` and `quantity` columns, and the `inventory` table has `product_id` and `stock_quantity` columns.

Example Answer:

DELIMITER //

CREATE TRIGGER CheckInventoryBeforeInsert

BEFORE INSERT ON orders

FOR EACH ROW

BEGIN

    DECLARE available_stock INT;

    SELECT stock_quantity INTO available_stock 

    FROM inventory 

    WHERE product_id = NEW.product_id;

    IF NEW.quantity > available_stock THEN

        SIGNAL SQLSTATE '45000' SET MESSAGE_TEXT = 'Insufficient stock for the product';

    END IF;

END //

DELIMITER ;
Explanation: This trigger executes before a new order is inserted into the `orders` table. It checks if the quantity being ordered exceeds the available stock in the `inventory` table. If the order quantity is greater than the available stock, the trigger prevents the insert operation by raising an error. This ensures that the database maintains data integrity and consistency, crucial for systems where inventory management is critical. It also reflects an understanding of how triggers can enforce business rules at the database level, which is a key consideration in robust database architecture.

Hard SQL server interview questions for senior developers (10+ years of experience)
High-availability solutions and disaster recovery strategies
Question: Can you describe a high-availability solution for an SQL Server environment, and how you would implement a disaster recovery plan to minimize downtime and data loss?

Example Answer: I would use Always On Availability Groups for high availability, setting up primary and secondary replicas across different servers, ideally in separate geographic locations. The primary replica handles transactions, while secondary replicas are kept in sync.

For disaster recovery, I’d configure a secondary replica in a remote data center with automatic failover. This setup ensures minimal downtime and no data loss if the primary server fails. I’d also establish regular backups and test the failover process to ensure reliability.

Performance tuning complex systems
Question: Can you walk me through your approach to diagnosing and resolving performance issues in a complex SQL Server system with multiple large databases?

Example Answer: I start by analyzing wait statistics to find bottlenecks like CPU or I/O issues. Then, I examine query execution plans to spot inefficiencies, such as unnecessary table scans.

For optimization, I may tune indexes, rewrite queries, or partition large tables. I also check system configurations, such as memory and I/O settings, and ensure regular maintenance tasks like index rebuilding are in place to keep performance stable.

Security best practices in SQL server management
Question: What are some of the security best practices you follow when setting up and managing SQL Server databases?

Example Answer: I follow the principle of least privilege, assigning minimal permissions needed for tasks. I integrate SQL Server with Active Directory for secure authentication and use encryption for sensitive data with tools like Transparent Data Encryption (TDE).

I also ensure SQL Server is regularly patched and perform security audits to monitor for unauthorized access. Regular reviews of activity logs help me quickly detect and respond to any security issues.

SQL performance tuning interview questions
Query optimization and execution plans analysis
Question: How do you approach optimizing a slow-running query in SQL Server, and what role do execution plans play in this process?

Example Answer: When optimizing a slow query, I start by analyzing its execution plan to identify bottlenecks like full table scans or expensive joins. The execution plan shows how SQL Server processes the query, helping me spot inefficiencies.

Based on the plan, I might rewrite the query, add or modify indexes, or adjust the query structure to reduce processing time. I continually review the updated execution plan to ensure the changes improve performance.

Index management and query optimization
Question: Can you explain your process for managing indexes to ensure efficient query performance in SQL Server?

Example Answer: I regularly monitor index usage to identify underutilized or missing indexes. If a query is slow, I check the execution plan to see if an index could improve performance.

I also evaluate existing indexes to ensure they are not redundant or overlapping, which could cause unnecessary overhead. Periodically, I perform index maintenance, such as rebuilding or reorganizing fragmented indexes, to keep the database performing optimally.

SQL server profiler and database tuning advisor
Question: How do you use SQL Server Profiler and Database Tuning Advisor to enhance database performance?

Example Answer: I use SQL Server Profiler to capture and analyze slow-running queries or resource-intensive operations. The trace data helps me identify patterns and specific queries that need optimization.

Then, I run these queries through the Database Tuning Advisor, which provides recommendations for indexing, partitioning, and other optimizations. This combination allows me to make data-driven decisions to enhance performance while avoiding guesswork.

Role-based SQL interview questions
SQL developer interview questions
Development environment setup and debugging SQL scripts
Question: Write a SQL script that sets up a development environment by creating a new schema named `dev_environment`, and within that schema, create a table `test_data` with columns `id` (INT, primary key) and `value` (VARCHAR). Then, include a statement to debug by inserting a sample record into the `test_data` table and verifying that the record was correctly inserted.

Example Answer:

CREATE SCHEMA dev_environment;

CREATE TABLE dev_environment.test_data (

    id INT PRIMARY KEY,

    value VARCHAR(100)

);

INSERT INTO dev_environment.test_data (id, value)

VALUES (1, 'Sample Data');

-- Debugging step: Check the inserted record

SELECT * FROM dev_environment.test_data WHERE id = 1;
Explanation: This script first creates a new schema named `dev_environment` to organize the development environment. It then creates a `test_data` table within that schema with an `id` column as the primary key and a `value` column for storing text data. The script includes a sample `INSERT` statement to add a record to the `test_data` table and a `SELECT` statement to verify that the insertion was successful. This approach helps in setting up a consistent development environment while also incorporating basic debugging practices.

Code versioning in SQL and best practices in database schema design
Question: Write a SQL script to create a version-controlled stored procedure that adds a new column `email` (VARCHAR) to an existing `users` table. Include comments that explain the purpose of the changes and a method to rollback the change if needed.

Example Answer:

-- Version 1.1: Adding an email column to users table

-- Purpose: To store email addresses of users

ALTER TABLE users

ADD email VARCHAR(255);

-- Rollback script: Remove the email column if the change needs to be undone

-- Version 1.1 Rollback

-- Purpose: To rollback the addition of the email column in case of issues

-- ALTER TABLE users

-- DROP COLUMN email;
Explanation: This script demonstrates best practices in code versioning and schema design. It includes an `ALTER TABLE` statement to add an `email` column to the `users` table, following a versioning format in the comments to track changes. The comments clearly explain the purpose of the update. Additionally, the script provides a rollback mechanism (commented out) to remove the `email` column if the change needs to be undone, promoting safe and controlled schema changes.

SQL interview questions for data analysts
SQL for data extraction and analytical functions in SQL
Question: Write a SQL query that extracts the total sales and calculates the average sales per month for each product in the `sales` table. The table contains `product_id`, `sale_date`, and `sale_amount` columns. Use SQL analytical functions to achieve this.

Example Answer:

WITH monthly_sales AS (

    SELECT 

        product_id, 

        EXTRACT(YEAR FROM sale_date) AS sale_year,

        EXTRACT(MONTH FROM sale_date) AS sale_month,

        SUM(sale_amount) AS monthly_total_sales

    FROM 

        sales

    GROUP BY 

        product_id, 

        EXTRACT(YEAR FROM sale_date), 

        EXTRACT(MONTH FROM sale_date)

)

SELECT 

    product_id,

    SUM(monthly_total_sales) AS total_sales,

    AVG(monthly_total_sales) AS avg_monthly_sales

FROM 

    monthly_sales

GROUP BY 

    product_id;
Explanation: This query uses SQL analytical functions to calculate the total sales and the average monthly sales for each product. The `SUM(sale_amount)` function aggregates the sales by `product_id`, month, and year. The `AVG()` function calculates the average of these monthly totals. This allows for a detailed analysis of sales patterns across products on a monthly basis.

Advanced reporting techniques and data visualization with SQL
Question: Write a SQL query to generate a report that shows the cumulative sales by month for the current year for each region. The `sales` table includes `region`, `sale_date`, and `sale_amount` columns. Ensure the report is ordered by region and month.

Example Answer:

SELECT 

    region, 

    EXTRACT(MONTH FROM sale_date) AS sale_month, 

    SUM(sale_amount) AS monthly_sales,

    SUM(SUM(sale_amount)) OVER (PARTITION BY region ORDER BY EXTRACT(MONTH FROM sale_date)) AS cumulative_sales

FROM 

    sales

WHERE 

    EXTRACT(YEAR FROM sale_date) = EXTRACT(YEAR FROM CURRENT_DATE)

GROUP BY 

    region, EXTRACT(MONTH FROM sale_date)

ORDER BY 

    region, sale_month;
Explanation: This query produces an advanced report that shows both monthly and cumulative sales by region for the current year. The `SUM(sale_amount)` function calculates the monthly sales per region. The cumulative sales are calculated using `SUM(SUM(sale_amount)) OVER (PARTITION BY region ORDER BY EXTRACT(MONTH FROM sale_date))`, which sums the monthly totals progressively. The report is ordered by region and then by month, making it useful for visualizations that track sales trends across regions over time.

SQL interview questions for data engineers
ETL processes and data quality + cleaning
Question: Write a SQL script that performs an ETL (Extract, Transform, Load) process to clean and load data from a `raw_sales` table into a `cleaned_sales` table. The `raw_sales` table contains `sale_id`, `sale_date`, `product_id`, `sale_amount`, and `customer_id`, where `sale_amount` may contain null or negative values. Clean the data by removing rows with null or negative `sale_amount`, and load the cleaned data into the `cleaned_sales` table.

Example Answer:

-- Step 1: Extract and Clean Data

INSERT INTO cleaned_sales (sale_id, sale_date, product_id, sale_amount, customer_id)

SELECT 

    sale_id, 

    sale_date, 

    product_id, 

    sale_amount, 

    customer_id

FROM 

    raw_sales

WHERE 

    sale_amount IS NOT NULL AND sale_amount > 0;

-- Step 2: Optional additional transformations can be applied here
Explanation: This script performs a basic ETL operation by extracting data from the `raw_sales` table, cleaning it by removing rows where `sale_amount` is null or negative, and then loading the cleaned data into the `cleaned_sales` table. This ensures that only valid sales data is stored in the `cleaned_sales` table, improving data quality for further analysis or reporting.

Data warehousing with SQL and SQL in data pipeline design
Question: Design a SQL query that aggregates daily sales data from a `daily_sales` table and loads it into a `monthly_sales_summary` table. The `daily_sales` table contains `sale_date`, `product_id`, and `sale_amount`. The `monthly_sales_summary` table should store `year`, `month`, `product_id`, and `total_sales`.

Example Answer:

-- Step 1: Aggregate Daily Sales into Monthly Totals

INSERT INTO monthly_sales_summary (year, month, product_id, total_sales)

SELECT 

    EXTRACT(YEAR FROM sale_date) AS year,

    EXTRACT(MONTH FROM sale_date) AS month,

    product_id,

    SUM(sale_amount) AS total_sales

FROM 

    daily_sales

GROUP BY 

    EXTRACT(YEAR FROM sale_date), EXTRACT(MONTH FROM sale_date), product_id;

-- Step 2: This data can now be used for reporting or further analysis
Explanation: This query aggregates daily sales data into monthly totals, which are then stored in the `monthly_sales_summary` table. The `EXTRACT(YEAR FROM sale_date)` and `EXTRACT(MONTH FROM sale_date)` functions are used to group the data by year and month. The `SUM(sale_amount)` function calculates the total sales per product for each month. This process is a common step in data warehousing, where data is aggregated and summarized for more efficient storage and faster querying.

Scenario-based SQL interview questions
Real-world problem-solving with SQL and handling corrupt data
Question: Can you describe how you would handle a situation where you find corrupt data in a critical production table, such as missing or invalid values in key columns?

Example Answer: If I encounter corrupt data in a production table, my first step would be to identify the extent of the corruption by running queries that check for anomalies like nulls in non-nullable columns or invalid data types. Once identified, I would create a backup of the affected data to ensure we have a recovery point.

Next, I’d isolate the problematic records and attempt to correct them, either by referencing backup data, if available, or by applying business rules to regenerate the correct values. If the corruption is widespread, I might consider restoring the table from a backup, followed by reapplying any subsequent valid changes. I would also investigate the root cause to prevent future occurrences, possibly by adding constraints or triggers to enforce data integrity.

Optimizing slow-running queries and simulating concurrency scenarios
Question: How would you approach optimizing a slow-running query in a high-traffic database, especially considering potential concurrency issues?

Example Answer: I would start by analyzing the query execution plan to identify inefficiencies like table scans, missing indexes, or suboptimal join operations. If the issue is related to indexing, I would add or adjust indexes to reduce the query’s execution time. Additionally, I’d consider query refactoring to eliminate unnecessary complexity.

Given the high-traffic environment, I’d also assess the query’s impact on concurrency. For example, I would check for locking or blocking issues that could be slowing down the system and might use techniques like query hints or isolation level adjustments to minimize contention. Finally, I would test the optimized query in a staging environment under simulated load to ensure that it performs well and doesn’t introduce new concurrency issues.

SQL for data migration tasks
Question: Can you walk me through your process for migrating large datasets from one SQL Server to another, ensuring minimal downtime and data integrity?

Example Answer: In a large-scale data migration, my first step is to plan and document the migration process, including identifying dependencies, assessing data volume, and estimating downtime. I usually start by performing a full backup of the source database to ensure we have a recovery point. 

To minimize downtime, I’d consider using techniques like log shipping or database mirroring to keep the target database up-to-date with changes made during the migration process. Before the final cutover, I’d perform a series of test migrations on a staging environment to verify that the data is correctly transferred and that the target environment functions as expected.

During the final migration, I’d carefully monitor the process, validating data integrity through checksums or row counts, and ensure that all necessary application connections are redirected to the new server. Post-migration, I’d run thorough tests to confirm everything is working correctly and that there are no data integrity issues.

Learning tip: Practice interview skills for behavioral interviews, recruiter screens, and panel interviews in CodeSignal Learn’s Behavioral Interview Practice for CS Students learning path. Engage in live mock interviews with an advanced AI agent and get immediate feedback on your performance from our AI tutor and guide, Cosmo.

Common SQL interview questions (if you have limited time to practice)
Essential SQL functions
Question: Write a SQL query to calculate the total number of orders and the average order amount from an `orders` table. The table contains columns `order_id`, `order_date`, and `order_amount`.

Example Answer:

SELECT 

    COUNT(order_id) AS total_orders, 

    AVG(order_amount) AS average_order_amount

FROM 

    orders;
Explanation: This query uses two essential SQL aggregate functions: `COUNT()` and `AVG()`. The `COUNT(order_id)` function calculates the total number of orders, while `AVG(order_amount)` calculates the average order amount across all orders. These functions are fundamental for summarizing data and generating insights from an SQL table.

SQL debugging
Question: You’ve written a query that doesn’t return the expected results. Describe how you would debug the issue, assuming you are dealing with a simple `SELECT` statement.

Example Answer:

-- Original query

SELECT * FROM customers WHERE last_name = 'Smith';

-- Debugging steps

-- 1. Check if the condition is too restrictive or misspelled

SELECT * FROM customers WHERE last_name LIKE '%Smith%';

-- 2. Verify the data

SELECT DISTINCT last_name FROM customers;

-- 3. Test a simplified version of the query

SELECT * FROM customers WHERE 1 = 1;

-- 4. Check for case sensitivity issues (if the database is case-sensitive) 

SELECT * FROM customers WHERE LOWER(last_name) = 'smith'; 

-- 5. Ensure there are no leading/trailing spaces 

SELECT * FROM customers WHERE TRIM(last_name) = 'Smith';
Explanation: The debugging process involves several steps. First, I’d check if the condition might be too restrictive or if there’s a typo by using a broader condition, like `LIKE`. Then, I’d verify the data by querying distinct values to see if the data matches the expected condition. Next, I’d run a simplified version of the query (`WHERE 1 = 1`) to confirm the basic query structure is sound. If your database is case-sensitive, Smith and smith would be treated differently. To avoid case sensitivity issues, you can use LOWER(last_name) = ‘smith’ or UPPER(last_name) = ‘SMITH’.  Finally, data might have leading or trailing spaces that affect the match. Using TRIM(last_name) = ‘Smith’ ensures that such spaces are removed before comparison. These steps help quickly identify common issues.  

Efficient query writing and key SQL clauses
Question: Write an efficient SQL query to retrieve all unique product names from a `products` table that has a `product_name` column, and ensure the results are sorted alphabetically.

Example Answer:

SELECT DISTINCT product_name

FROM products

ORDER BY product_name ASC;
Explanation: This query retrieves all unique product names using the `DISTINCT` clause, ensuring that no duplicates appear in the results. The `ORDER BY` clause sorts the product names alphabetically (`ASC`). Using `DISTINCT` in combination with `ORDER BY` is a common practice to write efficient queries that provide meaningful, well-organized results.

Critical performance factors
Question: Given a `sales` table with millions of records, write an optimized SQL query to retrieve the total sales amount for each `region` from the current year. The table includes `sale_id`, `region`, `sale_date`, and `sale_amount` columns.

Example Answer:

SELECT 

    region, 

    SUM(sale_amount) AS total_sales

FROM 

    sales

WHERE 

    EXTRACT(YEAR FROM sale_date) = EXTRACT(YEAR FROM CURRENT_DATE)

GROUP BY 

    region;
Explanation: This query efficiently calculates the total sales amount for each `region` by limiting the dataset to the current year using the `EXTRACT(YEAR FROM sale_date)` function in the `WHERE` clause. The `SUM(sale_amount)` function aggregates the sales for each `region`, and the `GROUP BY` clause organizes the results by region. This approach optimizes performance by reducing the data processed and ensures that the query scales well with large datasets.
Behavioral Interview Questions for Data Analysts
Behavioral questions in data analyst interviews ask about specific situations you’ve been in in which you had to apply specific skills or knowledge.

For many data analysts, behavioral questions can be fairly tough.

One tip: Always try to relate the question back to your experience and strengths.

1. Describe a time when you spotted an inconsistency. How did you respond?
Successful data analysts can help businesses identify anomalies and respond quickly.

For data sense questions, think about a time you spotted an inconsistency in data quality and how you eventually addressed it.

2. Talk about a time when you had to make a decision with a lot of uncertainty.
Interviewers want to see you demonstrate the following:

Decisiveness – Show the interviewer that you can make decisions and communicate your decision-making process.
Self-direction – Show that you are able to choose a path forward, deduce information, and create a plan of action.
Adaptability – Your response should show that you can adapt your decision-making in a challenging situation.
Here’s an example answer: “In my previous job, I worked on a sales forecasting problem under a strict deadline. However, I was missing the most recent data due to a processing error and only had 3-year-old sales figures. My strategy was applying the growth factor to the data to establish correct correlation and variances. This strategy helped me deliver a close forecast and meet the deadline.”

3. How would you convey insights and the methods you use to a non-technical audience?
Interviewers ask this question to see if you can make complex subjects accessible and that you have a knack for communicating insights in a way that persuades people. Here’s a marketing analytics example response:

“I was working on a customer segmentation project. The marketing department wanted to better segment users. I worked on a presentation and knew the audience wouldn’t understand some of the more complex segmenting strategies. I put together a presentation that talked about the benefits and potential trade-offs of segmenting options like K-means clustering. For each option, I created a slide to show how it worked, and after the presentation, we could have an informed discussion about which approach to use.”

4. How do you set goals and achieve them? Give us an example.
Interviewers want to see that you can set manageable goals and understand your process for achieving them. Don’t forget to mention the challenges you faced, which will make your response more dynamic and insightful. For example, you might say:

“Data visualization was something I struggled with in college. I didn’t have a strong design eye, and my visualizations were hard to read. In my last job, I made it a goal to improve, and there were two strategies that were most helpful. I took an online data visualization course, and I built a clip file of my favorite visualizations. The course was great for building my domain knowledge. However, I felt I learned the most by building my clip file and breaking down what made a good visualization on my own.”

5. Describe a time when you solved a conflict at work.
This question assesses your ability to remain objective at work, communicate effectively in challenging situations, and remain calm under fire. Here’s an example response:

“In my previous job, I was the project manager on a dashboard project. One of the BI engineers wasn’t meeting the deadlines I had laid out, and I brought that up with him. At first, he was defensive and angry with me. But I listened to his concerns about the deadlines and asked what I could do to help. I learned from our conversation that he had a full workload besides this project. I talked with the engineering manager, and we were able to reduce some of his workload. He caught up quickly, and we were able to finish the project on time.”

6. Give an example of a situation when you have shown effectiveness, empathy, humbleness, and adaptability.
This is a leadership question in disguise. If you can relate a time you were an effective leader, chances are you will easily incorporate all of these traits. For example:

“I was the lead on a marketing analytics project. We had a critical deadline to meet, but we were in danger of missing the deadline due to a data processing error. The team morale was low, so I held a quick meeting to lay out a schedule, answer questions, and rally the team. That meeting gave the team the jolt it needed. We made the deadline, and I ensured leadership knew how hard each contributor had worked.”

7. Give me an example of a time when you failed on a project.
This question tests your resilience, how you respond to adversity, and how you learn from your mistakes. You could say:

“I had to give a client a presentation about a data analytics project. I mistakenly assumed the audience had more technical knowledge than they did. The presentation was received with a lot of blank stares. However, I knew the material about our findings was strong. I stopped for questions, and then jumped ahead to the visualizations and findings. This helped get the presentation on track, and by the end, the client was impressed. Now, whenever I have a presentation, I take time to understand the audience before I start working on it.”

8. Talk about an occasion when you used logic to solve a problem.
A strong response to this question shows that you can solve problems creatively and that you don’t just jump at the first or easiest solution. One tip: Illustrate your story with data to make it more credible.

Here’s what you could say: “In my previous job, I was responsible for competitor research, and through my analysis, I noticed that our most significant competitors had increased sales by 5% during Q1. This deviated significantly from our sales forecasts for these accounts. I found that we needed to update our competitor sales models with more recent market research and historical data. I tested the model adjustments, and ultimately, I improved our forecasting accuracy by 15%.”

9. What do you do if you disagree with your manager?
Interviewers ask this question to gauge your emotional maturity, see that you can remain objective, and gain insights into your communication skills. Avoid subjective examples, such as my boss being a micromanager. Instead, you could say:

“One time, I disagreed with my manager over the process for building a dashboard, as their approach was to jump straight into the execution. I knew that it would be better to perform some planning in advance rather than feeling our way through and reacting to roadblocks as they arose, so I documented a plan that could potentially save us time in development. That documentation and planning showed where pitfalls were likely to arise, and by solving for future issues, we could launch the new dashboard three weeks early.”

10. How comfortable are you with presenting insights to stakeholders?
This question is asked to see how confident you are in your communication skills, and it provides insight into how you communicate complex technical ideas. With this question, talk about how you make data and analytics accessible. Try to answer these questions:

Do you create visualizations?
What do you do to prepare for a data analytics presentation?
What strategies do you use to make data more accessible?
What presentation tools do you use?
11. Talk about a time you were surprised by the results of an analytics project.
This question basically asks: Are you open to new ideas in your work? Analysts can get stuck trying to prove their hypothesis, even if the data says otherwise. A successful analyst is OK with being wrong and listens to the data. You could say:

“While working on a customer analytics project, I was surprised to find that a subsegment of our customer base wasn’t actually responding to our offers. We had lumped the subsegment into a larger customer bucket and had assumed that a broader segmentation wouldn’t make a difference. I relayed the insight to the marketing team, and we reduced churn among this subsegment.”

12. Why are you interested in working for this company?
This question is super common in analyst behavioral interviews. However, it still trips a lot of candidates up. Another variation of this question would be: why did you want to work in data analytics?

In your response, your goal should be to convey your passion for the work and discuss what excites you about the company/role. You might focus on the company’s culture, a mentor who inspired you, a recommendation you received, or someone in your network who’s connected with the company. A sample response:

“I’m excited by the possibility of using data to foster stronger social connections amongst friends and peers. I also like to ‘go fast’ and experiment, which fits into Meta’s innovative culture.”

13. Talk about a time when you had trouble communicating with stakeholders. How were you able to overcome it?
Interviewers ask questions like this to assess how you handle adversity and adapt. Don’t be afraid to share what went wrong. B. Describe what you learned and how you will apply it to future work.

Here’s a sample answer for a data analyst role: “I presented a data analytics project to non-technical stakeholders, but my presentation was far too technical. I realized that the audience wasn’t following the technical aspects, so I stopped and asked questions. I spent time clarifying the technical details until there were no questions left. I learned that it’s important to tailor presentations to the audience, so before I start a presentation, I always consider the audience.”
Coding / Technical (SQL & Python) Questions
Meta’s technical screens focus heavily on SQL and Python, with recent Meta data scientist interview questions 2024 centering around data wrangling, simulation, and optimization problems, similar to what you’d find in Facebook data scientist interview questions:

1. Find how much overlapping jobs are costing the company

To solve this, simulate the scenario by generating random start times for two jobs within the 300-minute window (7 pm to midnight) and check for overlaps. Repeat the simulation N times to estimate the probability of overlap, then multiply this probability by the annual cost factor (365 days * $1000 per overlap) to get the estimated annual cost.

2. Given a dictionary with weights, write a function random_key that returns a key at random with a probability proportional to the weights.

To solve this, construct a list where each key appears a number of times equal to its weight. Use Python’s random.choice() to select a key randomly from this list. This ensures the probability of selecting a key is proportional to its weight.

3. Write a query to get the average commute time for each commuter in New York

To solve this, use two subqueries: one to calculate the average commute time for each commuter in New York grouped by commuter_id, and another to calculate the average commute time across all commuters in New York. Use the TIMESTAMPDIFF function to calculate the time difference in minutes between start_dt and end_dt. Finally, perform a LEFT JOIN to combine the results of both subqueries.

4. Matrix Rotation

To rotate a matrix 90 degrees clockwise, first transpose the matrix and then reverse the order of its columns. Transposing swaps rows and columns, while reversing columns achieves the desired rotation. This approach is efficient and leverages simple matrix transformations.

5. Write a query to create a metric that can validate and rank the queries by their search result precision

To compute the precision metric, start by calculating the average rating for each query using AVG(rating) grouped by the query column. To incorporate position into the metric, use the inverse of the position as a weighted factor (1/position) multiplied by the rating. This ensures higher weights for top positions.

6. Write a Python function called max_profit to calculate the maximum profit with at most two buy/sell transactions

To solve this, iterate through the list of prices while maintaining four variables: buy1, profit1, buy2, and profit2. buy1 tracks the lowest price for the first transaction, and profit1 calculates the profit from the first transaction. buy2 adjusts for the profit from the first transaction, and profit2 calculates the maximum profit from two transactions. Return profit2 as the result.

Product & Experimentation / A/B Test Questions
A core part of the Meta data scientist product analytics interview involves product-sense and experimentation challenges that test your ability to define metrics, evaluate tests, and think like a product partner in a data-driven organization:

7. How would you assess the validity of the result in an AB test?

To assess the validity of the result, first ensure that the user groups were properly separated and randomly bucketed to avoid biases. Next, confirm that the variants were equal in all other aspects except the tested feature. Additionally, evaluate the measurement process, including sample size, duration of the test, and whether the p-value was monitored continuously, as this can lead to false positives or negatives. Finally, determine the minimum effect size and compute the required sample size and duration before starting the experiment to ensure statistical reliability.

8. Revenue Retention: Calculate the formula for the average lifetime value

To calculate the average lifetime value (LTV) for a subscription-based service, use the formula (LTV = \sum_{i=0}^{\infty} 100 \cdot 0.9^i), where 100 is the monthly subscription cost and 0.9 represents the retention rate after accounting for 10% churn. This formula sums the expected value of the customer over infinite months, converging to (LTV = $1000). The calculation accounts for retention rates and churn, providing a more accurate LTV than simply multiplying the average customer duration by the monthly cost.

9. Given a list of strings, write a function that returns the longest common prefix

To measure the percentage of fake news stories within 24 hours, start by defining what constitutes fake news, such as spam, bot-generated content, or incorrect media reporting. Then, use unsupervised methods to identify fake accounts, analyze article sharing patterns, and validate media outlets’ credibility. Metrics like user engagement, account authenticity, and cross-referencing news titles can help infer the percentage of fake news stories.

10. Given a team wants to A/B test multiple changes through a sign-up funnel, how would you set up this test?

To set up this test, you would create a multivariate test with four variants: red button at the top, red button at the bottom, blue button at the top, and blue button at the bottom. Randomly assign users to each variant and calculate the sample size needed to reach statistical significance. Alternatively, chaining A/B tests could be considered, but this approach would not account for interaction effects between the variables.

11. Given customer spending data, how would you determine the next partner card?

To determine the next partner card, analyze customer transaction data by grouping spending by merchants and identifying high-spending merchants without existing partnerships. Consider metrics like average transaction price and frequency to optimize for recurring spending. Additionally, examine year-over-year growth trends to forecast future high-spending merchants and conduct external research to identify merchants popular outside the current dataset for customer acquisition.

Behavioral & Culture-Fit Questions
The data scientist Meta interview also includes behavioral prompts designed to assess collaboration, resilience, and alignment with Meta’s values—key qualities that Facebook data science interview loops look for in high-performing candidates:

12. Describe an analytics experiment that you designed. How were you able to measure success?

In a Meta context, this question is often about demonstrating your ability to design robust, scalable experiments that drive product decisions. You might describe designing an A/B test to evaluate the impact of a new ranking algorithm on user engagement within the News Feed.

Success would be measured through metrics like time spent, click-through rates, or meaningful social interactions. You should explain how you set up the control and treatment groups, ensured randomization, and accounted for bias or variance. Mention tools like Hive, PyTorch, or Meta’s internal experimentation platforms, and show how you used p-values or confidence intervals to validate findings and inform product rollout decisions.

13. Talk about a time when you had trouble communicating with stakeholders. How were you able to overcome it?

At Meta, stakeholders range from product managers to engineers and designers, and they often have different levels of technical fluency. You could describe a time when you presented a complex causal inference model, but the team found it too abstract.

To overcome this, you simplified the narrative using data visualizations and drew parallels to user behavior they were more familiar with. You might mention how you scheduled follow-ups or embedded yourself in sprint reviews to build context. Emphasize how this experience improved your stakeholder management and ultimately helped align teams on the insights and next steps.

14. Why Do You Want to Work With Us

For a Metadata Scientist role at Meta, you should go beyond generic enthusiasm. Highlight how Meta’s commitment to open science, large-scale experimentation, and infrastructure for data scientists aligns with your interests.

For example, you could mention your excitement about working on products that impact billions of users or the opportunity to contribute to innovations in content discovery, trust and safety, or generative AI. Be specific about how Meta’s scale and emphasis on measurement excellence match your goals. This is also a good place to reflect on Meta’s core values like “Focus on Long-Term Impact” or “Move Fast.”

15. Describe a data project you worked on. What were some of the challenges you faced?

This is a chance to show your ability to drive a project end-to-end. For a Meta role, you could describe working on a project where you built a machine learning model to predict user churn or optimized a recommendation pipeline.

Explain the technical scope, like data ingestion with Presto, model training using PyTorch, and how you collaborated with product teams to define success. Then describe real-world challenges—maybe you faced incomplete logging or had to backfill data to simulate historical results. Talk about how you navigated ambiguity, validated your approach with stakeholders, and iterated to ship a solution that drove measurable impact.
Courses
Courses in this learning path are:

Introduction to Data Science
Introduction to Data Science
Learn how to prepare for the wide range of questions that come up in data science interviews.

0 of 8 Completed

Easy SQL Questions
Easy SQL Questions
Get started on tackling easy level SQL questions involving aggregations, joining multiple tables, and pulling data for beginning analytical reports.

0 of 12 Completed

Medium SQL Questions
Medium SQL Questions
Medium level SQL questions utilize more advanced concepts like sub-queries, window functions, and solving case study problems.

0 of 19 Completed

Hard SQL Questions
Hard SQL Questions
Let's tackle advanced SQL interview questions that focus on multi-joins and layers of data interpretation. These questions may come up in take-home challenges and senior level interviews.

0 of 9 Completed

Data Structures
Data Structures
Data structures in Python attempt to be more intuitive and flexible than traditional data structures in other programming languages.

0 of 9 Completed

Common DS Packages
Common DS Packages
As we said in the first section of this course, a major benefit of using Python for data science in comparison to other programming languages is the availability of a large number of useful packages that are distributed under a free license.

0 of 11 Completed

Python Questions: Hard
Python Questions: Hard
Let's try some hard Python questions that you would see in tougher data science interviews and many machine learning interviews.

0 of 6 Completed

Basic Probability
Basic Probability
Probability Theory is the branch of mathematics that deals with uncertainty, underpinning all of statistics and machine learning.

0 of 10 Completed

Discrete Distributions
Discrete Distributions
All areas of study in math can roughly be divided into two camps: discrete mathematics and continuous mathematics. Perhaps the best way to describe the difference between the two is to talk about what each of the branches means by "number."

0 of 12 Completed

Continuous Distributions
Continuous Distributions
Continuous probability distribution: A probability distribution in which the random variable X can take on any value (is continuous).

0 of 6 Completed

Sampling Theorems
Sampling Theorems
Thus far in this course, we have considered random variables under an idealized scenario where we know the distribution of the random variable.

0 of 7 Completed

Hypothesis Testing
Hypothesis Testing
Hypothesis testing covers the fundamental theory and background behind A/B Testing. In this course we'll cover Z and T test, multiple hypothesis testing, and the different type errors.

0 of 11 Completed

Confidence Intervals
Confidence Intervals
Confidence intervals help us deal with this imprecision by giving us a way to talk about a range of values with some certainty where the true value of the statistic is contained in.

0 of 6 Completed

A/B Testing & Experiment Design
A/B Testing & Experiment Design
Let's start with a general framework for A/B testing. In practice, an A/B testing and experimentation all follow a step by step process of setting metrics and designing experiments.

0 of 10 Completed

A/B Testing Common Scenarios
A/B Testing Common Scenarios
The next couple of chapters will cover common scenarios and concepts involved in A/B testing. As A/B testing involves statistical concepts, there may be terms that you need refreshing on.

0 of 9 Completed

A/B Testing Tradeoffs
A/B Testing Tradeoffs
There are scenarios where A/B testing is not necessarily the best course of action. Often, there are technical, infrastructure, or practical concerns that come up while planning an A/B test.

0 of 6 Completed

Statistics
Statistics
This is a refresher on some important statistical concepts that will help us with A/B testing and beyond. While by no means a comprehensive guide, this chapter will go over some important basics about statistical testing and probability distributions.

0 of 11 Completed

Data Analytics Fundamentals: Causal Inference
Data Analytics Fundamentals: Causal Inference
In this course we’ll go over the core concepts of causality, significance, and analyzing data. This is meant as a quick refresher and a high level overview of causal inference basics to eventually apply them in data analytics problems.

0 of 9 Completed

Diagnosing and Investigating Metrics
Diagnosing and Investigating Metrics
Investigating metrics is a type of product intuition problem that will come up frequently in interviews. Examples of this are typically phrased along the lines of - If X metric is up/down by Y percent, how would you investigate it?

0 of 12 Completed

Measuring Success
Measuring Success
Measuring the success of products is critical to data science and analytics interviews. Generally, this question is an encapsulation of every time a product manager or executive asks the question: “So, how is it doing?”.

0 of 11 Completed

Feature Change
Feature Change
Before launching a feature, we can imagine that the first step we’d have to take is analyzing the existing data in our product to make a decision about exactly what to build. This process is what creates the building or change of a feature problem that gets asked in product interviews.

0 of 10 Completed

Metric Trade-Offs
Metric Trade-Offs
Metric trade-off type questions can occur on their own in product interviews or as part of a larger product or AB testing interview discussion.

0 of 4 Completed

Modeling Case Study
Modeling Case Study
The machine learning and modeling case study is the most common type of interview question that tests a combination of modeling intuition and business application.

0 of 2 Completed

Data Pre-Processing
Data Pre-Processing
Data processing and analysis is the first step that we need to consider once we've clarified details and started down the path of building the model.

0 of 5 Completed

Feature Selection
Feature Selection
Feature selection and feature engineering is the second part of the data processing step. Once we've understood what our data looks like, we need to begin to theorize the kinds of features we would use to build the model.

0 of 4 Completed

Model Selection
Model Selection
Model selection is usually the crux of any modeling case study problem. We want to be able to select a model or machine learning algorithm that will combine a bunch of factors to become the most optimal algorithm for the problem.

0 of 4 Completed

Machine Learning Algorithms
Machine Learning Algorithms
We have touched on the different machine learning algorithms throughout this lesson, but haven't yet dived deep into each one. The prior for this course is that you, as a candidate, have an idea of basic machine learning concepts, and the different modeling algorithms are one such example of them.

0 of 7 Completed

Model Evaluation
Model Evaluation
Most machine learning model deployment requires some technical details and implementation to doing so. But we can abstract away from that in an interview when we’re focusing on the model roll out.

0 of 9 Completed

Applied Modeling
Applied Modeling
Applied modeling is a type of case question asked about practical machine learning. The most common type of question framework is: Given an example scenario with a machine learning system or model, how would you analyze and fix the problem?

0 of 5 Completed

Generalized Linear Models and Regression
Generalized Linear Models and Regression
Regression models are used to predict the value of a dependent variable from one or more independent variables.


data modeling
data engineering
data warehouses
	Data Warehouse Design Patterns
data lakes
dbt
data mart


Relational Databases and SQL
OLAP and Recursion
Modeling and Theory
Semistructured Data

Airflow and Cloud Composer


Containerization (Docker)
Docker Mastery: with Kubernetes +Swarm from a Docker Captain

Machine Learning Fundamentals

Product Strategy
Behavioral
Analytical
Experimentation
Statistics
Concept
Data Analysis & Reporting (DAR)




CORE TOPICS





















Analytics Value Chain & Core Functions (AVC.CF)




BROAD TOPIC
SUB-TOPICS
DETAILED TOPICS






Data Acquisition
Event & API tracking, Fivetran/Airbyte pipelines
Raw landing tables, CDC streams
Data Engineering
dbt/SQL ELT, column-level lineage, tests
Curated marts, entity-resolution tables
Analytics & BI
Ad-hoc SQL/Python, descriptive stats, dashboards
Exploratory notebooks, Looker/Power BI reports
Modeling & ML
Feature engineering, MLflow experiments, MLOps
Forecasts, propensity scores, segmentations
Experimentation & Causal
A/B test design, CUPED, diff-in-diff
Lift reports, decision memos
Activation
Reverse-ETL, marketing pixels, product triggers
Audiences in CDP/CRM, automated actions
Governance & Quality
Data contracts, Monte Carlo/Soda monitors
SLA reports, incident retros
Storytelling & Strategy
Executive read-outs, OKR alignment
Roadmaps, North-star metrics, ROI models



Data Visualization
Visualization Tools
Tableau
- Connecting to data sources
- Calculated fields and parameters
- Dashboards and storyboards




Power BI
- Data modeling and DAX
- Report building
- Sharing and collaboration features


Visualization Principles
Design Best Practices
- Choosing chart types (bar, line, pie)
- Color theory and accessibility
- Avoiding misleading visuals




Storytelling with Data
- Structuring a narrative
- Highlighting key insights
- Tailoring visuals for stakeholders





 3. Analytics & Analysis Domain

Core Skills and Competencies

Technical Skills

- Statistical Analysis [Skill] - Descriptive statistics, inferential statistics, hypothesis testing, regression analysis
- Data Modeling [Skill] - Conceptual modeling, predictive modeling, statistical modeling
- Programming Languages [Skill] - Python, R, SQL, SAS, MATLAB, Scala, Java
- Machine Learning [Skill] - Supervised learning, unsupervised learning, deep learning, ensemble methods
- Database Management [Skill] - SQL querying, database design, data warehousing, NoSQL databases

Business Skills

- Data Storytelling [Skill] - Narrative construction, insight communication, stakeholder engagement
- Business Intelligence [Skill] - KPI development, dashboard creation, performance measurement
- Analytics Strategy [Skill] - Analytics roadmap development, ROI measurement, data governance

Key Methodologies and Frameworks

- CRISP-DM [Framework] - Cross-Industry Standard Process for Data Mining (6 phases: Business Understanding, Data Understanding, Data Preparation, Modeling, Evaluation, Deployment)
- SEMMA [Framework] - Sample, Explore, Modify, Model, Assess (SAS-developed methodology)
- Descriptive Analytics [Methodology] - Historical data analysis, reporting, data aggregation
- Predictive Analytics [Methodology] - Forecasting, machine learning models, statistical modeling
- Prescriptive Analytics [Methodology] - Optimization, simulation, decision modeling

Essential Tools and Software Platforms

- Tableau [Tool] - Advanced data visualization, interactive dashboards, self-service analytics
- Python [Tool] - Versatile programming language with data science libraries (pandas, numpy, scikit-learn)
- R [Tool] - Open-source statistical computing with extensive package ecosystem
- SAS [Tool] - Enterprise statistical software with advanced analytics capabilities
- Google Cloud Platform [Tool] - BigQuery, Dataflow, Cloud ML Engine, Looker
- Snowflake [Tool] - Cloud data platform, data warehousing, analytics workloads

Sub-disciplines and Specializations

- Customer Analytics [Specialization] - Customer segmentation, lifetime value modeling, churn prediction
- Marketing Analytics [Specialization] - Campaign optimization, attribution modeling, A/B testing
- Financial Analytics [Specialization] - Risk modeling, fraud detection, credit scoring
- Data Science [Specialization] - Machine learning, artificial intelligence, experimental design
- Business Intelligence [Specialization] - Data warehousing, OLAP, reporting, dashboard development

Emerging Trends and Technologies

- Agentic AI [Trend] - Autonomous AI systems capable of independent decision-making
- Generative AI for Analytics [Trend] - AI-powered report generation, automated insight discovery
- Real-time Analytics [Trend] - Stream processing, edge computing, instantaneous decision-making
- MLOps [Trend] - Machine learning operations, model lifecycle management




1. ANALYSIS PLANNING & STRATEGY
Business Question Definition
Reverse-engineering from decisions needed
Stakeholder requirement gathering
Success criteria establishment
Scope and constraint documentation
Hypothesis Development
Formulating testable hypotheses
Identifying key variables and relationships
Defining success/failure metrics
Creating analysis roadmaps
Resource Planning
Data source identification
Tool and technology selection
Timeline and milestone setting
Team role assignment
2. DATA MANAGEMENT & QUALITY
Data Quality Assessment
Completeness checks (missing values, gaps)
Accuracy validation (range checks, business rules)
Consistency verification (cross-source validation)
Timeliness evaluation (data freshness)
Data quality scorecarding
Data Governance
Source documentation
Data lineage tracking
Access control and security
Version control implementation
Metadata management
Data Preparation Standards
ETL process documentation
Transformation logic standardization
Error handling procedures
Data validation checkpoints
3. ANALYTICAL FRAMEWORKS
Progressive Depth Analysis
Descriptive: What happened?
Summary statistics
Historical trending
Current state assessment
Diagnostic: Why did it happen?
Root cause analysis
Variance decomposition
Driver identification
Predictive: What will happen?
Forecasting models
Risk assessment
Scenario planning
Prescriptive: What should we do?
Optimization analysis
Recommendation development
Impact estimation
Segmentation & Cohort Analysis
Customer segmentation strategies
Behavioral cohort creation
Performance tier analysis
Geographic/demographic breakdowns
Time-based cohort tracking
Comparative Analysis
Period-over-period comparisons
Benchmark analysis (internal/external)
Competitive positioning
Best practice identification
Gap analysis
4. REPORTING STRUCTURES & STANDARDS
Report Architecture
Executive Dashboards
KPI summaries
Trend visualizations
Exception highlighting
Action triggers
Operational Reports
Detailed metrics
Process performance
Resource utilization
Bottleneck identification
Analytical Deep Dives
Investigation findings
Statistical analysis
Predictive insights
Recommendation details
Pyramid Principle Structure
Lead with conclusions/recommendations
Support with key evidence
Provide detailed analysis
Include technical appendices
Visual Design Standards
Chart type selection matrix
Color coding conventions
Layout templates
Annotation guidelines
Accessibility requirements
5. INSIGHT GENERATION METHODOLOGIES
Pattern Recognition Techniques
Trend identification
Seasonality detection
Anomaly flagging
Correlation analysis
Cluster identification
Variance Analysis Framework
Actual vs. Plan/Forecast
Volume/Price/Mix decomposition
Time series variance
Geographic variance
Product/Service variance
Statistical Rigor
Significance testing
Confidence intervals
Sample size validation
Bias identification
Uncertainty quantification
6. QUALITY ASSURANCE & VALIDATION
Analysis Validation
Peer review processes
Sensitivity testing
Scenario stress-testing
Back-testing against historical data
Cross-validation techniques
Documentation Standards
Methodology documentation
Assumption cataloging
Limitation disclosure
Source attribution
Calculation transparency
Reproducibility Framework
Code versioning
Environment documentation
Data snapshot preservation
Process step recording
Audit trail maintenance
7. STAKEHOLDER MANAGEMENT
Communication Frameworks
Audience Segmentation
Executive briefings
Manager updates
Technical deep-dives
Cross-functional sharing
Delivery Cadence
Real-time alerts
Daily operational updates
Weekly business reviews
Monthly strategic reports
Quarterly business reviews
Feedback Integration
Iterative review cycles
Requirement refinement
Finding validation sessions
Action plan development
Impact measurement
8. ACTION & IMPLEMENTATION
Recommendation Framework
Specific action identification
Impact quantification
Resource requirement estimation
Implementation timeline
Success metrics definition
Decision Support Tools
What-if scenario modeling
ROI calculators
Risk assessment matrices
Priority scoring models
Implementation roadmaps
Performance Tracking
KPI definition and tracking
Initiative impact measurement
Continuous improvement cycles
Learning documentation
Best practice capture
9. TECHNOLOGY & TOOLS UTILIZATION
Analytics Platform Management
Tool selection criteria
Platform integration strategies
Performance optimization
Automation implementation
Scalability planning
Self-Service Analytics
User training programs
Template development
Governance frameworks
Usage monitoring
Quality control measures
10. PROFESSIONAL DEVELOPMENT
Skill Building Areas
Statistical methods
Business domain expertise
Communication skills
Technology proficiency
Project management
Industry Best Practices
Professional standards adoption
Methodology updates
Tool evaluation
Network building
Continuous learning



4. Function-Specific Deep Dives
Marketing Analytics Specialization
Customer Analytics Excellence
Segmentation Mastery: Behavioral segmentation, psychographic profiling, and dynamic segmentation Analytics Vidhya365 Data Science
Lifecycle Analysis: Customer journey mapping, cohort analysis, and retention modeling 365 Data Science
Value Modeling: Customer Lifetime Value (CLV) calculation, predictive CLV modeling, and value optimization Analytics Vidhya365 Data Science
Attribution and Measurement
Multi-Touch Attribution: Data-driven attribution modeling, cross-device tracking, and incrementality testing Medium +3
Marketing Mix Modeling: Adstock and saturation curves, media effectiveness measurement, and budget optimization Adsmurai +3
Experimental Design: A/B testing for marketing campaigns, multivariate testing, and causal inference 365 Data Science
Advanced Applications
Personalization Engines: Real-time recommendation systems, dynamic content optimization, and behavioral targeting Ironhack
Campaign Optimization: Automated bidding algorithms, creative optimization, and budget allocation
Brand Analytics: Brand sentiment analysis, brand lift measurement, and competitive brand analysis Analytics Vidhya

Different Types of Data Analyst Interview Questions
Interview Query regularly analyzes the contents of data analyst interviews. By tagging common keywords and mapping them back to question topics for over 10K+ tech companies, we’ve found that SQL questions are asked most frequently.

In fact, in interviews for data analyst roles, SQL and data manipulation questions are asked 85% of the time.

Here are the types of technical interview questions data analysts get asked most frequently:

Behavioral interview questions
SQL and data processing
Data analytics case studies
Python, algorithms, and coding questions
Statistics and probability
A/B testing and experimentation
Product metrics
Additionally, for more traditional data analyst roles, expect interview questions around the following:

Excel
Data Visualization
Let’s first dive into how to approach and answer behavioral interview questions.

Question	Topic	Difficulty	Ask Chance
Causal Inference Without A/B
A/B Testing & Experimentation
Medium
Very High
Department Expenses
SQL
Medium
Very High
Algorithm Reliability
ML Ops & Training Pipelines
Hard
Very High
This feature requires a user account
Sign up to access this feature.

feature
Access 1000+ data science interview questions

feature
30,000+ top company interview guides

feature
Unlimited code runs and submissions



Behavioral Interview Questions for Data Analysts
Behavioral questions in data analyst interviews ask about specific situations you’ve been in in which you had to apply specific skills or knowledge.

For many data analysts, behavioral questions can be fairly tough.

One tip: Always try to relate the question back to your experience and strengths.

1. Describe a time when you spotted an inconsistency. How did you respond?
Successful data analysts can help businesses identify anomalies and respond quickly.

For data sense questions, think about a time you spotted an inconsistency in data quality and how you eventually addressed it.

2. Talk about a time when you had to make a decision with a lot of uncertainty.
Interviewers want to see you demonstrate the following:

Decisiveness – Show the interviewer that you can make decisions and communicate your decision-making process.
Self-direction – Show that you are able to choose a path forward, deduce information, and create a plan of action.
Adaptability – Your response should show that you can adapt your decision-making in a challenging situation.
Here’s an example answer: “In my previous job, I worked on a sales forecasting problem under a strict deadline. However, I was missing the most recent data due to a processing error and only had 3-year-old sales figures. My strategy was applying the growth factor to the data to establish correct correlation and variances. This strategy helped me deliver a close forecast and meet the deadline.”

3. How would you convey insights and the methods you use to a non-technical audience?
Interviewers ask this question to see if you can make complex subjects accessible and that you have a knack for communicating insights in a way that persuades people. Here’s a marketing analytics example response:

“I was working on a customer segmentation project. The marketing department wanted to better segment users. I worked on a presentation and knew the audience wouldn’t understand some of the more complex segmenting strategies. I put together a presentation that talked about the benefits and potential trade-offs of segmenting options like K-means clustering. For each option, I created a slide to show how it worked, and after the presentation, we could have an informed discussion about which approach to use.”

4. How do you set goals and achieve them? Give us an example.
Interviewers want to see that you can set manageable goals and understand your process for achieving them. Don’t forget to mention the challenges you faced, which will make your response more dynamic and insightful. For example, you might say:

“Data visualization was something I struggled with in college. I didn’t have a strong design eye, and my visualizations were hard to read. In my last job, I made it a goal to improve, and there were two strategies that were most helpful. I took an online data visualization course, and I built a clip file of my favorite visualizations. The course was great for building my domain knowledge. However, I felt I learned the most by building my clip file and breaking down what made a good visualization on my own.”

5. Describe a time when you solved a conflict at work.
This question assesses your ability to remain objective at work, communicate effectively in challenging situations, and remain calm under fire. Here’s an example response:

“In my previous job, I was the project manager on a dashboard project. One of the BI engineers wasn’t meeting the deadlines I had laid out, and I brought that up with him. At first, he was defensive and angry with me. But I listened to his concerns about the deadlines and asked what I could do to help. I learned from our conversation that he had a full workload besides this project. I talked with the engineering manager, and we were able to reduce some of his workload. He caught up quickly, and we were able to finish the project on time.”

6. Give an example of a situation when you have shown effectiveness, empathy, humbleness, and adaptability.
This is a leadership question in disguise. If you can relate a time you were an effective leader, chances are you will easily incorporate all of these traits. For example:

“I was the lead on a marketing analytics project. We had a critical deadline to meet, but we were in danger of missing the deadline due to a data processing error. The team morale was low, so I held a quick meeting to lay out a schedule, answer questions, and rally the team. That meeting gave the team the jolt it needed. We made the deadline, and I ensured leadership knew how hard each contributor had worked.”

7. Give me an example of a time when you failed on a project.
This question tests your resilience, how you respond to adversity, and how you learn from your mistakes. You could say:

“I had to give a client a presentation about a data analytics project. I mistakenly assumed the audience had more technical knowledge than they did. The presentation was received with a lot of blank stares. However, I knew the material about our findings was strong. I stopped for questions, and then jumped ahead to the visualizations and findings. This helped get the presentation on track, and by the end, the client was impressed. Now, whenever I have a presentation, I take time to understand the audience before I start working on it.”

8. Talk about an occasion when you used logic to solve a problem.
A strong response to this question shows that you can solve problems creatively and that you don’t just jump at the first or easiest solution. One tip: Illustrate your story with data to make it more credible.

Here’s what you could say: “In my previous job, I was responsible for competitor research, and through my analysis, I noticed that our most significant competitors had increased sales by 5% during Q1. This deviated significantly from our sales forecasts for these accounts. I found that we needed to update our competitor sales models with more recent market research and historical data. I tested the model adjustments, and ultimately, I improved our forecasting accuracy by 15%.”

9. What do you do if you disagree with your manager?
Interviewers ask this question to gauge your emotional maturity, see that you can remain objective, and gain insights into your communication skills. Avoid subjective examples, such as my boss being a micromanager. Instead, you could say:

“One time, I disagreed with my manager over the process for building a dashboard, as their approach was to jump straight into the execution. I knew that it would be better to perform some planning in advance rather than feeling our way through and reacting to roadblocks as they arose, so I documented a plan that could potentially save us time in development. That documentation and planning showed where pitfalls were likely to arise, and by solving for future issues, we could launch the new dashboard three weeks early.”

10. How comfortable are you with presenting insights to stakeholders?
This question is asked to see how confident you are in your communication skills, and it provides insight into how you communicate complex technical ideas. With this question, talk about how you make data and analytics accessible. Try to answer these questions:

Do you create visualizations?
What do you do to prepare for a data analytics presentation?
What strategies do you use to make data more accessible?
What presentation tools do you use?
11. Talk about a time you were surprised by the results of an analytics project.
This question basically asks: Are you open to new ideas in your work? Analysts can get stuck trying to prove their hypothesis, even if the data says otherwise. A successful analyst is OK with being wrong and listens to the data. You could say:

“While working on a customer analytics project, I was surprised to find that a subsegment of our customer base wasn’t actually responding to our offers. We had lumped the subsegment into a larger customer bucket and had assumed that a broader segmentation wouldn’t make a difference. I relayed the insight to the marketing team, and we reduced churn among this subsegment.”

12. Why are you interested in working for this company?
This question is super common in analyst behavioral interviews. However, it still trips a lot of candidates up. Another variation of this question would be: why did you want to work in data analytics?

In your response, your goal should be to convey your passion for the work and discuss what excites you about the company/role. You might focus on the company’s culture, a mentor who inspired you, a recommendation you received, or someone in your network who’s connected with the company. A sample response:

“I’m excited by the possibility of using data to foster stronger social connections amongst friends and peers. I also like to ‘go fast’ and experiment, which fits into Meta’s innovative culture.”

13. Talk about a time when you had trouble communicating with stakeholders. How were you able to overcome it?
Interviewers ask questions like this to assess how you handle adversity and adapt. Don’t be afraid to share what went wrong. B. Describe what you learned and how you will apply it to future work.

Here’s a sample answer for a data analyst role: “I presented a data analytics project to non-technical stakeholders, but my presentation was far too technical. I realized that the audience wasn’t following the technical aspects, so I stopped and asked questions. I spent time clarifying the technical details until there were no questions left. I learned that it’s important to tailor presentations to the audience, so before I start a presentation, I always consider the audience.”

Question	Topic	Difficulty	Ask Chance
Causal Inference Without A/B
A/B Testing & Experimentation
Medium
Very High
Department Expenses
SQL
Medium
Very High
Algorithm Reliability
ML Ops & Training Pipelines
Hard
Very High
This feature requires a user account
Sign up to access this feature.

feature
Access 1000+ data science interview questions

feature
30,000+ top company interview guides

feature
Unlimited code runs and submissions



SQL Interview Questions for Data Analysts
SQL Interview Questions for Data Analysts

Question	Topic	Difficulty	Ask Chance
Causal Inference Without A/B
A/B Testing & Experimentation
Medium
Very High
Department Expenses
SQL
Medium
Very High
Algorithm Reliability
ML Ops & Training Pipelines
Hard
Very High
This feature requires a user account
Sign up to access this feature.

feature
Access 1000+ data science interview questions

feature
30,000+ top company interview guides

feature
Unlimited code runs and submissions



Data analysts use SQL to query data to solve complex business problems or find answers for other employees. In general, SQL data analyst questions focus on analytics and reporting:

Basic SQL Questions - These include the basics, e.g., definitions and beginner SQL queries.
Analytics Questions – For analytics-based questions, you might have to understand what kind of report or graph to build first and then write a query to generate that report. So, it’s an extra step on top of a regular SQL question.
Reporting Questions – SQL reporting questions replicate the work many data or business analysts do daily, e.g., writing queries.
Reporting interview questions focus on writing a query to generate an already-known output, such as producing a report or a metric given some example table.

For analytics-based questions, you might have to understand what kind of report or graph to build first and then write a query to generate that report. So, it’s an extra step on top of a regular SQL question.

Basic SQL Interview Questions
14. What are the different ways of handling NULL when querying a data set?
To handle such a situation, we can use three different operations:

IS NULL − This operator returns true if the column value is NULL.
IS NOT NULL − This operator returns true if the column value is not NULL.‘
<=> − This operator compares values, which (unlike the = operator) is true even for two NULL values.
15. What’s the difference between UNION and UNION ALL? (Asked by Facebook)
UNION and UNION ALL are SQL operators used to concatenate 2 or more result sets. This allows us to write multiple SELECT statements, retrieve the desired results, and then combine them together into a final, unified set.

The main difference between UNION and UNION ALL is that:

UNION: only keeps unique records
UNION ALL: keeps all records, including duplicates
16. What is the difference between an SQL view and a table? (Asked by Kaiser Permanente)
A table is structured with columns and rows. A view is a virtual table extracted from a database by writing a query.

17. What’s the difference between an INNER and OUTER JOIN?
The difference between an inner and outer join is that inner joins result in the intersection of two tables, whereas outer joins result in the union of two tables.

18. What is the difference between WHERE and HAVING?
The WHERE clause is used to filter rows before grouping, and HAVING is used to exclude records after grouping.

19. When do you use the CASE WHEN function?
CASE WHEN lets you write complex conditional statements on the SELECT clause and also allows you to pivot data from wide to long formats.

20. What is the difference between DELETE TABLE and TRUNCATE TABLE in SQL?
Although they’re both used to delete data, a key difference is that DELETE is a Database Manipulation Language (DML) command, while TRUNCATE is a Data Definition Language (DDL) command.

Therefore, DELETE is used to remove specific data from a table, while TRUNCATE removes all the table rows without maintaining the table’s structure.

Another difference is that DELETE can be used with the WHERE clause, but TRUNCATE cannot. In this case, DELETE TABLE would remove all the data from the table while maintaining the structure. TRUNCATE would delete the entire table.

21. How would you pull the date from a timestamp in SQL?
EXTRACT allows us to pull temporal data types like date, time, timestamp, and interval from date and time values.

22. Write an SQL query to select all employees’ records with last names between “Bailey” and “Frederick.”
For this question, assume the table is called “Employees” and the last name column is “LastName”.

SELECT * FROM Employees WHERE LastName BETWEEN 'Bailey' AND 'Frederick'
23. What is the ISNULL function? When would you use it?
The ISNULL function returns an alternative value if an expression is NULL. Therefore, if you wanted to add a default value for NULL values, you would use ISNULL. For example, in the statement:

SELECT name, ISNULL(price, 50) FROM PRODUCTS
NULL price values would be replaced with 50.

Reporting SQL Questions
24. We have a table with an ID and name field. The table holds over 100 million rows, and we want to sample a random row in the table without throttling the database. Write a query to sample a row from this table randomly.
big_table

Column	Type
id	INTEGER
name	VARCHAR
In most SQL databases, there exists a RAND() function, which normally we can call:

SELECT * FROM big_table
ORDER BY RAND()
The function will randomly sort the rows in the table. This function works fine and is fast if you only have, let’s say, around 1,000 rows. It might take a few seconds to run at 10K. And then at 100K, maybe you have to go to the bathroom or cook a meal before it finishes.

What happens at 100 million rows?

Someone in DevOps is probably screaming at you.

Random sampling is important in SQL with scale. We don’t want to use the pre-built function because it wasn’t meant for performance. But maybe we can re-purpose it for our own use case.

We know that the RAND() function actually returns a floating point between 0 and 1. So, if we were to instead call:

SELECT RAND()
We would get a random decimal point to some Nth degree of precision. RAND() essentially allows us to seed a random value. How can we use this to select a random row quickly?

Let’s try to grab a random number using RAND() from our table that can be mapped to an ID. Given we have 100 million rows, we probably want a random number from 1 to 100 million. We can do this by multiplying our random seed from RAND() by the maximum number of rows in our table.

SELECT CEIL(RAND() * (
    SELECT MAX(id) FROM big_table)
)
We use the CEIL function to round the random value to an integer. We must return to our existing table to get the value.

What happens if we have missing or skipped ID values, though? We can solve this by running the join on all the IDs that are greater or equal than our random value and selecting only the direct neighbor if a direct match is impossible.

Once one row is found, we stop (LIMIT 1). And we read the rows according to the index (ORDER BY id ASC). Now, our performance is optimal.

SELECT r1.id, r1.name
FROM big_table AS r1
INNER JOIN (
    SELECT CEIL(RAND() * (
        SELECT MAX(id)
        FROM big_table)
    ) AS id
) AS r2
    ON r1.id >= r2.id
ORDER BY r1.id ASC
LIMIT 1
25. Given a table of job postings, write a query to break down the number of users that have posted their jobs once versus the number of users that have posted at least one job multiple times.
Hint: We want the value of two different metrics, the number of users that have posted their jobs once and the number of users that have posted at least one job multiple times. What does that mean exactly?

26. Write a query to get the current salary for each employee.
More context. Let’s say we have a table representing a company payroll schema.

Due to an ETL error, the employees table, instead of updating the salaries every year when doing compensation adjustments, did an insert instead. The head of HR still needs the current salary of each employee.

27. Write a query to get the total amount spent on each item in the ‘purchases’ table by users that registered in 2023.
More context. Let’s say you work at Costco. Costco has a database with two tables. The first is users, composed of user information, including their registration date, and the second table is purchases, which has the entire item purchase history (if any) for those users.

Here’s a process you can use to solve this question:

You can use INNER JOINor JOIN to connect tables users and purchases on the user_id column
You can filter the results by using the WHERE clause
Use GROUP BY to aggregate items and apply the SUM() function to calculate the amount spent
28. Write a query to get the cost of all transactions by user ordered by total cost descending.
Here’s a code solution:

SELECT
 u.name
 ,u.id AS user_id
 ,ROUND(SUM(p.price * t.quantity ) ,2) AS total_cost
FROM users u
INNER JOIN transactions t
    ON u.id = t.user_id
INNER JOIN products p
    ON p.id = t.product_id
GROUP BY u.name
ORDER BY total_cost DESC
29. Given a table of transactions and a table of users, write a query to determine if users tend to order more to their primary address versus other addresses.
Hint: This question has been asked in Amazon data analyst interviews, and the first step is getting data from the users table to the transactions table. This can be done using a JOIN based on a common column between the tables. How do we identify when the addresses match? We can use the CASE WHEN statement to produce a flag for further calculations. Finally, we need the percentage of all the transactions made to the primary address rounded to two decimals.

30. Write a query to get the top three users with the most upvotes on their comments.
You’re provided with three tables representing a forum of users and their comments on posts and are asked to find the top three users with the most upvotes in the year 2020. Additionally, we’re told that upvotes on deleted comments and upvotes that users make on their own comments don’t matter.

Hint: The trickiest part about this question is performing your JOINs on the proper fields. If you join two of our tables on the wrong key, you could make things difficult, or even impossible, for yourself later on.

31. Write a query to identify customers who placed more than three transactions in 2019 and 2020.
In this question, you’re given two transactions and users.

Hint: Start by joining the transactions and users tables. Use INNER JOIN or JOIN.

Analytics SQL Questions
32. Given a table of search results, write a query to compute a metric to measure the quality of the search results for each query.
search_results table

Column	Type
query	VARCHAR
result_id	INTEGER
position	INTEGER
rating	INTEGER
You want to be able to compute a metric that measures the precision of the ranking system based on position. For example, if the results for dog and cat are….

query	result_id	position	rating	notes
dog	1000	1	2	picture of hotdog
dog	998	2	4	dog walking
dog	342	3	1	zebra
cat	123	1	4	picture of cat
cat	435	2	2	cat memes
cat	545	3	1	pizza shops
…we would rank ‘cat’ as having a better search result ranking precision than ‘dog’ based on the correct sorting by rating.

Write a query to create a metric to validate and rank the queries by their precision of search results, round the metric (avg_rating column) to 2 decimal places.

33. Given the two tables, write an SQL query that creates a cumulative distribution of a number of comments per user. Assume bin buckets class intervals of one.
Hint: What is a cumulative distribution exactly? What would the dataset look like if we were to imagine our output and figure out what we wanted to display on a cumulative distribution graph?

34. We are given a table of bank transactions with three columns: user_id, a deposit or withdrawal value (determined if the value is positive or negative), and created_at time for each transaction.
Write a query to get the total three-day rolling average for daily deposits.

Usually, if the problem states to solve for a moving/rolling average, we’re given the dataset as a table with two columns: the date and the value.

This problem, however, is taken one step further with a table of just transactions with values conditioned to filtering for only deposits and removing records representing withdrawals denoted by a negative value (e.g., 10).

35. Given a table of user experiences representing each person’s work experiences, write a query to determine if a data scientist gets promoted faster or if they switch jobs more frequently.
More context. Let’s say we’re interested in analyzing the career paths of data scientists. Job titles are bucketed into data scientist, senior data scientist, and data science manager. We’re interested in determining if a data scientist who switches jobs more often gets promoted to a manager role faster than a data scientist who stays at one job longer.

This question has been asked in Google data analyst interviews, and it requires a bit of creative problem-solving to understand how we can prove or disprove the hypothesis. The hypothesis is that data scientists who end up switching jobs more often get promoted faster.

Therefore, in analyzing this dataset, we can prove this hypothesis by separating the data scientists into specific segments based on how often they jump into their careers. How would you do that?

36. Write a query to get the distribution of the number of conversations created by each user by day in the year 2020.
Our focus is getting our key metric of the number of new conversations created daily in a single query. To get this metric, we have to group by the date field, and then group by the distinct number of users messaged. Afterward, we can then group by the frequency value and get the total count of that as our distribution.

37. Write a query that could display the percentage of users on our forum that would be acting fraudulently in this manner.
More context. We’re given three tables representing a forum of users and their comments on posts. We want to determine if users create multiple accounts to upvote their comments. What kind of metrics could we use to figure this out?

38. Uber users are complaining that the pick-up map is wrong. How would you verify how frequently this is actually happening?
Hint. What metric would help you investigate this problem?

39. What strategies could we try to implement to increase the outreach connection rate?
More context. Let’s say that Facebook account managers cannot reach business owners after repeated calls to try to onboard them onto a new Facebook business product. Assume that we have training data on all of the account manager’s outreach in terms of calls made, calls picked up, time of call, etc…

One option would be to investigate when calls are most likely to be connected. Could changing our approach here improve the connection rate?

40. You’re analyzing churn on Facebook. How would you investigate if a disparity exists in retention on different Facebook platforms?
Follow-up question. How would you investigate the causes of such a disparity?

Data Analytics Case Study
Data analysis case study questions combine a rotating mix of product intuition, business estimation, and data analytics.

Case questions come up in interviews when the job responsibilities lean to more of a heavy analytics space with an emphasis on solving problems and producing insights for management.

Many times, data analysts will transition into a heavy analytics role when they’re required to take on more scope around the product and provide insights that upper-level management can understand and interpret.

So, data analytics case study questions will focus on a particular problem, and you will be judged on how you break down the question, analyze the problem, and communicate your insights.

Here’s an example data analytics case study question:

41. Given a table of Stack Overflow posts data, suggest three metrics to monitor the community’s health.
Community members can create a post to ask a question, and other users can reply with answers or comments to that question. The community can express their support for the post by upvoting or downvoting.

post_analytics table:

Column	Type	Description
id	int	Primary key of posts table
user_id	int	ID of the user who created the post
created_at	datetime	Timestamp of the post
title	string	Title of the post
body	string	Text content of the post
comment_count	int	Total number of the comments on a post
view_count	int	Total number of the views on a post
answer_count	int	Total number of answers on a post
upvotes	int	Total number of upvotes on the post
More context. You work at Stack Overflow on the community team that monitors the platform’s health. Community members can create a post to ask a question, and other users can reply with answers or comments to that question. The community can express their support for the post by upvoting or downvoting.

42. Write the queries for these metrics in SQL.
This is a classic data analytics case study. A question like this is designed to assess your data intuition, product sense, and ability to isolate key metrics.

Remember: There isn’t one correct answer, but usually, the conversation should head in a similar direction.

For example, this question asks about community health. Broadly, there are several metrics you’ll want to consider: Growth rate, engagement, and user retention, which would provide insights into the community’s health.

The challenge with this question is determining how to measure those metrics with the provided data.

43. Describe an analytics experiment that you designed. How were you able to measure success?
Case questions sometimes take the form of behavioral questions. Data analysts get tasked with experimenting with data to test new features or campaigns. Many behavioral questions will ask about experiments but also tap into how you approach measuring your results.

With questions like these, be sure to describe the objective of the experiment, even if it is a simple A/B test. Don’t be afraid to get technical and explain the metrics and process you used to quantify the results.

44. An online marketplace introduces a new feature that lets buyers and sellers conduct audio chats. Write a query to indicate whether the feature is successful or not.
Bonus question. How would you measure the success of this new feature?

See a step-by-step solution to this data analytics case study problem.

45. Write a query to prove or disprove the hypothesis: CTR depends on the search result rating.
More context. You’re given a table that represents search results from searches on Facebook. The query column is the search term, the position column represents each position the search result came in, and the rating column represents the human rating from 1 to 5, where 5 is high relevance, and 1 is low relevance.

Each row in the search_eventstable represents a single search, with the has_clicked column representing whether a user clicked on a result or not. We hypothesize that the CTR depends on the search result rating.

46. A revised new-user email journey boosts conversion rates from 40% to 43%. However, a few months prior, CVR was 45%. How would you investigate if the new email journey caused the increase in CVR?
See a step-by-step solution to this problem on YouTube.

Python Coding Questions for Data Analysts
Python coding questions for data analysts are usually pretty simple and not as difficult as the ones seen on Leetcode. Most interviewers want to test their basic knowledge of Python to the point that they can write scripts or some basic functions to move data between SQL and Excel or onto a dashboard. These can then be said to only be the basic Python interview questions.

Most data analysts never write production code, and their code is never under scrutiny because it’s not holding a website up or performing some critical business function.

Therefore, most coding questions for data analyst interviews are generally easier and mostly test basic functions required for data manipulation. Pandas questions may also be asked in this round of the interview.

Here’s an example Python coding question:

47. Write a function that can take a string and return a list of bigrams. (Asked by Indeed)
sentence = "Have free hours and love children?"

output = [
 ('have', 'free'),
 ('free', 'hours'),
 ('hours', 'and'),
 ('and', 'love'),
 ('love', 'children')
]

Bigrams are two words that are placed next to each other. To parse them out of a string, we must first split the input string.

We would use the Python function .split() to create a list with each individual word as an input. Create another empty list that will eventually be filled with tuples.

Then, once we’ve identified each individual word, we need to loop through k-1 times (if k is the number of words in a sentence) and append the current word and subsequent word to make a tuple. This tuple gets added to a list that we eventually return. Remember to use the Python function .lower() to turn all the words into lowercase!

def find_bigrams(sentence):
  input_list = sentence.split()
  bigram_list = []

  # Now we have to loop through each word
  for i in range(len(input_list)-1):
    #strip the whitespace and lower the word to ensure consistency
    bigram_list.append((input_list[i].strip().lower(), input_list[i+1].strip().lower()))
  return bigram_list
48. Explain negative indexing. What purpose does it serve?
Negative indexing is a function in Python that allows users to index arrays or lists from the last element. For example, the value -1 returns the last element, while -2 returns the second-to-last element. It is used to display data from the end of a list or to reverse a number or string.

Example of negative indexing:

a = "Python Data Analyst Questions"
print (a[-1])
>> s
49. What is a compound data type in Python?
Compound data structures are single variables that represent multiple values. Some of the most common in Python are:

Lists - A collection of values where the order is important.
Tuples - A sequence of values where the order is important.
Sets - A collection of values where membership in the set is important.
Dictionaries - A collection of key-value pairs where you can access values based on their keys.
50. What is the difference between Python lists, tuples, and sets? When should you use one over the other?
Lists, tuples, and sets are compound data types that serve a similar purpose: storing collections of items in Python. However, knowing the differences between each of them is crucial for computing and memory efficiency.

Lists are mutable collections that are ordered and allow duplicate elements. They are versatile and offer various operations, such as accessing, adding, and removing items. They are suitable when the order of items matters or when you need to change the collection over time.
Tuples are similar to lists in order collections. However, they are immutable, meaning you cannot change their content once defined. Tuples are faster than lists and can be used in situations where the content will remain constant.
Sets are unordered collections that do not allow duplicate elements. Because they are unordered, you cannot access elements by an index. Sets are faster than lists and tuples for membership testing, i.e., checking if an item is in the collection. They are also beneficial when removing duplicates from a collection or performing mathematical set operations such as union, intersection, and difference.
51. How would you find duplicate values in a dataset for a variable in Python?
You can check for duplicates using the Pandas duplicated() method. This will return a boolean series, which is TRUE only for unique elements.

DataFrame.duplicated(subset=None,keep='last')
52. What is list comprehension in Python? Provide an example.
List comprehension defines and creates a list based on an existing one. For example, if we wanted to separate all the letters in the word “retain” and make each letter a list item, we could use list comprehension:

r_letters = [ letter for letter in 'retain' ]
print( r_letters)
We can also use list comprehension for filtering. For example, to get all the vowels in the word “retain,” we do the following:

vowels = [vowel for vowel in 'retain' if vowel in ('a', 'e', 'i', 'o', 'u')]
print(vowels)
If you are concerned about duplicate values, you can opt for sets instead by replacing “[]” with “{}”.

unique_vowels = {vowel for vowel in 'retain' if vowel in ('a', 'e', 'i', 'o', 'u')}
print(unique_vowels)
53. What is sequence unpacking? Why is it important?
Sequence unpacking is a Python operation that allows you to de-structure the elements of a collection and assign them directly to variables without the need for iteration. It provides a terse method for mapping variables to the elements of a compound data structure. For example:

# instead of:
x = coordinates[0]
y = coordinates[1]

# we can unpack a list:
x, y = coordinates

# we can also do the same for sets, tuples, and dictionaries. 
We can even swap the elements of two variables without the use of a third variable:

a = 3
b = 2
a, b = b, a

assert a == 2
assert b == 3

# no assertion errors
If the size of a collection is unclear, you can use the * operator on a variable to assign all extra items to said variable:

food = ('apples', 'oranges', 'carrots', 'cabbages', 'lettuce')
apples, oranges, *vegetables = food
# apples = 'apples', oranges = 'oranges',
# vegetables = ('carrots', 'cabbanges', 'lettuce')
54. Write a function that takes in a list of dictionaries with a key and a list of integers and returns a dictionary with the standard deviation of each list.
Hint: need to use the equation for standard deviation to answer this question. Using the equation allows us to take the sum of the square of the data value minus the mean over the total number of data points, all in a square root.

55. Given a list of timestamps in sequential order, return a list of lists grouped by week (7 days) using the first timestamp as the starting point.
This question sounds like it should be an SQL question, doesn’t it? Weekly aggregation implies a form of GROUP BY in a regular SQL or pandas question. In either case, aggregation on a dataset of this form by week would be pretty trivial.

56. Given two strings A and B, return whether or not A can be shifted some number of times to get B.
Example:

A = 'abcde'
B = 'cdeab'
can_shift(A, B) == True
A = 'abc'
B = 'acb'
can_shift(A, B) == False
Hint: This problem is relatively simple if we work out the underlying algorithm that allows us to easily check for string shifts between the strings A and B.

57. Given two strings, string1 and string2, write a function is_subsequence to determine if string1 is a subsequence of string2.
Hint: Notice that in the subsequence problem set, one string in this problem will need to be traversed to check for the values of the other string. In this case, it is string2.

Statistics and Probability Interview Questions
Statistics and probability questions for data analysts will usually come up on an onsite round as a test of basic fundamentals.

Statistics questions are more likely than probability questions to show up, as statistics are the fundamental building blocks for many analyst formulas and calculations.

58. Given uniform distributions X and Y and the mean 0 and standard deviation 1 for both, what’s the probability of 2X > Y? (Asked by Snapchat)
Given that X and Y both have a mean of 0 and a standard deviation of 1, what does that indicate for the distributions of X and Y?

Let’s look at this question a little closer.

We’re given two normal distributions. The values can either be positive or negative, but each value is equally likely to occur. Since we know the mean is 0 and the standard deviation is 1, we understand that the distributions are also symmetrical across the Y-axis.

In this scenario, we are equally likely to randomly sample a value that is greater than 0 or less than 0 from the distribution.

Now, let’s take examples of random values that we could get from each scenario. There are about six different scenarios here.

X > Y: Both positive
X > Y: Both negative
X < Y: Both positive
X < Y: Both negative
X > Y: X is positive, Y is negative
X < Y: X is negative, Y is positive
We can simulate a random sampling by equating that all six are equally likely to occur. If we play out each scenario and plug the variables into 2X > Y, then we see about half of the time the statement is true, or 50%.

Why is this the case? Generally, let’s return to the fact that both distributions are symmetrical across the Y-axis. We can intuitively understand that if both X and Y are random variables across the same distribution, we will see 2X as being, on average, double positive or double negative the value that Y is.

59. What is an unbiased estimator, and can you provide an example for a layman to understand?
To answer this question, consider how a biased estimator looks. Then, think about how an unbiased estimator differs. Ultimately, an estimator is unbiased if its expected value equals the true value of a parameter, meaning that the estimates are in line with the average.

60. Let’s say we have a sample size of N. The margin of error for our sample size is 3. How many more samples would we need to decrease the margin of error to 0.3?
Hint: In order to decrease our margin of error, we’ll probably have to increase our sample size. But by how much?

61. What’s the Difference Between Correlation and Covariance?
Covariance measures the linear relationship of variables, while correlation measures the strength and direction of the relationship. Therefore, correlation is a function of covariance. For example, a correlation between two variables does not mean that the change in variable X caused the change in variable Y’s value.

62. How would you describe probability distribution to a non-technical person?
Probability distributions represent random variables and associated probabilities of different outcomes. In essence, a distribution maps the probability of various outcomes.

For example, a distribution of test grades might look similar to a normal distribution, AKA bell curve, with the highest number of students receiving Cs and Bs and a smaller percentage of students failing or receiving a perfect score. In this way, the center of the distribution would be the highest, while outcomes at either end of the scale would fall lower and lower.

63. What is a non-normal distribution? Provide an example.
A probability distribution is abnormal if most observations do not cluster around the mean, forming the bell curve. An example of a non-normal probability distribution is a uniform distribution, in which all values are equally likely to occur within a given range. A random number generator set to produce only the numbers 1-5 would create such a non-normal distribution, as each value would be equally represented in your distribution after several hundred iterations.

64. What is the probability that it’s raining in Seattle?
More context. You are about to get on a plane to Seattle. You call 3 random friends in Seattle and ask each other if it’s raining. Each has a 2⁄3 chance of telling you the truth and a 1⁄3 chance of messing with you by lying. All 3 friends tell you that “yes” it is raining.

Hint: There are several ways to answer this question. Given that a frequentist approach operates on the set of known principles and variables given in the original problem, you can logically deduce that P(Raining)= 1-P(Not Raining).

Since all three friends have given you the same answer as to whether or not it’s raining, what can you determine about the relationship between P(Not Raining) and the probability that each of your friends is lying?

65. How would you design a function to detect anomalies if given a univariate dataset? What if the data is bivariate?
Before jumping into anomaly detection, discuss the meaning of a univariate dataset. Univariate means one variable. For example, travel time in hours from your city to 10 other cities is given in an example list below:

12, 27, 11, 41, 35, 22, 18, 43, 26, 10

This kind of single column data set is called a univariate dataset. Anomaly detection is a way to discover unexpected values in datasets. The anomaly means data exists that is different from the normal data. For example, you can see below the dataset where one data point is unexpectedly high intuitively:

12, 27, 11, 41, 35, 22, 76767676, 18, 43, 26, 10

66. You want to look at the mean and median for a dataset. When would you use one measure over the other? How do you calculate the confidence interval of each measure?
You should answer these questions in your response:

Which measure has the widest application?
What happens when the dataset has values that are way above or below most other values?
How would your choice of metric be influenced by the non-continuous data?
67. You have a biased and unbiased coin. You select a random coin and flip it two times. What is the probability that both flips result in the same side?
Hint: The first step in solving this problem is to separate it into two instances– one where you grab the fair coin and one where you grab the biased coin. Solve for the probabilities of flipping the same side separately for both.

68. What could be the cause of a capital approval rate decrease?
Capital approval rates have gone down compared to our overall approval rate. Let’s say last week it was 85%, and the approval rate went down to 82% this week, which is a statistically significant reduction.

The first analysis shows that all approval rates stayed flat or increased over time when looking at the individual products.

Product 1: 84% to 85% week over week
Product 2: 77% to 77% week over week
Product 3: 81% to 82% week over week
Product 4: 88% to 88% week over week
Hint: This would be an example of Simpson’s Paradox, a phenomenon in statistics and probability. Simpson’s Paradox occurs when a trend shows in several groups but either disappears or is reversed when combining the data.

69. How would you explain confidence intervals?
In probability, confidence intervals refer to a range of values you expect your estimate to fall between if you rerun a test. Confidence intervals are a range that is equal to the mean of your estimate plus or minus the variation.

For example, if a presidential popularity poll had a confidence interval of 93%, encompassing a 50%-55% approval, it would be expected that, if you re-polled your sample 100 more times, 93 times the estimate would fall between the upper and lower values of your interval. Those other seven events would fall outside, which is to say either below 50% or above 55%. More polling would allow you to get closer to the true population average and narrow the interval.

70. You must draw two cards from a shuffled deck, one at a time. What’s the probability that the second card is not an ace?
One question to add: does order matter here? Is drawing an ace on the second card the same as drawing an ace on the first and still drawing a second card? Let’s see if we can solve this and prove it.

We can generalize to two scenarios when drawing two cards to get an ace:

Drawing an ace on the first card and an ace on the second card
Drawing not an ace on the first card and an ace on the second card
If we model the probability of the first scenario, we can multiply the two probabilities of each occurrence to get the actual probability.

Question	Topic	Difficulty	Ask Chance
Causal Inference Without A/B
A/B Testing & Experimentation
Medium
Very High
Department Expenses
SQL
Medium
Very High
Algorithm Reliability
ML Ops & Training Pipelines
Hard
Very High
This feature requires a user account
Sign up to access this feature.

feature
Access 1000+ data science interview questions

feature
30,000+ top company interview guides

feature
Unlimited code runs and submissions



A/B Testing and Experimentation
A/B testing and experimentation questions for data analysts tend to explore the candidate’s ability to conduct A/B tests properly. You should have a strong knowledge of p-values and confidence intervals and be able to assess the experiment’s validity.

71. The PM checks the results of an A/B test (standard control and variant) and finds a .04 p-value. How would you assess the validity of the result? How would you assess the validity of the result?
In this particular question, you’ll need to clarify the context of how the A/B test was set up and measured.

If we have an A/B test to analyze, there are two main ways in which we can look for invalidity. We could likely re-phrase the question: How do you correctly set up and measure an A/B test?

Let’s start out by answering the first part of figuring out the validity of the setup of the A/B test:

1. How were the user groups separated?

Can we determine that the control and variant groups were sampled according to the test conditions?

If we’re testing changes to a landing page to increase conversion, can we compare the two different users in the groups to see different metrics in which the distributions should look the same?

For example, if the groups were randomly bucketed, would the distribution of traffic from different attribution channels still look similar, or would the variant A traffic channel come primarily from Facebook ads and the variant B from email? If testing group B has more email traffic, that could be a biased test.

2. Were the variants equal in all other aspects?

The outside world often has a much larger effect on metrics than product changes do. Users can behave very differently depending on the day of the week, the time of year, the weather (especially for a travel company like Airbnb), or whether they learned about the website through an online ad or found it organically.

If variant A’s landing page has a picture of the Eifel Tower and the submit button on the top of the page, and variant B’s landing page has a large picture of an ugly man and the submit button on the bottom of the page, then we could get conflicting results based on the change to multiple features.

Measurement

Looking at the actual measurement of the p-value, we understand that the industry standard is .05, which means that 19 out of 20 times that we perform that test, we’re going to be correct that there is a difference between the populations.

However, we have to note some things about the test in the measurement process.

What was the sample size of the test?

How long did it take before the product manager measured the p-value? Lastly, how did the product manager measure the p-value, and did they do so by continually monitoring the test?

If the product manager ran a T-test with a small sample size, they could very well easily get a p-value under 0.05. Often, the source of confusion in AB testing is how much time you need to make a conclusion about the results of an experiment.

The problem with using the p-value as a stopping criterion is that the statistical test that gives you a p-value assumes that you designed the experiment with a sample and effect size in mind. If we continuously monitor the development of a test and the resulting p-value, we are very likely to see an effect, even if there is none. The opposite error is common when you stop an experiment too early before an effect becomes visible.

The most important reason is that we perform a statistical test every time you compute a p-value, and the more you do it, the more likely you are to find an effect.

How long should we recommend running an experiment for then? To prevent a false negative (a Type II error), the best practice is to determine the minimum effect size that we care about and compute based on the sample size (the number of new samples that come every day) and the certainty you want, how long to run the experiment for, before starting the experiment.

72. How can you effectively design an A/B test? Are there times when A/B testing shouldn’t be used?
Split testing fails when you have unclear goals. That’s why it’s imperative to start backward with that goal. Is it to increase conversions? Are you trying to increase engagement and time spent on the page? Once you have that goal, you can start experimenting with variables.

73. How much traffic would you need to drive to a page for the result of an A/B test to be statistically significant?
Statistical significance - having 95% confidence in the results - requires the right volume of data. That’s why most A/B tests run for 2-8 weeks. Comparing metrics like conversions is fairly easy to calculate. In fact, most A/B tools have built-in calculators.

74. How would you conduct an experiment to test a new ETA estimate feature in Uber? How would you know if your results were significant?
Hint: This question asks you to think hypothetically about A/B testing. But the format is the same: Walk the interviewer through setting up the test and how you arrive at a statistically relevant result.

75. How would you explain P-value to someone who is non-technical?
The p-value is a fundamental concept in statistical testing. First, why does this kind of question matter? What an interviewer is looking for here is whether you can answer this question in a way that conveys your understanding of statistics and can also answer a question from a non-technical worker who doesn’t understand why a p-value might matter.

For example, if you were a data scientist and explained to a PM that the ad campaign test has a .08 p-value, why should the PM care about this number?

76. Your company wants to test new marketing channels. How would you design an A/B test for the most efficient marketing spend?
The new channels include YouTube ads, Google search ads, Facebook ads, and direct mail campaigns.

First, you’d want to follow up with clarifying questions and make some assumptions. Let’s assume, for example, that the most efficient means the lowest cost per conversion and that we’ve been asked to spend evenly across all platforms.

77. You want to run an experiment but find that the distribution of the dataset is not normal. What kind of analysis would you run, and how would you measure which variant won?
Understanding whether your data abides by or violates a normal distribution is an important first step in your subsequent data analysis.

This understanding will change which statistical tests you want to use if you need to look for statistical significance immediately. For example, you cannot run a t-test if your distribution is non-normal since this test uses mean/average as a way to find differences between groups.

78. You want to A/B test pricing levels for subscriptions. The PM asks you to design a two-week test. How do you approach this? How do you determine if the pricing increase is a good business decision?
Hint: Is A/B testing a price difference a good idea? Would it encourage users to opt out of your test if they saw different product prices?

Is there a better way to test pricing?

79. A survey shows that app users who use an optional location-sharing feature are “less happy” with the app as a whole. Is the feature actually causing users to be unhappy?
Causal relationships are hard to come by, and truly determining causality is tough as the world is full of confounding variables. Because of this, instead of causality, we can dissect the correlation between the location-sharing feature and the user unhappiness level.

At its core, this interview question tests how to dig into the science and statistics behind their assumption. The interviewer is essentially asking a small variation of a traditional experimental design with survey research and wants to know how you would either validate or disprove this claim.

Product Metrics Data Analyst Questions
Metrics is a common product analyst interview question subject, and you’ll also see this type of question in product-oriented data analyst roles. In general, these questions test your ability to choose metrics to investigate problems or measure success. These questions require a strong product sense to answer.

80. You’re given a list of marketing channels and their costs. What metrics would you use to determine the value of each marketing channel?
The first thing we’ll want to do when faced with an interview question like this one is to ask a few clarifying questions. Answer these questions first:

What is the company’s business model?
Is there one product or many?
Let’s say it’s a SaaS business that offers a free Studio model of their product but makes their money selling enterprise subscriptions. This gives us a better sense of how they’re approaching their customers. They’re saying: here’s a good free tool, but you can pay to make it even better.

How many marketing channels are there?
Imagine what your analysis would look like if the answer to this question was “a few.” Now imagine what your analysis would look like if the answer to this question was “hundreds.”

Are some marketing channels bigger than others? What’s the proportion?
Mode could be spending 90% of its marketing budget on Facebook Ads and 10% on affiliate marketing, or vice versa. We can’t know unless we ask.

What is meant by “the value of each marketing channel?”
Here’s where we start getting into the meat of the question.

81. A PM at Facebook comes to you and tells you that friend requests are down 10%. What do you do?
This question has been asked in Facebook data analyst interviews. See an example solution to this question on YouTube.

82. What are some reasons why the average number of comments per user would be decreasing, and what metrics would you look into?
More context. Let’s say you work for a social media company that has just done a launch in a new city. Looking at weekly metrics, you see a slight decrease in the average number of comments per user from January to March in this city. The company has been consistently growing new users in the city from January to March.

Let’s model an example scenario to help us see the data.

Jan: 10000 users, 30000 comments, 3 comments/user
Feb: 20000 users, 50000 comments, 2.5 comments/user
Mar: 30000 users, 60000 comments, 2 comments/user
We’re given information that the total user count is increasing linearly, which means that the decreasing comments/user is not an effect of a declining user base creating a loss of network effects on the platform. What else can we hypothesize, then?

83. How would you measure the success of Facebook Groups?
Start here: What is the point of Facebook Groups? Primarily we could say Facebook Groups provides a way for Facebook users to connect with other users through a shared interest or real-life/offline relationship.

How could we use the goals of Facebook Groups to measure success?

84. What kind of analysis would you conduct to recommend UI changes?
More context. You have access to a set of tables summarizing user event data for a community forum app. You’re asked to conduct a user journey analysis using this data with the eventual goal of improving the user interface.

85. How would you measure the success of Uber Eats?
See a step-by-step solution for this question on YouTube.

86. What success metrics would you be interested in for an advertising-driven consumer product?
With this question, you might define success in terms of advertising performance. A few metrics you might be interested in are:

CTR
CPC
Pageviews or daily actives (for apps)
Conversion rate
Number of purchases
Cost per conversion
87. How do success metrics change by product type?
Let’s look at two examples: An eCommerce product like Groupon vs. a subscription product like Netflix.

E-commerce metrics tend to be related to conversions and sales. Therefore, you might be interested in the number of purchases, conversion rate, quarterly or monthly sales, and cost of goods sold.

Subscription products tend to focus more on subscriber costs and revenue, like churn rates, cost of customer acquisition, average revenue per user, lifetime value, and monthly recurring revenue.

88. Given a dataset of raw events, how would you come up with a measurement to define what a “session” is for the company?
More context. Let’s say you’re given event data from users on social networking sites like Facebook. A product manager is interested in understanding the average number of “sessions” that occur every day. However, the company has not technically defined what a “session” is yet.

The best the product manager can do is illustrate an example of a user browsing Facebook in the morning on their phone and then again during lunch as two distinct “sessions.” There must be a period of time when the user leaves Facebook to do another task before coming back again anew.

89. Some of the success metrics for the LinkedIn newsfeed algorithm are going up, while others are going down. What would you look at?
See a solution for this question on YouTube.

90. The number of products or subscriptions sold is declining. How would you investigate this problem?
This question provides you with a chance to show your expertise in analyzing sale metrics and KPIs. Some of the challenges you might bring up include competitor price analysis, examining core customer experiences, and investigating evolving customer desires. Your goal in your response should be to outline how you would perform root cause analysis.

Tip. Start with some clarifying questions like, What is the product? Who is the audience? How long has the decline in sales persisted?

91. You’re asked to investigate how to improve search results. What metrics would you investigate? What would you look at to determine if current search results are effective?
More context. Specifically, we want to improve search results for people looking for things to do in San Francisco.

92. Let’s say you work on the growth team at Facebook and are tasked with promoting Instagram from within the Facebook app. Where and how could you promote Instagram through Facebook?
This product question is more focused on growth and is very much used for Facebook’s growth marketing analyst technical screen. Here are a couple of things that we have to remember.

Like usual product questions where we are analyzing a problem and coming up with a solution with data, we have to do the same with growth, except we have to come up with solutions in the form of growth ideas and provide data points for how they might support our hypothesis.

93. How would you measure success for Facebook Stories?
Measuring the success of Facebook Stories requires an integrated approach that examines how users interact with the feature and its impact on the platform. Key to this evaluation is understanding engagement levels, which are reflected through metrics such as the total number of story views and unique viewers, alongside interactions like replies and reactions. These figures are pivotal because they indicate not just how many people are watching, but how actively they are engaging with the content.

Excel Interview Questions
Excel is still a widely used tool by data analysts, and in interviews, Excel questions typically focus on advanced features. These questions might ask for definitions, or you may be required to perform some Excel tasks.

Data analysts should also have strong knowledge of data visualization. Data visualization interview questions typically focus on design and presenting data, and may be more behavioral in nature. Be prepared to talk about how you make data accessible on dashboards.

94. Explain the Excel VLOOKUP function. What are the limitations of VLOOKUP?
This function allows users to find data from one column, and return a corresponding value from another.

For example, if you were analyzing a spreadsheet of customer data, you might use VLOOKUP to find a customer name and the corresponding phone number.

One limitation of VLOOKUP is that it only looks to the right of the column you are analyzing. For example, you couldn’t return a value from column A, if you used column B as the lookup column.

Another limitation is that VLOOKUP only returns the first value; if the spreadsheet contains duplicate records, you won’t see any duplicates.

95. What is conditional formatting in Excel? When is a good time to use conditional formatting?
Conditional formatting allows users to change the appearance of a cell based on specified conditions.

Using conditional formatting, you can quickly highlight cells or ranges of cells, based on your conditions. Data analysts use conditional formatting to visualize data, identify patterns or trends, or detect potential issues.

96. What are your favorite data visualization tools?
Data analysts will be asked what tools they have experience with. Choose a few you’re most comfortable with and explain the features you like.

97. What are some challenges you’ve experienced working with large volumes of data?
One tip: Think of questions like this in terms of Big Data’s 5 Vs: volume, velocity, variety, veracity, and value.

98. Can you use multiple data formats in pivot tables?
Data can be imported from a variety of sources by selecting the Data tab and clicking Get External Data > From Other Sources. Excel worksheet data, data feeds, text files, and other such data formats can be imported, but you will need to create relationships between the imported tables and those in your worksheet before using them to create a pivot table.

99. When creating a visualization, you suspect data is missing. What do you do?
In your answer, provide an overview of your data validation process. For example, you might say, “The first step I would do would be to prepare a data validation report, which reveals why the data failed.” Then, you might talk through strategies for analyzing the dataset or techniques to process missing data, like deletion or mean/median/mode imputation.

Question	Topic	Difficulty	Ask Chance
Causal Inference Without A/B
A/B Testing & Experimentation
Medium
Very High
Department Expenses
SQL
Medium
Very High
Algorithm Reliability
ML Ops & Training Pipelines
Hard
Very High
This feature requires a user account
Sign up to access this feature.

feature
Access 1000+ data science interview questions

feature
30,000+ top company interview guides

feature
Unlimited code runs and submissions



Visualization Interview Questions
Data visualization involves presenting data in a graphical or pictorial format. This allows viewers to see data trends and patterns that may not be easy to understand in text-based data. Tableau, Power BI, and Python libraries such as Matplotlib and Seaborn are some of the most commonly used tools for data visualization.

100. Discuss your experience creating visualizations using Tableau, Power BI, or Python tools. What distinct features have you utilized in each?
This question requires you to detail your hands-on experience with the mentioned tools. It involves discussing specific features you have used in Tableau, Power BI, and Python, such as creating different types of charts, setting up dashboards, or using Python libraries like Matplotlib and Seaborn for custom visualizations.

101. What is DAX, and why is it important in Power BI?
DAX, or Data Analysis Expressions, is a library of functions and operators used to create formulas in Power BI, Analysis Services, and Power Pivot in Excel. These formulas, or expressions, are used to define custom calculations for tables and fields and to manipulate data within the model.

102. Imagine you’re working on a sales report and have a table of daily sales data. You want to calculate the monthly sales total. How could you use DAX to do this?
This question tests your understanding of DAX time-intelligence functions. A suitable response could be:

“I would combine the SUM and CALCULATE functions and a Date table. First, I would create a measure using the SUM function to total the sales. Then, I would use the CALCULATE function and the DATESMTD (Dates Month to Date) function to calculate the monthly total. The DAX expression would look something like this:

*Monthly Sales = CALCULATE(SUM(Sales[Daily Sales]), DATESMTD('Date'[Date]))“*

103. Suppose a company has collected a large dataset on customer behavior, including demographics, transaction data, browsing history, and customer service interactions. You are tasked with presenting this data to the executive team, which does not comprise data professionals. How would you go about this?
This question assesses your ability to analyze complex datasets and create straightforward, impactful visualizations. Your response might include:

“Understanding the audience is key. For an executive summary, it’s important to focus on high-level insights. I would start by performing exploratory data analysis to identify key trends and relationships within the data. From this, I could determine which aspects are most relevant to the executive team’s interests and strategic goals.

For visualization, I would use a tool like Tableau or Power BI, which is known for its user-friendly, interactive dashboards. To make the data more digestible, I would utilize various chart types, such as bar graphs for categorical data comparison, line graphs for trend analysis, or pie charts for proportions.

To add an interactive element, I’d implement filters to allow executives to view data for different demographics, products, or time periods. Keeping the design clean and ensuring the visuals tell a clear story is crucial. For the presentation, I would walk them through the dashboard, explain key insights, and address any questions.”

104. You are working for an e-commerce company that needs a real-time dashboard to monitor sales across various product categories. Would you use Tableau or Power BI for this task? How would you leverage the chosen tool’s features to create the dashboard?
Your response should demonstrate your knowledge of Tableau and Power BI and ability to select the most appropriate tool for a specific task.

“For real-time sales monitoring, both Tableau and Power BI can be effective. However, if the company uses Microsoft’s suite of products and requires extensive integration with these services, I would lean towards Power BI as it’s part of the same ecosystem.

Power BI has robust real-time capabilities. I would leverage Power BI’s DirectQuery feature to connect to the sales database, ensuring the data displayed on the dashboard is always up-to-date. The tool also allows for datasets that can be used to stream and update data continuously.

To visualize sales, I would design a dashboard that includes key metrics such as total sales, sales by product category, and changes in sales over time. I would also include slicers to allow users to filter data by region, time period, or other relevant dimensions.

Power BI also allows for creating alerts based on KPIs that could notify the team when a sales target is reached or when there are significant changes in sales trends.”
Entry-Level SQL Interview Questions for Data Analysts
These SQL interview questions for data analyst freshers tend to appear early in the hiring funnel—typically in recruiter screens, take-home challenges, or online assessments. While you might get sent a SQL interview questions for data analyst PDF as prep material, remember that companies are testing more than syntax memorization. They’re looking for clarity in how you approach data, how you structure logic, and how well you can generalize patterns across datasets.

Each of these basic SQL interview questions for data analyst roles targets specific foundational skills—whether it’s joining tables, applying aggregation, or extracting time-based insights. Mastering these builds a strong core for more advanced questions later in the process.

What query returns the largest salary in each department?

This exercise checks whether you can aggregate data and apply group-level filters. You need to GROUP BY department_id and use MAX(salary) to capture the highest value per group, then join to a dimension (or add a window function) if the interviewer also wants employee names. It reinforces the idea that every SELECT with a grouped aggregate must include only grouping columns or aggregates. Interviewers listen for discussion of NULL salaries and why casting to DECIMAL might be required when a table mixes currencies. Clear communication of these edge cases shows you understand how basic aggregation supports compensation dashboards.

How would you find the 2nd-highest salary in the engineering department?

A classic ranking problem that proves you can work with ordering and limits. Common answers use DENSE_RANK() or ROW_NUMBER() partitioned by department, filter on =2, and add WHERE dept_name='engineering'. Candidates should point out that ties at the top push the “true” second value down, making DENSE_RANK() the safer choice. Interviewers like to hear how you’d handle departments with fewer than two employees, perhaps returning NULL or excluding them entirely. Knowing when to use a window function versus a correlated subquery highlights core SQL literacy.

Which neighborhoods have zero registered users?

This anti-join problem tests understanding of NULL handling and set logic. A straightforward LEFT JOIN users u ON n.neighborhood_id = u.neighborhood_id WHERE u.user_id IS NULL surfaces empty areas. Explaining why NOT IN or NOT EXISTS could behave differently when NULLs appear shows grasp of three-valued logic. The query is common in churn or vacancy analyses where managers need to see untouched market segments. Interviewers may ask how adding an index on neighborhood_id speeds results on large city-wide datasets.

How can you return one random car manufacturer with equal probability?

Selecting a uniform random row validates awareness of database-specific random functions (ORDER BY RANDOM() in Postgres, TABLESAMPLE in BigQuery, etc.). Candidates should mention why adding LIMIT 1 is critical and discuss performance implications when the table grows—e.g., why full sorts can be expensive and how to use a precomputed sampling column. This maps to real features like “pick a random promo” or A/B bucket assignment. Good answers show you can translate business requests into performant SQL rather than relying on naïve approaches.

How much did 2022 sign-ups spend on every product?

Here you join users (filtered on registration_year = 2022) to purchases, then sum price * quantity per product_id. It reinforces join direction, date filtering, and grouped aggregation—bread-and-butter skills for any analyst. Explaining why you use an INNER JOIN versus LEFT JOIN (to exclude users with no purchases) demonstrates awareness of how join semantics affect totals. You might also discuss rounding currency and indexing on (user_id, product_id) to keep the query responsive.

How do you calculate the daily average downloads for free vs. paying accounts?

This question couples conditional aggregation with date grouping. You join accounts to downloads, apply COUNT(DISTINCT download_id) or simple COUNT(*), and divide by distinct account count per plan to get averages—rounding to two decimals. Interviewers expect mention of grouping by download_date and plan_type and why accounts with zero downloads should be excluded per spec. It mirrors real SaaS KPIs like DAU/MAU or usage per subscription tier.

What query returns the maximum quantity bought for every product each year?

The task blends date extraction (EXTRACT(YEAR FROM order_date)) with per-product aggregation. You need to group by year and product_id, selecting MAX(quantity) as max_quantity, then order by those keys. Bringing up indexing on (product_id, order_date) and partitioning large fact tables on year helps show you think about scale even for basic metrics. Such year-over-year comparisons are staple requests in retail analytics.

How many days separate each user’s first and last session in 2020?

This problem evaluates use of MIN() and MAX() in one pass per user, or window functions if you prefer. You filter on YEAR(session_date)=2020, compute the difference in days, and return user_id plus the gap. Candidates should highlight that results may be negative if data quality is bad and suggest placing a composite index on (user_id, session_date) to speed scanning billions of events. It echoes churn analytics where tenure length influences retention models.

How do you compute the average order value by gender?

Joining customer attributes to transaction totals and then grouping by gender tests basic join logic plus conditional counting (only users who have ever placed an order). You sum order_amount per user, divide by order count, and round to two decimals. Interviewers note whether you handle NULL genders and whether you use a subquery or CTE for clarity. Such demographic breakdowns appear daily in e-commerce BI work.

What share of Apple-platform actions ranked in the top-5 during November 2020?

You must filter on platform, restrict to November 2020, aggregate counts, then rank with DENSE_RANK. Handling ties properly and producing an ordered output shows mastery of grouping plus ranking logic in real engagement analyses.

How would you flag each purchase as either the customer’s first or a repeat in its product category?

Interviewers want to see whether you can leverage window functions (ROW_NUMBER() or MIN(id) OVER (PARTITION BY user_id, category)) to mark a “first” versus subsequent purchase, then cast that boolean into a tidy feature column. A good solution joins no extra tables, sorts by purchase time, and explains why session-level deduping isn’t needed. Mentioning that this repeat-purchase label later feeds retention analyses shows business awareness while keeping the SQL lightweight.

Given wireless packet logs, how can you return—per SSID—the largest number of packets any single device sent in the first ten minutes of 1 Jan 2022?

The query filters on the timestamp window, groups by both ssid and device_id, counts packets, then applies MAX() (or ROW_NUMBER() with DESC ordering) per SSID. Explaining that you choose an index on (ssid, created_at) to speed the time filter demonstrates practical sense, yet the core logic remains a straightforward aggregation—squarely entry-level.

What SQL statement gives each user’s total transaction cost, sorted from highest spender to lowest?

This test reinforces simple grouping (SUM(amount)) and ordering skills. Candidates should highlight that an INNER JOIN to a product table isn’t necessary unless unit prices live elsewhere, and that NULL amounts require COALESCE to keep sums correct. Such wallet-share rollups are daily fare for junior analysts in fintech or retail data teams.

How would you output, in one result set, the total transaction count, the number of distinct purchasers, the count of “paid” transactions ≥ $100, and the product with the highest paid revenue?

Interviewers are testing whether you can combine scalar subqueries or CTEs into a single select list. A neat answer uses four subqueries—each aggregating differently—while noting why unioning or multiple passes over the table would be less efficient. This “dashboard in one row” pattern appears often in recruiter screens.

Which five user actions ranked highest during Thanksgiving week 2020, and what were their ranks (ties allowed)?

The task mixes filtering on a date range, aggregating counts, and ranking with DENSE_RANK(). Candidates should explain tie handling and why ORDER BY action_count DESC before ranking is crucial. The scenario mirrors common engagement reporting—perfect for junior analysts who’ll build feature-usage tables.

How do you calculate the overall acceptance rate of friend requests, rounded to four decimals?

Solving requires counting total requests versus accepted ones—often via a join or a request_id IN (SELECT …) pattern. Key talking points include integer division pitfalls, the need to cast to DECIMAL, and whether to exclude self-friend edge cases. Simplicity keeps it entry-level, but the precision requirement checks attention to detail.

After discovering duplicate rows in employee_projects, how would you still identify the five priciest projects by budget-to-employee ratio?

A clean answer uses COUNT(DISTINCT employee_id) in the denominator, guarding against duplicates, then orders by the computed ratio and limits to five. The exercise spotlights practical data-quality thinking (deduping) without venturing into advanced optimization, making it a solid capstone basic query for new analysts.

SQL Interview Questions for Experienced Data Analysts
Once you’ve cleared the basics, most data analyst SQL interviews begin to probe deeper into query logic, edge-case reasoning, and optimization skills. For candidates with 3+ years of experience, the expectations go beyond just writing accurate queries. These are the types of SQL query interview questions for data analyst roles that assess how well you can translate business requests into accurate, testable SQL logic.

Expect a mix of hands-on live coding tasks and take-home SQL challenges—especially at companies like Meta, Amazon, or Netflix—where your ability to manipulate data at scale matters just as much as syntax fluency.

These types of SQL interview questions for experienced data analyst roles are especially common in companies handling terabytes of data daily—like e-commerce platforms, fintech firms, and data-driven marketplaces. If you’re looking to ace SQL interview questions for 3 years experience, focus on techniques like query planning, indexing strategy, and intelligent use of CTEs or window functions.

What is the last transaction recorded on each calendar day?

A banking table lists id, transaction_value, and created_at timestamps. Your goal is to pick, for every date, the single transaction with the latest timestamp and output its id, amount, and datetime. This entry-level task teaches the staple window-function pattern—ROW_NUMBER() OVER (PARTITION BY CAST(created_at AS DATE) ORDER BY created_at DESC)—and encourages candidates to consider tie-breakers when two entries share an identical timestamp. Interviewers gauge whether you can partition correctly, convert datetimes to dates, and deliver an ordered result set that business users can trust.

How would you pivot exam results so each student’s four test scores appear on a single row?

A table exam_scores records student-id, exam-id (1-4) and score. You’re asked to reshape the data into a wide format—one row per student, with separate columns for Exam 1 through Exam 4. The prompt reinforces essential entry-level skills: conditional aggregation (or filtered pivots) and null handling when scores are missing. Interviewers love it because it surfaces your mental model of grouping, selective aggregation with CASE, and output formatting for downstream dashboard use. Getting it right demonstrates that you can translate a reporting requirement into clean SQL without over-engineering the solution.

How would you pull a truly random row from a 100-million-row table without overloading the database?

A naïve ORDER BY RANDOM() causes full sorts, so seasoned beginners mention more efficient tricks—sampling by id range, using a random modulo predicate, or leveraging database sampling clauses like TABLESAMPLE BERNOULLI. The question pushes you to reason about performance trade-offs and estimate how long a query might lock. It also opens discussion on why approximate randomness is often “good enough” for dashboards or QA spot-checks.

Which customers have placed more than three transactions in both 2019 and 2020?

You aggregate by (user_id, year) with COUNT(*), filter on >3 using a CTE, then GROUP BY user_id HAVING COUNT(DISTINCT year)=2. The task ensures you understand grouping, HAVING filters, and how to pivot year-level conditions into a single pass. It’s the kind of simple cohort query recruiters expect juniors to nail quickly.

Which shipments were delivered during a customer’s membership period, and which were not?

You’re a data scientist on Amazon’s distribution team and must tag each shipment as Y (delivered while the customer was an active member) or N (delivered outside that window). The exercise checks your comfort with conditional joins and date-range logic: you’ll join a customers table that stores membership start and end dates to a shipments table, compare shipment dates against those ranges, and return a tidy report. A correct answer shows you can reason about inclusive vs. exclusive boundaries (edge-case shipments sent on the exact start or end date) and format a boolean output column. It’s a classic entry-level test of CASE expressions, simple joins, and clear communication of business rules. Mastering this pattern prepares you for common “flag-this-row” analytics tasks that pop up in day-to-day work.

How would you list only the duplicate rows in a users table?

Data-cleaning is core to analyst work, and this task checks your ability to spot duplicates using COUNT(*) > 1 in a grouped CTE or ROW_NUMBER() > 1 in a window. You must decide which columns define “duplicate”—often all columns except a surrogate key—and explain why hashing or concatenating fields can be handy. The interviewer looks for discussion of removing rather than just identifying duplicates, highlighting the importance of reproducible ETL pipelines. Mentioning how to add a composite unique index to prevent recurrence shows practical thinking.

Who are the top three highest-earning employees in each department?

Using employees and departments, build a ranked list of the three largest salaries per department, outputting employee full name, department name, and salary. The question probes intermediate query construction: joining reference tables, applying RANK() or DENSE_RANK(), and handling departments with fewer than three staff. A solid answer shows familiarity with window functions, tie-breaking rules, and ordering by multiple fields—skills that quickly separate candidates who only know basic aggregation from those who can craft polished reporting queries.

How many customers were upsold after their initial purchase?

Given a purchases table with timestamps, determine the number of users who bought additional products after their first purchase date (same-day multiple items don’t count). You’ll apply MIN(purchase_date) in a CTE, join back, and filter on later dates. The scenario tests logical thinking around customer behavior funnels and event ordering, plus competence with CTEs and date comparisons. It’s a favorite mid-screen question because the correct query is short yet requires careful reasoning about “first purchase” vs. “later purchase” semantics.

Create a January-2020 histogram of comments per user, with one-comment bins.

From an events table, count how many comments each user left in January 2020, then bucket those counts (0, 1, 2, …) and tally users per bucket. This query forces use of subqueries or CTEs for per-user counts followed by either a GROUP BY on that derived count or a windowed approach. Interviewers want to see if you understand grouping on aggregated results, generating missing buckets (optional), and rounding percentages if requested. It’s representative of product-analytics tasks like building engagement histograms.

What item did each user purchase third?

A transactions table records every order with user_id, item, timestamp, and id. You must return, for every user, the item (or full row) corresponding to their third chronological purchase, breaking timestamp ties with the lower id. The exercise highlights ranking functions (ROW_NUMBER()) and tie-handling logic—core abilities for analysts who work with event streams. It also demonstrates your understanding of why deterministic ordering matters when timestamps collide.

How would HR total regular pay, overtime, and overall compensation per role?

Group the payroll table by role_title, SUM(regular_salary) AS reg_pay, SUM(overtime_pay) AS ot_pay, and compute total_comp = reg_pay + ot_pay. Presenting both component and aggregated figures helps budget planning, and comparing each role’s share to company averages can reveal inequities. The paragraph stresses validating that overtime isn’t double-counted and explains how currency conversions or multi-country payrolls complicate roll-ups.

What query totals IT, HR, Marketing, and Other departmental spend by 2023 fiscal quarter?

Create fiscal_qtr = DATE_TRUNC('quarter', txn_date) (or custom fiscal logic), then sum amounts with conditional aggregation: SUM(CASE WHEN dept='IT' THEN amt END) AS it_spend, and group by fiscal_qtr. An “Other” column sums any department not explicitly listed. Finance uses this snapshot to spot over-budget units quickly. Including a quarter index or partition improves performance, and noting how fiscal calendars can differ from calendar quarters shows analyst diligence.

How do you compute a three-day rolling average of steps per user, excluding the first two days?

Partition by user_id, order by step_date, then AVG(steps) OVER (PARTITION BY user_id ORDER BY step_date ROWS BETWEEN 2 PRECEDING AND CURRENT ROW) gives the moving average. Filter out rows where ROW_NUMBER() ≤ 2. Rounding the result matches dashboard display needs. The explanation cautions about missing dates: if gaps exist, analysts might need a calendar table to fill them before windowing.

Which query extracts every user’s third purchase, breaking timestamp ties by the lower transaction ID?

Apply ROW_NUMBER() OVER (PARTITION BY user_id ORDER BY purchase_ts, id) and filter for row_number = 3. Sorting by (user_id, id) afterwards yields a tidy list. Merchandisers watch this milestone because the third order often signals long-term retention. Highlight that users with fewer than three purchases won’t appear, and note that composite indexing on (user_id, purchase_ts, id) optimizes the window function.

How can you pivot worldwide branch sales so each year is a column?

Start by unifying all yearly branch tables—either with UNION ALL into a common-table expression or by querying a single long-format table if one already exists—so every row holds branch_id, year, total_sales. Apply a conditional aggregation: SUM(CASE WHEN year = 2021 THEN total_sales END) AS sales_2021, repeating for each year you need, and group by branch_id. This returns one row per branch with sales split across columns, letting executives compare multi-year performance at a glance. Including a fallback “other_year” column or dynamically generating the year list future-proofs the report. A composite index on (branch_id, year) keeps scans fast even with millions of rows.

What query finds the median household income for every city?

Use a window function: partition incomes by city, order by income, then assign row numbers from both the top and bottom (ROW_NUMBER() and COUNT(*) OVER). For each city keep rows where the two row numbers meet in the middle; average them when the count is even. This calculation handles any sample size and avoids subqueries that scan the same city repeatedly. Presenting the median rather than the mean gives a better sense of typical earnings when outliers skew the data. Be sure to cast incomes to a numeric type with the right precision to avoid truncation.

How do you compute a 3-day weighted moving average of product sales (0.5, 0.3, 0.2)?

Partition rows by product_id, order by sale_date, and grab the three-day window with LAG(); then calculate 0.5*curr + 0.3*prev1 + 0.2*prev2 only when both previous rows exist. Filtering with WHERE prev1 IS NOT NULL AND prev2 IS NOT NULL ensures you output dates that have two predecessors. This weighted view smooths volatility while still reacting quickly to recent shifts, which is why analysts prefer it for trend dashboards. Indexes on (product_id, sale_date) guarantee sequential access, and adding ROUND(value, 2) readies the figure for stakeholder slide decks.

How can you classify 2023 sales as Standard, Premium, or Promotional and sum them by region?

A CASE statement encodes the hierarchy: July overrides all as Promotional; East overrides the amount cut-off except in July; otherwise the 2 000 threshold splits Premium and Standard. Sum amount and COUNT(*) by region and the derived sale_type to build the report. Management uses these figures to tune pricing and seasonal promotions. Clearly documenting rule priority prevents logic drift, and partitioning by sale date enables parallel scans for year-long tables.

How would you calculate the number of unpurchased seats on every flight?

Joining flights, planes, and flight_purchases demands careful use of LEFT JOIN and COALESCE() to treat “no purchases” as zeros. Advanced candidates point out that a SKU-like key combining flight date, flight number, and seat number prevents double counting, and they’ll propose materialized views or incremental aggregation to keep the query sub-second for operations teams. Explaining how this metric feeds yield-management optimizations demonstrates business impact awareness.

What query reports 2022 total expenses and the company-wide average per department?

Senior analysts must aggregate at two levels in one result: per-department totals and an overall benchmark. A clean answer uses a window function (AVG(total_expense) OVER ()) or a cross-join to a sub-aggregate, and discusses why aligning fiscal vs. calendar years matters. Performance commentary—such as partitioning the fact table on expense_date and compressing low-cardinality dept_id—signals experience with real finance datasets.

Which cities have the lowest-quality search results (all ratings < 3)?

The challenge blends grouping with Boolean tests across result sets: you must confirm that all rows per query meet a condition, not just the average. A typical pattern uses MIN(rating) and checks if it’s >=3, then flips the logic. Interviewers expect discussion of the anti-join alternative and of clustering the table by (query, rating) so that the engine can skip irrelevant blocks—crucial for terabyte-scale search logs.

How many confirmation SMS responses do we receive by carrier and country on 28-Feb-2020?

Real-time marketing teams rely on this metric to detect deliverability issues, so latency and accuracy both matter. You join the latest “confirmation” message per phone number to the confirmations table, group by carrier, country, and count responses. Senior-level answers mention windowing to select “latest” per number, advocate a filtered index on type='confirmation', and discuss why time-zone normalization is vital when messages span regions.

What share of comments on each ad occurs in the feed versus the “moments” surface?

Solving this requires UNIONing two comment tables, tagging the source, grouping by ad_id, and computing percentages. Advanced interviewees justify using COUNT(DISTINCT comment_id) to avoid duplicates, suggest bitmap indexes on ad_id, and highlight the importance of consistent UTC timestamps when ads run in multiple regions. They may even propose pre-aggregating hourly to power real-time advertiser dashboards.

How do you retrieve each employee’s current salary after an ETL bug inserted yearly updates as new rows?

The fix leverages ROW_NUMBER() over (employee_id ORDER BY salary_effective_date DESC) to isolate the latest row. You then join this CTE back to employees for a clean, deduplicated view. Experienced analysts discuss adding a surrogate key plus ON CONFLICT handling to prevent future drift and consider a covering index (employee_id, salary_effective_date DESC) to support both the query and payroll reports.

How would you build a monthly customer KPI report for 2020 showing user count, transactions, and GMV?

The query aggregates different measures from separate tables, aligns them on a generated date spine, and outputs tidy month-level rows suited for dashboards. Senior answers cover why using a calendar table avoids missing-month gaps, how to handle late-arriving transactions via incremental backfills, and ways to index on (order_date) + (user_signup_date) to keep nightly ETL light.

Which users performed ATM withdrawals exactly 10 seconds apart—indicating possible fraud?

You use LAG() over (user_id ORDER BY created_at) to compute time deltas, then confirm that all consecutive gaps equal 10 seconds. Edge-case handling (single-transaction users) and ordering in the final result (ORDER BY user_id) show professionalism. Discussing a composite index (user_id, created_at) and partitioning on created_at to shorten forensic look-backs moves the answer into staff-level territory.

Rank departments with ≥10 employees by percent earning > 100 K, keep top 3.

The query mixes conditional aggregation, filtering on department size, and percentage calculation. Interviewers check numeric precision, use of window functions for ranking, and thoughtful exclusion of small departments—common executive-level reporting nuances.

Find the three lowest-paid employees who have finished ≥2 projects.

The answer requires joining employee, project, and assignment tables, filtering completed projects, and ranking salaries. Interviewers judge your join ordering and ability to eliminate duplicates that inflate project counts.

Fix an ETL bug: retrieve each employee’s latest salary despite duplicate rows.

Seasoned analysts must deploy ROW_NUMBER() OVER (PARTITION BY employee_id ORDER BY salary_date DESC) or a MAX-by-GROUP approach, then discuss auditing steps to prevent future duplication. It tests data-quality instincts alongside SQL chops.

Which users post the same job repeatedly vs. only once?

Counting per-user per-job occurrences, then pivoting into single vs. multiple posters, forces nuanced grouping and CASE aggregation. Senior roles often own marketplace anti-spam metrics like this.

Who are the daily top-3 downloaders, using window RANK?

Large download_fact tables demand efficient partitioning and thoughtful ordering; follow-ups often cover index or cluster key choices. Experienced analysts should anticipate scale-related pitfalls.

How would you integrate payment, behavior, and fraud logs to improve the system?

The prompt is open-ended: discuss data cleansing, schema unification, entity resolution, and choosing the right join keys. Interviewers want narrative structure—ingestion → validation → feature engineering → insight generation—illustrating that you can plan end-to-end analytics projects beyond a single SQL script.

Compute a weighted campaign score using 0.3 × open rate and 0.7 × click rate.

Beyond writing the SQL, strong candidates justify why click-heavy weighting matters, mention confidence intervals on small sends, and flag bias if opens are auto-filtered by email clients. It marries arithmetic SQL with marketing-domain reasoning.

Describe the data-model migration from a document DB to relational tables.

The scenario expects an architectural narrative: identifying entities (users, friendships, interactions), defining PK/FK constraints, planning backfill ETLs, and outlining read-pattern performance. It evaluates system-thinking more than pure query writing.

What insights and distribution metrics would you build on daily conversation counts?

After outlining potential KPIs (median conversations per user, power-law tails, churn predictors), you must deliver a SQL query producing the per-user daily conversation histogram. The question blends exploratory analytics reasoning with concrete querying—perfect for analysts who straddle data discovery and SQL execution

How would you calculate the total salary paid to employees who never finished a single assigned project ?

You must join employees to projects, flag unfinished work where end_dt IS NULL, and identify staff whose entire project set is unfinished. A CTE that counts finished projects per employee (COUNT(end_dt) FILTER (WHERE end_dt IS NOT NULL)) and filters on = 0 is a clean pattern. Summing their salaries shows the true cost of “slackers” and is often used in cost-reduction audits. Good answers also mention excluding employees with no projects and adding an index on (employee_id, end_dt) for speed.

What query returns the running (cumulative) sales total for every product, ordered by product and date?

This task highlights window functions: SUM(price) OVER (PARTITION BY product_id ORDER BY date ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) produces the cumulative field. Grouping isn’t needed—windowing keeps each purchase row intact—so analysts can trend the metric day by day. Discussing how to partition large sales tables (e.g., by month) shows scale awareness. Rounding and casting are minor but necessary finishing touches for reporting.

How many rows would each join type (INNER, LEFT, RIGHT, CROSS) return when you join all ads to the top-3 ads subquery?

You first build a CTE top_ads using ORDER BY popularity LIMIT 3, then create four separate SELECT COUNT(*) blocks labeled by join type. The puzzle forces you to reason about join mechanics—INNER should give 3, LEFT gives N, RIGHT gives 3, and CROSS gives 3 × N—rather than just write syntax. Seasoned answers mention why RIGHT joins aren’t supported in some systems and how nulls affect counts. It doubles as a quick sanity-check of a candidate’s join intuition.

Which user has the highest average number of unique item categories per order?

You’ll need a two-stage aggregate: first count distinct categories inside each order, then average that count per user. Ordering the result and limiting to 1 returns the winning user. The question checks comfort with nested aggregation, COUNT(DISTINCT …), and the difference between order-level and user-level granularity. Mentioning tie-handling and potential use of WITH TIES shows polish.

Which two students scored closest to each other on the SAT, and what was the score gap?

A neat approach self-joins the table on score differences or uses window functions with LAG() after ordering by score. You then pick the minimum absolute difference and, if ties remain, return the alphabetically higher pair. This tests ranking logic, tie-breaking, and string comparison. Explaining why an index on score speeds the inner difference scan gets bonus points.

What percentage of users held “Data Analyst” immediately before “Data Scientist”?

Using ROW_NUMBER() or LEAD() over each user’s job-history ordered by start date lets you compare adjacent titles. Count matches where the earlier row is “Data Analyst” and the next row is “Data Scientist,” divide by total distinct users, and round. The prompt tests your ability to work with ordered event data and compute conditional ratios. Call out pitfalls like overlapping date ranges or simultaneous titles.

On what earliest date did each user listen to their third unique song, and what was that song?

You partition by user_id, rank distinct song_id by first-play date, and filter for rank 3, leaving nulls for users below the threshold. Handling distinctness inside the window (ROW_NUMBER() OVER … PARTITION BY user_id ORDER BY MIN(play_dt)) shows finesse. Interviewers like hearing about edge cases—repeated plays of the same song—and why a surrogate key on (user_id, song_id) accelerates the query.

What fraction of 12/31/2019 active accounts closed on 1/1/2020?

You identify the active cohort on December 31, join to January 1 statuses, count those whose status changed to “closed,” and divide by the cohort total. Rounding to two decimals matches finance reporting standards. Discussing how to index (account_id, ds) for daily status snapshots demonstrates practical performance thinking. It also surfaces the need to guard against duplicate daily rows.

How would you label each user’s attribution as “paid” or “organic” based on prior Facebook or Google visits?

A subquery that checks EXISTS (SELECT 1 FROM visits WHERE user_id=… AND source IN ('facebook','google')) feeds a simple CASE statement. The exercise is tiny but reveals your clarity on boolean logic and set membership. Advanced candidates bring up deduplicating multi-channel visits and the importance of visit timing relative to conversion. It’s foundational for marketing analytics pipelines.

How do you assign session numbers to events when a session is ≤ 60 minutes of inactivity?

Use LAG(event_ts) per user to compute minute gaps, flag starts where gap > 60, then apply a running SUM() of those flags to generate session_id. This classic pattern tests mastery of window functions for stateful labeling. Mentioning timezone normalization and indexing (user_id, event_ts) shows real-world savvy. Edge-case awareness—like back-to-back identical timestamps—also impresses.

Which ongoing projects are forecast to go “over-budget” versus “within budget”?

You prorate each employee’s salary to project duration, sum per project, and compare to the budget. A CASE label outputs the status, making it useful for dashboards. The math forces candidates to convert annual salaries into daily costs and handle half-year examples correctly. Seasoned answers discuss assuming a 365-day divisor and suggest materializing salary snapshots for long projects.

What was the month-over-month revenue change for every month in 2019?

Aggregate revenue by month, then apply LAG(total_rev) to compute the change and ROUND(…,2) for presentation. Candidates must filter to 2019, handle January’s null prior month, and decide between absolute or percentage change. Performance-minded folks note that partitioning the transactions table by date keeps annual scans light.

Which products cost more than their own average transaction total?

A per-product CTE calculates AVG(price*quantity) as avg_total; the outer query joins to products and filters on product_price > avg_total. Rounding both numeric columns to two decimals matches stakeholder expectations. The problem checks understanding of self-referential filters and grouped aggregates. Bringing up indexed materialized views for large SKU catalogs adds senior-level depth.

If you’re preparing offline, you can also use a downloadable SQL practice questions for data analyst interview PDF or worksheet. The best prep resources go beyond copy-paste queries—they explain why each query matters in a business context.

Let these questions guide your prep, but make sure to actually write and run them in a real SQL editor. Practice under realistic constraints is what separates candidates who pass from those who almost do.

Scenario-Based and Advanced SQL Questions
As you move into mid-level and senior data analyst roles, SQL interviews go beyond basic queries. Companies expect you to handle multi-table joins, create optimized CTEs, work with window functions, and debug slow queries in production. These advanced SQL interview questions for data analysts reflect business-critical scenarios where precision, scalability, and business logic all matter.

Many SQL scenario-based interview questions for data analyst roles also test your ability to reason about trade-offs—e.g., filtering before or after joins, dealing with nulls, or ensuring referential integrity. Below are examples that simulate these real-world demands.

How many minutes did each plane spend in the air on every calendar day?

This problem forces you to convert departure and arrival timestamps into minute-level duration, group by both plane_id and flight date, and round down fractional minutes. It rewards analysts who can spot hidden pitfalls—overnight flights that straddle dates, missing data, or daylight-saving jumps—and who suggest placing a composite index on (plane_id, departure_ts) to avoid full table scans during daily ETL. Interviewers also look for discussion of window vs. aggregation trade-offs when the same table powers multiple metrics.

Which SQL pulls the 2nd-longest flight for every city pair?

Normalize routes by sorting the two city names and storing them as city_a, city_b so A-B equals B-A, then compute duration_minutes. Apply ROW_NUMBER() OVER (PARTITION BY city_a, city_b ORDER BY duration_minutes DESC) and filter for rank = 2; if a pair lacks two flights no row returns. Sorting the final output by flight_id meets the spec. Airlines inspect this list to schedule backup aircraft for long hauls, so accuracy matters. A multi-column index on normalized cities plus duration speeds the ranking even in decade-long flight logs.

How would you calculate first-touch attribution for each converting shopper?

Join attribution to user_sessions, filter to rows where conversion = TRUE, and for every user_id pick the earliest session (MIN(session_ts)) then capture its channel with FIRST_VALUE(channel) over an ORDER BY session_ts. This isolates the discovery channel that led to the eventual purchase, informing marketing spend. Mention deduplicating multiple same-timestamp sessions and handling users who clear cookies. Partitioning by user_id and indexing on (user_id, session_ts) make the scan feasible when logs exceed a billion rows.

How do you calculate a three-day rolling average of deposits in a bank-transactions table?

You first filter to positive transaction_values, group them by txn_date, then use AVG(daily_total) OVER (ORDER BY txn_date ROWS BETWEEN 2 PRECEDING AND CURRENT ROW) to smooth volatility in inflows. The explanation should mention why withdrawals are excluded, how to handle holidays that skip dates, and how indexing on txn_date keeps the query scalable for real-time dashboards.

How can you surface the five product pairs most frequently purchased together?

The technique explodes each transaction into product pairs via a self-join (enforcing p1 < p2 to avoid duplicates), counts occurrences, and orders by that count. The query illustrates market-basket analysis at scale—important for recommendation engines—so candidates should comment on deduplicating large intermediate joins (e.g., using sessionization or probabilistic sketches). Deterministic alphabetical ordering (p2) ensures tie-breaking for reproducible BI dashboards.

Calculate a 3-day rolling average of step counts per user.

The query uses AVG() OVER(ORDER BY date ROWS BETWEEN 2 PRECEDING AND CURRENT ROW) while filtering out rows lacking two predecessors. It gauges ability to apply window frames, convert floating output to whole numbers, and think through the first-two-days exclusion. Such rolling metrics surface constantly in health-tech dashboards, making this a practical advanced exercise.

How many right-swipes does each feed-ranking variant average among users with ≥ 10, 50 and 100 total swipes?

You’ll join the swipes log to the variants table, bucket users by total-swipe count (≥10, ≥50, ≥100), and for each cohort compute the mean of is_right_swipe separately for variants A and B. The challenge mixes conditional aggregation with a HAVING filter on swipe thresholds and showcases why integer division of right/total swipes yields a clearer metric than simply averaging booleans. Good answers also explain how excluding users below each threshold avoids small-sample noise and why indexing (user_id, variant) keeps the query performant on billions of swipes. Interviewers listen for discussion of statistical significance and whether further stratification (e.g., by geography) might alter conclusions.

What cumulative distribution (CDF) of comment counts per user can you build, using one-comment buckets?

First, aggregate the comments table to get each user’s total comment count; then group these counts into integer buckets and apply a running SUM() window to produce the cumulative percentage. This pattern teaches how to turn raw frequency data into a CDF that product managers can read at a glance (e.g., “90 % of users leave ≤ 3 comments”). Candidates should mention handling sparse tails, ensuring the bucket list is complete, and ordering the window correctly so cumulative values stay monotonic. Indexing on user_id and pre-materialising daily comment tallies are good scale optimisations to note.

How many “likers’ likers” does each user have on a dating platform?

The task requires a self-join: first find everyone who liked you, then count how many people liked them in turn. Using DISTINCT in the inner set avoids double-counting duplicates, and grouping by the original user_id produces a tidy fan-out metric. The exercise checks fluency with aliasing, multi-layer joins, and reasoning about graph-like relationships in SQL. Interviewers value explanations of edge cases (users with no inbound likes) and performance tips such as indexing (liker_id) to accelerate the second-level lookup.

What annual-cohort retention does an annually-billed SaaS achieve?

In annual_payments each row marks a yearly renewal; cohort users by their first payment_year, then compute the share who renew in year +1 and year +2. You’ll pivot counts into retention percentages and order by cohort year for an executive-friendly table. The query demonstrates date extraction, cohort labelling, and conditional aggregation—all staples of subscription analytics. Strong answers discuss missing renewals (NULL pay dates), churn bias, and how to handle users who upgrade mid-cycle.

How many push notifications does a user receive before converting, segmented across the user base?

Join notification_deliveries to the users table, cap each user’s timeline at their conversion_date, and count notifications sent before that moment (or to today if they never convert). Aggregating these counts into a histogram—e.g., 0-3, 4-7, 8+—reveals whether over-messaging hurts conversions. The prompt tests temporal joins, NULL handling for non-converters, and the ability to craft business-ready distribution outputs. Savvy candidates will discuss window functions to pick “last pre-purchase notification” and the impact of time-zone normalisation.

What is the three-month cohort retention for each subscription plan?

Using subscriptions, derive the signup month, then for months +1, +2, +3 flag whether the user remains active (end_date IS NULL or after the window). A UNION or conditional aggregation can roll these flags into retention rates per (start_month, plan_id, num_month). The problem blends date math, cohort framing, and ratio calculation, mirroring dashboards built for product-led-growth teams. Interviewers look for clear ordering, thoughtful null checks, and commentary on why 3-month retention predicts lifetime value.

Does click-through rate rise with higher human-rated relevance in Facebook search results?

Combine search_results with search_events, group by discrete rating bands, and compute both impressions and clicks to output CTR per rating. A subsequent regression or chi-square test (outside SQL) can confirm significance, but the query itself must deliver clean aggregates: total rows, clicked rows, and CTR rounded to two decimals. The exercise checks ability to join fact tables correctly, avoid double counting when multiple results share a query, and reason about causality versus correlation in product metrics. Mentioning confidence intervals and exposure thresholds shows senior-level insight.

MySQL and Tool-Specific SQL Questions
If you’re preparing for a role that specifically uses MySQL, expect questions on syntax, functions, and performance nuances unique to this platform. Many MySQL data analyst interview questions focus on functions like STR_TO_DATE, DATEDIFF, GROUP_CONCAT, and window functions such as ROW_NUMBER()—all critical for date parsing, ranking, and summarizing large datasets.

What are common MySQL functions used in analytics?
In data analyst interviews focused on MySQL, you’ll often be asked to demonstrate fluency with built-in functions that are essential for performing analytical tasks. These types of MySQL interview questions for data analyst roles are especially common at smaller startups and SaaS companies that rely on MySQL for powering dashboards and reporting pipelines.

You may also be asked to debug queries involving MySQL-specific behaviors, such as how NULL values are sorted or how execution plans are generated. To stand out, go beyond just syntax—be ready to explain how MySQL handles indexing, temporary tables, and query optimization under the hood.

What does GROUP_CONCAT() do in MySQL, and when would you use it?

The GROUP_CONCAT() function in MySQL combines multiple values from grouped rows into a single comma-separated string. It’s especially useful in data analyst tasks when summarizing categories or labels into one field, such as combining all product names per customer. This function isn’t supported natively in all databases, making it a frequent question in MySQL data analyst interview questions. You may also be asked to handle truncation or ordering issues with this function.

How does MySQL handle NULL values when using ORDER BY?

Unlike some databases that treat NULL as the lowest value, MySQL orders NULLs first in ascending order and last in descending order by default. This behavior can affect analytic reports or dashboards when sorting is involved, especially in rankings or priority queues. It’s a subtle point but commonly tested in MySQL interview questions for data analyst candidates who are expected to understand default behaviors and how to override them with IS NULL, IFNULL(), or COALESCE().

Explain the difference between LIMIT in MySQL and TOP in SQL Server.

MySQL uses LIMIT to restrict the number of rows returned, while SQL Server uses TOP. For example, SELECT * FROM users LIMIT 10 is equivalent to SELECT TOP 10 * FROM users in SQL Server. Understanding this difference is crucial for analysts who switch between platforms or work with ETL pipelines spanning multiple systems. It’s also a common question in MySQL-focused interviews, testing adaptability across SQL dialects.

How would you use DATE_FORMAT() to bucket a timestamp column into calendar months while preserving the sort order?

Candidates should explain that DATE_FORMAT(ts,'%Y-%m') converts a timestamp to a year-month string that still sorts chronologically, enabling straightforward GROUP BY or ORDER BY without additional casts. A strong answer mentions edge cases such as time-zone inconsistencies, how %b %Y would break lexical ordering, and why using DATE_TRUNC() equivalents (available only in other SQL dialects) isn’t an option in pre-8.0 MySQL. Interviewers also appreciate discussion of indexing on the raw timestamp versus the formatted string.

When aggregating revenue, why might you choose COALESCE(col,0) over IFNULL(col,0)?

Both functions replace NULL, but COALESCE is ANSI-standard and accepts more than two arguments, making queries portable and expressive. Analysts should note that in MySQL they perform identically on performance, yet COALESCE works inside window frames, nested selects, and casts without surprises. Highlighting subtle precedence rules and null-propagation pitfalls shows deeper fluency.

Explain how GROUP_CONCAT() can be used to create a “top products per user” string, and list two limitations of the function.

A complete answer covers concatenating sorted product names within a GROUP BY user_id, optionally controlling order with ORDER BY price DESC inside the function. Limitations include the default group_concat_max_len (often 1,024 bytes) and the inability to store more than one value per delimiter safely when downstream tools expect atomic columns. Mentioning a fallback to sub-queries or reporting tables demonstrates real-world experience.

Write a query that compares two dates in different time zones using CONVERT_TZ() and TIMESTAMPDIFF(). What could go wrong around daylight-saving changes?

Interviewers want to see CONVERT_TZ(order_ts,'UTC','America/New_York') followed by a TIMESTAMPDIFF(day, signup_ts, …) clause. A strong explanation calls out that DST gaps/overlaps may yield nulls if MySQL’s time-zone tables are stale, and that TIMESTAMPDIFF truncates toward zero. Acknowledging the need to refresh or mount mysql_tzinfo_to_sql shows operational maturity.

How can JSON_EXTRACT() (or >) be leveraged to filter events stored in a JSON column, and why is a virtual column sometimes preferable?

Good answers demonstrate WHERE JSON_EXTRACT(meta,'$.event') = 'click', discuss generated virtual columns for indexing, and note that MySQL cannot index deep keys directly without exposing them. They might also mention cost trade-offs of storing semi-structured data in JSON versus normalizing into relational tables.

Describe a scenario where LAG() combined with IFNULL() solves an analytics problem in MySQL 8.0.

Typical use-case: computing session gaps or day-over-day user metrics. The analyst must show LAG(val) OVER (PARTITION BY user ORDER BY ts) to fetch the prior value, then wrap in IFNULL() to set a default for the first row. Highlighting memory usage and the implications of window size indicates deeper expertise.

Why might SUBSTRING_INDEX() be safer than SUBSTRING() for parsing URLs into domain buckets?

SUBSTRING_INDEX(url,'/',3) grabs everything up to the third slash without relying on hard-coded positions, handling variable lengths robustly. Interviewers value commentary on search-engine tracking parameters, how leading “https://” changes offsets, and why pre-cleaning with LOWER() prevents case-sensitive mismatches.

Show how DENSE_RANK() differs from ROW_NUMBER() when extracting top-selling SKUs per month.

Candidates must articulate that DENSE_RANK preserves shared ranks for ties, whereas ROW_NUMBER forces uniqueness. Illustrating with revenue ties and explaining downstream dashboard impact (e.g., “top 3” may yield more than three rows) proves practical understanding.

What is the purpose of the CAST() function when joining a numeric user ID stored as a string to another table storing it as INT?

Beyond syntax, answers should discuss index usage: casting the smaller side (or using generated columns) maintains performance, whereas casting a bigint column in the WHERE clause negates indexes. Mentioning data-quality initiatives to align types highlights broader thinking.

How does MySQL sort NULL values by default, and what expression forces explicit ordering?

By default, ORDER BY col ASC places NULL first; adding ORDER BY col IS NULL, col pushes them last. A seasoned analyst notes MySQL 8.0’s

How to Prepare for a SQL Interview as a Data Analyst
Cracking the SQL portion of a data analyst interview requires more than just memorizing syntax—you need to solve business problems using structured logic, write efficient queries, and explain your reasoning clearly. Whether you’re prepping for your first job or have years of experience, most data analyst SQL interviews will test your grasp of real-world query writing, data interpretation, and optimization techniques. The good news? There’s a predictable structure to how these interviews are built—and with the right study plan, you can master it.

Study Topics to Master
If you’re preparing for a SQL test for data analyst roles, your first step is to build a strong foundation in the following areas:

Joins (inner, left, self joins) and understanding data relationships
Aggregations with GROUP BY, COUNT(), SUM(), AVG()
Filtering using WHERE, HAVING, and subconditions
Window functions like RANK(), ROW_NUMBER(), LAG() and LEAD()
Subqueries and CTEs, especially for nested logic or transformations
Indexes and performance optimization, especially on large datasets
For advanced roles, you’ll also be tested on query optimization, data modeling concepts, and the ability to break down business KPIs using SQL logic.

Want structure? Start with the SQL Learning Path on Interview Query—it guides you from basic queries to complex window functions and optimization.
Every data analyst knows the feeling: you’re staring at a complex query, laden with layers of subqueries and more JOIN clauses than you can count. It runs slow. It's impossible to debug. And worst of all, you're not entirely sure if the numbers are right.

Often, the problem isn’t your SQL skills; it’s the data model you’re working with.

Data modeling for analytics is the art and science of organizing data for efficient querying, clear reporting, and reliable insights. It’s the silent hero that makes your SQL clean, fast, and trustworthy. While often seen as a data engineer’s domain, understanding how data should be structured is the single most important skill an analyst can cultivate to elevate their work from basic data pulling to strategic business partnership.

This guide will demystify data modeling from an analyst’s perspective, focusing on the most powerful pattern for reporting: the Star Schema.

The “Why” of Data Modeling: Taming the Chaos
Why should you, as an analyst, care about how data is structured?

Cleaner, Faster Queries: Well-modeled data reduces the need for complex joins and subqueries, making your SQL simpler and more performant.
Reliable Insights: A consistent model ensures that metrics are calculated the same way every time, creating a “single source of truth.”
Understandable Data: It provides a clear, logical view of the business, making it easier to find and interpret data.
Empowered Self-Service: Enables business users to query and build reports more easily, reducing your workload.
Scalability: Good models are easier to scale as your data grows.
In essence, a good data model helps you stop drowning in joins and start swimming in insights.

The Golden Rule: The Star Schema
The Star Schema is the go-to data modeling pattern for analytical databases (data warehouses and data marts). It’s designed for speed and simplicity in querying. It consists of:

Fact Tables: These contain the quantitative or measurable data (e.g., sales amounts, number of clicks, event durations). Each row represents a single event or transaction, and they contain foreign keys to dimension tables.
Dimension Tables: These contain descriptive attributes related to the facts (e.g., customer name, product category, date components, employee details). They provide the “who, what, when, where, why” context for your facts.
Think of the fact table as the “star” and the dimension tables as the “points” of the star, all connected to the central fact table.

Building a Star Schema: A Practical SQL Example
Let’s imagine you have a messy transactional database for an e-commerce platform. We’ll use SQL to transform it into a clean star schema for analytics.

Original (Messy) Tables:

orders: order_id, customer_id, order_date, total_amount
order_items: order_item_id, order_id, product_id, quantity, price
products: product_id, product_name, category, brand
customers: customer_id, customer_name, email, city, state, signup_date
Goal: Create a Star Schema for Sales Analysis

We want one sales_fact table, and separate dim_customer, dim_product, dim_date tables.

Step 1: Create Dimension Tables (Descriptive Attributes)
These tables will hold the unique, descriptive attributes.

dim_customer

SQL

CREATE TABLE dim_customer AS
SELECT
    customer_id,
    customer_name,
    email,
    city,
    state,
    signup_date
FROM
    customers
GROUP BY -- Deduplicate if necessary
    customer_id, customer_name, email, city, state, signup_date;
dim_product

SQL

CREATE TABLE dim_product AS
SELECT
    product_id,
    product_name,
    category,
    brand
FROM
    products
GROUP BY -- Deduplicate if necessary
    product_id, product_name, category, brand;
dim_date (This is a common and powerful dimension to build once!)

SQL

CREATE TABLE dim_date AS
SELECT
    DISTINCT order_date AS date_key, -- Use the date itself as the key
    EXTRACT(YEAR FROM order_date) AS year,
    EXTRACT(MONTH FROM order_date) AS month,
    EXTRACT(DAY FROM order_date) AS day,
    EXTRACT(WEEK FROM order_date) AS week_of_year,
    TO_CHAR(order_date, 'Month') AS month_name,
    TO_CHAR(order_date, 'Day') AS day_of_week_name,
    CASE WHEN EXTRACT(DOW FROM order_date) IN (0, 6) THEN 'Weekend' ELSE 'Weekday' END AS is_weekend
FROM
    orders; -- Or any table containing dates you need
Step 2: Create the Fact Table (Measurable Events)
The fact table contains the metrics and foreign keys to your new dimension tables.

SQL

CREATE TABLE sales_fact AS
SELECT
    oi.order_item_id AS sales_event_id, -- A unique ID for each sales event
    o.order_id,
    o.customer_id AS customer_key, -- Foreign Key to dim_customer
    oi.product_id AS product_key,   -- Foreign Key to dim_product
    o.order_date AS date_key,       -- Foreign Key to dim_date
    oi.quantity,
    oi.price,
    (oi.quantity * oi.price) AS line_item_total_revenue
FROM
    order_items oi
JOIN
    orders o ON oi.order_id = o.order_id;
Now, instead of joining orders, order_items, products, and customers every time, you join sales_fact to dim_customer, dim_product, and dim_date directly.

Press enter or click to view image in full size

Conclusion: Empowering Your Analytics with Structure
Understanding and advocating for a well-designed data model is a superpower for an analyst. It means:

Faster Iteration: Quickly build new reports and analyses without wrestling with data structure.
Easier Collaboration: Other analysts and business users can understand your data models at a glance.
Trust in Numbers: Confidence that your aggregations and filters are applied correctly.
Strategic Impact: You can spend more time interpreting data and less time cleaning or debugging it.
Stop seeing data modeling as someone else’s job. Embrace it, and watch your analytical capabilities (and career) flourish.

What’s the biggest challenge you face when working with poorly modeled data, and how do you work around it? Share your experiences in the comments!
For Beginners: Building a Strong SQL Foundation
If you’re new in SQL or transitioning into a data-focused role, most interviewers will likely assess your foundational understanding and practical exposure to relational databases.

Background and Experience : Be ready to talk about how long you've worked with SQL, what database systems you've used (e.g. MySQL, PostgreSQL, SQL Server etc.) and your comfort level with writing and optimizing queries.
Core Concepts : Expect questions around what SQL is, its real-world applications, common data types and operators and basic syntax.
SQL Components : Interviewers may quiz you on different types of SQL statements (like SELECT, INSERT, UPDATE, DELETE) data definition commands (CREATE, ALTER, DROP) and basic types of queries.
Top SQL Interview Questions and Answers for Beginner

For Intermediate-Level Candidates
Before moving on Intermediate level it's recommended to read our Top Beginner level SQL Interview questions and answers

Once you’ve mastered the basics, the next step is showcasing your ability to solve real-world database problems efficiently and accurately. Intermediate-level interviews often focus on the following areas

SQL Functions : You should be comfortable explaining and using aggregate functions (SUM, COUNT, AVG etc.), scalar functions (UPPER(), LOWER(), LEN(), GETDATE()) and even custom user-defined functions9When and how to create and use them).
Database Design Principles : Expect questions that test your ability to optimize database structures, including: Normalization vs Denormalization along with purpose, advantages and trade-offs, Use cases for DELETE vs TRUNCATE vs DROP etc with when to use what and why?. Understanding of relational integrity and constraints
Advanced SQL Commands : Interviewers often explore your understanding of relationships and constraints through topics like: Joins (INNER, LEFT, RIGHT, FULL), Keys (Primary vs Foreign), Indexing strategies and their effect on query performance, Relationships between tables etc
Complex Query Writing : You might be asked to solve complex problems using subqueries (both nested and correlated), GROUP BY clauses or even retrieve the Nth highest record using various approaches like LIMIT, ROW_NUMBER() or common table expressions etc.
✅ Tip : Practice writing queries by hands-on exercise and explaining them out loud. Many interviews will include whiteboard rounds or live coding tasks where you’ll need to walk through your logic step by step and optimize queries while discussing performance implications.

General SQL Interview Questions for Intermediate to Advanced Users
As you progress in your SQL journey from beginner to Intermediate level, interviewers expect more than just knowledge of basic syntax. They’re looking for hands-on experience, depth of understanding and the ability to tackle real-world database challenges. These general interview questions help assess your proficiency, problem-solving skills and ability to work with large-scale data systems.

Below are some commonly asked general SQL interview questions for intermediate to advanced professionals, especially useful if you’re preparing for roles like Database Developer, Data Engineer, SQL Analyst or DBA.

1. What are the largest databases or datasets you’ve worked with?
Be ready to describe the scale of data (e.g., millions of records, terabytes of storage) and your role in managing it. Interviewers want to know how you handled challenges related to performance, indexing or storage optimization at scale.

Example
I worked with a PostgreSQL database storing over 20 million transaction records per year. I optimized partitioning and used materialized views for performance.

2. How do you approach query optimization?
Explain your methodology for diagnosing and fixing slow queries. Mention tools like EXPLAIN PLAN, ANALYZE or SQL Server Profiler and strategies like using indexes, avoiding unnecessary joins, filtering early or optimizing subqueries and CTEs.

Example
I use EXPLAIN to analyze the query plan, then check for missing indexes, full table scans and unnecessary sorts.

In aspect of Query Optimization can read blog SQL Performance Tuning

3. Can you describe your experience with indexing strategies?
Discuss types of indexes you’ve used like clustered, non-clustered, composite, covering or filtered indexes and when each is appropriate. Mention index maintenance tasks like rebuilding/reorganizing fragmented indexes.

Example
For read-heavy reporting queries, I implemented covering indexes on the sales table, reducing query time by 60%.

4. How do you ensure data integrity in a multi-user environment?
Talk about transaction isolation levels, locking mechanisms, constraints and handling concurrent updates. Be ready to discuss deadlocks, race conditions and consistency strategies like ACID compliance.

Example
To prevent dirty reads in critical financial systems, I use SERIALIZABLE isolation and optimistic concurrency controls

5. Have you worked on database normalization and denormalization?
Describe situations where you normalized data to eliminate redundancy or denormalized to improve performance. Include the impact on storage, query speed and complexity.

Example
For OLTP systems, I use 3NF. But for reporting dashboards, I denormalize heavily for performance.

6. How do you handle schema changes in production databases?
Explain your process for applying schema migrations with zero downtime, including version control, staging environments, rollback strategies and using tools like Liquibase, Flyway or custom scripts.

Example
I use Flyway to track schema versions, apply changes incrementally and test extensively in staging before production rollout.

7. Describe your experience with views, materialized views or CTEs.
Talk about your use cases for each like views for abstraction, CTEs for readability or recursion and materialized views for performance improvements.

Example
I used a materialized view to pre-aggregate daily user metrics, improving dashboard load times from 10s to under 1s.

8. Have you worked with partitioned tables or sharding?
Mention experience with table partitioning (range, list, hash) for managing large tables, and sharding strategies for horizontal scaling in distributed systems.

Example
In a high-traffic e-commerce app, I used range partitioning on the order date to archive older data and improve lookup speed.

9. What’s your approach to managing database backups and recovery?
Discuss backup strategies like full, incremental, point-in-time recovery and how you’ve tested or implemented disaster recovery plans (e.g., replication, failover clusters).

Example
For PostgreSQL, I set up daily base backups with WAL archiving and tested PITR using pgBackRest.

10. How do you monitor and troubleshoot database performance?
Explain the tools and techniques you use to detect issues like slow queries, blocking, deadlocks or CPU spikes. Mention tools like Prometheus, SQL Server DMVs, pg_stat_statements or performance schema in MySQL.

Example
I regularly monitor slow query logs and use pg_stat_statements to identify inefficient queries and optimize them.

✅ Tip : Use specific scenarios and metrics when answering these questions. Instead of saying “I optimized a slow query,” say “I reduced query execution time from 8s to 0.5s by indexing two columns and rewriting a subquery with a CTE.”

Technical SQL Interview Questions for Beginners
Let’s move on to technical questions to check your deeper understanding of SQL concepts, query optimization and database structures.

Tip : While answering try to give clear and practical answers. Focus on what works well and why. Don’t make things too complicated, just explain things in a simple way with examples if possible. Show that you understand how to use SQL effectively in real situations.

11. What’s the difference between WHERE and HAVING clause?
The WHERE clause filters individual rows before any grouping or aggregation takes place and The HAVING clause filters grouped/aggregated results after GROUP BY is applied.

Example : Real-World Scenario
Imagine you're managing an HR dashboard. You want to see only departments with more than 10 active employees

SELECT department, COUNT(*) 
FROM employees 
WHERE status = 'active' 
GROUP BY department 
HAVING COUNT(*) > 10;
Explanation
Use WHERE to ignore inactive or terminated employees. WHERE status = 'active' ensures only active employees are considered.
Use GROUP BY department groups active employees by department.
Use HAVING to find departments that are large enough. HAVING COUNT(*) > 10 only includes departments with more than 10 active employees.
12. What is the difference between ROW_NUMBER(), RANK() and DENSE_RANK() in SQL?
These three functions are window (analytic) functions used to assign a ranking or position to rows within a result set, based on a specified ORDER BY clause. They differ in how they handle ties (rows with the same value).

ROW_NUMBER() gives a unique row number without ties.
RANK() gives the same rank for ties but skips ranks.
DENSE_RANK() gives the same rank for ties but doesn’t skip ranks.
Example: Assume the leaderboard table
name	score
Alice	100
Bob	95
Charlie	95
David	90
SELECT name, score,
  ROW_NUMBER() OVER (ORDER BY score DESC) AS rn,
  RANK() OVER (ORDER BY score DESC) AS rnk,
  DENSE_RANK() OVER (ORDER BY score DESC) AS drnk
FROM leaderboard;
Output
name	score	rn	rnk	drnk
Alice	100	1	1	1
Bob	95	2	2	2
Charlie	95	3	2	2
David	90	4	4	3
13. How do you get the 2nd or Nth highest salary in SQL?
You can find the 2nd or Nth highest salary using window functions (RANK(), DENSE_RANK(), ROW_NUMBER()) or using subqueries. Each method has its own use case depending on whether ties (duplicate salaries) should be considered.

Method 1: Using DENSE_RANK() (Considers ties)
SELECT name, salary
FROM (
  SELECT name, salary, 
         DENSE_RANK() OVER (ORDER BY salary DESC) AS rnk
  FROM employees
) ranked
WHERE rnk = 2;
If two people have the same highest salary, then they will both be ranked 1 and the next will be ranked 2.
So, this returns all employees who share the 2nd highest salary that means if second highest salary is x then this return all employees whose salary is x.
Method 2: Using ROW_NUMBER() (No ties considered)
SELECT name, salary
FROM (
  SELECT name, salary, 
         ROW_NUMBER() OVER (ORDER BY salary DESC) AS rn
  FROM employees
) ranked
WHERE rn = 2;
This returns exactly one row , the second row after sorting salaries in descending order.
Use this when you want strict ranking without duplicates.
Method 3: Using a Subquery (Simple, only salary value)
SELECT MAX(salary)
FROM employees
WHERE salary < (
  SELECT MAX(salary)
  FROM employees
);
This gives you the 2nd highest salary value, not the employee(s).
To get the employee(s) with that salary
SELECT name, salary
FROM employees
WHERE salary = (
  SELECT MAX(salary)
  FROM employees
  WHERE salary < (
    SELECT MAX(salary)
    FROM employees
  )
);
14 . How can I display the top 3 earners in each department in an HR dashboard?
Use the RANK() or DENSE_RANK() window function with PARTITION BY department and order by salary in descending order.

SELECT employee_id, name, department, salary,
       RANK() OVER (PARTITION BY department ORDER BY salary DESC) AS salary_rank
FROM employees
WHERE status = 'active'
Then, filter using a subquery or CTE

WITH ranked_employees AS (
  SELECT employee_id, name, department, salary,
         RANK() OVER (PARTITION BY department ORDER BY salary DESC) AS salary_rank
  FROM employees
)
SELECT * FROM ranked_employees
WHERE salary_rank <= 3;
15. What if I want to show only one unique runner-up (no ties) per leaderboard in a contest?
If you want to strictly show only the 2nd place without any ties, use ROW_NUMBER() because it assigns a unique sequence to each row, even when there are duplicate scores.

WITH leaderboard AS (
  SELECT employee_id, name, department, salary,
         ROW_NUMBER() OVER (PARTITION BY department ORDER BY salary DESC) AS row_num
  FROM employees
)
SELECT * FROM leaderboard
WHERE row_num = 2;
ROW_NUMBER() assigns a unique ranking within each department, ordered by descending salary.
The WHERE row_num = 2 filters to show only the 2nd highest-paid employee in each department with no ties allowed.
16. How can you dynamically fetch the Nth highest salary in SQL (e.g., 4th, 5th)?
Use a Common Table Expression (CTE) or subquery with DENSE_RANK() and filter where the rank equals N.

In SQL Server (T-SQL)
DECLARE @N INT = 4;

WITH RankedSalaries AS (
  SELECT employee_id, name, salary,
         DENSE_RANK() OVER (ORDER BY salary DESC) AS rnk
  FROM employees
)
SELECT *
FROM RankedSalaries
WHERE rnk = @N;
In MySQL (v8+ with window functions)
SET @N = 4;

WITH RankedSalaries AS (
  SELECT employee_id, name, salary,
         DENSE_RANK() OVER (ORDER BY salary DESC) AS rnk
  FROM employees
)
SELECT *
FROM RankedSalaries
WHERE rnk = @N;
In PostgreSQL
-- Use a CTE with parameterized query
PREPARE get_nth_salary(INT) AS
WITH Ranked AS (
  SELECT employee_id, name, salary,
         DENSE_RANK() OVER (ORDER BY salary DESC) AS rnk
  FROM employees
)
SELECT * FROM Ranked WHERE rnk = $1;
EXECUTE get_nth_salary(4);
17. How can you fetch the Nth highest salary using LIMIT and OFFSET in MySQL or PostgreSQL?
You can use DISTINCT, ORDER BY, and LIMIT ... OFFSET to retrieve the Nth highest unique salary.

SELECT DISTINCT salary
FROM employees
ORDER BY salary DESC
LIMIT 1 OFFSET 1;  -- 2nd highest salary
How OFFSET Works

OFFSET Value	Result
OFFSET 0	Highest salary
OFFSET 1	2nd highest salary
OFFSET 2	3rd highest salary
...	Nth highest = OFFSET N-1
18. What is the different between Primary Key and Unique Key?
Both Primary Key and Unique Key are used to enforce uniqueness in a table, but they differ in how they handle NULLs, indexing behavior and their intended purpose.

A Primary Key is a column (or combination of columns) that uniquely identifies each row in a table and does not allow NULL values. A Unique Key also enforces uniqueness across rows but allows one NULL value and is typically used for enforcing alternate or secondary unique constraints.

Key differences between Primary Key and Unique Key

Feature	Primary Key	Unique Key
Uniqueness	Must be unique	Must be unique
NULL Values Allowed	Not allowed	Allowed (only one NULL per unique key in most DBMS)
Default Index	Automatically creates a clustered index (in many DBMSs like SQL Server)	Automatically creates a non-clustered index
Number per Table	Only one Primary Key per table	Can have multiple Unique Keys
Purpose	Main identifier for a row	Enforces uniqueness on other columns
Constraint Type	Primary Key constraint	Unique constraint
Composite Key Support	Yes (can be made of multiple columns)	Yes (can be made of multiple columns)
Enforces Entity Integrity	Yes	No
Syntax	PRIMARY KEY	UNIQUE
19. How do you create a Primary Key and Unique Keys in SQL?
You can define a Primary Key and one or more Unique Keys in a CREATE TABLE statement using the PRIMARY KEY and UNIQUE constraints.

CREATE TABLE Employees (
  emp_id INT PRIMARY KEY,          -- Primary Key: unique + not null
  email VARCHAR(100) UNIQUE,       -- Unique Key: unique, allows one NULL
  phone VARCHAR(15) UNIQUE         -- Unique Key: unique, allows one NULL
);
Read in depth Primary Key And Unique key

20. Can a table have multiple Primary Keys?
No, A table can have only one Primary Key constraint, but that Primary Key can be composite, i.e., made up of multiple columns.

Example
CREATE TABLE ProjectAssignments (
  employee_id INT,
  project_id INT,
  PRIMARY KEY (employee_id, project_id)
);
21. What kind of indexes are automatically created by Primary Key and Unique Key?
Primary Key
Automatically creates a unique clustered index in SQL Server, unless a clustered index already exists, in which case it creates a non-clustered unique index.
In PostgreSQL and MySQL (InnoDB), it creates a unique B-tree index.
In InnoDB, the primary key becomes the clustered index.
Unique Key
Automatically creates a unique non-clustered index in SQL Server.
In PostgreSQL and MySQL, it creates a unique B-tree index (non-clustered by behavior).
22. Is a UNIQUE constraint equivalent to using a DISTINCT clause?
No, UNIQUE enforces physical data integrity, while DISTINCT is a query-time filter on the result set.

UNIQUE : It prevents duplicates from being inserted.
DISTINCT : It removes duplicates from results.
23. Can a table have a UNIQUE constraint and a PRIMARY KEY on overlapping columns?
Yes, technically it's allowed, but it's usually redundant and inefficient.

CREATE TABLE Sample (
  id INT PRIMARY KEY,
  name VARCHAR(50),
  UNIQUE (id, name)  -- Redundant
);
id is already unique because it's the PRIMARY KEY.
The UNIQUE(id, name) constraint enforces uniqueness on the combination, but since id is already unique on its own, the pair (id, name) must also be unique. It's making the constraint redundant.
Why is it inefficient?
It creates an extra unique index that consumes storage and slows down inserts/updates.
It adds no additional integrity benefit unless you're enforcing a more complex uniqueness condition (e.g., composite uniqueness independent of the primary key).
When is it not redundant?
If you have a composite primary key or your UNIQUE constraint adds additional columns not already covered by the PK.

CREATE TABLE Orders (
  order_id INT,
  product_id INT,
  PRIMARY KEY (order_id, product_id),
  UNIQUE (order_id)  -- Not redundant
);
In this case, the UNIQUE constraint adds new business logic.

24. When is a UNIQUE constraint not redundant in a table that already has a PRIMARY KEY?
A UNIQUE constraint is not redundant when it enforces additional rules that are not covered by the primary key. This often happens when you need another column or set of columns to be unique, even though it's not the primary identifier of the row. So, a UNIQUE constraint is not redundant if

It enforces uniqueness on columns not already covered by the PRIMARY KEY.
Example

CREATE TABLE Users (
  UserID INT PRIMARY KEY,
  Username VARCHAR(50) UNIQUE
);
It enforces independent business rules.
Example

CREATE TABLE Products (
  ProductID INT PRIMARY KEY,
  SKU VARCHAR(100),
  CategoryID INT,
  UNIQUE (SKU)
);
Here SKU (Stock Keeping Unit) must be unique across all products — a separate business rule.

It uses different column combinations than the PK.
Example

CREATE TABLE Enrollments (
  StudentID INT,
  CourseID INT,
  EnrollmentDate DATE,
  PRIMARY KEY (StudentID, CourseID),
  UNIQUE (CourseID, EnrollmentDate)
);
These are different combinations of columns, so the UNIQUE constraint is not redundant.

25. Does enforcing UNIQUE or PRIMARY KEY constraints affect performance?
Yes, enforcing these constraints does impact performance, both positively and negatively

Performance Overhead
INSERT/UPDATE overhead : The database performs an index lookup to ensure no duplicate values exist. This lookup adds latency, especially on large tables or frequently updated keys.
Extra index maintenance : For every DML operation (INSERT, UPDATE, DELETE) associated unique indexes must be updated, adding to I/O and CPU cost.
Performance Benefits
Data integrity : Guarantees no duplicate values — a hard guarantee at the database level.
Faster SELECTs : These constraints automatically create unique indexes, which speed up queries on those columns (especially JOINs, WHERE and lookups).
Note : In write-heavy systems, it's important to balance data integrity vs performance. You can sometimes enforce uniqueness at the application level, but for critical business logic, database-level constraints are essential.

26. What is a CTE in SQL?
A CTE (Common Table Expression) is a temporary, named result set that exists only during the execution of a single SQL statement. It's defined using the WITH clause and can be referenced like a regular table or view within SELECT, INSERT, UPDATE or DELETE statements.

Example :

WITH cte_name AS (
  SELECT column1, column2
  FROM table_name
  WHERE condition
)
SELECT *
FROM cte_name
WHERE column2 > 100;
27. How is a CTE different from a Subquery?
Feature	CTE (Common Table Expression)	Subquery
Readability	✅ More readable for complex and multi-step logic	❌ Becomes hard to read when deeply nested
Reusability	✅ Can be referenced multiple times in the same query	❌ Must repeat logic if reused
Recursive Queries	✅ Supports recursion (especially in hierarchical data)	❌ Does not support recursion
Named Result	✅ Yes, using WITH cte_name AS (...)	❌ No, it's anonymous
Scope	✅ Available to the outer/main query and nested CTEs	❌ Limited to the immediate query part
Materialization (RDBMS-dependent)	May or may not be materialized — depends on optimizer	Usually optimized inline (not materialized)
Example Use Case
CTE Example
WITH HighEarners AS (
  SELECT * FROM Employees WHERE salary > 100000
)
SELECT name FROM HighEarners WHERE department = 'Engineering';
Subquery Equivalent
SELECT name
FROM (
  SELECT * FROM Employees WHERE salary > 100000
) AS HighEarners
WHERE department = 'Engineering';
✅ CTE is cleaner and allows for step-wise breakdown, especially when reused or made recursive.

28. When should I use a CTE over a subquery?
Most of the use cases of CTE (Common Table Expression) over subquery are

Breaking Down Complex Logic
CTEs allow you to structure large queries step by step, similar to writing modular code.
Great for multi-join, aggregated or multi-layered queries.
WITH Filtered AS (...), Aggregated AS (...) SELECT ... FROM Aggregated;
Reusing a Result Set
You can reference the same CTE multiple times in the main query without repeating logic.
Subqueries must be duplicated, making them harder to maintain.
Recursive Queries
CTEs support recursion, which is essential for

Tree structures
Hierarchical data
Graph traversals
Bill of materials (BOM)
WITH RECURSIVE OrgChart AS (...)
Subqueries can not do recursion.

Improving Readability &amp; Maintainability
CTEs are named, making the logic self-explanatory and the query easier to debug.
Especially helpful in team environments and code reviews.
29. Are Common Table Expressions (CTEs) better for performance compared to subqueries or temporary tables?
CTEs are great for readability, modularity and recursion, but they do not always offer performance improvements. In fact, depending on the database engine and how you use them, CTEs can sometimes be slower than subqueries or temp tables.

Technique	Readability	Performance	Materialization	Use Case
CTE (WITH clause)	✅ High	⚠️ Varies	Often materialized (engine-specific)	Recursive queries, modular logic
Subquery (inline)	🟡 Medium	✅ Can be better	Often optimized/folded by the engine	One-off usage
Temp Table	❌ Verbose	✅ Best for reuse/complex joins	Materialized and indexed	Multiple reuse or large datasets
Tip : CTEs are not inherently faster. Use them for clarity and recursive logic, but always check the execution plan. For performance-critical queries, compare with subqueries or temp tables and benchmark.

30. How do you detect and optimize slow SQL queries?
You detect slow SQL queries using query profiling tools, execution plans and performance metrics. Optimization involves analyzing the query execution plan, indexing strategy, table statistics, joins, filtering logic and server resource usage. You can follow below steps for Optimization Process

Analyze Execution Plan : Use tools like EXPLAIN, EXPLAIN ANALYZE or the execution plan viewer.
Check for Missing or Inefficient Indexes
Examine Joins and Filtering Logic : Ensure appropriate join types and push filters as early as possible in the plan.
Rewrite Inefficient Queries
Measure Query Stats : Look at runtime statistics, I/O, buffer cache hits and row estimates vs actuals.
If you want to dive deeper, explore SQL Performance Tuning

31. What is database normalization?
Database normalization is the process of organizing data in a relational database to minimize redundancy and ensure data integrity by dividing large, complex tables into smaller, well-structured tables. Each resulting table represents a single concept or entity and relationships between tables are maintained using foreign keys.

32. What are the differences Between DBMS and RDBMS
Both DBMS (Database Management System) and RDBMS (Relational Database Management System) are systems designed to manage data efficiently, but they differ significantly in terms of structure, functionality and use cases.

Feature	DBMS	RDBMS
Data Storage	Stores data as files, documents or tables	Stores data strictly in relational tables
Data Relationships	No relationships between data entities	Supports relationships via primary/foreign keys
Normalization	Not supported	Fully supports normalization to reduce redundancy
Data Integrity	Limited	Enforces integrity constraints (PK, FK, UNIQUE)
Multi-user Access	Limited or not efficient	Supports multiple concurrent users with consistency
ACID Transactions	Not guaranteed	Fully ACID-compliant for reliable transactions
Security & Access Control	Basic or minimal	Advanced user roles and permission handling
Examples	Microsoft Access, File System, XML DB	MySQL, PostgreSQL, Oracle, SQL Server
Scalability	Suitable for small apps or single users	Suitable for enterprise apps and large systems
Note : All RDBMS are DBMS, but ❌ not all DBMS are RDBMS.

RDBMS follows a strict relational model, while DBMS can be any system that stores and retrieves data.

33. Difference Between INNER JOIN and OUTER JOINs
Joins are used to combine rows from two or more tables based on a related column. Here's how different join types behave:

Join Type	Description
INNER JOIN	Returns only the rows with matching values in both tables
LEFT JOIN (Left Outer Join)	Returns all rows from the left table and matched rows from the right table (NULL if no match)
RIGHT JOIN (Right Outer Join)	Returns all rows from the right table and matched rows from the left table (NULL if no match)
FULL OUTER JOIN	Returns all rows from both tables. If there’s no match, fills with NULLs
Tip : Use INNER JOIN when you need only common data, and use OUTER JOINs when you want to retain unmatched records too.

34. Can you explain ACID properties in databases?
ACID is a set of four key properties that ensure reliable and consistent transactions in a database system.

Atomicity : A transaction is all-or-nothing. If one part fails, the entire transaction is rolled back. Partial changes are never saved.
Consistency : Ensures that a transaction brings the database from one valid state to another. All rules (constraints, triggers, FK) must be satisfied.
Isolation : Transactions are executed independently. Multiple transactions can run at the same time without interfering with each other.
Durability : Once a transaction is committed, changes are permanent even in case of power loss or crash.
35. When would you use a Stored Procedure vs a Trigger in SQL?
Both stored procedures and triggers are used to encapsulate logic in SQL, but they serve very different purposes

Use a Stored Procedure When
You need modular, reusable logic (e.g., sp_generate_invoice, sp_apply_discount).
You want to encapsulate complex business processes.
You need parameterized execution or conditional logic.
The process is called by an application or scheduler.
To be in control of when and how it's executed.
Example
CREATE PROCEDURE sp_update_salary (@emp_id INT, @bonus DECIMAL)
AS
BEGIN
  UPDATE employees
  SET salary = salary + @bonus
  WHERE emp_id = @emp_id;
END;
Use a Trigger When
You want automatic action on data changes (like INSERT, UPDATE or DELETE).
Enforcing audit logs
Replicate data changes to another table/log.
Enforcing hidden business rules at the database level.
Example
CREATE TRIGGER trg_audit_employees
AFTER UPDATE ON employees
FOR EACH ROW
BEGIN
  INSERT INTO audit_log(emp_id, old_salary, new_salary, changed_on)
  VALUES (OLD.emp_id, OLD.salary, NEW.salary, CURRENT_TIMESTAMP);
END;
36. What are constraints in SQL and why use them?
Constraints are rules applied to table columns to make sure the data is valid, accurate and consistent. They help prevent bad or incorrect data from being stored in the database.

PRIMARY KEY: Uniquely identifies each row
FOREIGN KEY: Maintains referential integrity (Ensures values match those in another table).
NOT NULL: Prevents a column from having NULL values.
UNIQUE: Ensures all values in a column are different i.e. Prevents duplicate values.
CHECK: Validates that values meet a specific condition
DEFAULT: Sets a default value if none is provided
Example
CREATE TABLE users (
  id INT PRIMARY KEY,
  email VARCHAR(100) UNIQUE NOT NULL
);
37. What are Indexes in SQL?
An index in SQL is a performance-boosting data structure that helps the database quickly find rows without scanning the entire table.

Think of it like a book index, instead of reading every page to find a topic, you look it up in the index and jump straight to the right page.

38. Why to use Indexes?
Indexes make data retrieval much faster and more efficient. Without an index, the database has to perform a full table scan, checking every row one by one to find a match. which can be slow, especially on large tables. With an index, the database can quickly jump to the rows it needs, improving the performance of queries like SELECT, JOIN, WHERE and ORDER BY.

In short : Indexes reduce the amount of work the database has to do, helping your queries run faster and more efficiently, especially on large datasets.

39. What are key Benefits of Using Indexes
Faster Data Retrieval : Speeds up SELECT queries, especially on large tables.
Better Use of Joins : Speeds up joins by indexing foreign and primary keys.
Improved Application Performance : Enhances read operations in applications, reducing response time.
Enforcing Uniqueness : Unique indexes prevent duplicate entries in a column (like email).
Efficient Sorting and Filtering : Helps in ORDER BY, GROUP BY and WHERE clause queries.
40. What is Trade-off using Indexs?
While indexes can significantly speed up read operations, they come with certain downsides that you should be aware of

Slower Write Operations : INSERT, UPDATE and DELETE take longer because indexes need to be updated too.
Increased Storage : indexes take up extra disk space, especially if there are many or large indexes.
Maintenance Overhead : More indexes mean more work to manage, monitor and optimize over time.
Diminishing Returns : Adding too many indexes can actually hurt performance by confusing the query planner or slowing down queries.
Use indexes only where needed - only on columns frequently used in WHERE, JOIN, ORDER BY or GROUP BY clauses. Avoid indexing every column, as that can slow down write performance and waste storage.

41. How Do Indexes Work?
When you create an index on a column (or multiple columns), the database builds a separate lookup structure (usually a B-tree) that keeps the data in sorted order.

So, instead of going through the entire table row by row, the database uses this structure to jump directly to the relevant data, just like finding a word using a dictionary.

42. How to Create Indexes in SQL?
You can create indexes either while creating the table or after the table is created.

While Creating a Table
CREATE TABLE Customers (
    CustomerId INT PRIMARY KEY,
    FirstName VARCHAR(50),
    LastName VARCHAR(50),
    Email VARCHAR(100),
    INDEX idx_lastname (LastName)
);
After Table Creation
CREATE INDEX idx_email
ON Customers (Email);
43. What all different types of indexes in SQL
Index Type	Key Features	Best Use Case
Primary Index	- Unique
- No nulls
- Only one per table	Used as the main row identifier (e.g., user_id)
Unique Index	- Prevents duplicate values
- Allows nulls	Enforce uniqueness on fields like email, phone, username
Composite Index	- Combines multiple columns
- Index works best when queries use leftmost column(s)	Queries filtering by multiple fields (e.g., first_name, last_name)
Clustered Index	- Physically sorts and stores data rows
- Only one per table	Efficient for range or ordered queries (e.g., by created_at)
Non-Clustered Index	- Stores pointers to data
- Doesn’t change physical row order	Fast lookup queries without affecting storage order
Full-Text Index	- Specialized for large text
- Supports MATCH/AGAINST in MySQL/PostgreSQL	Searching in articles, blog posts, product descriptions
Spatial Index	- For spatial/geographic data
- Supports spatial relationships (e.g., distance, contains)	Location-based apps, GIS systems
Bitmap Index	- Uses bitmaps for values
- Ideal for low-cardinality columns (few unique values)	Data warehousing, analytics on columns like gender, status
JSON/XML Index	- Indexes specific keys/paths inside JSON or XML
- Improves structured queries on unstructured data	APIs, modern web apps storing structured documents
44. What is Index Selectivity and Why Does It Matter?
Index Selectivity measures how uniquely a column’s values identify rows in a table.

Selectivity = (Number of distinct values) / (Total number of rows)
High selectivity (e.g. email, user_id): Index is very effective.
Low selectivity (e.g. gender, is_active): Index may be ignored by the optimizer
Why it matters
The query planner uses indexes only when they help filter down the data significantly. Indexes on low-selectivity columns can even slow things down due to overhead.

45. What is a covering index?
A covering index is an index that contains all columns needed by a query, so the DBMS doesn’t need to access the base table.

Example
SELECT FirstName, LastName FROM Customers WHERE Email = 'a@b.com';
if you create

CREATE INDEX idx_email_name ON Customers (Email, FirstName, LastName);
This index covers the query (WHERE + SELECT columns), resulting in better performance.

46. What is the difference between Index Scan and Index Seek in SQL?
Feature	Index Seek	Index Scan
Definition	Efficient search that goes directly to the matching entry in the index.	Reads the entire index from start to finish.
Performance	Fast and selective, minimal I/O.	Slower, especially on large tables.
Usage	When query filters match indexed columns precisely with high selectivity.	When filter is missing, too broad or column has low selectivity.
Data Access	Uses B-tree traversal to locate relevant rows.	Scans all index rows and checks them one by one.
Predicate	SARGable (Search ARGument Able) : filter can use index efficiently.	Often non-SARGable (e.g., functions, LIKE '%x', type mismatch).
Tip : Aim for Index Seeks by writing SARGable queries and creating targeted indexes.

47. What set operators do you know in SQL?
SQL provides the following set operators to combine the results of two or more queries

UNION
Returns distinct rows from the result of two queries (duplicates are removed).

✅ Use when you want to merge results without duplicates.

UNION ALL
Returns all rows, including duplicates, from both queries.

✅ Faster than UNION, as it skips the distinct sort operation.

INTERSECT
Returns only the rows that are common to both queries.

✅ Useful for identifying overlaps between datasets.

EXCEPT (called MINUS in Oracle)
Returns rows that are in the first query but not in the second.

✅ Used to find non-overlapping differences between datasets.

Note : All participating queries must return the same number of columns, with compatible data types and column order.

48. What operator is used in SQL for pattern matching?
SQL uses the LIKE operator for pattern matching, typically in combination with wildcard characters:

% (percent) → Matches zero or more characters Example: 'A%' matches 'A', 'Alex', 'Amazon'
_ (underscore) → Matches exactly one character Example: 'A_' matches 'AB', 'Ax', but not 'Alex'
49. In which order does the SQL interpreter logically execute the sections of a SELECT query?
The system executes logically :

→ FROM
→ JOIN / ON
→ WHERE
→ GROUP BY
→ HAVING
→ SELECT
→ DISTINCT
→ ORDER BY
→ LIMIT (or OFFSET / TOP)
50. Is this the correct order of appearance for the common clauses in a SELECT statement?
 → SELECT 
 → FROM
 → JOIN
 → ON
 → WHERE
 → GROUP BY
 → HAVING
 → ORDER BY
 → LIMIT
51. What is a View in SQL?
A View is a virtual table based on a SQL query that retrieves data from one or more real tables (or even other views). It doesn’t store data itself, but dynamically presents it from the underlying tables.

Example
CREATE VIEW active_customers AS
SELECT id, name, email
FROM customers
WHERE status = 'active';
Here’s a focused, two-part preparation guide—technical SQL practice and high-level 
product-analytics case strategy—tailored to the Airbnb Sr. Manager, Advanced Analytics, 
Marketing interview and the job description you shared. 
1⃣ SQL & Data-Analysis Prep 
Airbnb uses Hive/Presto (ANSI-SQL-like) syntax. The interview allows you to look up syntax, 
so focus on conceptual mastery and pattern recognition rather than rote memorization. 
Core Topics to Review 
Area 
Joins 
Key Patterns & Examples 
INNER, LEFT, RIGHT, FULL; self-joins (hierarchies, user→referrer). 
Aggregation 
Filtering 
GROUP BY, HAVING, DISTINCT counts, conditional aggregation 
(SUM(CASE WHEN …)), grouping sets/cube/rollup. 
WHERE vs HAVING, date filters, semi/anti joins. 
Window Functions ROW_NUMBER, RANK, DENSE_RANK, LAG/LEAD, SUM() OVER 
(PARTITION BY … ORDER BY … RANGE/ROWS). 
Subqueries & CTEs Correlated subqueries (e.g., latest record per user), multi-CTE 
pipelines. 
Set Ops 
Data Types & 
Casting 
Performance 
UNION vs UNION ALL, INTERSECT, EXCEPT. 
Timestamps/time zones (Hive/Presto often use UTC), string parsing 
(regexp_extract, split). 
Predicate pushdown, filtering before joining, using DISTINCT vs 
windowing. 
Recommended Practice Framework 
Organize practice around query archetypes so you can recognize patterns quickly: 
Archetype 
Top N per group 
SQL Pattern 
ROW_NUMBER() OVER (PARTITION BY g ORDER BY metric 
DESC) 
Deduplicate latest QUALIFY ROW_NUMBER() OVER (PARTITION BY id ORDER BY 
ts DESC)=1 
Sessionization 
LAG(ts) ... WHERE ts - LAG(ts) > 30m 
Retention / cohort First login per user → DATEDIFF(event_date, cohort_date) 
Rolling metrics 
Practice these with: 
SUM(metric) OVER (PARTITION BY id ORDER BY ts ROWS 
BETWEEN 6 PRECEDING AND CURRENT ROW) 
● LeetCode SQL 50, Mode Analytics SQL tutorial, and Airbnb open-source datasets 
(e.g., Inside Airbnb). 
2⃣ Product-Analytics / Case-Study Prep 
The “case study portion” focuses on structuring an analytical approach to measure product 
development and marketing impact. 
Framework for Case Answers 
A. Clarify the Objective 
● Business goal (e.g., improve guest conversion, host retention). 
● Primary KPIs (bookings, GMV, NPS, repeat rate). 
B. Map the Funnel / User Journey 
● Awareness → Search → Listing View → Booking → Review. 
● Identify key drop-off points and leverage marketing levers. 
C. Define Metrics 
● Core: conversion rate, activation rate, LTV, acquisition cost. 
● Supporting: time-to-book, search-to-book ratio, host response rate. 
D. Experimental / Measurement Strategy 
● A/B Testing: randomization, CUPED/stratified designs. 
● Quasi-experiments: geo holdouts, difference-in-differences when randomization isn’t 
possible. 
● Attribution: MTA vs. MMM for marketing spend. 
E. Data & Infrastructure 
● Event-level logs, Hive/Presto tables, dimensional model. 
● Guardrails: data quality checks, anomaly detection. 
F. Communicate & Drive Action 
● Build narrative: baseline → intervention → impact. 
● Visuals: funnel charts, retention curves. 
Example Mini-Case 
“Airbnb wants to assess the impact of a new search-ranking algorithm on booking 
conversions.” 
1. Hypothesis: Improved ranking increases booking rate. 
2. Design: 50/50 randomized bucket of search traffic. 
3. Metrics: primary = bookings/search session; guardrail = search latency. 
SQL Approach: 
SELECT bucket, 
COUNT(DISTINCT booking_id)/COUNT(DISTINCT search_session_id) AS conv_rate 
FROM search_experiment 
GROUP BY bucket; 
4.  
5. Deeper Dive: segment by market, device; run CUPED for variance reduction. 
3⃣ Marketing-Specific Angle 
Because this is Marketing Advanced Analytics, expect tie-ins such as: 
● Incrementality of paid channels (MMM, geo experiments). 
● Lifetime value modeling. 
● Marketplace balance (supply vs. demand). 
● Global seasonality & pricing effects. 
4⃣ Mock Interview Drill Plan 
Day 
Focus 
1–2 SQL review 
Action 
Work through 15 LeetCode/Mode problems across joins, windows, 
CTEs. 
3 
Product metrics Create funnels for sample Airbnb datasets; write metric definitions. 
4 
5 
Case studies 
Dry run 
Practice 2–3 mock product-measurement questions aloud; time 
yourself. 
Combine: 30-min SQL live coding + 30-min case presentation. 
Quick Reference Sheets 
Prepare a one-pager with: 
● Common window function syntax. 
● Funnel & retention metric formulas. 
● Experiment design checklist. 
Here’s a concise, single-page reference you can print or keep open during your Airbnb 
interview prep. 
SQL Window Functions – Quick Syntax 
Use Case 
Row numbering / 
ranking 
Example 
ROW_NUMBER() OVER (PARTITION BY user_id ORDER BY 
event_ts DESC) 
Rank with ties 
Percentile 
RANK() OVER (PARTITION BY region ORDER BY revenue 
DESC) 
PERCENT_RANK() OVER (PARTITION BY cohort ORDER BY 
spend) 
Running total / 
cumulative sum 
Moving average 
(7-day) 
Lag/Lead (prev/next 
event) 
First/last value 
Deduplicate latest 
SUM(bookings) OVER (PARTITION BY host_id ORDER BY 
date ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT 
ROW) 
AVG(metric) OVER (PARTITION BY id ORDER BY dt ROWS 
BETWEEN 6 PRECEDING AND CURRENT ROW) 
LAG(price,1) OVER (PARTITION BY listing_id ORDER 
BY ts) 
FIRST_VALUE(state) OVER (PARTITION BY guest_id 
ORDER BY check_in) 
QUALIFY ROW_NUMBER() OVER (PARTITION BY user_id 
ORDER BY updated_at DESC)=1 (Presto/Hive often use 
subquery instead of QUALIFY) 
Funnel & Retention Metrics 
Metric 
Step-to-step 
conversion 
Overall 
funnel 
conversion 
Formula (SQL-style) 
COUNT(DISTINCT 
users_at_step_n)/COUNT(DISTINCT 
users_at_step_{n-1}) 
COUNT(DISTINCT 
booked_users)/COUNT(DISTINCT 
entry_users) 
Notes 
Compute for each adjacent 
step (e.g., 
Search→View→Book). 
From first touch to booking. 
Activation 
rate 
Day-N 
retention 
Rolling 
retention 
Churn rate 
LTV (simple) 
activated_users / new_signups 
COUNT(DISTINCT user_id WHERE 
DATEDIFF(event_date, 
signup_date)=N)/COUNT(DISTINCT 
user_id WHERE signup_date = 
cohort_date) 
Same as above but DATEDIFF <= N 
1 - retention_rate 
SUM(net_revenue)/COUNT(DISTINCT 
users) 
Define “activated” (e.g., first 
booking). 
Repeat for N=1,7,30… 
Measures “active anytime up 
to N days.” 
Periodic. 
Segment by cohort. 
Tip: For cohort tables use a CTE to capture MIN(signup_date) per user, then join back on 
events. 
Experiment Design Checklist 
1. Define 
● Business Objective: e.g., “Increase booking conversion by 2 pp.” 
● Primary Metric: conversion rate, revenue/visitor, etc. 
● Guardrails: latency, cancellations, NPS. 
2. Hypothesis & Design 
● Randomized buckets (50/50 or multivariate). 
● Stratification if needed (market, device). 
● Unit of randomization (user, session, geo). 
3. Sample & Power 
● Minimum Detectable Effect (MDE). 
● Power ≥80%, α=0.05. 
● Duration estimate: use historical variance. 
4. Data Collection 
● Event logging spec: user_id, experiment_id, ts, variant. 
● Monitor for logging gaps. 
5. Analysis 
● Difference in means/prop tests or CUPED for variance reduction. 
● Segmentation (geo, platform). 
● Pre-checks: balance of key covariates. 
6. Interpretation 
● Statistical significance vs. practical impact. 
● Validate no negative impact on guardrail metrics. 
7. Rollout 
● Gradual exposure / ramp-up. 
● Post-launch monitoring and backtesting. 
Keep this one-pager handy during prep and mock interviews to quickly recall syntax, metric 
formulas, and the stepwise experiment framework. 
How to Pick the Right Chart Type for your Data
You have the right data, the next step is to pick an appropriate chart type to visualize the data. Picking the right chart type depends on exactly what you're attempting to show:

a relationship between categories
a distribution along one variable
composition of different variables
Chart Type	When to use
Metric Chart	Best to show a single value that's either increasing or decreasing.
Column Chart	Best to show differences between categories.
Bar Chart	Best to show differences between categories, with longer labels.
Stacked Column or Bar Chart	Best to show differences between sub-categories, and better than pie charts.
Pie Chart	Can show proportions of a whole, but use stacked column or bar charts instead.
Line Chart	Best to show changes in continuous data (such as time.)
Scatter Chart	Best to show the relationship between two series of data. Also called an XY chart.
Bubble Chart	Best to show relationship between three series of data by using bubble size and color.
Map Chart	Best to show geographic spread of your data.
I interviewed for Amazon recently and got asked these questions.

Joins, window functions type questions

Variations of gaps and islands problem

Window statistical distribution questions like median

Design schema for grocery store chain to be able to identify top selling products for each store in last 12 months to recommend best selling products to customers who have shopped for atleast 2 days

/


TheRiteGuy
•
5y ago
My last technical interview, which I botched horribly, wasn't any kind of formal questions. It was more like, Here's a hypothetical table and it has this kind of information. Then he went through setting up different scenarios for me to solve.

Things I needed in the interview:

Simple things like Select Distinct, Count, Count Distinct, Min/Max, Sum.

Windows functions like Lag, Over, Partition by

Joins, using the filter clause in join to filter to one result instead of using where.

Group by, Order By

/

Master different types of JOINs, CTEs (Common Table Expressions), and window functions.

Practice pattern recognition to identify similar problems and solutions.

Begin with basic SQL syntax and gradually move to more complex problems.

Translate real-world business problems into SQL statements, which requires "thinking in SQL."

Learn relational algebra, the Entity-Relationship model, database normalization, and translating those into object and class-oriented models.

Practice using real-world examples and data to build your understanding.

Be aware of the performance differences between CTEs and temporary tables, especially when working with large datasets.

Gain exposure to industry-standard data engineering technologies like Spark, Airflow, and DBt.

Brush up on common data engineering concepts like OLTP, OLAP, distributed file formats, and REST APIs.

/

Joins: Master inner, left, right, and full joins.

Aggregations: Know GROUP BY, HAVING, and functions like SUM(), COUNT(), etc.

Window Functions: Focus on ROW_NUMBER(), RANK(), LAG(), LEAD().

Subqueries: Learn how to handle subqueries within SELECT, WHERE, and FROM.

Common Table Expressions (CTEs): Understand how and when to use them.

Indexes and Performance: Learn indexing strategies and how to optimize query performance.

Data Modeling: Understand normalization, denormalization, and keys.

Complex Queries: Be able to write complex queries combining multiple concepts.

Real-world Scenarios: Be prepared to solve business problems with SQL.

Error Handling: Learn how to debug and fix common SQL issues.

/


Grouchy-Donut-726
•
1y ago
Select

Where

In,between, like

Order by

Case

Group by

Aggregate: sum, count, avg, min, max

Having

Inner join, outer join, right join, left join, self join

Sub query

Window functions: row number, over and partition by, lag, lead

Union, union all

CTE

temp tables

Stored procedures

/


I won’t lie - Data Science interviews are TOUGH, especially when you’re aiming to work at FAANG. But look no further, I’m here to break down 20 probability and statistic questions that come up during these high-stakes interviews.

I’ve broken down these questions into four sections: Easy, Medium, Hard, and Expert. Challenge yourself and solve as many questions as you can, and I’ll be here to guide you through the answers.

20 Probability and Statistics Questions and Answers

Probability and Statistics Concepts to Review for the Data Science Interview
Because probability & statistics are foundational to the field of Data Science, before the interview you should review:

Central Limit Theorem
Probability Distributions
Regression Analysis
Hypothesis Testing
If are unfamiliar with these concepts I recommend reading some of the books from the 13 Best Books for Data Scientists list.

Central Limit Theorem
Understanding the Central Limit Theorem is crucial. It states that the distribution of the sample mean of a large enough sample from any population will be approximately normally distributed, regardless of the population's underlying distribution. This theorem is fundamental when dealing with inferential statistics and hypothesis testing.

Probability Distributions
Hypothesis testing involves formulating null and alternative hypotheses, collecting data, and using statistical methods to determine whether there is enough evidence to reject the null hypothesis. You should be proficient in different types of hypothesis tests (e.g., t-tests, chi-squared tests) and their applications.

Regression Analysis
Familiarity with common probability distributions like the normal distribution, binomial distribution, and Poisson distribution is essential. You should understand their probability density functions, cumulative distribution functions, and how to use them in real-world scenarios.

Hypothesis Testing
Regression analysis is a fundamental statistical technique used for modeling relationships between variables. You should know about linear regression, multiple regression, logistic regression (for classification), and how to interpret regression coefficients, p-values, and R-squared values. Understanding regression allows you to make predictions and draw insights from data.

Beginner Probability and Statistic Questions and Answers asked by FAANG
1. What is the probability of rolling a 6 on a fair six-sided die?
The probability of rolling a 6 on a fair six-sided die is 1/6.

2. Calculate the expected value of a fair coin flip.
The expected value of a fair coin flip is 0.5 (or 1/2).

3. Explain the concept of simple random sampling in statistics.
Simple random sampling is a method where every member of the population has an equal chance of being selected in the sample.

4. Define the Central Limit Theorem and its significance in statistics.
The Central Limit Theorem states that the distribution of the sample mean approaches a normal distribution as the sample size increases, regardless of the shape of the original population distribution.

5. What is a p-value, and how is it used in hypothesis testing?
A p-value is a probability measure used in hypothesis testing that quantifies the evidence against a null hypothesis. A smaller p-value suggests stronger evidence against the null hypothesis.

More Questions
Not enough? Try these questions for FREE:

Two Consecutive Sixes
Coin Fairness Test
Medium Probability and Statistic Questions and Answers asked by FAANG
6. Given two events A and B, how do you calculate P(A|B) (the conditional probability of A given B)?
Conditional probability P(A|B) is calculated as the probability of both events A and B occurring (P(A ∩ B)) divided by the probability of event B occurring (P(B)).

7. Explain the Bayesian probability theory and its application in data science.
Bayesian probability is a framework that incorporates prior beliefs and updates them with new evidence using Bayes' theorem, allowing for probabilistic reasoning and decision-making.

8. What is the confidence interval, and how do you interpret a 95% confidence interval?
A 95% confidence interval means that if we were to take many random samples and construct confidence intervals from them, we would expect approximately 95% of those intervals to contain the true population parameter.

9. Describe the sampling distribution of the sample mean.
The sampling distribution of the sample mean is a normal distribution with the same mean as the population and a standard deviation equal to the population standard deviation divided by the square root of the sample size.

10. Calculate the z-score for a data point in a standard normal distribution.
The z-score for a data point in a standard normal distribution is calculated as (X - μ) / σ, where X is the data point, μ is the mean, and σ is the standard deviation.

More Questions
Not enough? Try these questions for FREE:

Consecutive Fives
Biased Coin?
Hard Probability and Statistic Questions and Answers asked by FAANG
11. Compare and contrast the Poisson and Binomial distributions.
The Poisson distribution models the number of events occurring in a fixed interval of time or space, while the Binomial distribution models the number of successes in a fixed number of independent trials.

12. What is the difference between Type I and Type II errors in hypothesis testing?
Type I error occurs when we reject a true null hypothesis, while Type II error occurs when we fail to reject a false null hypothesis.

13. Explain the concept of MLE and provide an example of its application.
Maximum Likelihood Estimation is a method used to estimate the parameters of a statistical model by maximizing the likelihood function. For example, in the case of a normal distribution, MLE estimates the mean and standard deviation.

14. What is covariance, and how does it differ from correlation?
Covariance measures the degree to which two variables change together, while correlation measures the strength and direction of the linear relationship between two variables.

15. Describe stratified sampling and its advantages over simple random sampling.
Stratified sampling involves dividing the population into subgroups or strata and then taking random samples from each stratum. It is advantageous when there is significant variation within strata.

More Questions
Not enough? Try these questions for FREE:

Product vs. Square
Minimum of Two Uniform Variables
Expert Probability and Statistic Questions and Answers asked by FAANG
Questions:
16. How does Monte Carlo simulation work, and what are its applications in data science?
Monte Carlo simulation is a computational technique that uses random sampling to solve complex problems or estimate numerical results. It has applications in finance, engineering, and optimization problems.

17. Define bootstrapping and discuss its use in estimating population parameters.
Bootstrapping is a resampling technique where samples are drawn with replacement from the observed data to estimate population parameters. It is useful when parametric assumptions are uncertain.

18. Explain the principles of Bayesian networks and their role in probabilistic graphical models.
Bayesian networks are graphical models that represent probabilistic relationships among a set of variables. They are used for probabilistic reasoning, decision-making, and risk analysis.

19. What are autoregressive (AR) and moving average (MA) models in time series analysis?
Autoregressive (AR) models describe a time series using its own past values while moving average (MA) models describe a time series using past forecast errors.

20. Discuss the concept of familywise error rate and methods to control it in multiple hypothesis testing scenarios.
Familywise error rate is the probability of making at least one Type I error when conducting multiple hypothesis tests. Methods to control it include Bonferroni correction and false discovery rate control.
If anyone wants a list of all the types of joins, here they are…. Sorry for the format.

Type: Logical

Join: INNER JOIN

Description: An INNER JOIN returns only the rows with matching values in both tables.

Type: Logical

Join: OUTER JOIN

Description: An outer join (LEFT OUTER JOIN/RIGHT OUTER JOIN) returns all the rows from one table and any matching rows from the other table. If there is no match, the result will contain NULL markers.

Type: Logical

Join: FULL OUTER JOIN

Description: A FULL OUTER JOIN returns all the rows from both tables. If there are no matching rows, the result will contain NULL markers.

Type: Logical

Join: CROSS JOIN

Description: A CROSS JOIN returns the Cartesian product of the two tables and returns every possible combination of rows from the two tables.

Type: Physical

Join: NESTED LOOP JOIN

Description: Nested loop join is a type of join algorithm that compares each row of one table with all rows of another table.

Type: Physical

Join: HASH JOIN

Description: Hash join is a join algorithm that uses a hash table to quickly match rows from one table with rows from another table.

Type: Physical

Join: MERGE SORT JOIN

Description: Merge sort join is a join algorithm that sorts both tables on the join column and then merges the sorted rows.

Type: Descriptive

Join: COMPLEX JOIN

Description: A complex join is a join operation that combines multiple tables using various comparison operators and often includes subqueries and aggregate functions to retrieve and combine data from different tables.

Type: Descriptive

Join: COMPOSITE JOIN

Description: A composite join is a join that uses multiple attributes from each table to retrieve and combine data from different tables.

Type: Descriptive

Join: MULTI-JOIN

Description: A multi-join refers to a query that involves joining more than two tables together.

Type: Descriptive

Join: SELF-JOIN

Description: A self-join joins a table to itself, using the same table twice with different aliases.

Type: Model

Join: SEMI-JOIN

Description: A semi-join returns only the rows from the first table with matching values in the second table.

Type: Model

Join: ANTI-JOIN

Description: An anti-join returns only the rows from the first table that do not have matching values in the second table.

Type: Model

Join: THETA-JOIN

Description: A theta-join is a join based on a binary operator (equality or inequality). Both equi and non-equi-joins are theta joins.

Type: Model

Join: EQUI-JOIN

Description: An equi-join is a join condition that involves only an equality operator.

Type: Model

Join: NON-EQUI-JOIN

Description: A non-equi-join is a join condition that involves any operator besides equality.

Type: Model

Join: NATURAL JOIN

Description: A NATURAL JOIN returns the rows where the values in the specified columns of both tables are equal and the column names are the same.
I'm a senior reporting analyst and I consider myself proficient, here's what we generally look for when hiring.

know the difference between joins (inner and left are 95% of pulling data, I never use right joins) and how they relate to what data is being pulled. This is basic SQL.

learn aggregate functions. Sometimes you want to count all widgets, sometimes you only want to count how many different widgets. Learn to use HAVING when grouping for aggregate function.

pick a formatting for code use it. Some people always put the comma after the column name in a select, some people put it before the name. Some people use tab/space to give it a visual pattern (I do) while some don't. It's OK to change your formatting if you find something does or does not work for you but generally be consistent.

add notes. I have a header statement I paste at the beginning of all scripts that states what the script does, any ticket #'s associated (and what the ticket request was changing), the date it was completed, and who the dev was. Force the habit of adding notes throughout the script to identify anything not obvious or to highlight import stuff. If the column name is [col1] then add a note that describes what data is in that column.

learn the difference between CTE's and temp or volatile tables, and when each should be used. If using MS/T-SQL using global temp tables helps a ton in troubleshooting.

learn a methodical way to troubleshoot and stick to it. This helps a ton in the beginning and will save you at least one head pounding on wall moment a week because you use "like" instead of = when dealing with integers. When starting g out with SQL I had a text file that was basically my manual for troubleshooting and added new steps when I came across a new issue.
In this 6,000-word SQL interview guide, I'm here to set the record straight. For context, my name is Nick Singh and I've worked in a variety of Data/Software Engineering roles at Facebook, Google, and Microsoft. I also wrote the best-selling book Ace the Data Science Interview.

Nick Singh About Me

The SQL interview tips in this 6,000-word guide directly come from my experience coaching hundreds of Data Analysts and Data Scientists to ace their SQL interviews.

The Ultimate SQL Interview Guide Covers:

Common SQL Commands Used in Interviews
SQL Questions from Tesla, Microsoft, TikTok, and Uber (with solutions)
Join SQL Interview Questions
Window Functions Used in SQL Interviews
Common Database Design Interview Questions
6-Step Process to Solve Hard SQL Questions
3 Best Books for SQL Interviews
Best Site to Practice SQL Interview Questions
How to Approach Open-Ended Take-Home SQL Challenges
4 Real Take-Home SQL Interview Challenges
Before we go into the exact topics SQL interviews cover, we need to get into the interviewer’s head and truly understand WHY tech companies ask SQL questions during interviews.

Why Do Data Interviews Ask SQL Questions?
While SQL might not be as glamorous and sexy as Python or R, SQL is an important language to master for Data Analysts and Data Scientists because your data lives in a database, and that’s where cleaning, filtering, and joining of large datasets can be done in a performant way. You don’t want to pull all the data into a Pandas or R dataframe, and crash your laptop, when you can efficiently shape and filter datasets thanks to SQL.

Just Use SQL Bellcurve Meme

That's why SQL is listed in 61% of data analytics jobs posted on Indeed, according to research done by DataQuest.io. In order to see if you can do the day-to-day work, hiring managers typically send candidates a SQL assessment during the Data Analyst or Data Science interview process.

But you might say: "Nick, I got into this field to do Deep Learning with Tensorflow, are you seriously telling me Data Science interviews cover boring old SQL?"

YES that's exactly what I'm saying!!

SQL?! I thought Data Science was about Neural Networks in Python?
Even at companies like Amazon and Facebook, which have massive amounts of data, most Data Scientists still spend most of their time writing SQL queries to answer business questions like "What are the top-selling products?", or "How do we increase ad click-through rates?".

For more insight into the importance of SQL for Data Scientists, you can read this infamous article "No, you don't need ML/AI – You need SQL" which concretely shows you how so many ML problems can just be solved in a fraction of the time with some heuristics and a SQL query.

What version of SQL is used in interviews?
SQL comes in a variety of versions (also known as flavors), like MySQL, SQL Server, Oracle SQL, and PostgreSQL. Because the SQL versions are pretty similar, most data job interviews don't require you to use a specific version of SQL during the interview. We recommend aspiring Data Analysts and Data Scientists practice their SQL interview questions in PostgreSQL, because it’s the most standards-compliant version of SQL out there, and one of the most popular flavors of SQL in the data industry.

However, if you are strongest in another flavor of SQL, it usually shouldn’t be a problem for SQL interviews. That’s because interviewers are more-so seeing if you understand how to write SQL queries and problem-solve – they know on the job you can just learn the version of SQL the company uses in a few days. As such, during live SQL interviews, a good interviewer won’t stress about minor syntactical errors or differences between different SQL versions.

What Do SQL Interviews Cover?
SQL interviews typically cover five main topics:

basic SQL commands
SQL joins
window functions
database design concepts
your ability to write SQL queries to answer business questions
While most other SQL interview question lists cover SQL trivia, like “What does DBMS stand for?” this guide focuses on what FAANG companies like Amazon and Google ask during interviews. I need to emphasize this point, because the first result on Google for "SQL interview questions" is a pop-up riddled website claiming "What is database?" is a legit interview question 😂.

Fake List of Questions on InterviewBit

Instead of asking conceptual questions, top Silicon Valley technology companies put you on the spot, and ask you to write a SQL query to answer a realistic business questions like "Find me the number of companies who accidentally posted duplicate job listings on LinkedIn?"

LinkedIn SQL Interview Question: Find Duplicate Job Listings

Before we can learn to apply SQL to these scenario-based questions, we need to cover some foundational SQL concepts like the most common SQL commands you need to know for interviews, what kinds of joins show up, and the most popular window functions for SQL interviews.

What are the most common SQL commands used in interviews?
Here’s the top 7 most common SQL commands tested during SQL interviews:

SELECT - used to select specific columns from a table
FROM - used to specify the table that contains the columns you are SELECT’ing
WHERE - used to specify which rows to pick
GROUP BY - used to group rows with similar values together
HAVING - used to specify which groups to include, that were formed by the GROUP BY clause.
ORDER BY - used to order the rows in the result set, either in ascending or descending order
LIMIT - used to limit the number of rows returned
However, 99% of Data Science & Data Analyst interviews at competitive companies won't just straight up ask you "What does GROUP BY do?". Instead you'll have to write a query that actually uses GROUP BY to solve a real-world problem. Check out the next section to see what we mean.

Group By Example: Tesla SQL Interview Question
In this real Tesla SQL Interview question, a Data Analyst was given the table called parts_assembly and asked to "Write a SQL query that determines which parts have begun the assembly process but are not yet finished?".

Tesla Data Analyst SQL Interview Question

To solve the question, realize that parts that are not yet finished can be found by filtering for rows with no data present in the finish_date column. This can be done using the SQL snippet:

WHERE finish_date IS NULL
Because some parts might be represented multiple times in the query data because they have several assembly steps that are not yet complete, we can GROUP BY to obtain only the unique parts.

Thus, the final answer to this Tesla SQL Interview question is:

SELECT part
FROM parts_assembly
WHERE finish_date IS NULL
GROUP BY part;
Hopefully, you've understood how just memorizing what WHERE or GROUP BY isn't going to cut it, and that to solve beginner SQL interview questions you still have to creatively apply the basic commands. To practice this Tesla SQL question yourself, click the image below:

Tesla SQL Question: Unfinished Parts

Now, let's cover another fundamental topic that's often combined with basic SQL commands: aggregate functions like COUNT() and SUM().

Aggregate Functions Used In SQL Interviews
Aggregate functions allow you to summarize information about a group of rows. For example, say you worked at JPMorgan Chase, in their Credit Card analytics department, and had access to a table called monthly_cards_issued. This table has data on how many credit cards were issued per month, for each different type of credit card that Chase offered.

JPMorgan Chase SQL Interview Question Data

To answer a question like “How many total cards were issued for each credit card” you’d use the SUM() aggregate function:

SELECT card_name,
       SUM(issued_amount)
FROM   monthly_cards_issued
GROUP  BY card_name; 
Entering this query on DataLemur yields the following output:

SUM() PostgreSQL Interview Question Example

Similarly, if you wanted to count the total number of rows, you could use the aggregate function COUNT(). To play around with this dataset, open the SQL sandbox for the JPMorgan SQL Interview Question.

JPMorgan SQL Interview Questions: Cards Issued Difference

While PostgreSQL technically has dozens of aggregate functions, 99% of the time you'll just be using the big five functions covered below.

What are the most common SQL aggregate functions?
The 5 most common aggregate functions used in SQL interviews are:

AVG() - Returns the average value
COUNT() - Returns the number of rows
MAX() - Returns the largest value
MIN() - Returns the smallest value
SUM() - Returns the sum
While array_agg() and string_agg() aggregate functions may show up in advanced SQL interviews, they are extremely rare. To learn more about these uncommon commands, visit the PostgreSQL documentation.

SQL Interview Questions On Joins
In real-world data science & data analytics, you don't just use aggregate functions on one table at a time. Because your data lives in multiple SQL tables, as an analyst you're constantly writing SQL joins to analyze all the data together in one go. As such, hiring managers frequently ask both conceptual questions about SQL joins, as well as give you practical scenarios and then ask you to write a SQL query to join two tables.

Microsoft SQL Interview Question Using JOIN
For a concrete example of how joins show up during SQL interviews, checkout this real SQL interview Question asked by Microsoft:

“Which Azure customer buys at least 1 Azure product from each product category?”

The data needed to answer this would be in two tables – a customer_contracts table, which details which companies buy which products, and a table of Azure products, which has details about what product category each Azure service belongs too.

Microsoft SQL Interview Question Dataset

To solve this question, you'd need to combine the customer_contracts and products tables with a SQL join, which is what the following SQL snippet does:

SELECT *
FROM customer_contracts
LEFT JOIN products 
    ON customers.product_id = products.product_id
To solve this real Microsoft Data Analyst SQL question yourself, and see the full solution give it a try on DataLemur:

Microsoft Join SQL Interview Question

What are the 4 different joins tested in SQL assessments?
There are four main ways to join two database tables, and one of the most frequently asked SQL interview questions is to distinguish between each kind:

INNER JOIN - combines rows from two tables that have matching values
LEFT JOIN - combines rows from the left table, even if there are no matching values in the right table
RIGHT JOIN - combines rows from the right table, even if there are no matching values in the left table
FULL JOIN - combines rows from both tables, regardless of whether there are matching values
Because a picture is worth a thousand words, checkout this neat infographic from DataSchool that explains joins visually:

SQL Joins Explained Visually

6 Most Common SQL Join Interview Questions
Besides having to write queries which use JOIN commands, you might also encounter the following commonly asked conceptual interview questions about SQL joins:

What is a self-join, and when would you use it?
What is an anti-join, and when would you use it?
What are the performance considerations of SQL join queries?
How do you optimize a slow join query?
How do you join more than two tables?
Does a join always have to be on two rows sharing the same value (non-equi joins)?
Many of these conceptual join questions closely relate to how databases are organized, and the costs and benefits of normalizing your tables. If you're interviewing for a Data Engineering, this topic is a must-know!

Do I need to know date/time functions for SQL assessments?
While it’s good to be familiar with date and time functions when preparing for a SQL interview, it isn’t absolutely mandatory to memorize the exact syntax for date/time functions because they differ greatly between SQL flavors. For example, SQL Server and MySQL have a DATEADD function, but PostgreSQL uses the keyword INTERVAL to get the same results.

Because of the varying syntax, interviewers often give you some leeway and allow you to look up the exact date/time SQL commands mid-interview, especially if you are interviewing in a version of SQL you aren’t accustomed to.

Most Common Date/Time Functions Used in SQL Interviews
The most common date/time functions to know for SQL interviews are:

NOW(): returns the current date and time
CURRENT_DATE(): returns the current date
INTERVAL: adds a specified time interval to a date
DATEDIFF: calculates the difference between two dates
EXTRACT: extracts a specific part of a date (e.g., month, day, year)
You should also know the following date/time operators:

+: adds a time interval to a date/time value
-: subtracts a time interval from a date/time value
||: concatenates two date/time values
Before a SQL assessment, it's also useful to be familiar with the various date/time types available in PostgreSQL, such as DATE, TIME, and TIMESTAMP.

Using Date/Time Functions In A TikTok SQL Assessment
To see PostgreSQL date/time operators in action, let’s solve this TikTok SQL Assessment Question called 2nd-day confirmation which gives you a table of text message and email signup data. You’re asked to write a query to display the ids of the users who confirmed their phone number via text message on the day AFTER they signed up (aka their 2nd day on Tik-Tok).

TikTok SQL Assessment: 2nd Day Confirmation

In the example data above, email_id 433 has a signup_date of 7/9/2022 and a confirmed action date of 7/10/2022. Hence, the user had a 1-day delay between the two events.

The answer to this TikTok SQL question utilizes the date/time operator INTERVAL to identify the 1-day gap between signup and confirmation. The snippet looks like this:

WHERE texts.action_date = emails.signup_date + INTERVAL '1 day'
The full solution also requires us to join the texts and emails table, and also filter down to text messages that were confirmed. Hence, the final solution is:

SELECT DISTINCT user_id
FROM emails 
INNER JOIN texts
  ON emails.email_id = texts.email_id
WHERE texts.action_date = emails.signup_date + INTERVAL '1 day'
  AND texts.signup_action = 'Confirmed';
Hard Date/Time SQL Interview Question From Stripe
If your up for a challenging date/time SQL interview question, try this very hard Stripe SQL Interview question asked in a final-round Data Science interview. The problem requires you to EXTRACT the EPOCH from a transaction timestamp.

Practice Problem
Stripe SQL Question: Write a SQL query to identify any payments made with the same credit card for the same amount within 10 minutes of each other.


If you have no idea how to solve this question, and reading the solution doesn't help, you probably need a refresher on window functions like LAG, conveniently covered up next!

Window Functions In SQL Interviews
Window functions are tricky, and hence show up constantly in advanced SQL interview questions to separate the beginners from the more experienced data analysts & data scientists.

At a high-level, a window function performs calculation across a set of rows that are related to the current row. This is similar to an aggregate function like SUM() or COUNT(), but unlike an aggregate function, a window function does not cause rows to become grouped into a single output row. Instead, you have control over the window (subset) of rows which are being acted upon.

ROW_NUMBER() Example From Google SQL Interview
For example the window function ROW_NUMBER() ranks selected rows in ascending order, but resets the ranks for each window. To demo this, let's analyze data from a real Google SQL Interview Question.

Google SQL Interview Question Odd Even

In the problem, you are given the measurements table which has data from an IoT sensor that collects multiple measurements per day:

measurements Example Input:
measurement_id	measurement_value	measurement_time
131233	1109.51	07/10/2022 09:00:00
135211	1662.74	07/10/2022 11:00:00
523542	1246.24	07/10/2022 13:15:00
143562	1124.50	07/11/2022 15:00:00
346462	1234.14	07/11/2022 16:45:00
You are asked to find the sum of the odd-numbered and even-numbered sensor measurements for each day. Before we start worrying about the odd measurements (1st, 3rd, 5th measurement of the day, etc.) and even measurements, we need to just understand what was the 1st, 2nd, 3rd, 4th, measurement of the day.

To do this we use ROW_NUMBER() to rank the rows BUT make the window only one-day wide. That means at the end of every day, the ranks reset back to 1. This is achieved with the following window function:

ROW_NUMBER() OVER (
    PARTITION BY CAST(measurement_time AS DATE) 
    ORDER BY measurement_time) AS measurement_num
When we run the code, you'll see at the end of each day the measurement number resets:Row_Number() Window Function Example.

From here, to get odd and even measurements, we just need to divide the measurement_num by 2 and check the remainder, but we'll leave it up to you to implement inside the SQL code sandbox for this Google Data Analyst SQL question.

For another example, let's dive into a practical exercise from an Uber Data Science assessment which also uses the ROW_NUMBER() window function.

Uber Window Function SQL Interview Question
Take for example this Uber SQL Interview Question about selecting a user's 3rd transaction made on the Uber platform.Uber SQL Interview Question: User's 3rd Transaction

At the core of this SQL question is the window function ROW_NUMBER() which assigns a number to each row within the partition. Essentially, we want to group/partition all the Uber transactions together based on which user_id made the transaction, and then order these transactions by when they occured (transaction_date), so that we can label the order in which they occured using ROW_NUMBER():

    ROW_NUMBER() OVER (
      PARTITION BY user_id ORDER BY transaction_date) AS row_num
    FROM transactions) AS trans_num 
Finally, using the output from the window function, we want to filter our results to only get the 3rd transaction for every user:

WHERE row_num = 3
This yields us the final solution:

SELECT 
  user_id,
  spend,
  transaction_date
FROM (
  SELECT 
    user_id, 
    spend, 
    transaction_date, 
    ROW_NUMBER() OVER (
      PARTITION BY user_id ORDER BY transaction_date) AS row_num
  FROM transactions) AS trans_num 
WHERE row_num = 3;
For more practice with SQL interview questions that use window functions select the 'Window Functions' filter on the DataLemur SQL interview questions.

Window Function SQL Interview Questions

What are the most common window functions for SQL interviews?
The top window functions used in SQL interviews are:

RANK() - gives a rank to each row in a partition based on a specified column or value
DENSE_RANK() - gives a rank to each row, but DOESN'T skip rank values
ROW_NUMBER() - gives a unique integer to each row in a partition based on the order of the rows
NTILE() - divides a partition into a specified number of groups, and gives a group number to each row
LAG() - retrieves a value from a previous row in a partition based on a specified column or expression
LEAD() - retrieves a value from a subsequent row in a partition based on a specified column or expression
NTH_VALUE() - retrieves the nth value in a partition
To understand each window function in more detail, check out Mode's SQL tutorial on Window Functions.

Now that you know the basic SQL commands that come up in interviews, along with intermediate SQL interview topics like joins and window functions, we're ready to cover database design and data modeling interview questions.

Database Design & Data Modeling Interview Questions
Database design and data modeling interview questions test you on how well you understand the inner-workings of databases, along with how to design your data warehouse. If you're preparing for a Data Engineering or Analytics Engineering interview, this section is just as important as being able to write SQL queries. However, we still think it’s an important topic for Data Analysts and Data Scientists to briefly cover too, especially if interviewing for a smaller startup where you’ll likely wear multiple hats and end up doing some Data Engineering work too.

Common Database Design Interview Questions
What is an index, and why does it speed up queries?
What are the dis-advantages of using indexes?
How do you troubleshoot a slow SQL query?
How do you CREATE, READ, UPDATE, and DELETE in SQL?
What is a stored procedure, and when do we use them?
What is normalization? Why might we want to also de-normalize some tables?
What is ACID, and how does a database enforce atomicity, consistency, isolation, durability?
What’s the difference between Star schema and Snowflake schema?
What are the different types of dimensions (e.g. junk dimensions, conformed dimensions, mini dimensions, shrunken dimensions)?
If you had to make a simple news feed, similar to the Facebook or LinkedIn feed, what are the main tables you’d have? Can you whiteboard a quick ER Diagram for it?
What is database sharding?
What are the advantages and disadvantages of relational vs. NoSQL databases?
How To Prep For Database Design Interview Questions
If these database design interview questions look super tough, I recommend reading the classic book Database Design for Mere Mortals because it covers topics like translating business needs into design specifications, how to determine what tables you need and their relationships, how to anticipate and mitigate performance bottlenecks, and how to ensure data integrity via field specifications and constraints.

Database Design for Mere Mortals on Amazon

To answer data warehousing interview questions, you need to memorize the dimension modeling bible The Data Warehouse Toolkit by Kimball and Ross. This book is gold because it features multiple data warehousing case studies, and shows you exactly how to design your dimensional databases for maintainability and performance.

And to learn basic DML (Data Markup Language) commands like CREATE, UPDATE, and DELETE checkout this blog post SQL CRUD operations.

Why Data Analysts And Data Scientists Should Study Database Design
While Data Analysts and Data Scientists might not be asked advanced database interview questions during their interview process, we still think it’s worth studying database design because:

knowing how databases are structured and indexed allows you to write more efficient SQL queries
understanding how databases enforce data integrity can help you troubleshoot issues with data quality
learning data modeling & warehouse design helps you collaborate more effectively with Data Engineering co-workers
at smaller companies you’ll wear multiple-hats which means there’s a very real chance you end up doing some data infrastructure work
With the fundamental SQL commands and database concepts out of the way, let's take a high-level approach to solving SQL interview questions.

How do you approach a SQL interview question?
SQL interviews are stressful, but if you approach each question with a structured approach, you’ll ace the SQL interview. Here’s the 6 steps to solve any SQL interview question:

Understand the question
Identify Relevant information
Break down the problem
Consider Edge Cases
Write queries to answer sub-problems
Test your final query
SQL Interview Step 1: Understand the Question
Often you might get a long SQL word problem, where you’ll have lots of extra details and it might not be clear what the interviewer is specifically asking you to query. So understanding and clarifying what specifically needs to be done is the best first step to take.

SQL Interview Step 2: Identify Relevant information
You might have extraneous columns, or even extra tables that aren’t needed for your SQL query. Interviewers do this on purpose, because in real-world SQL you'll often have thousands of tables, with hundreds of columns, and it's a skill to determine what information you actually need to query that's relevant to the problem. As such, Identify what’s actually needed to directly answer the SQL interview question at-hand.

SQL Interview Step 3: Break Down the Problem
Often, SQL interviews have a multi-part solution, consisting of multiple joins, unions, subqueries, and CTEs. Map out what are the smaller building blocks that are needed for the final solution. You want to verbalize this step, because it shows the interviewer that's watching you code that you are able to break-down complex problems into simpler sub-problems – a useful skill not just in SQL, but in Data Analytics & Data Science as a whole.

SQL Interview Step 4: Consider Edge Cases
You can’t forget edge cases, like if some value is null, or there is a tie in your results set. Make sure to think about this BEFORE you start writing your SQL query. Frequently, SQL interviews will purposely have tricky test cases which catch whether you’ve handled all edge cases.

SQL Interview Step 5: Write Queries for Sub-Problems
Write queries to answer sub-problems: don’t go after the question all at once. Write small subqueries that answer sub-problems. Test your solutions incrementally, and slowly combine your sub-problem results. If you try to answer the problem all in one go, your SQL query likely won’t run and you’ll overwhelm yourself trying to figure which of the 27 lines you wrote contains the error.

SQL Interview Step 6: Test Your Final Query
Run your SQL query, and validate that your output matches the expected output. From coaching hundreds of people, you won’t believe how many people think they have the final solution, but don’t realize there’s a slight difference between the expected results and what they produced.

What makes online SQL assessments difficult?
Online SQL assessments are difficult for three main reasons:

You need to solve the questions under time pressure.
You need to write clean SQL code which adheres to best-practices.
You need to know the SQL patterns required for the toughest SQL interview questions (which takes a ton of SQL interview practice)
Let's cover each tricky aspect, and how best to overcome these difficulties.

Handling Time Pressure During SQL Assessments
Online SQL assessments typically give you an hour to solve 2 to 3 tricky SQL questions. This time constraint significantly adds to the stress, which can make it difficult to think clearly. For live whiteboard SQL interviews, an interviewer is hovering over you, which further adds to the tension. Finally, for live SQL screens, you're expected to verbalize your thought process to the interviewer, which can make SQL interviews even more stressful.

Our advice to make this less nerve-wracking is to practice sql interview questions with a timer, and in the presence of a friend, so you can get used to writing SQL quickly while verbalizing your thoughts.

Write Clean SQL Queries During Interviews
It’s not enough to answer the interview question correctly – your SQL query needs to be written cleanly too! That means not taking shortcuts, like renaming columns and tables with short unhelpful names like “t” or “u”. It also means not overly nesting sub-queries, and instead using CTEs. This is especially true during take-home SQL assessments, where there is less time pressure and you have no excuse not to write clean SQL.

My soccer coach used to always tell me “You play like you practice and practice how you play” and the same mentality applies for SQL interviews too. When you are practicing sql interview questions, don’t take shortcuts, and put in the effort to make sure your SQL queries are written cleanly even if no one else is going to read them!

If you don’t know what constitutes clean, efficient SQL code read the article “10 Best Practices to Write Readable and Maintainable SQL Code”. You can also get feedback on your SQL queries, and learn from others, by seeing how other people solve the SQL interview exercises on DataLemur.

Learn SQL Interview Patterns
For advanced SQL interview questions, you’ll need to practice enough questions to internalize the most common SQL interview patterns out there. For example, there’s a non-intuitive way to apply Postgres’s GENERATE_SERIES() command that keeps coming up in SQL interview questions, yet most Data Analysts probably haven’t ever come across this function during their day-to-day SQL work.

That’s why we added SQL pattern tags to the sql interview questions on DataLemur, to help you notice and then intentionally practice the specific SQL interview patterns that come up.

Common SQL Interview Patterns

You can also read this article on common SQL interview patterns.

SQL Interview Patterns

Preparing for SQL Assessments
Simply knowing the SQL concepts that commonly show up in online SQL assessments isn't enough. I recommend creating a study plan that allots oodles of time to practice the concepts too if you want to crack the SQL interview.

How long does it take to study for a SQL interview?
From analyzing data from 12,000 DataLemur.com users, we found it takes SQL beginners 3-6 months to pass the toughest SQL interview questions. For Data Analysts and Data Scientists who’ve used SQL extensively at work, it takes 30 to 60 days to ace SQL interview questions at companies like Amazon, Google, and Facebook. However, if you’ve only got a few days or hours to cram for a SQL assessment, checkout this guide on how to cram for SQL assessments.

The best way to know the appropriate amount of time to dedicate to studying is by solving a real easy, medium, and hard SQL interview question from DataLemur. If you struggle on the easy question, you know you've got your work cut out for you!

Easy Amazon SQL Interview Question: Average Review Ratings
Medium Difficulty Spotify SQL Interview Question: Top 5 Artists
Hard Facebook SQL Interview Question: Active User Retention
I have a SQL assessment in a month. What should I study?
If you’ve got an interview in a month, but don’t know much SQL, check out this 30-day SQL learning roadmap which covers the best FREE online SQL resources (in what order to study them).

Learn SQL in 30 Days Roadmap

However, because SQL is so core to Data Analytics & Data Science, I recommend giving yourself more than a month to learn SQL, and sitting down with a more comprehensive book to learn SQL. And in case you find learning SQL boring, play these 4 SQL games to make learning more fun!

What are the best books for SQL interviews?
The 3 best books to get ready for a SQL interview are:

SQL for Data Scientists: A Beginner's Guide for Building Datasets for Analysis
Minimum Viable SQL Patterns: Hands on Design Patterns for SQL
Ace the Data Science Interview: 201 Real Data & SQL Interview Questions
3 Best Books To Prep For SQL Interviews

SQL for Data Scientists: The Best Book To Learn SQL For Data Nerds
The book "SQL for Data Scientists" is an excellent resource specifically designed for data nerds (compared to other more general books, which cover obscure database details geared towards database administrators. While not specifically geared towards SQL interview prep, it covers all the main topics which you'll find during an interview, like joins, window functions, subqueries, and data prep for ML.

Minimum Viable SQL Patterns: Best Book To Learn SQL Best Practices
The eBook Minimum Viable SQL Patterns will take your SQL code to the next level. This is for folks who want their SQL queries to be more efficient, readable, and maintainable – things that experienced hires are judged on during SQL interviews!

Ace the Data Science Interview: Best Book For SQL Exercises
Finally, the book Ace the Data Science Interview has an entire chapter with 30 real SQL & Database Interview questions, along with a guide on how to prepare for them. I like this book, but then again I’m biased because I wrote it!

To get a complete list of book recommendations, check out this list of the 17 best books for Data Analysts and the 13 best Data Science books.

What's the best site to practice SQL interview questions?
The 3 most popular sites to practice SQL interviews are:

LeetCode - https://leetcode.com/
HackerRank - https://www.hackerrank.com/
DataLemur – https://datalemur.com/
I believe that DataLemur is the best SQL interview platform because it is the most affordable option (half the cost of LeetCode), features the best solutions and hints, and has the most generous free tier.

Want proof? Start practicing with this free TikTok SQL question to see what I mean:

Practice Problem
TikTok SQL Question: Write a query to find the activation rate of new users


While practicing from these online SQL interview platforms is great, we want to acknowledge that tackling open-ended take-home SQL challenges is a whole other beast.

Open-Ended Take-Home SQL Challenges
What makes open-ended SQL tests trickier than online SQL assessments?
Open-ended SQL challenges typically use much larger, messier, and more realistic datasets than SQL assessments. For example, in a take-home challenge you might get some anonymized data from the company’s production database, which is filled with missing data or nulls. In timed SQL coding screens, you’re usually querying a clean toy dataset that might only be 20-30 rows big.

The scope of an open-ended SQL challenge is much bigger too. Whereas in an online SQL assessment, there’s a specific question with well-defined inputs and outputs, for take-home SQL challenges it might not even be obvious what question you need to answer! Some startups in their SQL take-home challenges just give you a large CSV file along with vague prompt like “From this data, what recommendations do you have for our business?” and it’s up to you to determine what specific questions you’ll ask of the dataset.

Of course, the timeframe for take-home SQL challenges is much longer too. Plus, you can usually pick which version of SQL to use too! However, this is both a blessing and a curse. While you typically have more freedom, the expectations around the cleanliness of your SQL code are much, much higher.

Finally, an open-ended SQL take-home challenge tests for much more than just raw SQL skills. Typically, you’ll have to write a report about what you did, which tests your written communication skills. You might even be asked to visualize the data, which tests your data visualization skills as well. Lastly, over a Zoom call, you might be asked to present your analysis, and defend the work you did, which evaluates your oral communication and presentation skills.

How do you prepare for take-home SQL interview challenges?
The best way to prepare for open-ended SQL interview challenges is by practicing real take-home SQL interview challenges, doing exploratory data analysis with Kaggle datasets. and reading books to improving your data analytics skills.

4 Real Take-Home SQL Interview Challenges
Because practice makes perfect, here’s 4 real take-home SQL interview challenges from PayPal, CVS Health, Asana, and UnitedHealth Group:

PayPal/Braintree Analytics Code Challenge
4-Part CVS Pharmacy Analytics SQL Challenge
Asana Take Home Data Science Challenge
4-Part UnitedHealth Group Patient Support SQL Analysis
You can also make your own open-ended SQL challenges using data from Kaggle if you want more practice!

Using Kaggle To Improve Your Ability To Answer Open-Ended SQL Questions
If you don’t know about Kaggle, you are missing out. While they typically host Data Science & Machine Learning competitions, where people build neural network models in Python or R, you can use Kaggle to improve your SQL skills too.

First find an interesting dataset on Kaggle and download the CSVs onto your laptop. Next, load the data into a free database tool like dBeaver so you can query it in the SQL flavor of your choice. Then brainstorm a list of questions you think a business stakeholder might have about the data. Finally, get querying – do the best you can to write SQL queries that answer these hypothetical open-ended analytics questions.

For bonus points, you can even visualize the results in an interactive Tableau dashboard, and turn this into a full-fledged data analytics portfolio project.

How To Translate Vague Business Questions Into SQL
To improve your ability to handle ambiguous data analytics take-home challenges like “Use SQL to find us some business insights in this dataset” my go-to resource is Lean Analytics.

Lean Analytics explains the most important metrics associated with business models like SaaS, freemium consumer apps, 2-sided marketplaces, and e-commerce brands. By knowing what numbers decision makers generally care about, you’ll be able to narrow down the scope of your SQL queries to only answer the questions that truly matter.

Read Lean Analytics to ace take-home SQL challenges

I also recommend improving your business-acumen by reading books like “The Personal MBA” and Boston Consulting Group’s book “On Strategy”, which you can find more details about in my list of the the best business books for Data Scientists.

Beyond SQL: Other Data Interview Topics
What technical concepts do Data Analyst interviews cover (besides SQL)?
SQL is just one tool in the Data Analyst toolbox, and anyways it’s not the tool that matters (or its size), it’s how you use it 😉. That’s why Data Analyst interviews go beyond just SQL questions, and ask technical interview questions like:

Probability Interview Questions:: Basics about conditional probability, Bayes' theorem, random variables
Statistics Interview Questions:: Topics include measures of central tendency (e.g., mean, median, mode), measures of dispersion (e.g., variance, standard deviation), hypothesis testing (t-tests, ANOVA, and chi-squared tests) and sometimes linear regression analysis for senior Data Analyst interviews.
Data Visualization Questions: Usually tested in-directly, by examining a data analytics portfolio project of yours, or judging a how you visualized the results from an open-ended SQL take-home test.
Business Acumen Questions: You’ll usually get an open-ended case question like “You're launching a new AWS database service. What metrics would you measure to know if the launch went well or not?”. Here you’ll have to walk through your knowledge of financial and product metrics, and be evaluated on your general business-savvy. You’ll also be asked about your past work experience, and how you communicated your data analysis results to stakeholders.
What do Data Science interviews cover besides SQL?
Much like a Data Analyst interview, Data Science interviews cover way more topics than just SQL. You can expected to be asked:

Probability & Statistics Questions
A/B Testing Questions
Machine Learning Questions
Database Design Questions
Coding Questions (usually in Python)
Product-Sense Questions
Behavioral Interview Questions
For a concrete example of what to expect
Introduction to Product Metrics
The first step in developing a product strategy is to identify the right metrics and KPIs to measure the success of your product. In this course, you will learn how to identify the right metrics for your product.

0 of 6 Completed

Diagnosing and Investigating Metrics
Diagnosing and Investigating Metrics
Investigating metrics is a type of product intuition problem that will come up frequently in interviews. Examples of this are typically phrased along the lines of - If X metric is up/down by Y percent, how would you investigate it?

0 of 12 Completed

Measuring Success
Measuring Success
Measuring the success of products is critical to data science and analytics interviews. Generally, this question is an encapsulation of every time a product manager or executive asks the question: “So, how is it doing?”.

0 of 11 Completed

Feature Change
Feature Change
Before launching a feature, we can imagine that the first step we’d have to take is analyzing the existing data in our product to make a decision about exactly what to build. This process is what creates the building or change of a feature problem that gets asked in product interviews.

0 of 10 Completed

Metric Trade-Offs
Metric Trade-Offs
Metric trade-off type questions can occur on their own in product interviews or as part of a larger product or AB testing interview discussion.

0 of 4 Completed

Growth
Growth
Learn what growth interview questions are and why they are distinct from all the previous questions we’ve covered.


Introduction to SQL Query Patterns
Get an overview of SQL query patterns and how they improve problem-solving in real-world tasks and interviews.

Imagine we are preparing for a data analytics interview.

One task is to find the most returned product by category in an online store. It is not a simple lookup involving joins, grouping, and filtering. We might not immediately remember the exact SQL syntax, but if we recognize the problem as a common Group bucket pattern, we know exactly how to approach it.

This demonstrates the power of SQL query patterns.

They provide a clear framework for solving real-world problems efficiently. In this lesson, we’ll discover SQL query patterns and why they matter in both interviews and on the job. We’ll learn how patterns offer reusable structures for solving common query challenges and how recognizing them can drastically improve our speed and clarity.

We’ll also introduce the concept of thinking in patterns, similar to how platforms like LeetCode encourage coding. This time, however, we’ll apply it to SQL.

In this lesson, we will:

Understand what SQL query patterns are.

Explain why patterns improve SQL problem-solving.

See how pattern recognition helps in interviews and real-world SQL tasks.

What are SQL query patterns?
When working with SQL, we often encounter the same problems repeatedly: finding totals per group, filtering for recent records, identifying gaps in sequences, comparing datasets, and more. Instead of writing queries from scratch each time, we can use query patterns, generalized templates that help solve recurring problems confidently.

These patterns aren’t about memorizing code; they’re about understanding how a problem can be solved and why certain query structures work. Much like design patterns in programming, SQL query patterns give us reusable solutions that adapt to different datasets and business questions.

Why are they important?
Using SQL patterns helps us:

Solve faster by starting with a structure already proven to work.

Avoid errors by following familiar, reliable query designs.

Focus on the logic of the problem instead of low-level syntax.

Communicate clearly with others by using consistent approaches.

For example, imagine we are asked, “Which product category had the highest average monthly sales last month?” This is not a unique problem; it is a variation of a common aggregation pattern where we group by category and calculate an average.

Once we recognize this pattern, the query becomes much more manageable.

Why do SQL patterns improve problem-solving?
Structured approaches help us navigate complex logic with greater confidence.

When we face new problems, we can draw on a mental “catalog” of patterns, like grouping and aggregating to get summary insights, or comparing two data sets to see differences. This speeds up our reasoning and reduces the chance of missing corner cases.

It also greatly benefits teams.

When everyone on a project understands common patterns, they share a vocabulary for discussing and refining queries. This boosts collaboration and helps new team members come up to speed faster because they work with familiar query templates.

This is powerful, especially in technical interviews.

Like LeetCode problems often fall into well-known categories (e.g., two-pointer methods, binary search), SQL challenges follow similar patterns. Mastering patterns means not solving from scratch; we are selecting the right approach from a toolkit.

LeetCode style pattern recognition
Coding platforms like LeetCode often present algorithmic challenges that hinge on recognizing certain problem shapes, whether sorting, searching, or using data structures. For example:

Sliding Window for string problems

Binary Tree Traversal for tree problems

SQL challenges follow a similar logic. When we learn common SQL patterns, we acquire a toolkit that helps us quickly map a new task to a known solution strategy. This is especially helpful in technical interviews, where time is limited and clarity is crucial.

We have curated the following SQL patterns to categorize problems:

Aggregation patterns for totals, averages, and counts

Filtering patterns for selecting data based on logic

Comparison patterns for identifying differences between rows or tables

Sequencing patterns for handling row order and relationships

Transformation patterns for reshaping data (like pivoting)
Introduction to SQL
Structured Query Language (SQL) is a powerful and widely used language for managing and manipulating relational databases. SQL allows you to interact with databases to store, retrieve, update, and delete data. In this section, we will cover the fundamental concepts and syntax of SQL.

Database Basics
A database is an organized collection of data stored in a structured format. It consists of tables, which hold the data, and relationships between the tables. Each table consists of rows (also known as records) and columns (also known as fields). Columns define the type of data that can be stored, such as text, numbers, or dates.

SQL Statements
SQL operates through various statements that allow you to perform different actions on the database. The most common SQL statements are:

SELECT: Retrieves data from one or more tables.
INSERT: Adds new data into a table.
UPDATE: Modifies existing data in a table.
DELETE: Removes data from a table.
Syntax and Structure
SQL statements follow a specific syntax and structure. Here's a basic structure of a SELECT statement:

SELECT column1, column2, ...
FROM table_name
WHERE condition;
SELECT specifies the columns you want to retrieve from the table.
FROM specifies the table you want to retrieve data from.
WHERE (optional) specifies the conditions for filtering the data.
Querying Data
To retrieve data from a database, you use the SELECT statement. You can specify the columns you want to retrieve and apply various conditions to filter the data. Here's an example of a simple SELECT statement:

SELECT column1, column2
FROM table_name;
This statement retrieves the values from column1 and column2 in the table_name table.

Filtering Data
You can filter the retrieved data using the WHERE clause. It allows you to specify conditions to match specific records. For example:

SELECT column1, column2
FROM table_name
WHERE condition;
The condition can be a comparison between columns or values using operators like =, <>, <, >, <=, >=. You can also use logical operators like AND, OR, NOT to combine multiple conditions.

Sorting Data
You can sort the retrieved data using the ORDER BY clause. It allows you to specify the columns to sort the data by. For example:

SELECT column1, column2
FROM table_name
ORDER BY column1 ASC, column2 DESC;
This statement sorts the data in ascending order based on column1 and descending order based on column2.

Modifying Data
In addition to retrieving data, SQL allows you to modify the data stored in a database. This section covers the basic SQL statements for inserting, updating, and deleting data, and explains their impact on the database.

Inserting Data
To add new data into a table, you use the INSERT statement. Here's an example:

INSERT INTO table_name (column1, column2, ...)
VALUES (value1, value2, ...);
This statement inserts a new row into table_name with the specified values for column1, column2, and so on.

Updating Data
The UPDATE statement is used to modify existing data in a table. Here's an example:

UPDATE table_name
SET column1 = value1, column2 = value2, ...
WHERE condition;
This statement updates the values of column1, column2, and so on in table_name that match the specified condition.

Deleting Data
To remove data from a table, you use the DELETE statement. Here's an example:

DELETE FROM table_name
WHERE condition;
This statement deletes the rows from table_name that match the specified condition.

Note: Modifying data in a database should be done with caution, as it can permanently alter or remove data. Always double-check your statements and ensure they are targeting the correct data before executing them.

Data Types and Constraints
SQL supports various data types to store different kinds of data in tables. Additionally, you can apply constraints to enforce rules and maintain data integrity. Here are some commonly used data types and constraints:

Data Types
INTEGER: Represents whole numbers.
FLOAT: Represents floating-point numbers.
VARCHAR: Represents variable-length character strings.
DATE: Represents a date without a time component.
BOOLEAN: Represents true or false values.
These are just a few examples, and different database systems may support additional data types.

Constraints
Primary Key: Ensures the uniqueness of a column's value in a table, typically used to uniquely identify each row.
Foreign Key: Establishes a relationship between two tables, enforcing referential integrity.
Unique Constraint: Ensures the uniqueness of values in one or more columns.
Check Constraint: Defines a condition that must be true for a row to be valid.
These constraints help maintain data integrity, enforce data relationships, and prevent invalid data from being inserted or modified.

Understanding data types and constraints is crucial for designing and creating well-structured databases that accurately represent the real-world entities and relationships.

This section has covered the basics of modifying data in a database using SQL statements. It has also introduced data types and constraints that help define the structure and integrity of the data.

As you progress, you'll explore more advanced techniques and features of SQL, including working with multiple tables, aggregating data, and optimizing query performance.

Joins and Relationships
In a relational database, data is often spread across multiple tables, and relationships are established between them. Understanding relationships and using JOIN statements allows you to retrieve related data from multiple tables efficiently.

Relationships in Databases
There are three common types of relationships in databases:

One-to-One: A relationship where each record in one table is associated with at most one record in another table.
One-to-Many: A relationship where each record in one table can be associated with multiple records in another table.
Many-to-Many: A relationship where records in both tables can be associated with multiple records in the other table.
Establishing proper relationships between tables helps organize and structure the data effectively.

JOIN Statements
JOIN statements are used to combine rows from different tables based on related columns. Here are the main types of JOINs:

INNER JOIN: Retrieves rows that have matching values in both tables being joined.
LEFT JOIN: Retrieves all rows from the left table and matching rows from the right table (if any).
RIGHT JOIN: Retrieves all rows from the right table and matching rows from the left table (if any).
FULL JOIN: Retrieves all rows from both tables, including matching and non-matching rows.
JOIN statements allow you to fetch data from multiple tables, leveraging the relationships established between them.

Aggregation and Grouping
Aggregation functions in SQL, such as SUM, AVG, COUNT, and others, enable you to summarize and calculate values from a set of rows. The GROUP BY clause is used in conjunction with these functions to group rows based on one or more columns.

Aggregating Data
Aggregate functions perform calculations on a set of rows and return a single result. For example:

SUM: Calculates the sum of a column's values.
AVG: Calculates the average of a column's values.
COUNT: Returns the number of rows in a group.
MIN: Retrieves the minimum value from a column.
MAX: Retrieves the maximum value from a column.
These functions allow you to derive meaningful insights and statistical calculations from your data.

Grouping Data
The GROUP BY clause is used to group rows based on one or more columns. It allows you to divide the data into logical subsets and apply aggregate functions to each group individually. For example:

SELECT column1, aggregate_function(column2)
FROM table_name
GROUP BY column1;
This statement groups the rows based on column1 and applies the aggregate function to each group.

Subqueries and Views
SQL subqueries provide a way to nest one query inside another. They can be used to create more complex queries and retrieve data from multiple tables simultaneously.

Views, on the other hand, are virtual tables based on the result of a query. They simplify data retrieval by providing a predefined query that can be treated as a table.

Subqueries
A subquery is a query embedded within another query. It can be used in the WHERE or FROM clause of the outer query to retrieve data based on intermediate results. Subqueries allow you to break down complex problems into smaller, more manageable parts.

Views
Views are saved queries that act as virtual tables. They can be created using a SELECT statement and provide an abstraction layer over the underlying tables. Views simplify data retrieval by encapsulating complex queries into a single, reusable entity.

This section has covered the concept of relationships in databases, JOIN statements to retrieve related data, aggregation functions and the GROUP BY clause for summarizing data, and the usage of subqueries and views to handle complex queries.

By understanding these concepts, you'll be able to work with more advanced SQL queries, manipulate data effectively, and gain valuable insights from your databases.

Indexing and Performance Optimization
Indexes play a crucial role in enhancing the performance of SQL queries by improving data retrieval speed. Understanding how to create and use indexes effectively is essential for optimizing database performance.

Importance of Indexes
Indexes are data structures that provide quick access to specific data within a table. They enable the database engine to locate data faster by reducing the number of rows that need to be scanned. Indexes are created on one or more columns and significantly enhance query performance, especially for large tables.

Creating Indexes
To create an index, you need to identify the columns that are frequently used in search conditions or join operations. Using the CREATE INDEX statement, you can specify the index name, the table on which the index will be created, and the column(s) to be indexed. For example:

CREATE INDEX idx_name ON table_name (column1, column2);
Creating indexes on appropriate columns can significantly speed up query execution.

Using Indexes Effectively
While indexes boost performance, they also come with some overhead. It's essential to strike a balance between the number of indexes and their impact on data modification operations (inserts, updates, and deletes). Remember to update indexes when modifying data to ensure their accuracy.

Regularly analyze query performance, monitor index usage, and consider adding or removing indexes based on actual usage patterns. Proper indexing strategy is crucial for optimizing database performance.

Transactions and Concurrency Control
In a multi-user database environment, transactions ensure data integrity and maintain consistency. Understanding transactions and concurrency control is vital when dealing with concurrent database operations.

Transactions and ACID Properties
A transaction is a logical unit of work that consists of one or more database operations. Transactions adhere to the ACID properties:

Atomicity: A transaction is treated as a single, indivisible unit of work. Either all operations within a transaction are committed, or none of them are.
Consistency: Transactions bring the database from one consistent state to another consistent state. The integrity of the data is maintained.
Isolation: Concurrently executing transactions are isolated from each other, ensuring that the intermediate states of transactions are not visible to other transactions.
Durability: Once a transaction is committed, its changes are permanently saved and can survive system failures.
Understanding the ACID properties helps ensure data integrity and reliability in database operations.

Isolation Levels
Isolation levels define the degree of isolation and concurrency control in database transactions. They determine how transactions interact with each other and impact data consistency.

Common isolation levels include:

Read Uncommitted: Allows dirty reads and has the lowest level of isolation.
Read Committed: Prevents dirty reads, but non-repeatable reads and phantom reads are possible.
Repeatable Read: Guarantees consistent reads within a transaction, but phantom reads may occur.
Serializable: Provides the highest level of isolation, ensuring that transactions are executed as if they were processed sequentially.
Understanding isolation levels helps manage concurrent transactions and maintain data consistency.

Advanced Topics
SQL offers advanced features that extend its capabilities beyond simple queries. Exploring these advanced topics opens up new possibilities for efficient data management and automation.

Stored Procedures
Stored procedures are precompiled SQL code that can be stored and executed on the database server. They encapsulate a set of SQL statements as a single unit, enabling code reuse, improved performance, and enhanced security. Stored procedures can accept input parameters and return output values.

Triggers
Triggers are special SQL constructs that automatically execute in response to specific database events, such as INSERT, UPDATE,

or DELETE operations on tables. Triggers enable you to enforce business rules, maintain data integrity, and automate complex database actions.

User-Defined Functions
User-defined functions (UDFs) allow you to extend SQL by creating custom functions. UDFs encapsulate specific logic and can be used within SQL statements just like built-in functions. They provide a way to modularize complex calculations or data transformations, improving code readability and reusability.

Exploring these advanced topics will expand your SQL skills and empower you to build more sophisticated database solutions.

Keep learning, practicing, and experimenting with SQL to become proficient in handling diverse data scenarios.

Best Practices
Writing efficient and maintainable SQL code is essential for building robust and scalable database applications. Here are some best practices to follow:

Naming Conventions
Use descriptive names for tables, columns, and other database objects. Choose names that accurately represent the data they store or the purpose they serve. Consistent and meaningful naming conventions improve code readability and maintainability.

Code Formatting
Consistent code formatting enhances readability and makes it easier to understand SQL statements. Indentation, proper spacing, and line breaks improve code structure and organization. Consider using a code formatter or adhering to a style guide for consistent formatting.

Error Handling
Implement error handling mechanisms in your SQL code to gracefully handle unexpected scenarios. Use structured error handling constructs provided by your database system, such as TRY-CATCH blocks, to catch and handle errors effectively. Proper error handling improves code reliability and maintainability.
Introduction to Statistics
Statistics For Data Science

11 min read
Descriptive Statistic

5 min read
What is Inferential Statistics?

7 min read
Bayes' Theorem

13 min read
Probability Data Distributions in Data Science

8 min read
Parametric Methods in Statistics

6 min read
Hypothesis Testing

9 min read
ANOVA for Data Science and Data Analytics

9 min read
Bayesian Statistics & Probability

6 min read
/

02Week 2: Maths for Data Analytics
Descriptive Statistics: Mean, Median, Mode, Variance, Standard Deviation
Range, Quartiles, Percentiles
Probability: Basic Concepts & Distributions (Normal, Binomial, Poisson, etc.)
Covariance and Correlation
Hypothesis Testing: CLT, Z-test, T-test, ANOVA, MANOVA
Non-parametric Tests: Mann-Whitney, Kruskal-Wallis
Data Skewness Detection and Handling


https://42dm.net/10-inbound-marketing-examples-for-b2b/
https://backlinko.com/templates/marketing
https://blog.coupler.io/marketing-dashboard/
https://careers.sapiens.com/job/New-Jersey%2C-Rochelle-Park-Marketing-Manager-NJ/957858201/
https://careers.sapiens.com/job/Remote-Marketing-Manager-OR-97458/957858201/
https://coschedule.com/content-marketing/types-of-content
https://creatorswithinfluence.com/wp-content/uploads/2025/02/AIC-Creators-Trailblazing-to-the-Top-List-Analyzed-by-HypeAuditor.pdf?utm_source=tldrmarketing
https://diggitymarketing.com/technical-seo-audit/
https://files.demandmetric.com/sites/demandmetric.com/files/images/marketing_strat_banner_.jpg
https://filestage.io/blog/marketing-operations/
https://grubhub.wd1.myworkdayjobs.com/External/job/USA-IL-Chicago/Marketing-Manager---Paid-Search-SEM_JR1000000522?source=Linkedin
https://hotdogmarketing.net/develop-marketing-growth-strategy-framework-success/
https://images.datacamp.com/image/upload/v1676302827/Marketing/Blog/Data_Wrangling_Cheat_Sheet.pdf
https://images.squarespace-cdn.com/content/v1/5e7eaf7501b2e061b53d2525/f438d6b9-3404-4871-ba52-f1182b559498/marketing-planning-process.png?format=500w
https://isbndb.com/search/books/marketing?edition=&year=2015&language=en&column=
https://medium.com/@mikeciccolella/7-tips-for-more-effective-b2b-demand-generation-lead-generation-marketing-sales-974137f9a43b
https://medium.com/@rikwalters/the-evolution-of-digital-marketing-strategy-in-the-enterprise-1b9687a85e09
https://medium.com/marketing-and-entrepreneurship/i-spent-30-000-testing-different-blog-designs-heres-what-i-found-8952bf057b8f
https://medium.com/strategic-content-marketing/do-you-need-a-marketing-technologist-to-run-demand-generation-campaigns-9cbf3c37bc29
https://medium.com/tag/growth-marketing
https://medium.ioninteractive.com/supercharge-your-demand-generation-with-interactive-content-marketing-c6ba65901677
https://mycareer.verizon.com/jobs/r-1006503/marketing-manager/?source=jb-linkedIn
https://nodeflair.com/companies/meta/interviews/growth-marketing-analyst
https://nvidia.wd5.myworkdayjobs.com/NVIDIAExternalCareerSite/job/US-CA-Santa-Clara/Senior-Product-Marketing-Manager--Data-Center-AI_JR1967423?source=jobboardlinkedin
https://pierreherubel.substack.com/p/my-full-b2b-positioning-playbook?utm_source=tldrmarketing
https://redwavelite.com/wp-content/uploads/2023/07/Types-of-Marketing.jpg.webp
https://search.worldcat.org/search?q=marketing&datePublished=2020-2024&inLanguage=eng&itemSubType=book-printbook%2Cbook-digital%2Cbook-thsis%2Cbook-mss%2Cbook-largeprint%2Cbook-continuing&limit=10&offset=1&orderBy=bestMatch&itemSubTypeModified=book-printbook%2Cbook-digital%2Cbook-thsis%2Cbook-mss%2Cbook-largeprint%2Cbook-continuing
https://taglab.net/product/looker-studio-marketing-data-visualization-guide-e-book/
https://towardsdatascience.com/building-a-marketing-data-science-team-from-scratch-9988fc30ad89
https://unito.io/blog/mastering-facebook-video-insights-a-comprehensive-guide-to-elevate-your-marketing-strategy/
https://userpilot.com/blog/growth-marketing-framework/
https://wpengine.wd1.myworkdayjobs.com/WP_Engine/job/WFH---Texas/Lead-Generation-Marketing-Manager_JR101835?__jvst=JobBoard&__jvsd=LinkedIn_Paid_Job&source=Linkedin
https://www.aimtechnologies.co/wp-content/uploads/2024/02/Types-of-Marketing-Research.jpg
https://www.algorithmicmarketingbook.com/
https://www.dashly.io/blog/inbound-marketing-funnel/
https://www.data-axle.com/resources/blog/the-ultimate-predictive-marketing-roadmap/
https://www.datadab.com/blog/multi-touch-attribution-models-the-what-why-and-how-for-marketing-automation-marketers/
https://www.demandmetric.com/content/marketing-strategy-framework
https://www.designrush.com/agency/digital-marketing/trends/meme-marketing?utm_source=tldrmarketing
https://www.google.com/imgres?imgurl=https%3A%2F%2Fthecannabismarketingassociation.com%2Fwp-content%2Fuploads%2F2019%2F09%2FDJ-Francis-Most-Successful-Top-of-Funnel-B2B-Demand-Gen-Tactics-Feb2019-300x167.png&tbnid=Jf-Nx_E7C1BTUM&vet=10CEsQMyjaAmoXChMIgP2Sn7nA_wIVAAAAAB0AAAAAEAU..i&imgrefurl=https%3A%2F%2Fthecannabismarketingassociation.com%2F3-inbound-marketing-tactics-to-drive-leads-to-your-cannabis-business%2F&docid=DrTcWgQGPB3r9M&w=300&h=167&q=b2b%20demand%20generation%20strategy&ved=0CEsQMyjaAmoXChMIgP2Sn7nA_wIVAAAAAB0AAAAAEAU
https://www.google.com/imgres?imgurl=https%3A%2F%2Fwww.marketingcharts.com%2Fwp-content%2Fuploads%2F2022%2F09%2FActivateMarketingCharts-B2B-Demand-Gen-Content-by-Funnel-Stage-Sept2022.png&tbnid=FAVK5TIoTlQjnM&vet=10CB8QMyh2ahcKEwigxs7XusD_AhUAAAAAHQAAAAAQAw..i&imgrefurl=https%3A%2F%2Fwww.marketingcharts.com%2Fcustomer-centric%2Flead-generation-and-management-227240&docid=1t3IKsR_jZJWGM&w=1640&h=912&q=b2b%20demand%20generation%20model&ved=0CB8QMyh2ahcKEwigxs7XusD_AhUAAAAAHQAAAAAQAw#imgrc=RGxa_jjaj-uooM&imgdii=OxTvfquMGnlknM
https://www.hubspot.com/marketing-statistics
https://www.idashboards.com/what-it-takes-to-build-a-marketing-data-strategy/
https://www.inturact.com/blog/the-saas-marketing-journey-convert
https://www.klipfolio.com/resources/kpi-examples/marketing
https://www.marketingcharts.com/wp-content/uploads/2014/10/SoftwareAdvice-B2B-Cost-per-Lead-by-Channel-Oct2014.png
https://www.mayple.com/resources/social-media-marketing/facebook-video-ads-best-practices
https://www.ninjacat.io/blog/marketing-data-strategy
https://www.owox.com/blog/articles/digital-marketing-metrics-and-kpis/
https://www.pdfdrive.com/digital-marketing-strategy-an-integrated-approach-to-online-marketing-e179690960.html
https://www.pdfdrive.com/search?q=marketing&pagecount=&pubyear=2015&searchin=&em=
https://www.semrush.com/blog/marketing-campaign/
https://www.shiftparadigm.com/insights/customer-marketing-framework-a-blueprint-for-success-2/
https://www.slidegeeks.com/media/catalog/product/cache/1280x720/O/r/Organizational_Marketing_Playbook_Marketing_Campaign_Data_Stack_And_Process_Flow_Template_PDF_Slide_1.jpg
https://www.smartinsights.com/digital-marketing-strategy/organize-campaign-plan-race-framework/
https://www.thinkwithgoogle.com/marketing-strategies/data-and-measurement/establishing-marketing-data-strategy-collection/
https://www.webfx.com/blog/marketing/how-marketers-can-use-deep-research/?utm_source=tldrmarketing
https://www.wrike.com/blog/the-key-marketing-metrics-your-cmo-actually-cares-about/
https://www.ycombinator.com/companies/optery/jobs/So1pCBQ-director-of-marketing-b2b?utm_source=syn_li
https://www.youtube.com/watch?v=i-R-B2dLRew&ab_channel=TortugaMarketing
https://42dm.net/10-inbound-marketing-examples-for-b2b/
https://backlinko.com/templates/marketing
https://blog.coupler.io/marketing-dashboard/
https://careers.sapiens.com/job/New-Jersey%2C-Rochelle-Park-Marketing-Manager-NJ/957858201/
https://careers.sapiens.com/job/Remote-Marketing-Manager-OR-97458/957858201/
https://coschedule.com/content-marketing/types-of-content
https://filestage.io/blog/marketing-operations/
https://medium.com/@mikeciccolella/7-tips-for-more-effective-b2b-demand-generation-lead-generation-marketing-sales-974137f9a43b
https://medium.com/@rikwalters/the-evolution-of-digital-marketing-strategy-in-the-enterprise-1b9687a85e09
https://medium.com/marketing-and-entrepreneurship/i-spent-30-000-testing-different-blog-designs-heres-what-i-found-8952bf057b8f
https://medium.com/strategic-content-marketing/do-you-need-a-marketing-technologist-to-run-demand-generation-campaigns-9cbf3c37bc29
https://medium.com/tag/growth-marketing
https://pierreherubel.substack.com/p/my-full-b2b-positioning-playbook?utm_source=tldrmarketing
https://search.worldcat.org/search?q=marketing&datePublished=2020-2024&inLanguage=eng&itemSubType=book-printbook%2Cbook-digital%2Cbook-thsis%2Cbook-mss%2Cbook-largeprint%2Cbook-continuing&limit=10&offset=1&orderBy=bestMatch&itemSubTypeModified=book-printbook%2Cbook-digital%2Cbook-thsis%2Cbook-mss%2Cbook-largeprint%2Cbook-continuing
https://taglab.net/product/looker-studio-marketing-data-visualization-guide-e-book/
https://towardsdatascience.com/building-a-marketing-data-science-team-from-scratch-9988fc30ad89
https://userpilot.com/blog/growth-marketing-framework/
https://www.algorithmicmarketingbook.com/
https://www.dashly.io/blog/inbound-marketing-funnel/
https://www.data-axle.com/resources/blog/the-ultimate-predictive-marketing-roadmap/
https://www.datadab.com/blog/multi-touch-attribution-models-the-what-why-and-how-for-marketing-automation-marketers/
https://www.demandmetric.com/content/marketing-strategy-framework
https://www.hubspot.com/marketing-statistics
https://www.idashboards.com/what-it-takes-to-build-a-marketing-data-strategy/
https://www.inturact.com/blog/the-saas-marketing-journey-convert
https://www.klipfolio.com/resources/kpi-examples/marketing
https://www.ninjacat.io/blog/marketing-data-strategy
https://www.owox.com/blog/articles/digital-marketing-metrics-and-kpis/
https://www.pdfdrive.com/digital-marketing-strategy-an-integrated-approach-to-online-marketing-e179690960.html
https://www.pdfdrive.com/search?q=marketing&pagecount=&pubyear=2015&searchin=&em=
https://www.semrush.com/blog/marketing-campaign/
https://www.shiftparadigm.com/insights/customer-marketing-framework-a-blueprint-for-success-2/
https://www.smartinsights.com/digital-marketing-strategy/organize-campaign-plan-race-framework/
https://www.thinkwithgoogle.com/marketing-strategies/data-and-measurement/establishing-marketing-data-strategy-collection/
https://www.webfx.com/blog/marketing/how-marketers-can-use-deep-research/?utm_source=tldrmarketing
https://www.wrike.com/blog/the-key-marketing-metrics-your-cmo-actually-cares-about/
https://www.youtube.com/watch?v=i-R-B2dLRew&ab_channel=TortugaMarketing
https://ahrefs.com/big-data
https://akool.com/?utm_source=tldrfounders
https://dashboard.mailmeteor.com/contact_lists?page_number=1
https://dashthis.com/seo-report-template/
https://makevisionclear.medium.com/maximum-your-lead-conversion-with-powerful-demand-generation-strategy-dd979f926948
https://martechmap.com/stackbuilder2023
https://medium.com/@billy-howell/the-internets-biggest-problem-saas-fatigue-7e15b9072111
https://medium.com/@duanebrown/difference-between-user-acquisition-demand-generation-cbf0939b5b22
https://medium.com/@kamilrextin/edition-9-demand-generation-funnel-e76efb95d2c0
https://medium.com/@mattbenati/turbocharge-your-demand-generation-impact-with-trigger-events-automation-90a95a187b63
https://medium.com/demand-karma/do-you-know-how-ai-will-affect-demand-generation-45138fb3ce3
https://medium.com/high-alpha/scaling-a-demand-generation-engine-4-key-components-9e5ae3c0bfa9
https://medium.com/illumination/hard-but-valuable-lessons-from-demand-generation-ced05ccf8fdc
https://medium.com/swlh/basics-of-demand-generation-aa8b95261656
https://medium.com/tag/conversion-optimization
https://medium.com/tag/conversions-and-funnels
https://medium.com/tag/growth-hacking-strategies
https://medium.com/tag/growth-strategy
https://snipfeed.co/?utm_term=sell%20digital%20products&utm_campaign=Casp+-+Selling+Online&utm_source=adwords&utm_medium=ppc&hsa_acc=2470120223&hsa_cam=21755207962&hsa_grp=171695714367&hsa_ad=714990358702&hsa_src=g&hsa_tgt=kwd-38233462&hsa_kw=sell%20digital%20products&hsa_mt=e&hsa_net=adwords&hsa_ver=3&gad_source=1&gclid=Cj0KCQjw05i4BhDiARIsAB_2wfBt3vliHXU46Ch1RqeMbavTa7yMiGZtGKt47lySIC_GO2BIqzZM3coaApeSEALw_wcB
https://www.adjust.com/glossary/media-mix-modeling/
https://www.elenaverna.com/p/how-to-build-brand-awareness-through?utm_source=tldrproduct
https://www.green-leads.com/learn/b2b-demand-generation
https://www.greenbook.org/market-research-companies/united-states-of-america
https://www.internalresults.com/blog/demand-generation-strategies
https://www.jasper.ai/?adgroupid=163111846019&campaignid=21338182930&utm_source=google&utm_term=jasper&utm_content=&utm_campaign=mm_search_brand_na&utm_medium=cpc&hsa_acc=5024544109&hsa_cam=21338182930&hsa_grp=163111846019&hsa_ad=700760558871&hsa_src=g&hsa_tgt=kwd-44639161&hsa_kw=jasper&hsa_mt=e&hsa_net=adwords&hsa_ver=3&gad_source=1&gclid=Cj0KCQiA88a5BhDPARIsAFj595gr_qwJQBjWRRT1QTh2_AtwJi29PNkAlf3HNaXcZ2Vge1bd3SLNqiMaAuHdEALw_wcB
https://www.lifesight.io/
https://www.linkedin.com/pulse/overreliance-failure-when-saas-companies-depend-too-much-drazdoff-o058e
https://www.relume.io/
https://www.relume.io/?via=Chwanc
https://www.youtube.com/watch?v=Ei-pYtYS6UY&ab_channel=Neo4j
https://www.youtube.com/watch?v=gKmwUuYcNEQ&ab_channel=NextBigIdeaClub
https://www.youtube.com/watch?v=rKOx5qlLyaA&ab_channel=TheDiaryOfACEO


Note: This post has now been turned into a book. You can get EPUB and PDF versions on Gumroad or get the Kindle version on Amazon

When I first learned SQL in college, it was truly from the ground up. The database course in the computer science curriculum had us studying relational algebra and set theory. We wrote all our answers to queries in a weird math notation and didn’t touch a database until mid-semester. I only learned SQL properly once I got a job and finally started using it.

If you pick up a SQL introductory book or course, it follows the same “pyramid” pattern. You start with the language basics, you learn the syntax, then you build up from there to increasingly complex concepts.

That way of learning rarely sticks.

If you think about anything you remember really well, you most likely learned it by mapping problems to solutions. More importantly, by mapping many problems to solutions, you start to learn patterns that allow you to recognize and solve certain types of problems instantly.

Experienced analysts and data scientists are able to solve complex queries quickly because they’ve built up a collection of patterns and best practices that go beyond the SQL syntax. They use these patterns to break down these complex queries into simple elements and solve them quickly.

Actually patterns exist in every field.

Chefs don’t create recipes from scratch. They use common cooking patterns, like sautéing vegetables, browning meat, making dough, using spices, etc. to create delicious meals.

Likewise fiction writers use character and plot patterns like romantic comedy, drama, red herring, foreshadowing, cliffhangers, etc.

In programming they’re called design patterns.

What are these design patterns?
They’re basically mental constructs that act like LEGO pieces for your mind. Like LEGO, they enable you to find solutions to complex problems by mixing and matching.

Whether you are aware of it or not, if you have a lot of experience in a field, you’re using patterns to quickly find solutions to novel problems in your field in just minutes.

Studying and learning patterns is the fastest way to level up in any field. The problem is that you need to spend years in that field to learn them and even when you do, you end up learning them haphazardly.

Also nobody teaches this way. You’d need an expert in the field to codify them in a way that are easy to learn and easy to teach.

I’ve been writing SQL for ~15 years. I’ve seen hundreds of thousands of lines of code. Over time I developed a set of patterns and best practices I always come back to when writing queries. I’ve codified and organized them into best practices so you can learn them and start using them right away.

If you’re starting out as a data analyst, scientist or engineer and you study these patterns you’ll be able to:

Level up your SQL skills in record time without taking yet another boring course
Write high-quality, production-ready SQL that’s easy to understand and maintain
Solve complex queries like an expert without having to wait decades to become on
The following are only a subset of what I’ve decoded so far.

So without further ado let’s get into it.

Top 10 SQL Design Patterns and Best Practices
Pattern 1: Always use CTEs
When writing a complex query it’s a good idea to break it down into smaller components. As tempting as it might be to solve the query in one step don’t. CTEs make your query easier to write and maintain in the future.

CTEs or Common Table Expressions are SQL queries you define inside a query and use as a temporary result. They help by breaking down large complex queries into easier, more manageable ones.

You define a single CTE using the WITH keyword like this:

WITH <cte_name> AS (
	SELECT col1, col2
	FROM table_name
)
SELECT *
FROM <cte_name>
You can define multiple CTEs again using WITH keyword like this:

WITH <cte1_name> AS (
	SELECT col1
	FROM table1_name
)
, <cte2_name> AS (
	SELECT col1
	FROM table2_name
)
SELECT *
FROM <cte1_name> AS cte1
JOIN <cte2_name> AS cte2 ON cte1.col1 = cte2.col1
This should suffice to whet your appetite. I can’t get into more details here because that would require a longer article.

Pattern 2: Keep CTEs small and single purpose
Your CTEs needs to be an encapsulated logical components that help you build your final query quickly and easily. They shouldn’t try to do too much. You can then mix and match the CTEs to solve just about any data request. This also makes the CTEs easy to test as you build them.

You can do just about anything you can do in SQL inside a CTE:

Pre-Filter data to the desired subset before joining later, which speeds up queries
Pre-Aggregate data in order to create custom groupings used later
Pre-Calculate a metric that’s used later in another calculation
etc.
Pattern 3: Start with the ground truth
The ground truth means to start with definitions. The definitions will lead you to the tables you will then need to build your final query.

Let’s say you’re being asked to figure out how many users visited a certain page on the website. Unless you’re very familiar with your data or have done this analysis before, your first questions should be:

What’s the definition of a user? Is it a web session or only those who have signed in to the website?
What’s the definition of a visit? Is it when they viewed the page? If so, what’s the minimum amount of time spent on the page in order for it to count as a visit?
How do we know which page they visited? Is it only available in the URL? If so is there a pattern in the URL that defines it or is it available elsewhere in the system?
Each of these definitions will tell you where to go and find the data. By building your queries on top of ground truth, you ensure that final result will likely be correct and true.

Pattern 4: Combine CTEs to solve any query
As we talked about earlier you can combine multiple CTEs to build up a solution to a complex query.

There are a couple of ways you can do that:

By chaining them
By nesting them
We saw an example of chaining earlier. This is where you define queries from separate tables in each CTE and then you combine them later.

WITH <cte1_name> AS (
	SELECT col1
	FROM table1_name
)
, <cte2_name> AS (
	SELECT col1
	FROM table2_name
)
SELECT *
FROM <cte1_name> AS cte1
JOIN <cte2_name> AS cte2 ON cte1.col1 = cte2.col1
This pattern is great for breaking up large queries into simpler elements

You can also define a CTE based on a previous CTE thereby nesting them. Just keep in mind many database systems have limits on how deep this nesting can be.

WITH <cte1_name> AS (
	SELECT col1
	FROM table1_name
)
, <cte2_name> AS (
	SELECT col1
	FROM <cte1_name>
)
SELECT *
FROM table2_name
JOIN <cte2_name> AS cte2 ON cte1.col1 = cte2.col1
This pattern is great for multi-step calculations.

Pattern 5: Don’t Repeat Yourself (DRY)
If you find yourself joining the same table multiple times or doing the same calculation, aggregation or filtering, that query chunk should be placed in a CTE. Not only does this refactoring make the code easier to read, but it can also identify chunks of code that could be used across multiple queries which can be made into views.

Pattern 6: Don’t mix layers of abstraction
Modern data warehouses have to deal with billions of rows of data and return results in seconds. One of the many ways this is achieved is by building base tables and views that de-normalize transactional data. Reporting is then built on top of these de-normalized tables.

Often there are two or more layers of intermediate tables added before a reporting table is generated because they’re combining data from multiple applications. This creates multiple levels of abstraction.

When an analyst or data scientist wants to add a missing column to a report, a quick solution would be to join the reporting table (that is 3 or 4 levels of abstraction downstream of source data) directly to a source table thus mixing layers of abstraction. This is one of the biggest sources of tech debt in data warehouses today.

It’s better (but more painful) to add the needed column to all the intermediate layers before exposing it to the reporting layer.

Pattern 7: Reduce your data before joining
When working with large tables, one of the easiest ways to speed up query performance is to reduce your data in a CTE by pre-filtering or pre-aggregating before joining it later.

For example if you only need one month of data from a large table, create a CTE that pre-filters the data down before joining it later. There might even be a partition or index set up on that data column making your query really fast.

Pattern 8: Only work with the smallest subset of columns you need
It’s very tempting to do SELECT * in all your queries or CTEs, especially if you don’t know which columns you need later.

Cloud data warehouse use columnar storage which means that you can make your query a lot faster by selecting only the subset of columns you need. Some of them even charge you based on how much data you’re scanning so when you only choose the columns you need, you’re also saving money in the long run.

Pattern 9: Expect the unexpected
From NULLs, to missing data, duplicate rows and random values, real world data is messy. A well-written query is robust enough to handle many of these cases without crashing or giving inaccurate results.

This means that you have to have a sensible replacement value for nulls/unknowns. If you know that your query will produce nulls, for example a CASE statement, a left join or a type conversion from a String to a Number, you should always use IFNULL() or COALESCE() to provide a sensible replacement value.

Pattern 10: Start with a left join
You never know if the column you’re joining on is fully represented in both tables. An inner join will filter out the non-matching rows and they could be important. By starting with a left join, you ensure that your query remains robust to future data changes.

If you liked these and want to learn more, I’m working on a book where I explain each one in more detail with specific examples for each, and we get into all the ones I didn’t have room for in this article. We also cover a few case studies where I break down large complex queries so you can see exactly how the patterns are applied in the real world.
Okay, here's a breakdown of subquery usage in different parts of a SQL query, with examples for each. I'll use a simplified scenario with two tables: `employees` (id, name, department_id, salary) and `departments` (id, name).

**1. Subquery in the `SELECT` Clause (Scalar Subquery)**

*   **Purpose:** To calculate a single value that is used as a column in the main query's output. The subquery *must* return only one row and one column.

*   **Example:**  Get the name of each employee and the average salary of *all* employees (not just within their department).

```sql
SELECT
    e.name,
    (SELECT AVG(salary) FROM employees) AS overall_average_salary  -- Subquery here
FROM
    employees e;
```

*   **Explanation:** The subquery `(SELECT AVG(salary) FROM employees)` calculates the overall average salary. This single value is then included as a new column (`overall_average_salary`) for each row in the `employees` table.

**2. Subquery in the `FROM` Clause (Derived Table/Inline View)**

*   **Purpose:** To create a temporary, named result set that can be treated as a table within the main query.  This is very useful for breaking down complex logic into smaller, more manageable steps.  An alias *must* be given to the derived table.

*   **Example:**  Calculate the average salary per department, and then find employees whose salary is higher than their department's average.

```sql
SELECT
    e.name,
    e.salary,
    da.avg_salary AS department_average_salary
FROM
    employees e
JOIN
    (SELECT department_id, AVG(salary) AS avg_salary  -- Subquery starts here
     FROM employees
     GROUP BY department_id) AS da  -- Alias 'da' for the derived table (MANDATORY)
ON
    e.department_id = da.department_id
WHERE
    e.salary > da.avg_salary;
```

*   **Explanation:**
    *   The subquery `(SELECT department_id, AVG(salary) AS avg_salary FROM employees GROUP BY department_id)` calculates the average salary for each department.  It produces a result set with two columns (`department_id`, `avg_salary`).
    *   This result set is given the alias `da` (for "department averages").  This is crucial; you *must* alias a derived table.
    *   The main query then joins the `employees` table (`e`) with the derived table (`da`) based on the `department_id`.
    *   Finally, the `WHERE` clause filters for employees whose salary exceeds their department's average.

**3. Subquery in the `WHERE` Clause**

*   **Purpose:** To filter the results of the main query based on conditions determined by the subquery. This is the most common use case for subqueries.

    *   **a) Using `IN` or `NOT IN`:**

        *   **Example:** Find employees who work in departments located in "New York" (assuming a `locations` table with `department_id` and `city`).

        ```sql
        SELECT name, department_id
        FROM employees
        WHERE department_id IN (SELECT id FROM departments WHERE location = 'New York');  -- Subquery here
        ```

        *   **Explanation:** The subquery `(SELECT id FROM departments WHERE location = 'New York')` returns a list of department IDs located in New York. The `IN` operator checks if an employee's `department_id` is present in that list.

    *   **b) Using Comparison Operators (`=`, `>`, `<`, etc.) with a Scalar Subquery:**

        *   **Example:** Find employees whose salary is greater than the average salary of *all* employees.

        ```sql
        SELECT name, salary
        FROM employees
        WHERE salary > (SELECT AVG(salary) FROM employees);  -- Subquery here
        ```

        *   **Explanation:** The subquery `(SELECT AVG(salary) FROM employees)` returns a single value (the overall average salary).  The `WHERE` clause then compares each employee's salary to this single value.  This is similar to the `SELECT` clause example, but used for filtering instead of displaying a calculated value.

    *   **c) Using `EXISTS` or `NOT EXISTS` (Correlated Subquery):**

        *   **Example:** Find departments that have *at least one* employee.

        ```sql
        SELECT d.name
        FROM departments d
        WHERE EXISTS (SELECT 1  -- Subquery starts here
                      FROM employees e
                      WHERE e.department_id = d.id);  -- Correlated: refers to 'd' from outer query
        ```

        *   **Explanation:**
            *   This is a *correlated subquery*. The subquery refers to `d.id` from the *outer* query (`departments d`).  The subquery is evaluated *for each row* of the outer query.
            *   `EXISTS` checks if the subquery returns *at least one row*. It doesn't care *what* the subquery returns (hence the `SELECT 1` – it's just a placeholder; any non-empty result will satisfy `EXISTS`).
            *   For each department (`d`), the subquery checks if there's any employee (`e`) whose `department_id` matches the department's `id`.  If there is at least one such employee, `EXISTS` returns true, and the department is included in the result.  If no matching employee is found, `EXISTS` is false, and the department is excluded.
        *  **`NOT EXISTS` is the opposite of `EXISTS`**.

    *   **d) Using `ANY` or `ALL` (less common, but useful):**
        *   **Example (ANY):** Find employees whose salary is greater than the salary of *at least one* employee in department 1.
            ```sql
            SELECT name, salary
            FROM employees
            WHERE salary > ANY (SELECT salary FROM employees WHERE department_id = 1);
            ```

        * **Example(ALL):** Find employees whose salary is greater than the salary of *every* employee in department 1.

            ```sql
            SELECT name, salary
            FROM employees
            WHERE salary > ALL (SELECT salary FROM employees WHERE department_id = 1);
            ```

        *   **Explanation:**
            *   `ANY` (or `SOME`, which is a synonym) checks if the comparison is true for *at least one* row returned by the subquery.
            *   `ALL` checks if the comparison is true for *every* row returned by the subquery.

**4. Subquery in the `HAVING` Clause**

*   **Purpose:** To filter the results of a `GROUP BY` query based on conditions involving aggregate functions.  `HAVING` is to `GROUP BY` what `WHERE` is to `SELECT`.

*   **Example:** Find departments where the average salary is greater than the overall average salary of *all* employees.

```sql
SELECT
    d.name,
    AVG(e.salary) AS avg_department_salary
FROM
    employees e
JOIN
    departments d ON e.department_id = d.id
GROUP BY
    d.name
HAVING
    AVG(e.salary) > (SELECT AVG(salary) FROM employees);  -- Subquery here
```

*   **Explanation:**
    *   The main query groups employees by department and calculates the average salary for each department (`avg_department_salary`).
    *   The `HAVING` clause then filters these grouped results.  The subquery `(SELECT AVG(salary) FROM employees)` calculates the overall average salary.
    *   The `HAVING` clause keeps only those departments where `avg_department_salary` is greater than the overall average.  You *cannot* use `WHERE` here because you are filtering based on an aggregate function (`AVG(e.salary)`).

**Key Takeaways and Best Practices:**

*   **Clarity:**  While subqueries are powerful, overuse can make queries hard to read and understand.  Consider using Common Table Expressions (CTEs) instead of complex nested subqueries in the `FROM` clause for better readability.
*   **Performance:** Correlated subqueries can sometimes be less efficient than joins.  Modern database optimizers are good at handling them, but it's worth testing different approaches if performance is critical.
*   **Scalar Subquery Return Value:** Remember that a scalar subquery (used in `SELECT` or with comparison operators in `WHERE`) *must* return exactly one row and one column.  Otherwise, you'll get an error.
*   **`EXISTS` Efficiency:** `EXISTS` can often be more efficient than counting rows with `COUNT(*)` in a subquery, as the database only needs to find *one* matching row to satisfy `EXISTS`.
* **Derived Table Aliasing**: Always provide an alias for derived tables in the `FROM` clause; it's required.

These examples cover the most common and useful ways to employ subqueries in SQL. They are a fundamental tool for writing complex and powerful queries to extract insights from your data.

Overview
This cheat sheet contains five sections. These are:

SQL Basics and Syntax
SQL Queries
SQL Functions
SQL Window Functions
SQL Data Types
You’ll find a brief explanation of clauses and functions in each section, plus examples. You can also see the summarized cheat sheet at the end of each section. Additionally, we have included a few tips on using this SQL cheat sheet effectively when preparing for your interview.

SQL Basics and Syntax
SQL is a declarative language. The statements only describe the desired result, not the exact steps that should be taken to achieve the result. In this section, we’ll start by looking at the basic SQL commands and the syntax used to apply them.

SELECT and FROM
The SELECT statement is used to retrieve data from one or multiple tables. In the syntax, you start by writing SELECT before listing the names of the columns you’d like displayed. The column names should be separated by commas.

The FROM clause comes after the column names, and it specifies which table(s) the data should be retrieved from. For example:

SELECT column1, column2
FROM table_name;
WHERE
The WHERE clause is used after the FROM statement. It’s used to filter results based on a specified condition. For example:

SELECT column1, column2
FROM table_name
WHERE column1 = 'value';
GROUP BY
This clause can come after the FROM statement or the WHERE clause (if present). It groups together rows with the same values in a specified column and outputs a summary of these rows. It’s often used with aggregate functions such as COUNT(), AVG(), and MAX() to produce summary statistics. The syntax is as shown below:

SELECT column1, COUNT(*)
FROM table_name
WHERE column1 > 10
GROUP BY column1;
HAVING
Where the GROUP BY statement is used, the HAVING clause will be required to filter results based on a condition because the WHERE clause doesn’t work with aggregate functions. For example:

SELECT column1, COUNT(*)
FROM table_name
GROUP BY column1
HAVING COUNT(*) > 10;
ORDER BY
The ORDER BY clause is used to sort results based on one or more columns in ascending or descending order. In terms of syntax, it comes after whichever of the previous statements (FROM, WHERE, GROUP BY, HAVING) is last. To specify the sorting order, you use ASC for ascending and DESC for descending. For example:

SELECT column1, column2
FROM table_name
WHERE column1 > 10
ORDER BY column1, column2 ASC;
Here’s a brief syntax cheat sheet of the commands discussed so far:

SELECT: Used to retrieve data from one or more tables.

Syntax: SELECT column1, column2, … FROM table_name;

FROM: Specifies the table(s) from which data should be retrieved.

Syntax: SELECT column1, column2, … FROM table_name;

WHERE: Filters the data based on a specified condition.

Syntax: SELECT column1, column2, … FROM table_name WHERE condition;

GROUP BY: Groups rows that have the same values in specified columns.

Syntax: SELECT column1, column2, … FROM table_name GROUP BY column1, column2, …;

HAVING: Filters the results of a GROUP BY query based on a specified condition.

Syntax: SELECT column1, column2, … FROM table_name GROUP BY column1, column2, … HAVING condition;

ORDER BY: Sorts the result set in ascending or descending order based on one or more columns.

Syntax: SELECT column1, column2, … FROM table_name ORDER BY column1 [ASC|DESC], column2 [ASC|DESC], …;

These commands are the basis of all SQL queries, and you’ll need to master them if you want to ace SQL tests during interviews and succeed in a data-centered role.

SQL Query Cheat Sheet
Understanding how to write queries makes it easy to demonstrate your SQL proficiency in interviews. In this section, our cheat sheet will show you some common types of queries you may come across in a job interview.

Retrieving Specific Data
In this type of problem, the goal is to write a query that retrieves specific pieces of data from a table. You may be asked to retrieve all the columns or just a few, as shown below:

If you only need specific types of data from a table, e.g., first names, email addresses, etc., you’ll need to write the names of the specific columns holding this information.
SELECT column1, column20
FROM table_name;
When you need to examine or analyze all the data in the table, the wildcard (*) is used after the SELECT statement instead of specific column names.
SELECT * 
FROM table_name;
Conditional Filtering
This type of problem requires you to only display results that meet a specific condition. It relies on the use of the WHERE clause. Conditional filtering helps to identify the most relevant records and identify or filter out outliers.

For example, if you’d like to only see data related to customers who have made purchases exceeding a certain amount, you’d use the WHERE clause followed by the name of the column holding transaction values, a greater-than sign, and the cut-off value:

SELECT *
FROM table_name
WHERE transaction_value_column > 'Cut-off value';
You can filter for different conditions, including equality (=), less than (<), not equal (!=), between a range (BETWEEN), similar to (LIKE), etc.

Sorting Data
Sorting data helps you organize data to make it easier to read or rank. For example, in a salaries table, you can sort the data from the lowest to the highest earners or vice versa. The ORDER BY clause is used together with the ASC and DESC keywords.

Sort records from lowest to highest earners.
SELECT *
FROM table_name
ORDER BY salary ASC;
Sort records from highest to lowest earners.
SELECT *
FROM table_name
ORDER BY salary DESC;
Joining Tables
Relational databases often hold data in multiple tables, which must be combined during analysis. In SQL, this requires the use of at least one JOIN statement. To join two tables, they must share a common feature/column. There are four types of JOIN statements, i.e.:

INNER JOIN - Shows records with matching values in both tables
SELECT table1.column1, table2.column2
FROM table1
INNER JOIN table2
ON table1.column1 = table2.column1;
LEFT JOIN - Shows all records in the left table and only matched records from the right table.
RIGHT JOIN - Shows all records in the right table and only matched records from the left table.
FULL OUTER JOIN - Shows all records from the joined tables whether they are matched or not. Unmatched values are shown as Null.
Grouping and Aggregating Data
Grouping and aggregating data helps with summarizing information for quick insights. To achieve this, the GROUP BY statement is used with an aggregating function. In the example below, this operation is used to return the average salary for each department in a company.

SELECT department, AVG(salary)
FROM table_name
GROUP BY department;
Limiting the Number of Rows in the Output
Limiting the number of rows or records shown helps you quickly analyze a dataset or focus on the most important records, e.g., the top ten best-selling products. This operation requires the use of the LIMIT clause followed by the desired number of rows:

SELECT * FROM table_name
LIMIT 10;
Here’s a brief syntax cheat sheet for the queries discussed in this section:

Retrieving specific data:
SELECT * FROM table_name; Retrieves all columns from the specified table.
SELECT column1, column2 FROM table_name; Retrieves specific columns from the specified table.
Filtering data using conditions:
SELECT * FROM table_name WHERE column1 = ‘value’; Retrieves rows where the value of column1 matches the specified value.
SELECT * FROM table_name WHERE column1 > ‘value’; Retrieves rows where the value of column1 is greater than the specified value.
Sorting data:
SELECT * FROM table_name ORDER BY column1 ASC; Retrieves data sorted by column1 in ascending order.
SELECT * FROM table_name ORDER BY column1 DESC; Retrieves data sorted by column1 in descending order.
Joining tables:
SELECT table1.column1, table2.column2 FROM table1 INNER JOIN table2 ON table1.column1 = table2.column1; Retrieves specific columns from two tables, joining them based on the specified condition.
Grouping and aggregating data:
SELECT column1, COUNT(*) FROM table_name GROUP BY column1; Groups data by column1 and counts the number of rows in each group.
Limiting the number of rows in the output
SELECT * FROM table_name LIMIT 10.
SQL Functions Cheat Sheet
The most commonly used SQL functions include aggregate functions, string functions, and date functions. These functions simplify complex operations and improve the speed of data processing.

Aggregate Functions
These functions perform calculations on a set of values and return one value, i.e., an aggregate such as an average, maximum, minimum, etc. They are often used with the GROUP BY statement or the PARTITION BY window function.

COUNT(): Counts the number of rows that match a specified condition.
SELECT COUNT(column_name)
FROM table_name
GROUP BY column_name;
SUM(): Adds up the values of a set of records in a specified column.
SELECT SUM(column_name)
FROM table_name
GROUP BY column_name;
AVG(): Calculates the average value of a set of records in a specified column.
SELECT AVG(column_name)
FROM table_name
GROUP BY column_name;
MIN(): Finds the smallest value in a specified column for a set of records.
SELECT MIN(column_name)
FROM table_name;
MAX(): Finds the largest value in a specified column for a set of records.
SELECT MAX(column_name)
FROM table_name;
When aggregate functions are used with the GROUP BY statement, calculations are done separately for each group. For example, the query below will return the total number of employees in each department. The result will be a summary table showing each department and the number of employees in each.

SELECT department, COUNT(employee_id) AS employee_count
FROM employees
GROUP BY department;
String Functions
String functions manipulate and transform text data. Common string functions include:

CONCAT(): Joins two or more strings together. For example, CONCAT('Hello', ' World') returns ‘Hello World’.
SELECT CONCAT(column1, column2)
FROM table_name;
SUBSTRING(): Extracts a part of a string. For example, SUBSTRING('Hello World', 1, 5) returns ‘Hello’.
SELECT SUBSTRING(column_name, start_position, length)
FROM table_name
LENGTH(): Returns the length of a string. For example, LENGTH('Hello') returns 5.
SELECT LENGTH(column_name)
FROM table_name;
REPLACE(): Replaces a specified part of a string with another string. For example, REPLACE('Hello World', 'World', 'There') returns ‘Hello There’.
SELECT REPLACE(column_name, 'old_string', 'new_string')
FROM table_name;
TRIM(column): Removes leading and trailing spaces from the specified column. For example, TRIM(' Hello World ') returns ‘Hello World’.
Date Functions
These functions help users manipulate and format date and time values in SQL. These come up often in job interviews because it’s common to work with data that contains one or more date/time values. Common date functions include:

NOW(): Returns the current date and time.
SELECT NOW()
DATE(): Extracts the date part of a date-time expression.
SELECT DATE(column_name)
FROM table_name;
DATEDIFF(): Calculates the difference between two dates.
SELECT DATEDIFF(date1, date2)
FROM table_name;
DATE_ADD(): Adds a specified time interval to a date.
SELECT DATE_ADD(date, INTERVAL value unit)
FROM table_name;
The “INTERVAL” keyword must be used. Value is the amount being added and unit is

used to specify whether the value is being added to the ‘DAY’, ‘MONTH’, ‘YEAR’,

‘HOUR’, ‘MINUTE’, or ‘SECOND’.

Here’s the syntax cheat sheet for this section:

Aggregate functions:
COUNT(column): Counts the number of non-null values in the specified column.
SUM(column): Calculates the sum of non-null values in the specified column.
AVG(column): Computes the average of non-null values in the specified column.
MIN(column): Finds the minimum value in the specified column.
MAX(column): Identifies the maximum value in the specified column.
String functions:
CONCAT(column1, column2): Concatenates two columns of strings.
SUBSTRING(column, start, length): Extracts a substring from the specified column, starting at the given position for the specified length.
LENGTH(column): Returns the string length for the value in the column.
REPLACE(column, search_string, replacement_string): Replaces all occurrences of the search_string with the replacement_string in the specified column.
TRIM(column): Removes leading and trailing spaces from the specified column.
Date functions:
NOW(): Returns the current date and time.
DATE(column): Extracts the date part of a datetime value in the specified column.
DATEDIFF(column1, column2): Calculates the difference in days between two dates.
DATE_ADD(date, INTERVAL value unit)
SQL Window Functions Cheat Sheet
SQL window functions enable users to perform calculations across a set of rows that share a common characteristic. This set of rows that serves as the input for the window function is defined using the OVER() clause. Window functions can be used to perform calculations such as cumulative sums and running averages while targeting specific groups in a dataset. Unlike the GROUP BY clause, the individual records are not rolled up into summaries. Their granularity is retained, which helps with data analysis. The syntax for window functions is:

 SELECT column1, column2,...,

 WINDOW FUNCTION() OVER() AS column_name

 FROM table1
For aggregate functions, you’ll need to specify the column on which calculation is done, i.e.:

 SELECT column1, column2,...,

 AGGREGATE FUNCTION(column_to_aggregate) OVER()

 FROM table1
ROW_NUMBER
This function simply assigns a sequential number to each row starting from one. Each row has a unique number. The syntax for applying this function is:

SELECT product_name, sales,
ROW_NUMBER() OVER (ORDER BY sales DESC) AS row_number
FROM products;
In the above example, the ORDER BY clause is optional. If the brackets are left empty, the records will be returned in the same order they appear in the products table. ROW_NUMBER is useful where data lacks a row with sequential numbering without missing values.

RANK()
RANK() assigns a unique rank to each row based on a particular column. Rows with equal values in that column are assigned the same rank, but the next number is skipped when assigning the next rank, i.e., 1,2,2,4,5. The syntax for this function is:

SELECT product_name, sales,
RANK() OVER (ORDER BY sales DESC) AS rank
FROM products;
In the above example, the ORDER BY clause ensures the ranking is done from the product with the highest number of sales to that with the lowest sales. A potential use case for this function is identifying the top 10 performing salespeople in a team.

DENSE_RANK
This function is similar toRANK(). Except when two numbers are assigned the same rank, the next number will not be skipped when assigning rank, i.e., 1,2,3,3,4,4,5,6…

NTILE
NTILE() divides the records into a number of specified groups of equal number and assigns different records to different groups based on a specified column. For example:

SELECT product_name, sales,
NTILE(4) OVER (ORDER BY sales DESC) AS quartile
FROM products;
In the above example, the records in the products table are divided into four groups. Each item is assigned to a group based on the number of sales. Therefore, items with equally high sales numbers are likely to end up in the same group.

PARTITION BY
PARTITION BY is a window function used to segment records into smaller groups on which other functions can be performed separately. For example, it can be used to calculate the average sales numbers for different product classes separately. The syntax for this would be:

SELECT product_name, sales,
AVG(sales) OVER(PARTITION BY product_class) AS class_average
FROM products;
Note that the PARTITION BY function is placed inside the OVER() clause. You can read more on the PARTITION BY clause in this post.
Here’s the syntax cheat sheet for this section:

ROW_NUMBER(): Assigns a unique, sequential number to each row within the result set, starting at 1.

Syntax: ROW_NUMBER() OVER (ORDER BY column)

RANK(): Assigns a unique rank to each row within the result set, with the same rank assigned to rows with equal values. Rows with equal values will leave a gap in the sequence.

Syntax: RANK() OVER (ORDER BY column)

DENSE_RANK(): Similar to RANK(), but without gaps in the rank sequence for equal values.

Syntax: DENSE_RANK() OVER (ORDER BY column)

NTILE(n): Divides the result set into a specified number of groups (n) and assigns a group number to each row.

Syntax: NTILE(n) OVER (ORDER BY column)

PARTITION BY: Divides the records into different groups based on one feature and allows functions to be applied separately to each group.

Syntax: OTHER_FUNCTION() OVER(PARTITION BY column)

SQL Data Types Cheat Sheet
Data tables often contain different types of information or information created and measured using different standards. This results in many data types, which you’ll need to know how to work with to write efficient queries and avoid conversion errors. Common SQL data types include:

INT: A whole number with a range depending on the database system.

FLOAT: A floating-point number, used to store approximate values with varying degrees of precision.

DECIMAL(p, s): A fixed-point number with user-defined precision (p) and scale (s). Scale represents the number of digits to the right of the decimal point.

VARCHAR(n): A variable-length character string with a maximum length of n characters.

DATE: Represents a date value (YYYY-MM-DD).

TIMESTAMP: Represents a date and time value (YYYY-MM-DD HH:MI:SS).
Practical SQL Puzzles That Will Level Up Your Skill
Three real-world SQL patterns that can be applied to many problems.

Mateus Trentz
Mar 4, 2025
14 min read
Share
An image of a blue puzzle missing a single piece (SQL puzzles concept)
Photo by Tanja Tepavac on Unsplash
There are some SQL patterns that, once you know them, you start seeing them everywhere. The solutions to the puzzles that I will show you today are actually very simple SQL queries, but understanding the concept behind them will surely unlock new solutions to the queries you write on a day-to-day basis.

These challenges are all based on real-world scenarios, as over the past few months I made a point of writing down every puzzle-like query that I had to build. I also encourage you to try them for yourself, so that you can challenge yourself first, which will improve your learning!

All queries to generate the datasets will be provided in a PostgreSQL and DuckDB-friendly syntax, so that you can easily copy and play with them. At the end I will also provide you a link to a GitHub repo containing all the code, as well as the answer to the bonus challenge I will leave for you!

I organized these puzzles in order of increasing difficulty, so, if you find the first ones too easy, at least take a look at the last one, which uses a technique that I truly believe you won’t have seen before.

Okay, let’s get started.

Analyzing ticket moves
I love this puzzle because of how short and simple the final query is, even though it deals with many edge cases. The data for this challenge shows tickets moving in between Kanban stages, and the objective is to find how long, on average, tickets stay in the Doing stage.

The data contains the ID of the ticket, the date the ticket was created, the date of the move, and the “from” and “to” stages of the move. The stages present are New, Doing, Review, and Done.

Some things you need to know (edge cases):

Tickets can move backwards, meaning tickets can go back to the Doing stage.
You should not include tickets that are still stuck in the Doing stage, as there is no way to know how long they will stay there for.
Tickets are not always created in the New stage.
CREATE TABLE ticket_moves (
    ticket_id INT NOT NULL,
    create_date DATE NOT NULL,
    move_date DATE NOT NULL,
    from_stage TEXT NOT NULL,
    to_stage TEXT NOT NULL
);
INSERT INTO ticket_moves (ticket_id, create_date, move_date, from_stage, to_stage)
    VALUES
        -- Ticket 1: Created in "New", then moves to Doing, Review, Done.
        (1, '2024-09-01', '2024-09-03', 'New', 'Doing'),
        (1, '2024-09-01', '2024-09-07', 'Doing', 'Review'),
        (1, '2024-09-01', '2024-09-10', 'Review', 'Done'),
        -- Ticket 2: Created in "New", then moves: New → Doing → Review → Doing again → Review.
        (2, '2024-09-05', '2024-09-08', 'New', 'Doing'),
        (2, '2024-09-05', '2024-09-12', 'Doing', 'Review'),
        (2, '2024-09-05', '2024-09-15', 'Review', 'Doing'),
        (2, '2024-09-05', '2024-09-20', 'Doing', 'Review'),
        -- Ticket 3: Created in "New", then moves to Doing. (Edge case: no subsequent move from Doing.)
        (3, '2024-09-10', '2024-09-16', 'New', 'Doing'),
        -- Ticket 4: Created already in "Doing", then moves to Review.
        (4, '2024-09-15', '2024-09-22', 'Doing', 'Review');

A summary of the data:

Ticket 1: Created in the New stage, moves normally to Doing, then Review, and then Done.
Ticket 2: Created in New, then moves: New → Doing → Review → Doing again → Review.
Ticket 3: Created in New, moves to Doing, but it is still stuck there.
Ticket 4: Created in the Doing stage, moves to Review afterward.
It might be a good idea to stop for a bit and think how you would deal with this. Can you find out how long a ticket stays on a single stage?

Honestly, this sounds intimidating at first, and it looks like it will be a nightmare to deal with all the edge cases. Let me show you the full solution to the problem, and then I will explain what is happening afterward.

WITH stage_intervals AS (
    SELECT
        ticket_id,
        from_stage,
        move_date 
        - COALESCE(
            LAG(move_date) OVER (
                PARTITION BY ticket_id 
                ORDER BY move_date
            ), 
            create_date
        ) AS days_in_stage
    FROM
        ticket_moves
)
SELECT
    SUM(days_in_stage) / COUNT(DISTINCT ticket_id) as avg_days_in_doing
FROM
    stage_intervals
WHERE
    from_stage = 'Doing';

The first CTE uses the LAG function to find the previous move of the ticket, which will be the time the ticket entered that stage. Calculating the duration is as simple as subtracting the previous date from the move date.

What you should notice is the use of the COALESCE in the previous move date. What that does is that if a ticket doesn’t have a previous move, then it uses the date of creation of the ticket. This takes care of the cases of tickets being created directly into the Doing stage, as it still will properly calculate the time it took to leave the stage.

This is the result of the first CTE, showing the time spent in each stage. Notice how the Ticket 2 has two entries, as it visited the Doing stage in two separate occasions.


With this done, it’s just a matter of getting the average as the SUM of total days spent in doing, divided by the distinct number of tickets that ever left the stage. Doing it this way, instead of simply using the AVG, makes sure that the two rows for Ticket 2 get properly accounted for as a single ticket.

Not so bad, right?

Finding contract sequences
The goal of this second challenge is to find the most recent contract sequence of every employee. A break of sequence happens when two contracts have a gap of more than one day between them. 

In this dataset, there are no contract overlaps, meaning that a contract for the same employee either has a gap or ends a day before the new one starts.

CREATE TABLE contracts (
    contract_id integer PRIMARY KEY,
    employee_id integer NOT NULL,
    start_date date NOT NULL,
    end_date date NOT NULL
);

INSERT INTO contracts (contract_id, employee_id, start_date, end_date)
VALUES 
    -- Employee 1: Two continuous contracts
    (1, 1, '2024-01-01', '2024-03-31'),
    (2, 1, '2024-04-01', '2024-06-30'),
    -- Employee 2: One contract, then a gap of three days, then two contracts
    (3, 2, '2024-01-01', '2024-02-15'),
    (4, 2, '2024-02-19', '2024-04-30'),
    (5, 2, '2024-05-01', '2024-07-31'),
    -- Employee 3: One contract
    (6, 3, '2024-03-01', '2024-08-31');

As a summary of the data:

Employee 1: Has two continuous contracts.
Employee 2: One contract, then a gap of three days, then two contracts.
Employee 3: One contract.
The expected result, given the dataset, is that all contracts should be included except for the first contract of Employee 2, which is the only one that has a gap.

Before explaining the logic behind the solution, I would like you to think about what operation can be used to join the contracts that belong to the same sequence. Focus only on the second row of data, what information do you need to know if this contract was a break or not?

I hope it’s clear that this is the perfect situation for window functions, again. They are incredibly useful for solving problems like this, and understanding when to use them helps a lot in finding clean solutions to problems.

First thing to do, then, is to get the end date of the previous contract for the same employee with the LAG function. Doing that, it’s simple to compare both dates and check if it was a break of sequence.

WITH ordered_contracts AS (
    SELECT
        *,
        LAG(end_date) OVER (PARTITION BY employee_id ORDER BY start_date) AS previous_end_date
    FROM
        contracts
),
gapped_contracts AS (
    SELECT
        *,
        -- Deals with the case of the first contract, which won't have
        -- a previous end date. In this case, it's still the start of a new
        -- sequence.
        CASE WHEN previous_end_date IS NULL
            OR previous_end_date < start_date - INTERVAL '1 day' THEN
            1
        ELSE
            0
        END AS is_new_sequence
    FROM
        ordered_contracts
)
SELECT * FROM gapped_contracts ORDER BY employee_id ASC;

An intuitive way to continue the query is to number the sequences of each employee. For example, an employee who has no gap, will always be on his first sequence, but an employee who had 5 breaks in contracts will be on his 5th sequence. Funnily enough, this is done by another window function.

--
-- Previous CTEs
--
sequences AS (
    SELECT
        *,
        SUM(is_new_sequence) OVER (PARTITION BY employee_id ORDER BY start_date) AS sequence_id
FROM
    gapped_contracts
)
SELECT * FROM sequences ORDER BY employee_id ASC;

Notice how, for Employee 2, he starts his sequence #2 after the first gapped value. To finish this query, I grouped the data by employee, got the value of their most recent sequence, and then did an inner join with the sequences to keep only the most recent one.

--
-- Previous CTEs
--
max_sequence AS (
    SELECT
        employee_id,
        MAX(sequence_id) AS max_sequence_id
FROM
    sequences
GROUP BY
    employee_id
),
latest_contract_sequence AS (
    SELECT
        c.contract_id,
        c.employee_id,
        c.start_date,
        c.end_date
    FROM
        sequences c
        JOIN max_sequence m ON c.sequence_id = m.max_sequence_id
            AND c.employee_id = m.employee_id
        ORDER BY
            c.employee_id,
            c.start_date
)
SELECT
    *
FROM
    latest_contract_sequence;

As expected, our final result is basically our starting query just with the first contract of Employee 2 missing! 

Tracking concurrent events
Finally, the last puzzle — I’m glad you made it this far. 

For me, this is the most mind-blowing one, as when I first encountered this problem I thought of a completely different solution that would be a mess to implement in SQL.

For this puzzle, I’ve changed the context from what I had to deal with for my job, as I think it will make it easier to explain. 

Imagine you’re a data analyst at an event venue, and you’re analyzing the talks scheduled for an upcoming event. You want to find the time of day where there will be the highest number of talks happening at the same time.

This is what you should know about the schedules:

Rooms are booked in increments of 30min, e.g. from 9h-10h30.
The data is clean, there are no overbookings of meeting rooms.
There can be back-to-back meetings in a single meeting room.

Meeting schedule visualized (this is the actual data). 

CREATE TABLE meetings (
    room TEXT NOT NULL,
    start_time TIMESTAMP NOT NULL,
    end_time TIMESTAMP NOT NULL
);

INSERT INTO meetings (room, start_time, end_time) VALUES
    -- Room A meetings
    ('Room A', '2024-10-01 09:00', '2024-10-01 10:00'),
    ('Room A', '2024-10-01 10:00', '2024-10-01 11:00'),
    ('Room A', '2024-10-01 11:00', '2024-10-01 12:00'),
    -- Room B meetings
    ('Room B', '2024-10-01 09:30', '2024-10-01 11:30'),
    -- Room C meetings
    ('Room C', '2024-10-01 09:00', '2024-10-01 10:00'),
    ('Room C', '2024-10-01 11:30', '2024-10-01 12:00');

The way to solve this is using what is called a Sweep Line Algorithm, or also known as an event-based solution. This last name actually helps to understand what will be done, as the idea is that instead of dealing with intervals, which is what we have in the original data, we deal with events instead.

To do this, we need to transform every row into two separate events. The first event will be the Start of the meeting, and the second event will be the End of the meeting.

WITH events AS (
  -- Create an event for the start of each meeting (+1)
  SELECT 
    start_time AS event_time, 
    1 AS delta
  FROM meetings
  UNION ALL
  -- Create an event for the end of each meeting (-1)
  SELECT 
   -- Small trick to work with the back-to-back meetings (explained later)
    end_time - interval '1 minute' as end_time,
    -1 AS delta
  FROM meetings
)
SELECT * FROM events;

Take the time to understand what is happening here. To create two events from a single row of data, we’re simply unioning the dataset on itself; the first half uses the start time as the timestamp, and the second part uses the end time.

You might already notice the delta column created and see where this is going. When an event starts, we count it as +1, when it ends, we count it as -1. You might even be already thinking of another window function to solve this, and you’re actually right!

But before that, let me just explain the trick I used in the end dates. As I don’t want back-to-back meetings to count as two concurrent meetings, I’m subtracting a single minute of every end date. This way, if a meeting ends and another starts at 10h30, it won’t be assumed that two meetings are concurrently happening at 10h30.

Okay, back to the query and yet another window function. This time, though, the function of choice is a rolling SUM.

--
-- Previous CTEs
--
ordered_events AS (
  SELECT
    event_time,
    delta,
    SUM(delta) OVER (ORDER BY event_time, delta DESC) AS concurrent_meetings
  FROM events
)
SELECT * FROM ordered_events ORDER BY event_time DESC;

The rolling SUM at the Delta column is essentially walking down every record and finding how many events are active at that time. For example, at 9 am sharp, it sees two events starting, so it marks the number of concurrent meetings as two!

When the third meeting starts, the count goes up to three. But when it gets to 9h59 (10 am), then two meetings end, bringing the counter back to one. With this data, the only thing missing is to find when the highest value of concurrent meetings happens.

--
-- Previous CTEs
--
max_events AS (
  -- Find the maximum concurrent meetings value
  SELECT 
    event_time, 
    concurrent_meetings,
    RANK() OVER (ORDER BY concurrent_meetings DESC) AS rnk
  FROM ordered_events
)
SELECT event_time, concurrent_meetings
FROM max_events
WHERE rnk = 1;

That’s it! The interval of 9h30–10h is the one with the largest number of concurrent meetings, which checks out with the schedule visualization above!

This solution looks incredibly simple in my opinion, and it works for so many situations. Every time you are dealing with intervals now, you should think if the query wouldn’t be easier if you thought about it in the perspective of events.

But before you move on, and to really nail down this concept, I want to leave you with a bonus challenge, which is also a common application of the Sweep Line Algorithm. I hope you give it a try!

Bonus challenge
The context for this one is still the same as the last puzzle, but now, instead of trying to find the period when there are most concurrent meetings, the objective is to find bad scheduling. It seems that there are overlaps in the meeting rooms, which need to be listed so it can be fixed ASAP.

How would you find out if the same meeting room has two or more meetings booked at the same time? Here are some tips on how to solve it:

It’s still the same algorithm.
This means you will still do the UNION, but it will look slightly different.
You should think in the perspective of each meeting room.
You can use this data for the challenge:

CREATE TABLE meetings_overlap (
    room TEXT NOT NULL,
    start_time TIMESTAMP NOT NULL,
    end_time TIMESTAMP NOT NULL
);

INSERT INTO meetings_overlap (room, start_time, end_time) VALUES
    -- Room A meetings
    ('Room A', '2024-10-01 09:00', '2024-10-01 10:00'),
    ('Room A', '2024-10-01 10:00', '2024-10-01 11:00'),
    ('Room A', '2024-10-01 11:00', '2024-10-01 12:00'),
    -- Room B meetings
    ('Room B', '2024-10-01 09:30', '2024-10-01 11:30'),
    -- Room C meetings
    ('Room C', '2024-10-01 09:00', '2024-10-01 10:00'),
    -- Overlaps with previous meeting.
    ('Room C', '2024-10-01 09:30', '2024-10-01 12:00');
If you’re interested in the solution to this puzzle, as well as the rest of the queries, check this GitHub repo.

Conclusion
The first takeaway from this blog post is that window functions are overpowered. Ever since I got more comfortable with using them, I feel that my queries have gotten so much simpler and easier to read, and I hope the same happens to you.

If you’re interested in learning more about them, you would probably enjoy reading this other blog post I’ve written, where I go over how you can understand and use them effectively.

The second takeaway is that these patterns used in the challenges really do happen in many other places. You might need to find sequences of subscriptions, customer retention, or you might need to find overlap of tasks. There are many situations when you will need to use window functions in a very similar fashion to what was done in the puzzles.

The third thing I want you to remember is about this solution to using events besides dealing with intervals. I’ve looked at some problems I solved a long time ago that I could’ve used this pattern on to make my life easier, and unfortunately, I didn’t know about it at the time.
Probability and Statistics Concepts to Review for the Data Science Interview
Because probability & statistics are foundational to the field of Data Science, before the interview you should review:

Central Limit Theorem
Probability Distributions
Regression Analysis
Hypothesis Testing
If are unfamiliar with these concepts I recommend reading some of the books from the 13 Best Books for Data Scientists list.

Central Limit Theorem
Understanding the Central Limit Theorem is crucial. It states that the distribution of the sample mean of a large enough sample from any population will be approximately normally distributed, regardless of the population's underlying distribution. This theorem is fundamental when dealing with inferential statistics and hypothesis testing.

Probability Distributions
Hypothesis testing involves formulating null and alternative hypotheses, collecting data, and using statistical methods to determine whether there is enough evidence to reject the null hypothesis. You should be proficient in different types of hypothesis tests (e.g., t-tests, chi-squared tests) and their applications.

Regression Analysis
Familiarity with common probability distributions like the normal distribution, binomial distribution, and Poisson distribution is essential. You should understand their probability density functions, cumulative distribution functions, and how to use them in real-world scenarios.

Hypothesis Testing
Regression analysis is a fundamental statistical technique used for modeling relationships between variables. You should know about linear regression, multiple regression, logistic regression (for classification), and how to interpret regression coefficients, p-values, and R-squared values. Understanding regression allows you to make predictions and draw insights from data.

Beginner Probability and Statistic Questions and Answers asked by FAANG
1. What is the probability of rolling a 6 on a fair six-sided die?
The probability of rolling a 6 on a fair six-sided die is 1/6.

2. Calculate the expected value of a fair coin flip.
The expected value of a fair coin flip is 0.5 (or 1/2).

3. Explain the concept of simple random sampling in statistics.
Simple random sampling is a method where every member of the population has an equal chance of being selected in the sample.

4. Define the Central Limit Theorem and its significance in statistics.
The Central Limit Theorem states that the distribution of the sample mean approaches a normal distribution as the sample size increases, regardless of the shape of the original population distribution.

5. What is a p-value, and how is it used in hypothesis testing?
A p-value is a probability measure used in hypothesis testing that quantifies the evidence against a null hypothesis. A smaller p-value suggests stronger evidence against the null hypothesis.

More Questions
Not enough? Try these questions for FREE:

Two Consecutive Sixes
Coin Fairness Test
Medium Probability and Statistic Questions and Answers asked by FAANG
6. Given two events A and B, how do you calculate P(A|B) (the conditional probability of A given B)?
Conditional probability P(A|B) is calculated as the probability of both events A and B occurring (P(A ∩ B)) divided by the probability of event B occurring (P(B)).

7. Explain the Bayesian probability theory and its application in data science.
Bayesian probability is a framework that incorporates prior beliefs and updates them with new evidence using Bayes' theorem, allowing for probabilistic reasoning and decision-making.

8. What is the confidence interval, and how do you interpret a 95% confidence interval?
A 95% confidence interval means that if we were to take many random samples and construct confidence intervals from them, we would expect approximately 95% of those intervals to contain the true population parameter.

9. Describe the sampling distribution of the sample mean.
The sampling distribution of the sample mean is a normal distribution with the same mean as the population and a standard deviation equal to the population standard deviation divided by the square root of the sample size.

10. Calculate the z-score for a data point in a standard normal distribution.
The z-score for a data point in a standard normal distribution is calculated as (X - μ) / σ, where X is the data point, μ is the mean, and σ is the standard deviation.

More Questions
Not enough? Try these questions for FREE:

Consecutive Fives
Biased Coin?
Hard Probability and Statistic Questions and Answers asked by FAANG
11. Compare and contrast the Poisson and Binomial distributions.
The Poisson distribution models the number of events occurring in a fixed interval of time or space, while the Binomial distribution models the number of successes in a fixed number of independent trials.

12. What is the difference between Type I and Type II errors in hypothesis testing?
Type I error occurs when we reject a true null hypothesis, while Type II error occurs when we fail to reject a false null hypothesis.

13. Explain the concept of MLE and provide an example of its application.
Maximum Likelihood Estimation is a method used to estimate the parameters of a statistical model by maximizing the likelihood function. For example, in the case of a normal distribution, MLE estimates the mean and standard deviation.

14. What is covariance, and how does it differ from correlation?
Covariance measures the degree to which two variables change together, while correlation measures the strength and direction of the linear relationship between two variables.

15. Describe stratified sampling and its advantages over simple random sampling.
Stratified sampling involves dividing the population into subgroups or strata and then taking random samples from each stratum. It is advantageous when there is significant variation within strata.

More Questions
Not enough? Try these questions for FREE:

Product vs. Square
Minimum of Two Uniform Variables
Expert Probability and Statistic Questions and Answers asked by FAANG
Questions:
16. How does Monte Carlo simulation work, and what are its applications in data science?
Monte Carlo simulation is a computational technique that uses random sampling to solve complex problems or estimate numerical results. It has applications in finance, engineering, and optimization problems.

17. Define bootstrapping and discuss its use in estimating population parameters.
Bootstrapping is a resampling technique where samples are drawn with replacement from the observed data to estimate population parameters. It is useful when parametric assumptions are uncertain.

18. Explain the principles of Bayesian networks and their role in probabilistic graphical models.
Bayesian networks are graphical models that represent probabilistic relationships among a set of variables. They are used for probabilistic reasoning, decision-making, and risk analysis.

19. What are autoregressive (AR) and moving average (MA) models in time series analysis?
Autoregressive (AR) models describe a time series using its own past values while moving average (MA) models describe a time series using past forecast errors.

20. Discuss the concept of familywise error rate and methods to control it in multiple hypothesis testing scenarios.
Familywise error rate is the probability of making at least one Type I error when conducting multiple hypothesis tests. Methods to control it include Bonferroni correction and false discovery rate control.
Product & Experimentation / A/B Test Questions
A core part of the Meta data scientist product analytics interview involves product-sense and experimentation challenges that test your ability to define metrics, evaluate tests, and think like a product partner in a data-driven organization:

7. How would you assess the validity of the result in an AB test?

To assess the validity of the result, first ensure that the user groups were properly separated and randomly bucketed to avoid biases. Next, confirm that the variants were equal in all other aspects except the tested feature. Additionally, evaluate the measurement process, including sample size, duration of the test, and whether the p-value was monitored continuously, as this can lead to false positives or negatives. Finally, determine the minimum effect size and compute the required sample size and duration before starting the experiment to ensure statistical reliability.

8. Revenue Retention: Calculate the formula for the average lifetime value

To calculate the average lifetime value (LTV) for a subscription-based service, use the formula (LTV = \sum_{i=0}^{\infty} 100 \cdot 0.9^i), where 100 is the monthly subscription cost and 0.9 represents the retention rate after accounting for 10% churn. This formula sums the expected value of the customer over infinite months, converging to (LTV = $1000). The calculation accounts for retention rates and churn, providing a more accurate LTV than simply multiplying the average customer duration by the monthly cost.

9. Given a list of strings, write a function that returns the longest common prefix

To measure the percentage of fake news stories within 24 hours, start by defining what constitutes fake news, such as spam, bot-generated content, or incorrect media reporting. Then, use unsupervised methods to identify fake accounts, analyze article sharing patterns, and validate media outlets’ credibility. Metrics like user engagement, account authenticity, and cross-referencing news titles can help infer the percentage of fake news stories.

10. Given a team wants to A/B test multiple changes through a sign-up funnel, how would you set up this test?

To set up this test, you would create a multivariate test with four variants: red button at the top, red button at the bottom, blue button at the top, and blue button at the bottom. Randomly assign users to each variant and calculate the sample size needed to reach statistical significance. Alternatively, chaining A/B tests could be considered, but this approach would not account for interaction effects between the variables.

11. Given customer spending data, how would you determine the next partner card?

To determine the next partner card, analyze customer transaction data by grouping spending by merchants and identifying high-spending merchants without existing partnerships. Consider metrics like average transaction price and frequency to optimize for recurring spending. Additionally, examine year-over-year growth trends to forecast future high-spending merchants and conduct external research to identify merchants popular outside the current dataset for customer acquisition.

Behavioral & Culture-Fit Questions
The data scientist Meta interview also includes behavioral prompts designed to assess collaboration, resilience, and alignment with Meta’s values—key qualities that Facebook data science interview loops look for in high-performing candidates:

12. Describe an analytics experiment that you designed. How were you able to measure success?

In a Meta context, this question is often about demonstrating your ability to design robust, scalable experiments that drive product decisions. You might describe designing an A/B test to evaluate the impact of a new ranking algorithm on user engagement within the News Feed.

Success would be measured through metrics like time spent, click-through rates, or meaningful social interactions. You should explain how you set up the control and treatment groups, ensured randomization, and accounted for bias or variance. Mention tools like Hive, PyTorch, or Meta’s internal experimentation platforms, and show how you used p-values or confidence intervals to validate findings and inform product rollout decisions.

13. Talk about a time when you had trouble communicating with stakeholders. How were you able to overcome it?

At Meta, stakeholders range from product managers to engineers and designers, and they often have different levels of technical fluency. You could describe a time when you presented a complex causal inference model, but the team found it too abstract.

To overcome this, you simplified the narrative using data visualizations and drew parallels to user behavior they were more familiar with. You might mention how you scheduled follow-ups or embedded yourself in sprint reviews to build context. Emphasize how this experience improved your stakeholder management and ultimately helped align teams on the insights and next steps.

14. Why Do You Want to Work With Us

For a Metadata Scientist role at Meta, you should go beyond generic enthusiasm. Highlight how Meta’s commitment to open science, large-scale experimentation, and infrastructure for data scientists aligns with your interests.

For example, you could mention your excitement about working on products that impact billions of users or the opportunity to contribute to innovations in content discovery, trust and safety, or generative AI. Be specific about how Meta’s scale and emphasis on measurement excellence match your goals. This is also a good place to reflect on Meta’s core values like “Focus on Long-Term Impact” or “Move Fast.”

15. Describe a data project you worked on. What were some of the challenges you faced?

This is a chance to show your ability to drive a project end-to-end. For a Meta role, you could describe working on a project where you built a machine learning model to predict user churn or optimized a recommendation pipeline.

Explain the technical scope, like data ingestion with Presto, model training using PyTorch, and how you collaborated with product teams to define success. Then describe real-world challenges—maybe you faced incomplete logging or had to backfill data to simulate historical results. Talk about how you navigated ambiguity, validated your approach with stakeholders, and iterated to ship a solution that drove measurable impact.
Q29. Listing conversion rate percentile among listings.
 Presto:
WITH v AS (
  SELECT listing_id, COUNT(*) AS views
  FROM events WHERE event_name='view_listing' GROUP BY 1
),
b AS (
  SELECT listing_id, COUNT(*) AS conf
  FROM bookings WHERE status='confirmed' GROUP BY 1
),
cr AS (
  SELECT v.listing_id,
         CASE WHEN v.views>0 THEN COALESCE(b.conf,0)*1.0/v.views ELSE 0 END cr
  FROM v LEFT JOIN b ON v.listing_id=b.listing_id
)
SELECT listing_id, cr,
       cume_dist() OVER (ORDER BY cr) AS cr_percentile
FROM cr ORDER BY cr DESC;





Q30. Small tables as inline views vs CTE: show both patterns.
 (Conceptual) Use CTEs for readability & reuse. Inline views fine for one-off
 short logic; prefer CTEs when referenced multiple times to avoid duplication.

7) Strings, JSON, Arrays/Maps (3)
Q31. Extract search query & guests from event properties JSON.
 Presto:
SELECT session_id, event_ts,
       json_extract_scalar(properties,'$.q')      AS query,
       CAST(json_extract_scalar(properties,'$.guests') AS INTEGER) AS guests
FROM events
WHERE event_name='search';





Q33. Parse UTM-like source/medium into a normalized channel.
 (Example of CASE normalization)
SELECT session_id,
  CASE
    WHEN source='google' AND medium='cpc' THEN 'SEM'
    WHEN source='facebook' AND medium='cpc' THEN 'Facebook'
    WHEN source='email'   THEN 'Email'
    WHEN medium='organic' THEN 'SEO'
    ELSE 'Direct/Other'
  END AS channel_norm
FROM sessions;




Q34. 4-step funnel (page_view → search → view_listing → start_booking) by day.
 Presto:
WITH steps AS (
  SELECT DATE(e.event_ts) AS day, e.session_id, e.user_id,
    MAX(CASE WHEN event_name='page_view'     THEN 1 END) AS s1,
    MAX(CASE WHEN event_name='search'        THEN 1 END) AS s2,
    MAX(CASE WHEN event_name='view_listing'  THEN 1 END) AS s3,
    MAX(CASE WHEN event_name='start_booking' THEN 1 END) AS s4
  FROM events e GROUP BY 1,2,3
)
SELECT day,
  COUNT(*)                                   AS sessions,
  SUM(CASE WHEN s1=1 THEN 1 ELSE 0 END)      AS step1,
  SUM(CASE WHEN s2=1 THEN 1 ELSE 0 END)      AS step2,
  SUM(CASE WHEN s3=1 THEN 1 ELSE 0 END)      AS step3,
  SUM(CASE WHEN s4=1 THEN 1 ELSE 0 END)      AS step4,
  step4*1.0/NULLIF(step1,0)                  AS funnel_conv
FROM steps
GROUP BY day
ORDER BY day;




Q35. Monthly signup cohorts and D30 retention (had any session by D30).
 Presto:
WITH cohorts AS (
  SELECT user_id, date_trunc('month', signup_date) AS cohort
  FROM users
),
ret AS (
  SELECT u.user_id,
         MIN(DATE(s.session_start)) AS first_seen
  FROM users u JOIN sessions s ON u.user_id=s.user_id
  GROUP BY u.user_id
)
SELECT c.cohort,
  COUNT(*) AS users,
  SUM(CASE WHEN date_diff('day', c.cohort, r.first_seen) <= 30 THEN 1 ELSE 0 END)
    AS d30_retained
FROM cohorts c LEFT JOIN ret r ON c.user_id=r.user_id
GROUP BY c.cohort
ORDER BY c.cohort;





Q36. Biggest drop-off step between S1→S4 (same steps as Q34).
 Use Q34 CTE and compute rates step(k+1)/step(k); smallest rate = largest drop.



Q37. Path length distribution (unique event names per session).
SELECT
  COUNT(DISTINCT event_name) AS uniq_events,
  COUNT(DISTINCT session_id) AS sessions
FROM events
GROUP BY uniq_events
ORDER BY uniq_events;





Q38. New vs repeat bookers by normalized channel.
WITH first_book AS (
  SELECT user_id, MIN(booking_ts) AS first_bkg_ts
  FROM bookings WHERE status='confirmed' GROUP BY 1
),
sess AS (
  SELECT b.user_id, b.booking_ts, c.channel
  FROM bookings b
  JOIN sessions s ON b.user_id=s.user_id
  LEFT JOIN campaigns c ON s.campaign_id=c.campaign_id
  WHERE b.status='confirmed'
)
SELECT COALESCE(channel,'Organic/Direct') AS channel,
       SUM(CASE WHEN b.booking_ts = fb.first_bkg_ts THEN 1 ELSE 0 END) AS new,
       SUM(CASE WHEN b.booking_ts <> fb.first_bkg_ts THEN 1 ELSE 0 END) AS repeat
FROM sess b JOIN first_book fb ON b.user_id=fb.user_id
GROUP BY 1 ORDER BY 1;


9) Attribution & Marketing Metrics (4)




Q39. 7-day last-click attribution from click → booking.
 Presto:
WITH lc AS (
  SELECT user_id, campaign_id, click_ts,
         ROW_NUMBER() OVER (PARTITION BY user_id ORDER BY click_ts DESC) rn
  FROM ad_clicks
),
b AS (
  SELECT booking_id, user_id, booking_ts, price_usd
  FROM bookings WHERE status='confirmed'
)
SELECT b.booking_id, b.user_id, b.price_usd, lc.campaign_id
FROM b
JOIN lc ON b.user_id=lc.user_id AND lc.rn=1
WHERE date_diff('day', DATE(lc.click_ts), DATE(b.booking_ts)) BETWEEN 0 AND 7;




Q40. Simple position-based attribution (40-20-40) in-session paths.
 (Interview-style sketch; actual calc requires ordered path of touches. Explain
 you’d explode an ordered array of touches per user, use positions to weight:
 first=0.4, last=0.4, mids share 0.2. Sum weights per campaign, then multiply by
 booking revenue. Provide steps; exact code omitted for brevity in live round.)



Q41. ROAS by campaign (confirmed bookings / spend).
 (Combine revenue by campaign from sessions→bookings with spend_daily as in Q10.)



Q42. Incrementality outline (geo holdout or time-based DiD).
 (Conceptual) Form matched geo pairs or date windows; compute
 (treat_post - treat_pre) - (ctrl_post - ctrl_pre) on bookings/revenue
 per capita; validate pre-period balance.






Q43. A/B revenue per assigned user; diff and lift.
WITH agg AS (
  SELECT ea.variant,
         AVG(em.metric_value) AS avg_rev_per_user
  FROM experiment_assignment ea
  LEFT JOIN experiment_metrics em
    ON ea.user_id=em.user_id AND ea.exp_name=em.exp_name
   AND em.metric_name='gross_revenue'
  WHERE ea.exp_name='NewCheckout'
  GROUP BY ea.variant
)
SELECT
  MAX(CASE WHEN variant='treatment' THEN avg_rev_per_user END) AS trt,
  MAX(CASE WHEN variant='control'   THEN avg_rev_per_user END) AS ctl,
  (MAX(CASE WHEN variant='treatment' THEN avg_rev_per_user END) -
   MAX(CASE WHEN variant='control'   THEN avg_rev_per_user END)) AS abs_diff,
  CASE WHEN MAX(CASE WHEN variant='control' THEN avg_rev_per_user END) > 0
       THEN (MAX(CASE WHEN variant='treatment' THEN avg_rev_per_user END) /
             MAX(CASE WHEN variant='control'   THEN avg_rev_per_user END) - 1)
  END AS rel_lift
FROM agg;




Q44. CUPED (conceptual with SQL hint).
 Compute pre-period covariate X (e.g., prior spend/sessions) per user and
 treatment indicator T. Estimate theta = cov(Y,X)/var(X) (via a small SQL
 aggregation over both arms). Then adjust Y* = Y - theta*(X - mean(X)).
 Compare means of Y* across variants. (Note: run regressions outside SQL or
 via Presto functions if allowed).



Q45. Guardrail: CTR difference across variants.
WITH a AS (
  SELECT ea.variant,
         COUNT(DISTINCT i.user_id) AS imps_users,
         COUNT(DISTINCT c.user_id) AS click_users
  FROM experiment_assignment ea
  LEFT JOIN ad_impressions i ON ea.user_id=i.user_id
  LEFT JOIN ad_clicks c ON ea.user_id=c.user_id
  WHERE ea.exp_name='NewCheckout'
  GROUP BY ea.variant
)
SELECT variant,
       click_users*1.0/NULLIF(imps_users,0) AS user_ctr
FROM a;

(Discuss statistical test done outside SQL or via UDFs.)



Q46. SRM check (Sample Ratio Mismatch).
SELECT variant, COUNT(DISTINCT user_id) AS n
FROM experiment_assignment
WHERE exp_name='NewCheckout'
GROUP BY variant;

(Compare to planned split 50/50; flag if chi-square p-value small.)






Q47. WHERE vs HAVING.
Use WHERE to filter rows before aggregation (faster, less data).

Use HAVING to filter after GROUP BY on aggregated results.

Prefer partition pruning (e.g., WHERE day BETWEEN ...) when tables are
 partitioned.






Q48. Join null-safe + defaulting channels.
In paid/organic mixes, campaign_id may be NULL. Use LEFT JOIN and
 COALESCE(channel,'Organic/Direct'). Avoid inner joins that drop organic.





Q49. Data skew mitigation.
Skewed keys (e.g., campaign_id = 201 huge) cause reducers to overload.




Q50. Safe casting & dates.
Avoid implicit casts; use CAST explicitly.


Standardize to DATE(...) and TIMESTAMP consistently.


For timezones, normalize to UTC on write; convert on read for reporting.



Quick sanity checks you can run now
Who booked?


SELECT user_id, price_usd FROM bookings WHERE status='confirmed';

Channel normalized path (sessions):


SELECT session_id, source, medium,
  CASE
    WHEN source='google' AND medium='cpc' THEN 'SEM'
    WHEN source='facebook' AND medium='cpc' THEN 'Facebook'
    WHEN source='email'   THEN 'Email'
    WHEN medium='organic' THEN 'SEO'
    ELSE 'Direct/Other'
  END AS channel_norm
FROM sessions ORDER BY session_id;


How to practice effectively (Airbnb-aligned)
Emphasize window functions (ranking, dedupe, funnels),
 anti-joins, sessionization, cohorts/retention, and paid vs
 organic blending with campaign lookups.


Be ready to:

Build a day-level funnel and identify the drop-off step (Q34, Q36).
Attribute bookings to last click within a window (Q39).
Compute DAU/WAU/MAU and MoM growth (Q11, Q24).
Normalize channels robustly (Q33, Q26).
Summarize experiment outcome & articulate validity checks (Q43–Q46).




Section 1: The Advanced SQL Questions
These questions go beyond simple syntax and test your ability to build logical, multi-step solutions.

Question 1: How would you find the month-over-month growth rate for a company’s total revenue?
Sample Table: transactions (transaction_id, transaction_date, revenue)
The SQL Answer:
SQL
WITH MonthlyRevenue AS (
    SELECT
        STRFTIME('%Y-%m', transaction_date) AS sales_month,
        SUM(revenue) AS total_revenue
    FROM
        transactions
    GROUP BY
        sales_month
)
SELECT
    sales_month,
    total_revenue,
    LAG(total_revenue, 1) OVER (ORDER BY sales_month) AS previous_month_revenue,
    (total_revenue - LAG(total_revenue, 1) OVER (ORDER BY sales_month)) * 100.0 / LAG(total_revenue, 1) OVER (ORDER BY sales_month) AS growth_rate
FROM
    MonthlyRevenue
ORDER BY
    sales_month;
Interviewer’s Intent: This is a classic question that tests your ability to perform time-series analysis. They are looking for your knowledge of window functions, specifically LAG(), which allows you to access a row from a previous record in the same result set without a complex self-join. The use of a CTE also demonstrates your ability to break down a multi-step problem into a readable solution.
Question 2: Given a table of employees, how would you find each employee’s manager’s name?
Sample Table: employees (employee_id, employee_name, manager_id)
The SQL Answer:
SQL
SELECT
    e1.employee_name AS employee,
    e2.employee_name AS manager
FROM
    employees e1
JOIN
    employees e2 ON e1.manager_id = e2.employee_id;
Interviewer’s Intent: This question tests your understanding of self-joins. The interviewer wants to see if you can join a table to itself to handle hierarchical or relational data. The use of clear table aliases (e1 and e2) is key here to make the query understandable.
Question 3: How would you pivot a table to show total sales by product category and region, with regions as columns?
Sample Table: sales_data (product_category, region, sales_amount)
The SQL Answer (using Conditional Aggregation):
SQL
SELECT
    product_category,
    SUM(CASE WHEN region = 'North' THEN sales_amount ELSE 0 END) AS total_sales_north,
    SUM(CASE WHEN region = 'South' THEN sales_amount ELSE 0 END) AS total_sales_south,
    SUM(CASE WHEN region = 'East' THEN sales_amount ELSE 0 END) AS total_sales_east
FROM
    sales_data
GROUP BY
    product_category;
Interviewer’s Intent: This tests your ability to transform data from a tall format to a wide format. They are looking for the common and portable solution of using GROUP BY with conditional aggregation (CASE WHEN) instead of a database-specific PIVOT function. This shows you can prepare data for business intelligence tools.
Question 4: You have a log of user sessions. How would you find the average time between a user’s consecutive logins?
Sample Table: sessions (user_id, session_start_time)
The SQL Answer:
SQL
WITH SessionDurations AS (
    SELECT
        user_id,
        session_start_time,
        LAG(session_start_time, 1) OVER (PARTITION BY user_id ORDER BY session_start_time) AS previous_session_time
    FROM
        sessions
)
SELECT
    AVG(JULIANDAY(session_start_time) - JULIANDAY(previous_session_time)) AS average_time_between_logins_days
FROM
    SessionDurations
WHERE
    previous_session_time IS NOT NULL;
Interviewer’s Intent: This is a comprehensive problem that combines several advanced concepts. The interviewer is testing your ability to use LAG() with PARTITION BY. The PARTITION BY user_id is crucial, as it tells the function to restart the time-series calculation for each unique user, which is what's needed for the correct answer. It also tests your ability to handle date calculations.
Question 5: Write a query to find the top 3 best-selling products in each sales region.
Sample Table: sales (sale_id, region, product_id, quantity)
The SQL Answer:
SQL
WITH ProductSales AS (
    SELECT
        region,
        product_id,
        SUM(quantity) as total_quantity
    FROM
        sales
    GROUP BY
        region, product_id
),
RankedSales AS (
    SELECT
        region,
        product_id,
        total_quantity,
        RANK() OVER (PARTITION BY region ORDER BY total_quantity DESC) as ranking
    FROM
        ProductSales
)
SELECT
    region,
    product_id,
    total_quantity
FROM
    RankedSales
WHERE
    ranking <= 3;
Interviewer’s Intent: This is the classic “Top-N per Group” problem. The interviewer is testing your mastery of window functions (RANK()) and CTEs (WITH). An answer that uses PARTITION BY and RANK() is far more efficient and scalable than a solution that tries to use subqueries or self-joins to achieve the same result.
Advanced SQL Cheat Sheet
This guide provides a quick, scannable summary of the key concepts for your post.

LAG/LEAD: Compare a value to the previous or next row in a result set.
Syntax: LAG(column, offset) OVER (ORDER BY column)
Use Case: Calculating month-over-month revenue growth.
PARTITION BY: Divide a table’s rows into partitions, to which a window function is applied.
Syntax: RANK() OVER (PARTITION BY column ORDER BY column)
Use Case: Finding the top N products in each region.
Self-Joins: Join a table to itself.
Syntax: SELECT ... FROM table t1 JOIN table t2 ON t1.id = t2.parent_id
Use Case: Finding each employee’s manager in a single table.
CTEs (Common Table Expressions): Define a temporary named result set within the scope of a single query.
Syntax: WITH CTE_Name AS (SELECT ...) SELECT ... FROM CTE_Name
Use Case: Breaking down a complex, multi-step query into readable parts.
Conditional Aggregation: Aggregate data based on specific conditions.
Syntax: SUM(CASE WHEN condition THEN column ELSE 0 END)
Use Case: Pivoting rows into columns (e.g., total sales per region).
Press enter or click to view image in full size

Conclusion: Go in with Confidence
These questions might seem intimidating, but they are all solvable with a few key concepts. Mastering them demonstrates not only your technical skills but also your logical thinking and ability to break down complex problems.

Go in with confidence. You have the skills. You know the concepts. Now, you also know what the interviewer is really looking for.
Six-step framework to solve hard SQL questions
Selecting transcript lines in this section will navigate to timestamp in the video
- Top tech companies ask really hard SQL interview questions. So if you don't follow a logical process for solving them you'll probably mess up when you're asked to write a 50 line query on the spot. That's why to solve hard SQL interview questions use this six step problem solving framework. First, understand the question being asked. That means asking your interviewer some clarifying questions and also exploring the input data. Second, identify the relevant tables and columns that you'll need to solve the problem. Because companies often give you extraneous tables and columns, it's best to filter out the irrelevant information early on. Third, consider edge cases. Before we write some code, check for nulls in the dataset, non-unique data, and figure out how you're going to handle ties in the output. Fourth, break down the problem to smaller chunks. Because your final solution will have multiple CTs and sub-queries, it's way too much to solve the question in one big swing. Now, the fifth step is to actually start solving these sub-problems. You heard me right. The first four steps was to make sure we're heading down the right path. It's only now on the fifth step do we actually start writing SQL code to start solving these sub-problems. And now the sixth and final step, testing. You got to actually run and test your code. Ideally, testing the code incrementally for each of the smaller sub-problems because it is way easier doing that than having a big jumble of SQL code and testing it all at the end. So there you go, the six step framework you need to use to solve hard SQL interview questions.
Smoothing Data
Raw data is inherently noisy and presenting it as-is can be distracting. Use different types of moving averages to present a streamlined and consistent story.

Calculating Running Total
Calculating Running/Moving Average
Calculating Weighted Moving Average
Calculating Exponential Moving Average

Calculations Per Group
Key business decisions are made based on comparisons across various product, customer and employee groups. Use SQL window functions to calculate sales compensation plans and quotas, year-to-date comparisons and hot selling products across categories.

Calculating Percentage (%) of Total Sum
Calculating Differences from Beginning/First Row
Calculating Top N items per Group
Calculating Top N items and aggregating (sum) the remainder into "All other"
Calculating Distinct & Unique items per Group

Growth Rates
Growth rates are present in every business report. Model month-over-month and exponential growth rates and Pareto charts in order to compare and focus your efforts.

Calculating Month-over-Month Growth Rate
Calculating Exponential Growth Rate
Creating Pareto Charts to visualize the 80/20 principle

Summarizing Data
When you get your hands on a dataset, you'll want to quickly get a feel for the data. Looking at the shape of the data, bucketing into groups, finding outliers, calculating relationships and correlations lets you do that.

Calculating Summaries with Descriptive Statistics
Calculating Summaries with Histogram Frequency Distributions
Calculating Relationships with Correlation Matrices
Calculating N-tiles (quartiles, deciles and percentiles)
Calculating Z-Scores
Gap analysis to find missing values in a sequence

Ranking your Best and Worst Customers
Understanding your best and worst customers is key to profitable growth. Use lead-scoring and net promoter score surveys to rank your customers.

Analyzing Recency, Frequency and Monetary value to index your best customers
Segmenting and Lead scoring your email list
Analyzing Net Promoter Score (NPS) surveys to improve customer satisfaction & loyalty


Forecasting & Predicting the future
Accurate forecasting of future activity is incredibly useful when provisioning resources and maintaining sufficient lead time.

Calculating Linear Regression Coefficients
Forecasting in presence of Seasonal effects using the Ratio to Moving Average method


SQL for marketing
Marketing teams are overwhelmed with data. Understand return on ad spend, attribute revenue to marketing programs and payback periods of different marketing channels.

Multichannel Marketing Attribution Modeling
Funnel Analysis


Data Cleansing (aka "Wrangling")
Dirty data can lead you astray. Understand pattern matching (eg: business emails), filling missing data, removing duplicates and empty values to sufficiently deal with messy data.

Finding Duplicate Rows
Filling Missing Data & Plugging Gaps by Generating a Continuous Series
Finding Patterns & Matching Substrings using Regular Expressions
Concatenating Rows of String Values for Aggregation
SQL's NULL values: comparing, sorting, converting and joining with real values



Business Model Analysis
Using SQL to analyze Bitcoin, Ethereum & Cryptocurrency Performance
Estimating Demand Curves and Profit-Maximizing Pricing
Account-level CRM analytics for B2B SaaS companies

Comparing Means with Statistical Testing
Calculating Medians
Calculating Fractional and Ordinal Rank
Calculating n-grams
Calculating Funnel drop-off metrics
Cohort charts for Retention Analysis
Understanding EXPLAIN ANALYZE
Pivoting and Unpivoting data
Intrusion Dection with IP addresses
GIS/Spatial queries
Correlated and Uncorrelated subqueries

Solve enough SQL interview questions, and you’ll start to see the same patterns come up over-and-over again. As the author of Ace the Data Science Interview, and founder of SQL interview platform DataLemur, I’ve solved over 300 SQL interview questions myself and seen just about every type of SQL interview question that gets asked.

To save you time, I put together this guide on the top patterns you’ll find in SQL interview questions:

Give Me All X, BUT Filter It Down
Compute a Rolling Average
Analyze Pairs Of Things With Self-Joins
Find Top X Things (Rank Functions)
Optimizing Query Performance
Scroll down to dive into each pattern👇👇

SQL Interview Patterns

SQL Interview Pattern 1: Give Me All X, BUT Filter It Down.
To answer the “gimme all X BUT make sure they’re Y and Z” questions, you’ll have to SELECT all records that match a certain condition. Sometimes, instead of simply SELECT’ing records, you might have to give the count, average, or sum of all the records that match some condition but the logic is mostly the same.

Irregarldess, the crux of the question is about filtering down the dataset correctly based on one or several of these conditions: Filter based on data in another table (via join) Filter based on string matching (tests your use of regex and LIKE syntax) Filter based on timestamps (tests your ability to use date/time operators)

An example Meta interview question would be: “-For each month, find me the count of all facebook users who created their account that month-”

Your solution use the following snippet:

  EXTRACT(
    MONTH FROM created_at
    ) AS "Month";
While date/time commands aren’t as common in SQL interviews (since the syntax varies so greatly between different SQL flavors), in my sql interview guide I shortlisted some of the most important PostgreSQL date/time functions for interviews.

SQL Interview Pattern 2: Rolling Average
You’ll be asked to find the rolling average, or trailing average for some set of data. While it’s common to have some type of orders, purchases, or transaction type dataset, for Product Data Science roles you might get a user analytics question like “What’s the 7-day rolling average Daily Active User count (DAU)”.

You could use a query like the following:

SELECT date, active_users,
       AVG(active_users) OVER (ORDER BY date ROWS BETWEEN 6 PRECEDING AND CURRENT ROW) AS rolling_average
FROM daily_active_users
These questions are popular SQL interview questions because in-real world analytics, there is so much seasonality in day-to-day metrics. That’s why accounting for weekend effects, most of the time you're asked to create a rolling 7-day or 28-day metric.

Fun fact: using a 30-day rolling average (aka a monthly rolling average) isn’t a great idea at the biggest tech companies, because weekday vs. weekend behavior is super different. And unfortunately, in a 30-day period there might be 4 weekends, 5 weekends, or 4.5 weekends, which can throw off your numbers completely. Source: worked at FB’s growth team, and messed up an important analysis this way…..woops.

SQL Interview Pattern 3: Analyze Pairs Of Things With Self-Joins
Frequently you’ll be asked to analyze pairs of things – maybe pairs of social media users (to see how much they message each other), or pairs of products (to see how often they are bough together), etc.

For these SQL questions, always think of self-joins (where you join a table to itself).

For example, say you had website visitor data exported from a company's Google Analytics account, and you had to analyze pairs of pages for UX or navigational issues. As part of that analysis, you wanted to generate all pairs of URLs, but needed to avoid pairs where both the URLs were the same since that's not a valid pair.

The self-join query would look like the following:

SELECT page1.url AS page_url, page2.url AS referred_from
FROM google_analytics AS page1
JOIN google_analytics AS page2 ON page1.referrer_id = page2.id
WHERE page1.id <> page2.id;
This query returns the url of each page (page1.url) along with the url of the page that referred to it (page2.url). The self-join is performed using the referrer_id field, which specifies the id of the page that referred the visitor to the current page, and avoids any pages that referred themself (aka data anomalies).

To try a self-join yourself, try this real SQL question from a Walmart interview about finding frequently purchased pairs of items!

Walmart SQL Interview Question: Frequent Pairs of Items

SQL Interview Pattern 4: Find Top X Things
Another common SQL interview question pattern is being asked to find the top X things. For example:

find me the top 3 employee salaries in each department
find me the 2nd most sold product in each Amazon category
find me the 3rd transaction made for every Uber rider
For these questions, you’ll want to immediately think of window functions like RANK() and ROW_NUMBER().

To test this pattern out yourself, checkout this Uber SQL Assessment Question about selecting a user's 3rd transaction made on the Uber platform.Uber SQL Interview Window Function Assessment

SQL Interview Pattern 5: Optimizing Query Performance
Usually, for Data Analysts and Data Scientists, questions about improving SQL query performance aren’t asked by themself. Instead, after you write a query to solve an earlier problem, the interviewer will follow-up and ask you to optimize your SQL query.

Here they are testing to see how deep your knowledge of database internals goes – are you used to writing simple queries, or have you had to analyze performance bottlenecks in complex SQL queries, and truly understand the DBMS you’re working on top of?

While there’s a ton of info out there on SQL query optimization tips, for 90% of Data Analyst and Data Science interviews, you’ll pass by mentioning these three things: Don’t SELECT * random fields - only get the columns you need Try to use database indexes Avoid joins if possible – try denormalizing your database

Of course, for proper Data Engineering and Database Administration roles, you’ll need to have more in-depth answers for which you can prep for by reading our guide on database design interview questions and by reading the classic book Database Design for Mere Mortals
SQL for Data Science
Last Updated : 23 Jul, 2025
Mastering SQL (Structured Query Language) has become a fundamental skill for anyone pursuing a career in data science. As data plays an increasingly central role in business and technology, SQL has emerged as the most essential tool for managing and analyzing large datasets. Data scientists rely on SQL to efficiently query, manipulate, and extract insights from vast amounts of information. With SQL, professionals can interact with databases, filter data, and perform complex operations that are crucial for data analysis and decision-making.

SQL-for-Data-Science
As companies shift toward a more data-centric approach, SQL is becoming a vital part of the data science workflow. Learning SQL not only opens doors to career opportunities in this high-demand field, but it also empowers individuals to unlock valuable insights from complex datasets. Whether you’re working with databases, building predictive models, or creating reports, SQL provides the foundation for data-driven decision-making. This article will guide you through the key SQL concepts and skills every data scientist should master to excel in the industry.

Getting Started with SQL for Data Science
This section introduces SQL as the foundational tool for data analysis in data science. It covers the basic concepts of relational databases, the structure of SQL queries, and the importance of SQL in extracting, manipulating, and storing data. Students will learn to set up their environment and begin writing simple queries to interact with data

Installing MySQL/PostgreSQL
Understanding SQL Commands
SQL CREATE DATABASE
SELECT
SQL INSERT INTO
SQL UPDATE
SQL DELETE
SQL ALTER TABLE
DROP and TRUNCATE in SQL
Basic SQL Queries for Data Science
In this section, we will dive into the essential SQL commands needed for data manipulation, such as SELECT, FROM, WHERE, ORDER BY, and LIMIT. Data scientists will learn how to filter, sort, and retrieve data from databases to answer basic analytical questions. It includes examples like filtering data based on conditions and selecting specific columns.

Select Distinct
Select Individual Columns
Retrieving All Columns (SELECT *)
WHERE Clause
SQL HAVING Clause
SQL | BETWEEN & IN Operator
SQL Comparison Operators
SQL Logical Operators
SQL LIKE Operator
Wildcard Pattern Matching
SQL IS NULL Operator
NULL values in SQL
SQL ORDER BY
SQL Multiple Column Ordering
SQL LIMIT Clause
SQL TOP, LIMIT, FETCH FIRST Clause
SQL | Aliases
Aggregate Functions and Grouping Data
Now let's cover SQL’s aggregate functions like COUNT(), SUM(), AVG(), MIN(), and MAX(). It explains how to group data using the GROUP BY clause and filter grouped results with HAVING. This is essential for summarizing data, such as calculating averages, totals, or finding trends across categories

SQL Aggregate functions
SQL COUNT(), AVG() and SUM() Function
SQL | GROUP BY
How to Group and Aggregate Data Using SQL?
HAVING With Aggregate Functions
Difference Between WHERE and HAVING
Joining Data from Multiple Tables
Data often resides in different tables, and this topic teaches how to combine them using JOIN operations. This includes INNER JOIN, LEFT JOIN, RIGHT JOIN, and FULL JOIN, allowing users to retrieve and merge data from multiple related tables, which is crucial for analyzing relationships between datasets.

What is a JOIN?
SQL Inner Join
SQL Self Join
SQL LEFT JOIN
SQL RIGHT JOIN
SQL FULL JOIN
SQL CROSS JOIN
SQL Full Outer Join Using Where Clause
Multiple Joins in SQL
Data Cleaning and Transformation for Data Science
In real-world datasets, data is often messy or incomplete. This topic introduces SQL methods for cleaning and transforming data, such as removing duplicates, handling missing values, and normalizing data. It’s essential for preparing datasets for analysis and ensuring accuracy in results.

SQL Query to Delete Duplicate Rows
SQL | Remove Duplicates without Distinct
SQL | NULL functions
IFNULL VS COALESCE
Conversion Function in SQL
SQL Query to Convert Datetime to String
SQL Data Types
Modifying existing data in SQL
SQL Date and Time Functions
How to Get Current Date and Time in SQL?
SQL Query to Check Given Format of a Date
SQL | String functions
SQL | Character Functions with Examples
SQL | Concatenation Operator
SQL Query to Match Any Part of String
Working with Large Datasets
Data scientists frequently work with massive datasets, and this section covers techniques for optimizing queries and managing large datasets. Topics include pagination, indexing, and partitioning. The goal is to improve query performance and minimize resource usage when dealing with big data.

SQL Performance Tuning
Best Practices For SQL Query Optimizations
SQL Query Complexity
SQL Indexes
Query Execution Plan in SQL
Query-Evaluation Plan in SQL
Query Processing in SQL
SELECT Data from Multiple Tables in SQL
SQL CROSS JOIN with Examples
Recursive Join in SQL
Hierarchical Data and How to Query
Transforming Rows to Columns in sql
Pivot and Unpivot in SQL
Performance Tuning and Best Practices
Now, let's focus on improving SQL query performance. It covers indexing, query optimization, and understanding execution plans. It’s vital for data scientists to write efficient queries, especially when working with large datasets, to ensure fast and scalable data processing

Writing Efficient SQL Queries
How to Limit Query Results in SQL?
CREATE and DROP INDEX Statement in SQL
SQL Queries on Clustered and Non-Clustered Indexes
EXPLAIN in SQL
SQL Stored Procedures
Data Visualization and Reporting with SQL
Although SQL is not a visualization tool, it can be used to prepare data for reporting and visualization. This section explores how to aggregate and format data to create meaningful reports and how SQL can be integrated with tools like Tableau, Power BI, or Python libraries to generate visual insights.

Exporting SQL query results to CSV or Excel.
Connecting SQL with visualization tools (e.g., Python libraries like pandas and matplotlib, Tableau, Power BI).
SQL Query to Make Month Wise Report
SQL Visualization Tools for Data Engineers
Data Analytics Training using Excel, SQL, Python & PowerBI
SQL for Data Science in Machine Learning
SQL is integral to machine learning workflows, especially for feature engineering and data preparation. This section shows how SQL can be used to preprocess and clean datasets before applying machine learning models. It includes techniques like filtering data, creating new features, and joining data sources to build robust datasets.

SQL for Machine Learning
Data Preprocessing, Analysis, and Visualization for building a Machine learning model
SQL using Python
SQL for Advanced Data Science Tasks
This section goes deeper into more complex SQL techniques, such as window functions, recursive queries, and common table expressions (CTEs). These advanced tools are powerful for performing tasks like time series analysis, ranking, and complex aggregations that are often required in data science.

SQL | Advanced Functions
Calculate Running Total in SQL
SQL LAG() Function
SQL Engine
Hierarchical Data and How to Query It in SQL?
Time-Series Data Analysis Using SQL
How to Conduct Time Series Forecasting with SQL
Simple Trend and Anomaly Detection with SQL
Market Basket Analysis with SQL
Advanced SQL For Data Analytics
Calculate Moving Averages in SQL
Analyzing Big Data with SQL
SQL for Data Scientists: The Best Book To Learn SQL For Data Nerds
The book "SQL for Data Scientists" is an excellent resource specifically designed for data nerds (compared to other more general books, which cover obscure database details geared towards database administrators. While not specifically geared towards SQL interview prep, it covers all the main topics which you'll find during an interview, like joins, window functions, subqueries, and data prep for ML.

Minimum Viable SQL Patterns: Best Book To Learn SQL Best Practices
The eBook Minimum Viable SQL Patterns will take your SQL code to the next level. This is for folks who want their SQL queries to be more efficient, readable, and maintainable – things that experienced hires are judged on during SQL interviews!

Ace the Data Science Interview: Best Book For SQL Exercises
Finally, the book Ace the Data Science Interview has an entire chapter with 30 real SQL & Database Interview questions, along with a guide on how to prepare for them. I like this book, but then again I’m biased because I wrote it!

To get a complete list of book recommendations, check out this list of the 17 best books for Data Analysts and the 13 best Data Science books.

What's the best site to practice SQL interview questions?
The 3 most popular sites to practice SQL interviews are:

LeetCode - https://leetcode.com/
HackerRank - https://www.hackerrank.com/
DataLemur – https://datalemur.com/
I believe that DataLemur is the best SQL interview platform because it is the most affordable option (half the cost of LeetCode), features the best solutions and hints, and has the most generous free tier.

Want proof? Start practicing with this free TikTok SQL question to see what I mean:

Practice Problem
TikTok SQL Question: Write a query to find the activation rate of new users


While practicing from these online SQL interview platforms is great, we want to acknowledge that tackling open-ended take-home SQL challenges is a whole other beast.

Open-Ended Take-Home SQL Challenges
What makes open-ended SQL tests trickier than online SQL assessments?
Open-ended SQL challenges typically use much larger, messier, and more realistic datasets than SQL assessments. For example, in a take-home challenge you might get some anonymized data from the company’s production database, which is filled with missing data or nulls. In timed SQL coding screens, you’re usually querying a clean toy dataset that might only be 20-30 rows big.

The scope of an open-ended SQL challenge is much bigger too. Whereas in an online SQL assessment, there’s a specific question with well-defined inputs and outputs, for take-home SQL challenges it might not even be obvious what question you need to answer! Some startups in their SQL take-home challenges just give you a large CSV file along with vague prompt like “From this data, what recommendations do you have for our business?” and it’s up to you to determine what specific questions you’ll ask of the dataset.

Of course, the timeframe for take-home SQL challenges is much longer too. Plus, you can usually pick which version of SQL to use too! However, this is both a blessing and a curse. While you typically have more freedom, the expectations around the cleanliness of your SQL code are much, much higher.

Finally, an open-ended SQL take-home challenge tests for much more than just raw SQL skills. Typically, you’ll have to write a report about what you did, which tests your written communication skills. You might even be asked to visualize the data, which tests your data visualization skills as well. Lastly, over a Zoom call, you might be asked to present your analysis, and defend the work you did, which evaluates your oral communication and presentation skills.

How do you prepare for take-home SQL interview challenges?
The best way to prepare for open-ended SQL interview challenges is by practicing real take-home SQL interview challenges, doing exploratory data analysis with Kaggle datasets. and reading books to improving your data analytics skills.

4 Real Take-Home SQL Interview Challenges
Because practice makes perfect, here’s 4 real take-home SQL interview challenges from PayPal, CVS Health, Asana, and UnitedHealth Group:

PayPal/Braintree Analytics Code Challenge
4-Part CVS Pharmacy Analytics SQL Challenge
Asana Take Home Data Science Challenge
4-Part UnitedHealth Group Patient Support SQL Analysis
You can also make your own open-ended SQL challenges using data from Kaggle if you want more practice!

Using Kaggle To Improve Your Ability To Answer Open-Ended SQL Questions
If you don’t know about Kaggle, you are missing out. While they typically host Data Science & Machine Learning competitions, where people build neural network models in Python or R, you can use Kaggle to improve your SQL skills too.

First find an interesting dataset on Kaggle and download the CSVs onto your laptop. Next, load the data into a free database tool like dBeaver so you can query it in the SQL flavor of your choice. Then brainstorm a list of questions you think a business stakeholder might have about the data. Finally, get querying – do the best you can to write SQL queries that answer these hypothetical open-ended analytics questions.

For bonus points, you can even visualize the results in an interactive Tableau dashboard, and turn this into a full-fledged data analytics portfolio project.

How To Translate Vague Business Questions Into SQL
To improve your ability to handle ambiguous data analytics take-home challenges like “Use SQL to find us some business insights in this dataset” my go-to resource is Lean Analytics.

Lean Analytics explains the most important metrics associated with business models like SaaS, freemium consumer apps, 2-sided marketplaces, and e-commerce brands. By knowing what numbers decision makers generally care about, you’ll be able to narrow down the scope of your SQL queries to only answer the questions that truly matter.

Read Lean Analytics to ace take-home SQL challenges

I also recommend improving your business-acumen by reading books like “The Personal MBA” and Boston Consulting Group’s book “On Strategy”, which you can find more details about in my list of the the best business books for Data Scientists.

Beyond SQL: Other Data Interview Topics
What technical concepts do Data Analyst interviews cover (besides SQL)?
SQL is just one tool in the Data Analyst toolbox, and anyways it’s not the tool that matters (or its size), it’s how you use it 😉. That’s why Data Analyst interviews go beyond just SQL questions, and ask technical interview questions like:

Probability Interview Questions:: Basics about conditional probability, Bayes' theorem, random variables
Statistics Interview Questions:: Topics include measures of central tendency (e.g., mean, median, mode), measures of dispersion (e.g., variance, standard deviation), hypothesis testing (t-tests, ANOVA, and chi-squared tests) and sometimes linear regression analysis for senior Data Analyst interviews.
Data Visualization Questions: Usually tested in-directly, by examining a data analytics portfolio project of yours, or judging a how you visualized the results from an open-ended SQL take-home test.
Business Acumen Questions: You’ll usually get an open-ended case question like “You're launching a new AWS database service. What metrics would you measure to know if the launch went well or not?”. Here you’ll have to walk through your knowledge of financial and product metrics, and be evaluated on your general business-savvy. You’ll also be asked about your past work experience, and how you communicated your data analysis results to stakeholders.
What do Data Science interviews cover besides SQL?
Much like a Data Analyst interview, Data Science interviews cover way more topics than just SQL. You can expected to be asked:

Probability & Statistics Questions
A/B Testing Questions
Machine Learning Questions
Database Design Questions
Coding Questions (usually in Python)
Product-Sense Questions
Behavioral Interview Questions
For a concrete example of what to expect, check out some TikTok Data Scientist Interview Questions.

TikTok Data Science Interview Questions

You can also expect to get take-home Data Science interview projects, which test your data cleaning and exploration skills, along with your data visualization and communication skills.

For a comprehensive way to prep for Data Science Interviews, go read the paperback book Ace the Data Science Interview which covers all these topics.
SQL has been a must-have tool in the arsenal of any aspiring data scientist for years and still is in 2025.

However, employers are now prioritizing real-world problem-solving over textbook syntax. Also, recent years have brought the rise of platforms that allow live coding, so the technical interviews have increasingly become timed, interactive, and pair-programmed with a live engineer or recruiter.

And the question itself? The golden era of companies hiring anyone who knows how to turn on the computer for a data science role is long gone. Companies are forced to focus on efficiency in recent years, and that’s reflected in the coding questions: they are more performance-focused. This means it’s not enough anymore to write a code that works; you’ll also have to think about how to optimize it.

As collected data increases, up with it goes the data messiness, so expect for a lot of SQL interview questions to be about data cleaning and analysis preparation.

In the following sections, we will provide you with an outline for learning, preparing, and acing your next SQL interview for a data science role.

We will explore why SQL is so widely used even in 2025, with all the cloud technology and AI surges.

Then, we will give you a breakdown of SQL skills needed by each role – Data Analyst, Data Scientist, Machine Learning Engineer, etc.

Further, we provide you with real interview examples from the StrataScratch platform illustrating a few of these skills and provide you with a step-by-step learning guide to become proficient with SQL even if you are not too familiar with SQL concepts and get your dream job.

So, let us start off by explaining why SQL is so widely used in the data science world.



The Prevalence of SQL
Relational databases are still the most popular types of databases, as shown in the image below.


why sql interview questions are important

Source: https://db-engines.com/en/ranking
NoSQL databases like MongoDB or Cassandra have gained traction with big data requirements, real-time applications, and the increasing prevalence of unstructured data.

However, there’s no running away from the facts: four out of the five most popular database engines are relational databases. In the top ten, relational databases take seven places.

They maintain popularity because they are optimized for storing structured data, which is still everywhere. Relational databases’ tabular format for data storage makes it easy to visualize databases as large spreadsheets with millions and millions of rows and columns.

NoSQL databases are also not discarding SQL. In fact, NoSQL databases have positioned themselves as such to highlight their support for SQL and increase their acceptance in organizations where traditional SQL-based databases are already used. (It’s a common misconception that NoSQL means literally ‘no SQL’, when in fact it means ‘not only SQL’).

And SQL? It’s a programming language created specifically for handling data in relational databases, which makes its popularity self-explanatory.


why sql interview questions are important

Source: https://survey.stackoverflow.co/2024/technology#1-programming-scripting-and-markup-languages
Another reason for the popularity of SQL is the ease of use. The syntax is very easy to understand, and anyone can pick it up very quickly. It is very easy to comprehend what the below statements do.

SELECT name FROM EMPLOYEES;
SELECT name FROM EMPLOYEES
WHERE age >= 35;
SELECT state, AVG(age) as average_age FROM EMPLOYEES
WHERE age >= 35
GROUP BY state;
With this popularity among data scientists, virtually every job interview will test your SQL skills. But which ones?

SQL Usage in Data Science by Role

SQL Usage in Data Science by Role

Not every position in Data Science uses the same SQL concepts. While some roles will focus heavily on queries and query optimization, others will tend towards data architecture and ETL processes. One can divide SQL Commands asked in Data Science Interviews into the following categories.

Data Definition Language (DDL)
CREATE
ALTER
DROP
RENAME
TRUNCATE
COMMENT
Data Query Language (DQL)
SELECT
Data Manipulation Language (DML)
INSERT
UPDATE
DELETE
MERGE
CALL
EXPLAIN PLAN
LOCK TABLE
Data Control Language (DCL)
GRANT
REVOKE
A Data Analyst or a Data Scientist will be expected to mostly be working with the SELECT statement and associated advanced level concepts like subqueries, Grouping / Rollups, Window Functions and CTEs. If you work as an analyst, you probably already use SQL. It does not matter if you are a data analyst, reporting analyst, product analyst, or even financial analyst. Your position generally requires handling the raw data and using your skills to provide the management and other stakeholders with the decision-making insight.

Data engineers work closely with data scientists and data analysts. Their main task is building and maintaining the data architecture and creating algorithms to allow data scientists and analysts easier access to data. By doing their job, they are helping data scientists to do their job too. What you cannot avoid as a data engineer is knowing SQL, often on an advanced level compared to data analysts and scientists. To be a good data engineer, you need to be an SQL master. That is why some questions you will be asked are the same as with data analysts and scientists. Besides the DQL commands, you are expected to be proficient in database modeling as well, hence you should know the DDL, DML and DCL commands in detail as well.

Machine learning engineers are hybrid experts who are bridging the gap between data scientists and software engineers. As they serve as a bridge between those two positions, they need to have a certain set of skills from both worlds. They use those skills to design, build, and maintain the machine learning systems. To achieve that, they usually use several sets of skills:

statistics
mathematics
data mining
data analysis
predictive analytics
Machine learning engineers usually need to know Python and/or R. However, since machine learning engineers and data scientists have some skills in common (data mining, data analysis), it is quite often required from the machine learning engineers to know SQL as well. That way, they are able to make their own analyses and use the data according to their needs. They do not need some intermediary who’ll pull out the data and analyze it for them. For example, here is a question from Uber on creating a Naive Forecasting Model.​

Naive Forecasting
Last Updated: December 2020

Uber
Hard
ID 10313
Data Engineer
Data Scientist
BI Analyst
Data Analyst
ML Engineer
Software Engineer
Some forecasting methods are extremely simple and surprisingly effective. Naïve forecast is one of them; we simply set all forecasts to be the value of the last observation. Our goal is to develop a naïve forecast for a new metric called "distance per dollar" defined as the (distance_to_travel/monetary_cost) in our dataset and measure its accuracy.


Our dataset includes both successful and failed requests. For this task, include all rows regardless of request status when aggregating values.


To develop this forecast,  sum "distance to travel"  and "monetary cost" values at a monthly level before calculating "distance per dollar". This value becomes your actual value for the current month. The next step is to populate the forecasted value for each month. This can be achieved simply by getting the previous month's value in a separate column. Now, we have actual and forecasted values. This is your naïve forecast. Let’s evaluate our model by calculating an error matrix called root mean squared error (RMSE). RMSE is defined as sqrt(mean(square(actual - forecast)). Report out the RMSE rounded to the 2nd decimal spot.

Table: uber_request_logs
Develop a naive forecast for a new metric: "distance per dollar". Distance Per Dollar is defined as the (distance_to_travel/monetary_cost) in our dataset. Calculate the metric and measure its accuracy.

Algorithm Outline:

"Sum the "distance to travel" and "monetary cost" values at a monthly level and calculate "distance per dollar". This is the actual value for the current month.
Next, populate the forecasted value for each month. To achieve this, take the previous month’s value.
Now, we have actual and forecasted values. This is our naive forecast. Now evaluate our model by calculating the root mean squared error (RMSE). RMSE is the square root of the difference mean squared differences between the actual and the forecast values. Report the RMSE rounded to the 2nd decimal place."

SQL interviews for software engineers are also usually at an intersection of various skill sets, such as computer science, engineering, and mathematics. They use those different disciplines to design, write, test, and maintain the software. Like the machine learning engineers, they will also need to work with various departments and clients. That is why they too need a high level of business and technical skills, even though their primary task is not data analysis. Why is that? When they build the interface, they have to lean on the database(s) that run in the background. They need to use those databases and analyze data during the implementation of new software.

You can refer to this excellent article on the various roles and responsibilities in Data Science to learn more. The article breaks down the most common requirements for each role that you can expect on a daily basis.

Let us look at the most frequent SQL areas tested in Data Science Interviews.

Technical Concepts tested in SQL Interview Questions
Since SQL roles differ widely, the testing areas vary quite a bit as well. Depending on the type of role that you are applying for and the organization, you can expect one or more of these SQL data science interview question types.


Technical Concepts tested in SQL Interview Questions

Fundamental SQL Concepts
The emphasis of these SQL interview questions is on your understanding and knowledge of basic database and SQL terminology. One is not required to write any code. These are the broad concepts and some example questions that one should be familiar with.

General information about SQL
What is SQL?
What are the different flavors of SQL?
What is a primary key?
Relational Databases and how they work?
What are the top RDBMS engines?
How is an RDBMS different from a No-SQL database?
SQL sublanguages and their main keywords
What do DDL, DCL, and DML stand for?
Give examples of commands for each.
Data types and how SQL handles it (including blanks and NULLs)
What are the common data types in SQL?
Does an SQLite database support date time objects?
Attribute constraints
What are attribute constraints, and explain them?
types of JOINs
What is the difference between inner join and left outer join?
What is the difference between UNION and UNION ALL?
Aggregation and Rollup functions
When should one use a CTE over a subquery?
What are window functions?
Knowledge of various SQL functions
What is the difference between WHERE and HAVING? Examples of where one should use one over the other
What does the COALESCE function do?
While one may not find them in the interviews initially, these SQL interview questions might be asked as follow up questions to the coding solutions submitted by you. For example, if you used an inner join in your solution, you might be asked why you did not use a left join or what would have happened if you did?

SQL Basics Interview Questions
The SQL basics questions will require you to put some of the above theoretical concepts into practice. That doesn’t necessarily mean that these questions have to be coding questions. They can be descriptive, too. But they usually cover concepts you will need to know if you want to write code.

Those concepts are shown in the image below.


Concepts for SQL basic interview questions

Questions Example

Here is an example from a Postmates SQL Data Science Interview​

SQL Interview Question #1: Customer Average Orders
Customer Average Orders
Last Updated: January 2021

Postmates
Easy
ID 2013
Data Engineer
Data Scientist
BI Analyst
Data Analyst
ML Engineer
How many customers placed an order and what is the average order amount?

Table: postmates_orders
Solution: To answer this SQL interview question you’ll have to use the table postmates_orders.

SELECT count(DISTINCT customer_id), avg(amount)
FROM postmates_orders
Go to the question on the platform
PostgreSQL
Tables: postmates_orders
Reset
Run Code
Check Solution
Use Alt + Enter to run query
Highlight some code to execute selection only

As one can see, this is really an easy one. It tests the aggregate functions COUNT() and AVG(), and it also requires the knowledge of how to use the DISTINCT clause.

Here’s another one, this time from Credit Karma:

SQL Interview Question #2: Submission Types
Submission Types
Last Updated: December 2020

MetLife
Credit Acceptance
Credit Karma
Easy
ID 2002
Data Engineer
Data Scientist
BI Analyst
Data Analyst
ML Engineer
Write a query that returns the user ID of all users that have created at least one ‘Refinance’ submission and at least one ‘InSchool’ submission.

Table: loans
Solution: To answer this interview question you’ll have to use the table loans.

SELECT user_id
FROM loans
WHERE TYPE in ('Refinance', 'InSchool')
GROUP BY user_id
HAVING count(DISTINCT TYPE) =2
Go to the question on the platform
PostgreSQL
Tables: loans
Reset
Run Code
Check Solution
Use Alt + Enter to run query
Highlight some code to execute selection only

The code selects the column user_id from the table loans where the value equals one of the two values: “Refinance” and “InSchool”. Since we need to ensure that there are submissions in each type, we need to use the DISTINCT clause. One also needs to appreciate the difference between WHERE and HAVING. Since we are using the DISTINCT clause in the aggregate function, we have used HAVING instead of WHERE here.

SQL Aggregation Interview Questions
Aggregation functions are widely used for reporting metrics and evaluating summary results, something data analysts and data scientists do a lot.

The purpose of aggregation is to transform data into information. This information is presented as reports, dashboards, and charts. What you report are different metrics that need to be monitored. So, basically, your two main tasks will be aggregating and filtering data and performing various calculations on that data.

Strictly speaking, data analysts and data scientists are not reporting but analyzing the data. However, there are a lot of instances where one needs to monitor and report metrics on a regular basis.

If you are interviewing for an SQL data analyst or data scientist position, you are expected to be familiar with the concepts below.


SQL aggregation interview questions

Questions Example

This is a moderate SQL interview question from Zillow that tests your understanding of aggregations functions

SQL Interview Question #3: Cities With The Most Expensive Homes
Cities With The Most Expensive Homes
Last Updated: December 2020

Zillow
Medium
ID 10315
Data Engineer
Data Scientist
BI Analyst
Data Analyst
ML Engineer
Write a query that identifies cities with higher than average home prices when compared to the national average. Output the city names.

Table: zillow_transactions
Solution: To answer this question you’ll have to use the table zillow_transactions.

SELECT city
FROM zillow_transactions a
GROUP BY city
HAVING avg(a.mkt_price) >
  (SELECT avg(mkt_price)
   FROM zillow_transactions)
ORDER BY city ASC
Go to the question on the platform
PostgreSQL
Tables: zillow_transactions
Reset
Run Code
Check Solution
Use Alt + Enter to run query
Highlight some code to execute selection only

To answer this SQL interview question, we write a subquery in the HAVING clause. The inner query calculates the overall average market price of all the transactions. The main query then filters only those cities where the average market price is greater than the overall average market price. Since we need to report only the cities in the final output, only those metrics are reported.

Here is another one. This one is a little more complex from Lyft.

SQL Interview Question #4: Distances Traveled
Distances Traveled
Last Updated: December 2020

Lyft
Medium
ID 10324
Data Engineer
Data Scientist
BI Analyst
Data Analyst
ML Engineer
Find the top 10 users that have traveled the greatest distance. Output their id, name and a total distance traveled.

Tables: lyft_rides_log, lyft_users
Solution: To answer this SQL interview question you will have to use the tables lyft_rides_log, lyft_users.

SELECT user_id,
       name,
       traveled_distance
FROM
  (SELECT lr.user_id,
          lu.name,
          SUM(lr.distance) AS traveled_distance,
          rank () OVER (
                        ORDER BY SUM(lr.distance) DESC) AS rank
   FROM lyft_users AS lu
   INNER JOIN lyft_rides_log AS lr ON lu.id = lr.user_id
   GROUP BY lr.user_id,
            lu.name
   ORDER BY traveled_distance DESC) sq
WHERE rank <= 10;
Go to the question on the platform
PostgreSQL
Tables: lyft_rides_log, lyft_users
 
Reset
Run Code
Check Solution
Use Alt + Enter to run query
Highlight some code to execute selection only

This SQL question tests subqueries, window functions, and joins. We start off by joining the two tables with an inner join as we need those users who have used the services and remove the rides with incomplete user information. We then proceed to rank the sum of the total distances travelled by these users. We finally select the top 10 users by rank and output their names and total distances travelled.

To improve your coding skills for SQL Data Science Interviews, you can refer to this video where we discuss some top data science interview questions, how to solve them and avoid common mistakes.


Open Ended SQL Interview Questions
You will encounter these types of SQL interview questions in Data Analyst or Data Scientist positions that require some work experience. The greatest challenge in these questions is the lack of any specified metric that needs to be calculated, as in the case of SQL Aggregation Interview Questions. You will still be required to write an SQL query that will return some metric(s) as a result. However, there is one big difference. In the Open-Ended SQL Interview questions, you will be asked to find the insight.

It is entirely up to you to understand your data and what calculation answers what you are being asked. For example, you will have to find out if some product launch campaign succeeded, or new calling procedure saves costs, or if new vehicles improved the users’ satisfaction. You will have to come up with metrics to define “success”, “saving”, or “improvement”.

Compared to the SQL Aggregation questions, these questions have this extra dimension designed to test your thinking in solving the problem. Regarding the coding part of the Open-Ended SQL interview questions, they test all the concepts you will use in the basic level and the SQL Aggregation type questions.

Questions Example

Have a look at a question asked by Uber:

SQL Interview Question #5: Percentage Of Revenue Loss
Percentage Of Revenue Loss
Last Updated: May 2021

Uber
Medium
ID 2048
Data Engineer
Data Scientist
BI Analyst
Data Analyst
ML Engineer
For each service, calculate the percentage of incomplete orders along with the percentage of revenue loss from incomplete orders relative to total revenue.


Your output should include:


•  The name of the service
•  The percentage of incomplete orders
•  The percentage of revenue loss from incomplete orders

Table: uber_orders
Solution: To answer this question, we must use the table uber_orders.

SELECT service_name,
       (lost_orders_number/total_orders)*100 AS orders_loss_percent,
       (lost_profit/possible_profit)*100 AS profit_loss_percent
FROM
  (SELECT service_name,
          SUM(number_of_orders) AS total_orders,
          SUM(number_of_orders) FILTER (
                                        WHERE status_of_order != 'Completed') AS lost_orders_number,
          SUM(monetary_value) AS possible_profit,
          SUM(monetary_value) FILTER (
                                      WHERE status_of_order != 'Completed') AS lost_profit
   FROM uber_orders
   GROUP BY 1) tmp;
Go to the question on the platform
PostgreSQL
Tables: uber_orders
 
Reset
Run Code
Check Solution
Use Alt + Enter to run query
Highlight some code to execute selection only

In order to solve this problem, we need to define the revenue loss percentage. Percentage means that two values will be divided, but which two? We should compare the lost revenue with the possible revenue. Now, how to define these two metrics? The possible revenue is the sum of all the orders' monetary value, regardless of their status. Lost revenue will therefore equal the sum of monetary values for all orders that were not completed.

The same needs to be done with the percentage of incomplete orders, but this is more straightforward: divide the number of not completed orders with the number of completed.

The next question is how we define the completed orders? It’s almost a lucky guess to think that the completed orders will have the status ‘Completed’. It could have been something else or even a combination of several statuses. Also, some of the orders with the status ‘Completed’ have a NULL monetary value. Maybe they shouldn’t have been considered as completed?

Based on the definitions, we calculate the metrics in the subquery. It uses the SUM() aggregate function and FILTER modifier. Then, in the main query, we use the subquery’s result to get the required metrics as percentages.

This is an open-ended SQL question because you were not given the meaning of the revenue loss nor how to find the completed orders. Also, the query considers that all the orders with a status other than ‘Completed’ are lost. However, the question doesn’t say only those orders should be considered as completed. You could apply different criteria. For example, there’s also the status ‘Other’. Since we are not told what this status means, it wouldn’t be a mistake to count such orders as completed, too. The important thing is that you state this explicitly in your assumptions.

Data Transformation Interview Questions
These are the SQL interview questions that one can expect to be asked very frequently for a data engineer or a machine learning engineer position. They also may not be out of place in a data scientist interview for positions that require more experience.

Data transformation is extensively performed within an ETL (Extract, Transform and Load) process, which is used to collect data from various sources (extract), changing it according to the business rules (transform), and then loading such extracted and transformed data into a database.

When the data is extracted, it is done so from various data sources that, more often than not, store data in completely different formats.

The data is then transformed into the format appropriate for reporting and analysis. This is done via data aggregation, filtering, sorting, joining, a calculation based on the rules set for business needs, etc.

Such data is loaded into another database or table that the analysts or any other users might use.

The ETL is heavily used in data warehouses, which serve as the central source of the integrated data, with data flowing into it from one or more separate sources. If you want to perform well at the SQL job interview, these are the concepts you need to know.


SQL data transformation interview questions

Question Examples

This is one of the easiest and yet frequently asked questions from the Southwest Airlines Data Science SQL Interview:

SQL Interview Question #6: DELETE and TRUNCATE
DELETE and TRUNCATE
Last Updated: March 2019

Southwest Airlines
British Airways
Easy
Technical
ID 2084
What is the difference between DELETE and TRUNCATE?

Link to the question: https://platform.stratascratch.com/technical/2084-delete-and-truncate

Answer:

DELETE is a DML statement. TRUNCATE is a DDL statement. The DELETE statement can be used to delete all rows or only some rows. To delete some rows, you’ll have to use the WHERE clause. While doing this, every row removed will be logged as an activity by the database. On the other hand, TRUNCATE is used only for deleting the whole table, which will be logged as only one action. That’s why TRUNCATE is faster than DELETE, which shows when deleting a table with a huge amount of data. Also, you can’t use TRUNCATE if there’s a foreign key in the table.

Another common SQL interview question to appear is

"How do you change a column name by writing a query in SQL?"

Answer: Assuming you are using PostgreSQL. For a hypothetical table say product, one of the columns is named year, but I want to rename it to description. The query that will do that is:

ALTER TABLE product
RENAME year TO description;
Another example of Data Transformation SQL Interview question will be:

"How do you create a stored procedure?"

Answer: We will solve this for Microsoft SQL Server. For example, if you are using a table named employee. Your procedure should help you get the employees that work in a certain department. The code would be

CREATE PROCEDURE employee_deparment @deparment nvarchar(50)
AS
SELECT * FROM employees WHERE department = @department
GO;
Once the procedure is created, I can invoke it in the following manner:

EXEC employee_deparment @department = 'Accounting';
Database Modeling Interview Questions
These SQL interview questions are designed to test how good you are at database design or database modeling. What is meant by that? You need to show the ability to design and build the database from scratch according to the business processes and business needs. This requires a high level of both technical and business knowledge. You will be working with both technical and non-technical colleagues. So, you need to understand both the business side of their requirement and how to, in the soundest way, technically cater to their business needs regarding the data. Generally, this is a process that goes through these steps (at least in the ideal world):

defining the database purpose
collecting and defining users’ requirements
creating a conceptual model
creating the logical model
creating the physical model
Question Examples

One of the typical questions that occur in the SQL interviews is this one by Audible:

SQL Interview Question #7: Build a Recommendation System
Build a Recommendation System
Last Updated: February 2020

Easy
System Design
ID 2148
Can you walk us through how you would build a recommendation system?

Link to the question: https://platform.stratascratch.com/technical/2148-build-a-recommendation-system

Answer: Since there is a wide variety of approaches to answer this question, we will leave you to come up with your own way of building one.

The database design question can also include SQL coding, such as this one from Facebook:

SQL Interview Question #8: GROUP or ORDER BY
GROUP or ORDER BY
Last Updated: January 2021

Medium
System Design
ID 2170
Write a SQL query to compute a frequency table of a certain attribute involving two joins. What if you want to GROUP or ORDER BY some attribute? What changes would you need to make? How would you account for NULLs?

Link to the question: https://platform.stratascratch.com/technical/2170-group-or-order-by

Answer: Due to the nature of the question, we will let you answer this one on your own.

Software Engineering SQL Interview Questions
These are questions that require SQL knowledge, but usually, what is being asked may not be widely applicable in practice. These SQL interview questions come up during the interview because even though as a software engineer, you might not be writing SQL codes every day, you still need to interact with your peers who use SQL daily and follow what they are trying to achieve and implement their needs and SQL logic into the software development. These questions test your logical skills more than coding skills. One such SQL interview question that could be asked during your interview could be

Imagine you’re working with two tables. The one is the product table, which has the following data:

id
product_name
manufacturer_id
The second table is manufacturer with the following data:

id
manufacturer
There are 8 records in the first table and 4 in the second one.

How many rows will the following SQL code return:

SELECT *
FROM product, manufacturer
Answer: The query will return 32 rows. Whenever the WHERE clause is omitted, the default result is CROSS JOIN or a Cartesian product. This means the query will return every combination of rows from the first table with every combination of rows from the second table.

The SQL Interview Questions Asked by the FAANG Companies

The SQL Interview Questions Asked by the FAANG Companies

FAANG is an acronym for the five most famous tech companies: Facebook, Amazon, Apple, Netflix, and Google. Why would you specially prepare for the questions asked by those companies, except being in awe of the possibility of working for them? They might seem or even be attractive, but that is not the main reason why you would pay special attention if you want to work at those companies.

The main reason is their SQL interview questions are a bit different. As tech companies, their business heavily relies on data. And where there is data, there is SQL which the FAANG companies often use. Hence they want to be absolutely certain that their employees know SQL in depth. You will always get SQL interview questions with a little twist. The twist being their questions are more practical and concerning a case study with real problems and data a certain company is facing in their everyday business. These are arguably the next level of the Open Ended SQL Interview Questions that we saw earlier.

Have a look at this example from Google:

SQL Interview Question #9: Activity Rank
Activity Rank
Last Updated: June 2021

Google
Medium
ID 10351
Data Engineer
Data Scientist
BI Analyst
Data Analyst
ML Engineer
Find the email activity rank for each user. Email activity rank is defined by the total number of emails sent. The user with the highest number of emails sent will have a rank of 1, and so on. Output the user, total emails, and their activity rank.


•	Order records first by the total emails in descending order.
•	Then, sort users with the same number of emails in alphabetical order by their username.
•	In your rankings, return a unique value (i.e., a unique rank) even if multiple users have the same number of emails.

Table: google_gmail_emails
Solution: To answer this question, you’ll need to use the google_gmail_emails table.

SELECT  from_user, 
        COUNT(*) as total_emails, 
        ROW_NUMBER() OVER (ORDER BY COUNT(*) DESC, from_user ASC)
    FROM google_gmail_emails 
    GROUP BY from_user
    ORDER BY 2 DESC, 1
Go to the question on the platform
PostgreSQL
Tables: google_gmail_emails
Reset
Run Code
Check Solution
Use Alt + Enter to run query
Highlight some code to execute selection only

As you can see, this SQL interview question tests your aggregate functions and window functions knowledge, along with the GROUP BY and ORDER BY clauses. But they also do that on real-life problems you’ll probably have to work on if you get a job.

Here is another example of such question, this time from Netflix:

SQL Interview Question #10: Find the nominee who has won the most Oscars
Find the nominee who has won the most Oscars
Spotify
Netflix
Medium
ID 9750
Data Engineer
Data Scientist
BI Analyst
Data Analyst
ML Engineer
Find the nominee who has won the most Oscars.
Output the nominee's name alongside the result.

Table: oscar_nominees
Solution: To answer this question, you must use the oscar_nominees table. As you can see, the code specifically uses the RANK() window function. You could achieve the same result with DENSE_RANK() but not ROW_NUMBER().

WITH cte AS
  (SELECT nominee,
          RANK() OVER (
                       ORDER BY count(winner) DESC) AS rnk,
                      count(winner) AS n_times_won
   FROM oscar_nominees
   WHERE winner = TRUE
   GROUP BY nominee)
SELECT nominee,
       n_times_won
FROM cte
WHERE rnk = 1
Go to the question on the platform
PostgreSQL
Tables: oscar_nominees
 
Reset
Run Code
Check Solution
Use Alt + Enter to run query
Highlight some code to execute selection only

Here’s a quick overview of the differences between those SQL ranking window functions.


ROW_NUMBER() vs. RANK() vs. DENSE_RANK()

Again, this SQL interview question tests some usual concepts. But the problem set is something that you expect to work on a daily basis. If you work at Netflix on an SQL job, you will for sure analyze some data that contains some Oscar nominations and winners.

What to study for your SQL Data Science Interviews?

What to study for your SQL Interviews

You would have probably noticed that the technical SQL interview questions overlap with other SQL questions. That is because one does not work without the other. There is no point in knowing the theory without being able to put it into practice, i.e., the SQL code. Conversely, you need to describe the technical concepts behind the code that you wrote. While the SQL concepts you should know depend on your position, years of experience, and the company you want to work at, we have looked at some concepts that are useful across roles. While this is not an exhaustive list, it is definitely something that you are expected to know if you are attending an SQL Data Science Interview.

SQL & Database Generalities
SQL definition

SQL stands for “Structured Query Language”. It is a programming language used for creating database structure, retrieving and manipulating data in it.

Types of the SQL commands

Data Definition Language (DDL)
CREATE
ALTER
DROP
RENAME
TRUNCATE
COMMENT
Data Query Language (DQL)
SELECT
Data Manipulation Language (DML)
INSERT
UPDATE
DELETE
MERGE
CALL
EXPLAIN PLAN
LOCK TABLE
Data Control Language (DCL)
GRANT
REVOKE
Relational database

A relational database is one based on the relational data model. This means the database is a collection of relations. Those relations are shown as tables, which consist of columns, rows, and values. The relational database aims to minimize or completely avoid data redundancy, leading to data integrity and speeding up its retrieval.

Relationships in the database

The relationship defines the type of connection between the tables in the database. There are three main types of relationships:


SQL relational database interview questions

Database normalization

Database normalization is a process of organizing data in the database to achieve its purpose: data integrity, its non-redundancy, and speed of retrieval.

Constraints

The constraints are the rules that define what type of data can and can’t be entered as a value in the database.

These are the most common attributes:


Most Common Database Attributes in SQL Interview Questions

Indexes

The indexes are structures in the databases created to tune the database performance. They are used to speed up data retrieval.

Here are the index types.


Database Indexes SQL Interview Questions

View

A view is a virtual table containing data from one or more tables resulting from a SQL statement.

Stored procedure

A stored procedure is an SQL code consisting of one or several SQL statements that are saved and can be called and executed whenever required.

Trigger

A trigger is a special type of a stored procedure. It is automatically executed (triggered) whenever some special event occurs in the database.

Joining Tables & Queries
Inner join

An Inner join returns only those rows where the data from one table matches the data from the second table.

Left outer join

The left join is a table join that will retrieve all the rows from the left table and only the matching rows from the right table.

Right outer join

This join is the one that returns all the rows from the right table and only the matching rows from the left table.

Full outer join

The full outer join will join the data so that the result will include all the rows from one table and all the rows from the second table.

Cross join

This results in a Cartesian product. This means it will return all the combinations of rows from one table with all the combinations of rows from the other table.

Union

This is an SQL command that will combine the result of one query with the result of another query. Therefore, it will show only unique records.

Union all

This one also combines the results from two or more queries. The difference between UNION and UNION ALL is that it will also include duplicates.

Aggregating and Grouping Data
Aggregate functions

The SQL aggregate functions perform a calculation on a data set and return a single value as a result. Examples of aggregate functions are:


SQL Aggregation Interview Questions

GROUP BY clause

The SQL GROUP BY clause allows you to group data according to the defined (one or more) criteria.

Filtering & Ordering Data
DISTINCT clause

The DISTINCT clause is a clause that will return only distinct or unique values, i.e., there will be no duplicate values in the result.

WHERE clause

The WHERE clause is used to filter data according to the specified criteria.

HAVING clause

The HAVING clause also filters data according to the specified criteria. The difference compared to the WHERE clause is that the HAVING clause works with the aggregate functions. Therefore, if used, it always follows the GROUP BY clause and precedes the ORDER BY clause.

ORDER BY clause

The ORDER BY clause is used to order the query result according to a certain data column.

CASE statement

The SQL CASE statement returns a defined value based on certain criteria. It is the SQL statement that allows you to apply the IF-THEN logic. Instead of IF, you use WHEN. And for THEN, you use THEN. Check out our comprehensive guide to CASE WHEN statements in SQL.

Subqueries, Common Table Expressions (CTEs) & Window Functions
Subquery

A subquery is a query found within the query. It can occur in a SELECT clause, FROM clause, or WHERE clause.

CTE

A SQL CTE or a Common Table Expression is a temporary result set returned by a query and used by another query. In that way, it’s similar to subquery. But the main difference is CTE can be named and can reference itself.

Window functions

The SQL window functions are the functions performing calculations over the defined set of rows (a window). Compared to the aggregate functions, which return a single value as a result, the window functions allow you to add the aggregated value to each row in a separate column. This means the rows are not grouped and all the rows are kept as a query result.

The window functions in SQL are shown below.


SQL Interview Questions on Window Functions

How to Organize your SQL Data Science Interview Solution?

How to Organize your SQL Interview

Being good at SQL is the prerequisite to do well at the job interview. However, it is not the only skill. Questions can be tricky, designed to put you off or doubt your knowledge by being seemingly too complicated or too simple. That’s why it’s important to have a clear strategy on what to do in certain situations.

1. Make Sure you Understand What is Required
If you don’t understand what the question is and what is expected from you, you will likely get the wrong answer. To avoid that, make sure you understand what is asked of you. Repeat the requirements out loud and ask the interviewer to confirm you understood the question correctly. Don’t be afraid to do that. The interviewers are people too. They can also unintentionally be unclear, make a mistake, or forget to give you enough details for you to answer correctly.

2. Outlay Your Approach
Before you start answering, especially if you are writing SQL code, outlay your approach. That way, you will be able to find the solution faster and or find the holes in the code you intended to write. You should do that to allow the interviewer to lead you through in case you missed the point of the question. It is always better to be corrected before presenting the final solution.

3. Try to Visualize the Output
This is something that can help you in writing the problem-solving code. Sometimes, when you clarify how the output should look and compare it with initial data, the approach and the solution reveal themselves.

4. Write the SQL Code
At some point, you will need to start writing the code. As we discussed, you should not jump headfirst into it. But you cannot keep procrastinating on the best approach to write it. After you have gone through all those previous steps, and you are still not sure if you have the right solution, simply start writing the code.

One of the reasons is, sometimes there is no solution at all. Meaning the question is too complex to be solved in the time you’re being given. In such cases, the interviewer is not interested in your solution. Instead, he or she is interested in your way of thinking and how you approach the problem. There are usually multiple ways to use SQL for problem-solving, and this is what some interviewers are interested in: the process, not the goal.

5. Code in Logical Parts
When you’re writing the code, pay attention to its structure. Divide the code into logical parts. That way, you will make your code easier to read, which is also one of the requirements to get the job. There is no point in writing a correct code that is a mess, and nobody can read it and understand it after you write it. Not even you!! If your code is divided into logical parts, it will be easier for you to explain to the interviewer what you did.

6. Optimize Code
It is also important to have the code optimization in mind. If your code is complex, of course, you are not going to be able to optimize it as you write. But you should pay attention to some general optimization guidelines, so your code is reasonably optimized. You can also discuss with the interviewer what other steps you will have to take to optimize your code in the aftermath. This is also the job requirement, similarly to the previous point. There is no point writing the code that will get you the required result, but takes forever to execute.

7. Explain Your Answer and Assumptions
Even if you did not get the required answer, it does not mean you failed the interview. That is why you should always know why you did something and explain why you did it. Maybe you did not get the answer to the question they asked, but you did get the answer to some questions. So make sure that you state your assumption and explain why you did what you did. Again, they may be looking exactly for that: the right reasoning in line with assumptions, even though the assumptions were wrong. That also shows you know what you are doing, even if it is not what they asked.

Also, one of the reasons for explaining the assumptions is there may be a loophole in the question. So imagine pointing at it right there at the interview by simply explaining why you did something while you thought you were all wrong.

Here is the video where Nate from StrataScratch shares some tips on how to organize your SQL interview solution:


Bonus SQL Interview Tips
Here are some additional tips that might help you to be a success at the upcoming SQL interview.

Get to Know the Your Potential Employer
This is important in general, not only for the SQL part of the interview. It is important to be informed about your future employer, their products, and their industry. It is especially important when the SQL questions are regarded. Why is that?

As we discussed earlier, the FAANG companies will usually ask you very practical SQL coding questions that will have you use the same data and solve the same problems as you would have to when you get employed. The FAANG companies are not the only ones who do that. So when you prepare for the interview, try to think which data is important to this company, how their database could look like, etc. When you practice the SQL interview questions, try to find the real questions from the companies you are interested in or at least from their competitors. If the companies are in the same industry, it’s quite likely the data they use will be more or less the same.

Be Prepared for a Whiteboard
It is quite usual to be asked to write SQL code on a whiteboard. It can be shocking to some people, which is understandable. You are probably used to writing code in a real RDBMS, on real data, which allows you to regularly check if the query works. Not even the greatest masters of SQL can write a code without running it to see if it works at all or if it returns the desired result. However, in the SQL interview, the criteria are a little higher.

While it can be scary, it is also understandable. Writing the code on a whiteboard shows that you know how to write your code. Reading your (or someone else’s code) is also important. This is the skill that is also tested on a whiteboard. If you can read a code and say if it will give you the desired result without relying on the database to tell you that, then working with a real database and SQL environment will be easier for you.

Write a Clean Code
We are not talking about your handwriting. There is not much you can do if your handwriting is messy. But that does not mean your code has to be unreadable. When you write a code, try to format it so that it is easier for you and the interviewers to read it and check your solution.

"Code is read more often than it is written. Code should always be written in a way that promotes readability.
- Guido Van Rossum, the creator of Python"​
Regularly use spacing and line breaks to make your code easier to read. If you need to (re)name tables and columns, be consistent with the naming convention you choose. Add comments whenever needed. Try to use aliases whenever possible, but try to make them sound logical and not some random decision when you do.

Here are also some useful tips from Nate on how to organize lengthy SQL codes.


Write in the Company’s SQL Dialect Only if you are comfortable with it
If you have experience with multiple SQL databases (Oracle, PostgreSQL, Microsoft SQL Server, MySQL), try to adapt and write in a dialect of the database that is used at your future employer. That would be nice and could show your versatility, especially if you know what dialect they prefer. However, if you are familiar with only one dialect, do not think that it is the end of the interview. For example, if you were using only PostgreSQL and the company is using Microsoft SQL Server, there may be different keywords for the same command in those two databases. It is also possible that PostgreSQL has some functions that aren’t allowed in Microsoft SQL Server and vice versa.

Ask the interviewer if it is possible to write a code in, say, PostgreSQL instead of Microsoft SQL Server since you are more familiar with it. It is always better if you know several dialects. But it is also better if you write in a familiar dialect, even though “the wrong one”, than mess up the code just because you were too afraid to ask if you can write in a dialect you are comfortable with. The differences between the dialects are not that huge. So if you know SQL, you’ll easily and quickly adapt to a new database.

Communication. Confidence. Collaboration.
While the interview is evaluating you, there are other things that employers look for in an employee besides just coding ability. You will be working as a part of a team and hence your ability to confidently put across your ideas, be open to positive feedback about your codes and ability to work as a team is equally important. Employers try to gage these even during something as technical as coding.

It is vital that you ask for help in case you are stuck. Asking for help shows confidence and is not a sign of weakness. Keep the interviewer in the loop regarding what your thought process is so that she might be able to help you in case you are stuck or omitted some information unintentionally. Listen to any explanations provided to ensure that you have taken care of all the edge cases that might arise.

These are skills that every Data Scientist must possess and unfortunately not a lot of candidates focus on them. An interviewer is more likely to hire a Data Scientist with sound basic understanding of SQL and willing to adapt to changes and pick up additional skills on the way over a prodigious but rigid one. The employer is looking at the potential for a long term relationship. If you give the right signals, you might just land your dream job. We recommend checking out these 5 tips on how to prepare for a Data Science interview.

We have also gathered some advanced-level SQL interview questions asked by real companies in 2021 that you can find in our Advanced SQL Questions You Must Know How to Answer article.

Conclusion
In this article, we looked at the various aspects of an SQL Data Science Interview. We started off by understanding why SQL is so popular in the Data Science world and the different roles that are available in the industry. We then provided a detailed overview of the type of SQL interview questions that you can expect for each position and what to learn for your becoming proficient with SQL for Data Science Interviews.

Even if you have just started with SQL, all that it takes to become interview ready is persistence, patience and lots of practice. If you do not have much real-life experience with SQL, it is very important that you practice writing SQL codes. Do it a lot, and regularly. Continuity is very important. Try to answer as many as possible SQL interview questions, be it hypothetical or, even better, the real ones from the company you want to work at. Only by writing a lot of code, you’ll gain experience, grasp some typical problems that need to be solved by SQL, and the syntax will become like second nature.

Even if you are vastly experienced in SQL and use it in a business environment, it is always good to prepare for an interview and brush up your skills. Nobody knows everything about SQL. Generally, people know what they need, what they regularly use every day. So it is possible that after several years at your job, you became a master of a certain aspect of SQL. Do not let it make you think you know it all. It could be that your new job will be asking you to know some different SQL functions and possibilities, the ones you are not exactly versed at.

In the StrataScratch community, you will be able to compete and collaborate with other aspiring data scientists working towards achieving their dream job at top tech companies and hottest start-ups across the world. We have over 400+ Real Life SQL Data Science Interview questions categorized by difficulty levels, frequency, recency, companies, et al. Or you can choose questions according to the topics, such as Window Functions, Manipulating Text, Manipulating Datetime. Join a community of over 20,000 learners and be interview ready.
SQL Interview Questions: A Data Analyst's Guide for Success
Written by Coursera Staff • Updated on Jul 9, 2024
Share
Prepare for the SQL portion of your interview with example questions and a framework for answering them.

[Featured image] A person in a gray button-down sits at a computer workstation running SQL queries on a laptop
Being able to use SQL, or Structured Query Language, ranks among the most important skills for data analysts to have. As you prepare to interview for data analyst jobs, you can expect SQL to come up during the interview process.

With this guide, you’ll learn more about SQL technical screenings, what type of screening you might encounter, and some common types of questions that can arise during these interviews. You’ll also find some example questions, a step-by-step guide for writing SQL code during your interview, and tips for success. Let’s get started.  

What to expect from SQL technical screenings
Data analysts use SQL to communicate with relational databases to access, clean, and analyze data. At the time of writing, more than 230,000 jobs on LinkedIn included SQL in the listing. 

Since it’s such a critical skill, it’s common for data analyst interviews to include a SQL technical screening. This portion of the interview tasks you with solving real-world problems using SQL. While you may be asked some definitional questions that are more typical of a standard interview, the real point here is for the interviewer to verify that you can actually use SQL, not just talk about it. 

These screenings typically take one of three forms:

1. Whiteboard test: The most common type of SQL screening is the whiteboard interview. In this type of screening, you’re given a whiteboard and a marker to write out your queries by hand. Since you won’t have a computer alerting you to any syntax or logical errors in your code, this is more about demonstrating that you can think through a problem and know the right SQL concepts to solve it. 

2. Live coding: With this type of screening, you’ll be given SQL problems to solve in a live coding environment. This allows you to run your queries and check your work as you go, but syntax will matter since you’re running your code. Since different databases use different tools, this type of screening isn’t as common as the whiteboard screening.

3. Take-home assignment: With this less-common screening technique, you’ll be given a problem or series of problems to take home and solve within a given period. This lets you write your queries in the comfort of your home, without the pressure of an interviewer looking over your shoulder. On the other hand, the coding challenges are often more complex.

Related: 5 SQL Certifications for Your Data Career

Types of SQL interview questions for data analysts
Just as there are three formats technical screenings might take, there are also three broad categories of questions you’ll typically be asked during this portion of the interview. We’ve arranged them here from the most simple to the most complex. Generally speaking, the easier, definitional questions will be fewer and less important than the live coding questions—something to keep in mind as you prepare.

IBM
specialization

Data Analysis and Visualization Foundations
Get ahead w/ Data Analysis & Visualization skills. Enhance your career by learning to analyze data using Excel spreadsheets, and create stunning visualizations and interactive dashboards with Cognos.

4.8

(3,125 ratings)

46,663 already enrolled

Beginner level

Average time: 1 month(s)

Learn at your own pace

Skills you'll build:

Excel Formulas, Apache Hive, Data Analysis, Data Visualization, Tree Maps, Big Data, Apache Hadoop, Data Visualization Software, Data Science, Dashboard, Data Mining, Statistical Analysis, IBM Cognos Analytics, Data Wrangling, Data Collection, Data Storytelling, Data Warehousing, Microsoft Excel, Data Mart, Data Cleansing, Pivot Tables And Charts, Histogram, Scatter Plots, Data Quality, Exploratory Data Analysis, Data Manipulation, Data Import/Export, Information Privacy, Data Integrity, Google Sheets, Spreadsheet Software, Apache Spark, Data Lakes, Analytics

1. Define a SQL term
If you’re interviewing for a data analyst role, you probably know what SQL is (and your interviewer assumes you know this). It’s possible you’d be asked what SQL is, but you’ll more likely be asked to explain more technical concepts in SQL, the difference between two (or more) related concepts, or how a concept is used. This is not an exhaustive list, but here are some examples of terms you should be ready to explain:

Trigger: a procedure stored within a database, which automatically happens whenever a specific event occurs.

Index: a special lookup table within a database to increase data retrieval speed.

Cursor: a pointer, or identifier, associated with a single or group of rows.

Constraints: rules used to limit the type of data allowed within a table. Common constraints include primary key, foreign key, unique key, and NOT NULL.

ETL (Extract, transform, and load): a data integration process used to combine multiple data sources into one data store, such as a data warehouse.

Primary key, foreign key, and unique key: constraints used to identify records within a table.

Normalization vs. denormalization: techniques used to either divide data into multiple tables to achieve integrity ("normalization") or combine data into a table to increase the speed of data retrieval ("denormalization").

RDBMS vs. DBMS: two types of database management systems. Within a relational database management system (RDBMS) data is stored as a table, while in a database management system (DBMS) its stored as a file.


Clustered vs. non-clustered index: two types of indices used to sort and store data. A clustered index sorts data based on their key values, while a non-clustered index stores data and their records in separate locations.


Forms definitional questions may take:

What is the purpose of an index in a table? Explain the different types.

What are the types of joins in SQL?

What is the difference between DROP, TRUNCATE, and DELETE statements?

How do you use a cursor?

What is the difference between a HAVING clause and a WHERE clause?

Read more: SQL vs. MySQL: Differences, Similarities, Uses, and Benefits


2. Questions about a query
This second category gives you an SQL query and asks you a question about it. This tests your ability to read, interpret, analyze, and debug code written by others.

Forms query analysis questions may take:
Given a query,


Put the clauses in order by how SQL would run them.

Identify the error and correct it.

Predict what the query will return.

Explain what problem the query is meant to solve.


Learn more: SQL vs. NoSQL: The Differences Explained + When to Use Each

3. Write a query
The questions most commonly associated with the SQL technical screening ask you to solve a given problem by writing out a query in SQL. You’ll typically be given one or more tables and asked to write queries to retrieve, edit, or remove data from those tables.

The difficulty of questions will likely vary based on the company and the role (entry-level vs. advanced). In general, you should be comfortable writing queries using the following concepts, statements, and clauses:

Categorization, aggregation, and ratio (CASE, COUNT, or SUM, numerator and denominator)

Joining two tables (JOIN inner vs. left or right)

Modifying a database (INSERT, UPDATE, and DELETE)

Comparison operators (Less than, greater than, equal to)

Organizing data (ORDER BY, GROUP BY, HAVING)

Subqueries 

Forms query-writing questions may take:
Given a table or tables with a few sample rows,

List the three stores with the highest number of customer transactions.

Extract employee IDs for all employees who earned a three or higher on their last performance review.

Calculate the average monthly sales by product displayed in descending order.

Find and remove duplicates in the table without creating another table.

Identify the common records between two tables.

Video placeholder
Play Video
Eine Vorlesung aus dem Data Analytics Professional Certificate von Google.
Google
Analyze Data to Answer Questions
Google
4.6 (11,986 ratings)

 | 

740K Students Enrolled

Course 5 of 9 in the Google Data Analytics Professional Certificate

SQL interview tips for success
In addition to the process above, here are some tips for preparing for your SQL interview.

Talk through your process out loud. Your interviewer may or may not know SQL themselves, so be sure to explain the what, how, and why of each step.

Include written comments on what each step of your query is meant to accomplish. This can help you keep track of where you are in the problem and make your code easier to understand. If you’re coding in a live environment, you can type comments using a double hash (--). On a whiteboard, write your comments off to the side.

Use correct formatting. While your problem-solving ability is more important than precise syntax, you can avoid confusing the interviewer (and yourself) by keeping your hand-written code organized.

Embrace the awkwardness. It’s okay if the room is silent while you think through a problem. As you’re thinking out loud, you may find yourself re-starting sentences with a better way to explain something. That’s okay, too.

Read more: How to Prepare for an Interview

Six-step strategy for your SQL interview
Sometimes, the best way to calm nerves before an interview is to walk into the screening with a clear action plan. No matter what type of query you’re asked to write, you can use this six-step process to organize your thoughts and guide you to a solution, even when you’re nervous.

1. Restate the question to ensure you understand what you’re asked to do.

2. Explore the data by asking questions. What data type is in each column? Do any columns contain unique data (such as user ID)? 

3. Identify the columns you’ll need to solve the problem. This helps you focus on the data that matters so you’re not distracted by data irrelevant to the query.

4. Think about what your answer should look like. Are you looking for a single value or a list? Will the answer be the result of a calculation? If so, should it be a float or an integer? Do you need to account for this in your code?

5. Write your code one step at a time. It can help to outline your approach first. By writing down the steps you plan to take, you’ll have a clear outline once you start writing your query (and you’ll give the interviewer a chance to correct you if there’s an issue with your approach).

Then, code in increments, taking one step of your outline at a time. After you’re happy with your code for the first step, build onto that code with the second step. 

6. Explain your solution as a whole.  If there’s a more efficient way you could have written your code—using subqueries, for example—explain that. And remember to answer the original question.
SQL Interview Questions at a Glance
Throughout this article, we'll explore a range of SQL interview questions and answers for practitioners at the beginner and intermediate levels. If you're looking for an overview, we've compiled some top tips below:

For beginners
General questions. Expect questions about your experience, the SQL flavors you're familiar with, and your level of proficiency.
Technical questions. These will cover the basics of SQL, such as what it is, its applications, SQL statements, SQL commands, and types of SQL queries, among others.
For intermediate practitioners
Functions in SQL. You should know about aggregate and scalar functions, as well as built-in and user-defined functions.

Advanced commands. Questions may cover topics like joins, primary and foreign keys, indexes, and SQL relationships.

Database design. Expect questions on normalization, denormalization, and the differences between various SQL statements like DELETE, TRUNCATE, and DROP.

Advanced queries. You may be asked about subqueries, both nested and correlated, as well as how to perform specific tasks like finding the nth highest value in a column.

General SQL Interview Questions for Beginners
Before asking you technical questions, your interviewer may ask you some general questions about your overall experience with SQL. You can expect the following questions:

What SQL flavors are you familiar with?
How can you estimate your level of proficiency in SQL?
For how long have you been working in SQL?
While this information can be mentioned in your resume, be ready to talk about it. Naturally, there are no "right" answers to such questions, and there is no need to make up things when answering them.

Don't worry if your experience in SQL is limited: this is something your interviewer, most probably, already knows from your resume. Since they are interested in talking to you anyway, your profile was considered a good fit for their company.

Also, it's perfectly fine if you have only worked with one SQL flavor. Remember that all SQL dialects are fairly similar. Therefore, being familiar with only one of them is a solid basis for you to learn any others.

Technical SQL Interview Questions for Beginners 
Now, let's move on to the technical SQL interview questions and some potential answers to them.

When answering technical questions, the best strategy is to give as precise answers as possible. It may look like an attempt to deviate from the main topic. In addition, it may provoke additional questions about which you can feel less confident.

1. What is SQL?
It stands for Structured Query Language, and it's a programming language used for interaction with relational database management systems (RDBMS). This includes fetching, updating, inserting, and removing data from tables.

2. What are SQL dialects? Give some examples.
The various versions of SQL, both free and paid, are also called SQL dialects. All the flavors of SQL have a very similar syntax and vary insignificantly only in additional functionality. Some examples are Microsoft SQL Server, PostgreSQL, MySQL, SQLite, T-SQL, and Oracle.

3. What are the main applications of SQL?
Using SQL, we can:

create, delete, and update tables in a database
access, manipulate, and modify data in a table
retrieve and summarize the necessary information from a table or several tables
add or remove certain rows or columns from a table
All in all, SQL allows querying a database in multiple ways. In addition, SQL easily integrates with other programming languages, such as Python or R, so we can use their combined power.

4. What is an SQL statement? Give some examples.
Also known as an SQL command. It's a string of characters interpreted by the SQL engine as a legal command and executed accordingly. Some examples of SQL statements are SELECT, CREATE, DELETE, DROP, REVOKE, and so on.

5. What types of SQL commands (or SQL subsets) do you know?
Data Definition Language (DDL) – to define and modify the structure of a database.
Data Manipulation Language (DML) – to access, manipulate, and modify data in a database.
Data Control Language (DCL) – to control user access to the data in the database and give or revoke privileges to a specific user or a group of users.
Transaction Control Language (TCL) – to control transactions in a database.
Data Query Language (DQL) – to perform queries on the data in a database to retrieve the necessary information from it.
6. Give some examples of common SQL commands of each type.
DDL: CREATE, ALTER TABLE, DROP, TRUNCATE, and ADD COLUMN

DML: UPDATE, DELETE, and INSERT

DCL: GRANT and REVOKE

TCL: COMMIT, SET TRANSACTION, ROLLBACK, and SAVEPOINT

DQL: – SELECT

7. What is a database?
A structured storage space where the data is kept in many tables and organized so that the necessary information can be easily fetched, manipulated, and summarized.

8. What is DBMS, and what types of DBMS do you know?
It stands for Database Management System, a software package used to perform various operations on the data stored in a database, such as accessing, updating, wrangling, inserting, and removing data. There are various types of DBMS, such as relational, hierarchical, network, graph, or object-oriented. These types are based on the way the data is organized, structured, and stored in the system.

9. What is RDBMS? Give some examples of RDBMS.
It stands for Relational Database Management System. It's the most common type of DBMS used for working with data stored in multiple tables related to each other by means of shared keys. The SQL programming language is designed to interact with RDBMS. Some examples of RDBMS are MySQL, PostgreSQL, Oracle, MariaDB, etc.

10. What are tables and fields in SQL?
A table is an organized set of related data stored in a tabular form, i.e., in rows and columns. A field is another term for a column of a table.

11. What is an SQL query, and what types of queries do you know?
A query is a piece of code written in SQL to access or modify data from a database.

There are two types of SQL queries: select and action queries. The first ones are used to retrieve the necessary data (this also includes limiting, grouping, ordering the data, extracting the data from multiple tables, etc.), while the second ones are used to create, add, delete, update, rename the data, etc.

12. What is a subquery?
Also called an inner query, a query placed inside another query, or an outer query. A subquery may occur in the clauses such as SELECT, FROM, WHERE, UPDATE, etc. It's also possible to have a subquery inside another subquery. The innermost subquery is run first, and its result is passed to the containing query (or subquery).

13. What types of SQL subqueries do you know?
Single-row – returns at most one row.
Multi-row – returns at least two rows.
Multi-column – returns at least two columns.
Correlated – a subquery related to the information from the outer query.
Nested – a subquery inside another subquery.
14. What is a constraint, and why use constraints?
A set of conditions defining the type of data that can be input into each column of a table. Constraints ensure data integrity in a table and block undesired actions.

15. What SQL constraints do you know?
DEFAULT – provides a default value for a column.

UNIQUE – allows only unique values.

NOT NULL – allows only non-null values.

PRIMARY KEY – allows only unique and strictly non-null values (NOT NULL and UNIQUE).

FOREIGN KEY – provides shared keys between two or more tables.

16. What is a join?
A clause used to combine and retrieve records from two or multiple tables. SQL tables can be joined based on the relationship between the columns of those tables. Check out our SQL joins tutorial for more context, plus our dedicated guide to SQL joins interview questions. 

17. What types of joins do you know?
(INNER) JOIN – returns only those records that satisfy a defined join condition in both (or all) tables. It's a default SQL join.

LEFT (OUTER) JOIN – returns all records from the left table and those records from the right table that satisfy a defined join condition.

RIGHT (OUTER) JOIN – returns all records from the right table and those records from the left table that satisfy a defined join condition.

FULL (OUTER) JOIN – returns all records from both (or all) tables. It can be considered as a combination of left and right joins.

Note: FULL OUTER JOIN is supported by PostgreSQL, SQL Server, Oracle, and MySQL 8.0 and above, but MySQL only allows it through UNION patterns; meanwhile, SQLite does not support RIGHT JOIN, which can be emulated using LEFT JOIN in combination with UNION.

18. What is a primary key?
A column (or multiple columns) of a table to which the PRIMARY KEY constraint was imposed to ensure unique and non-null values in that column. In other words, a primary key is a combination of the NOT NULL and UNIQUE constraints. The primary key uniquely identifies each record of the table. Each table can define at most one PRIMARY KEY (which may be composite). A PRIMARY KEY  is strongly recommended but not strictly required by all engines.

19. What is a unique key?
A column (or multiple columns) of a table to which the UNIQUE constraint was imposed to ensure unique values in that column, including a possible NULL value (the only one).

Note: 

SQL Server: Only one NULL allowed unless you use a filtered index.
PostgreSQL / Oracle / MySQL: Multiple NULLs allowed because NULL <> NULL.
20. What is a foreign key? 
A column (or multiple columns) of a table to which the FOREIGN KEY (or UNIQUE key) constraint was imposed to link this column to the primary key in another table (or several tables). The purpose of foreign keys is to keep connected various tables of a database.

21. What is an index?
A special data structure related to a database table and used for storing its important parts and enabling faster data search and retrieval. Indexes are especially efficient for large databases, where they significantly enhance query performance.

22. What types of indexes do you know?
Unique index – doesn't allow duplicates in a table column and hence helps maintain data integrity.
Clustered index – defines the physical order of records of a database table and performs data searching based on the key values. A table can have only one clustered index.
Non-clustered index – keeps the order of the table records that don't match the physical order of the actual data on the disk. It means that the data is stored in one place and a non-clustered index – in another one. A table can have multiple non-clustered indexes.
23. What is a schema?
A collection of database structural elements such as tables, stored procedures, indexes, functions, and triggers. It shows the overall database architecture, specifies the relationships between various objects of a database, and defines different access permissions for them. Read our database schema guide for a deeper understanding.

24. What is a SQL comment?
A human-readable clarification of what a particular piece of code does. SQL code comments can be single-line (preceded by a double dash --) or span over multiple lines (as follows: /*comment_text*/). When the SQL engine runs, it ignores code comments. The purpose of adding SQL code comments is to make the code more comprehensive for those people who will read it in the future.

25. What is a SQL operator?
A reserved character, a combination of characters, or a keyword used in SQL queries to perform a specific operation. SQL operators are commonly used with the WHERE clause to set a condition (or conditions) for filtering the data.

26. What types of SQL operators do you know?
Arithmetic (+, -, *, /, etc.)

Comparison (>, <, =, >=, etc.)

Compound (+=, -=, *=, /=, etc.)

Logical (AND, OR, NOT, BETWEEN, etc.)

String (%, _, +, ^, etc.)

Set (UNION, UNION ALL, INTERSECT, and MINUS (or EXCEPT))

27. What is an alias?
A temporary name given to a table (or a column in a table) while executing a certain SQL query. Aliases are used to improve the code readability and make the code more compact. An alias is introduced with the AS keyword:

SELECT col_1 AS column
FROM table_name;

Powered By 
28. What is a clause?
A condition imposed on a SQL query to filter the data to obtain the desired result. Some examples are WHERE, LIMIT, HAVING, LIKE, AND, OR, ORDER BY, etc.

29. What are some common statements used with the SELECT query?
The most common ones are FROM, GROUP BY, JOIN, WHERE, ORDER BY, LIMIT, and HAVING.

30. How to create a table?
Using the CREATE TABLE statement. For example, to create a table with three columns of predefined datatypes, we apply the following syntax:

CREATE TABLE table_name (col_1 datatype,
                         col_2 datatype,
                         col_3 datatype);

Powered By 
31. How to update a table?
Using the UPDATE statement. The syntax is:

UPDATE table_name
SET col_1 = value_1, col_2 = value_2
WHERE condition;

Powered By 
32. How to delete a table from a database?
Using the DROP TABLE statement. The syntax is: DROP TABLE table_name;.

33. How to get the count of records in a table?
Using the COUNT() aggregate function with the asterisk passed as its argument: SELECT COUNT(*) FROM table_name;.

34. How to sort records in a table?
Using the ORDER BY statement:

SELECT * FROM table_name
ORDER BY col_1;

Powered By 
We can specify that we need a descending order using the DESC keyword; otherwise, the order will be ascending by default. Also, we can sort by more than one column and specify for each one, ascending or descending order separately. For example:

SELECT * FROM table_name
ORDER BY col_1 DESC, col_3, col_6 DESC;

Powered By 
35. How to select all columns from a table?
Using the asterisk * with the SELECT statement. The syntax is: SELECT * FROM table_name;.

36. How to select common records from two tables?
Using the INTERSECT statement:

SELECT col1, col2 FROM table_1
INTERSECT
SELECT col1, col2 FROM table_2;

Powered By 
Note: INTERSECT requires the same number of columns and compatible types.

37. What is the DISTINCT statement and how do you use it?
This statement is used with the SELECT statement to filter out duplicates and return only unique values from a column of a table. The syntax is:

SELECT DISTINCT col_1
FROM table_name;

Powered By 
38. What are relationships? Give some examples.
Relationships are the connections and correlations between entities, basically meaning how two or more tables of a database are related to one another. For example, we can find an ID of the same client in a table on sales data and in a customer table.

39. What is a NULL value? How is it different from zero or a blank space?
A NULL value indicates the absence of data for a certain cell of a table. Instead, zero is a valid numeric value, and an empty string is a legal string of zero length.

40. What is the difference between SQL and NoSQL?
SQL databases are relational, structured, and use tables with predefined schemas, while NoSQL databases are non-relational, schema-less, and designed to handle unstructured or semi-structured data.

41. What are some common challenges when working with SQL databases?
Challenges include performance tuning for large datasets, managing indexing strategies, ensuring data integrity with constraints, handling concurrent transactions, and optimizing query execution.

Intermediate SQL Interview Questions
In this section, we take a look at the most popular intermediate SQL questions and answers so that you'll know what to expect from your interviewer.

42. What is a function in SQL, and why use functions?
A database object representing a set of SQL statements frequently used for a certain task. A function takes in some input parameters, performs calculations or other manipulations on them, and returns the result. Functions help improve code readability and avoid repetition of the same code snippets.

43. What types of SQL functions do you know?
Aggregate functions – work on multiple, usually grouped records for the provided columns of a table, and return a single value (usually by group).
Scalar functions – work on each individual value and return a single value.
On the other hand, SQL functions can be built-in (defined by the system) or user-defined (created by the user for their specific needs).

44. What aggregate functions do you know?
AVG() – returns the average value

SUM() – returns the sum of values

MIN() – returns the minimum value

MAX() – returns the maximum value

COUNT() – returns the number of rows, including those with null values

FIRST() – returns the first value from a column

LAST()– returns the last value from a column

45. What scalar functions do you know?
LEN() (in other SQL flavors – LENGTH()) – returns the length of a string, including the blank spaces

UCASE() (in other SQL flavors – UPPER()) – returns a string converted to the upper case

LCASE() (in other SQL flavors – LOWER()) – returns a string converted to the lower case

INITCAP() – returns a string converted to the title case (i.e., each word of the string starts from a capital letter)

MID() (in other SQL flavors – SUBSTR()) – extracts a substring from a string

ROUND() – returns the numerical value rounded to a specified number of decimals

NOW() – returns the current date and time

46. What are case manipulation functions? Give some examples.
Case manipulation functions represent a subset of character functions, and they're used to change the case of the text data. With these functions, we can convert the data into the upper, lower, or title case.

UCASE() (in other SQL flavors – UPPER()) – returns a string converted to the upper case

LCASE() (in other SQL flavors – LOWER()) – returns a string converted to the lower case

INITCAP() – returns a string converted to the title case (i.e., each word of the string starts from a capital letter)

47. What are character manipulation functions? Give some examples.
Character manipulation functions represent a subset of character functions, and they're used to modify the text data.

CONCAT() – joins two or more string values appending the second string to the end of the first one

SUBSTRING()/SUBSTR() – returns a part of a string satisfying the provided start and end points

LENGTH() (in other SQL flavors – LEN()) – returns the length of a string, including the blank spaces

REPLACE() – replaces all occurrences of a defined substring in a provided string with another substring

INSTR() – returns the numeric position of a defined substring in a provided string

LPAD() and RPAD() – return the padding of the left-side/right-side character for right-justified/left-justified value

TRIM() – removes all the defined characters, as well as white spaces, from the left, right, or both ends of a provided string

48. What is the difference between local and global variables?
Local variables can be accessed only inside the function in which they were declared. Instead, global variables, being declared outside any function, are stored in fixed memory structures and can be used throughout the entire program.

49. What is the difference between SQL and PL/SQL?
SQL is a standard language for querying and managing relational databases, primarily used for data manipulation and retrieval. PL/SQL (Procedural Language/SQL) is an extension of SQL used in Oracle databases that includes procedural programming constructs like loops, conditions, and exception handling, allowing for complex business logic to be implemented within the database. We have another article on the Top 20 PL/SQL Interview Questions and Answers which is a good review if you know you will be asked about your knowledge of Oracle. 

50. What is the difference between LEFT JOIN and LEFT OUTER JOIN?
There is no difference between LEFT JOIN and LEFT OUTER JOIN. They are interchangeable. SQL allows the OUTER keyword to be optional, so LEFT JOIN is simply a shorthand for LEFT OUTER JOIN. Both return all records from the left table and the matching records from the right table.

51. What is indexing in SQL, and how does it improve performance?
Indexing creates a special data structure that speeds up data retrieval by allowing the database to find rows more efficiently. It works like an optimized lookup table, reducing the need for full table scans. However, excessive indexing can slow down insert, update, and delete operations due to the need for index maintenance.

52. What is a stored procedure, and how is it different from a function?
A stored procedure is a precompiled set of SQL statements executed as a unit to perform a task. Procedures can modify data or schema objects, manage transactions, and return zero or more result sets. Functions, on the other hand, are typically used in SQL expressions, must return a value (scalar or table-valued), and in many databases are restricted from side effects. Exact behavior differs by DB (e.g., T-SQL has scalar and table-valued functions; PostgreSQL distinguishes between functions and procedures).

53. What is the default data ordering with the ORDER BY statement, and how do you change it?
Default is ascending (NULLS FIRST/LAST varies by DB). Use ASC/DESC per column keyword as follows:

SELECT * FROM table_name 
ORDER BY col_1 DESC;

Powered By 
54. What set operators do you know?
UNION – returns the records obtained by at least one of two queries (excluding duplicates)

UNION ALL – returns the records obtained by at least one of two queries (including duplicates)

INTERSECT – returns the records obtained by both queries

EXCEPT (called MINUS in MySQL and Oracle) – returns only the records obtained by the first query but not the second one

55. What operator is used in the query for pattern matching?
The LIKE operator in combination with the % and _ wildcards. The % wildcard represents any number of characters including zero, while _ – strictly one character.

56. What is the difference between a primary key and a unique key?
While both types of keys ensure unique values in a column of a table, the first one uniquely identifies each record of the table, and the second one prevents duplicates in that column.

57. What is a composite primary key?
The primary key of a table, based on multiple columns.

58. What is the order of appearance of the common statements in the SELECT query?
SELECT – FROM – JOIN – ON – WHERE – GROUP BY – HAVING – ORDER BY – LIMIT

59. In which order does the interpreter execute the common statements in the SELECT query?
Here is the SQL order of execution: 

FROM → ON → JOIN → WHERE → GROUP BY → HAVING → SELECT → ORDER BY → LIMIT/OFFSET (FETCH)

60. What is a view, and why use it?
A virtual table containing a subset of data retrieved from one or more database tables (or other views). Standard views store no data; materialized views do. Views can simplify queries, encapsulate logic, and, combined with privileges, restrict columns/rows. They can join/aggregate data from multiple tables.

61. Can we create a view based on another view?
Yes. This is also known as nested views. However, we should avoid nesting multiple views since the code becomes difficult to read and debug.

62. Can we still use a view if the original table is deleted?
No. Any views based on that table will become invalid after deleting the base table. If we try to use such a view anyway, we'll receive an error message.

63. What types of SQL relationships do you know?
One-to-one – each record in one table corresponds to only one record in another table
One-to-many – each record in one table corresponds to several records in another table
Many-to-many – each record in both tables corresponds to several records in another table
64. What are the possible values of a BOOLEAN data field?
In some SQL flavors, such as PostgreSQL, the BOOLEAN data type exists explicitly and takes values TRUE, FALSE, or NULL. In other flavors, such as Microsoft SQL Server, the BIT datatype is used to store Boolean values as integers 1 (true) or 0 (false).

65. What is normalization in SQL, and why use it?
Normalization is a process of database design that includes organizing and restructuring data in a way to reduce data redundancy, dependency, duplication, and inconsistency. This leads to enhanced data integrity, more tables within the database, more efficient data access and security control, and greater query flexibility.

66. What is denormalization in SQL, and why use it?
Denormalization is the process opposite of normalization: it introduces data redundancy and combines data from multiple tables. Denormalization optimizes the performance of the database infrastructure in situations when read operations are more important than write operations since it helps avoid complex joins and reduces the time of query running.

67. What is the difference between renaming a column and giving an alias to it?
Renaming a column means permanently changing its actual name in the original table. Giving an alias to a column means giving it a temporary name while executing an SQL query, with the purpose to make the code more readable and compact.

68. What is the difference between nested and correlated subqueries?
A correlated subquery is an inner query nested in a bigger (outer) query that refers to the values from the outer query for its execution, meaning that a correlated subquery depends on its outer query. Instead, a non-correlated subquery doesn't rely on the data from the outer query and can be run independently of it.

69. What is the difference between clustered and non-clustered indexes?
While a clustered index defines the physical order of records of a table and performs data searching based on the key values, a non-clustered index keeps the order of records that do not match the physical order of the actual data on the disk. A table can have only one clustered index but many non-clustered ones.

70. What is the CASE() function?
The way to implement the if-then-else logic in SQL. This function sequentially checks the provided conditions in the WHEN clauses and returns the value from the corresponding THEN clause when the first condition is satisfied. If none of the conditions is satisfied, the function returns the value from the ELSE clause in case it's provided, otherwise, it returns NULL. The syntax is:

CASE
    WHEN condition_1 THEN value_1
    WHEN condition_2 THEN value_2
    WHEN condition_3 THEN value_3
    ...
    ELSE value
END;

Powered By 
71. What is the difference between the DELETE and TRUNCATE statements?
DELETE is a DML (Data Manipulation Language) command used to remove one or more rows from a table based on a condition in the WHERE clause. It is reversible only if wrapped in a transaction (via ROLLBACK).

TRUNCATE is a DDL (Data Definition Language) command that removes all rows from a table by deallocating pages. It is faster but generally irreversible, and cannot be used on tables that are referenced by a foreign key.

72. What is the difference between the DROP and TRUNCATE statements?
DROP deletes a table from the database completely, including the table structure and all the associated constraints, relationships with other tables, and access privileges. TRUNCATE deletes all rows from a table without affecting the table structure and constraints.  Both are DDL. DROP removes the table and its metadata; TRUNCATE deletes all rows but keeps the table definition. Performance and transactional behavior depend on the DB engine.

73. What is the difference between the HAVING and WHERE statements?
The first one works on aggregated data after they are grouped, while the second one checks each row individually. If both statements are present in a query, they appear in the following order: WHERE – GROUP BY – HAVING. The SQL engine interprets them also in the same order.

74. How do you add a record to a table?
Using the INSERT INTO statement in combination with VALUES. The syntax is:

INSERT INTO table_name
VALUES (value_1, value_2, ...);

Powered By 
75. How do you delete a record from a table?
Using the DELETE statement. The syntax is:

DELETE FROM table_name
WHERE condition;

Powered By 
In this way, we can also delete multiple records if they satisfy the provided condition.

76. How do you add a column to a table?
Using the ALTER TABLE statement in combination with ADD. The syntax is:

ALTER TABLE table_name
ADD column_name datatype;

Powered By 
77. How do you rename a column of a table?
Using the ALTER TABLE statement in combination with RENAME COLUMN ... TO ... The syntax is:

ALTER TABLE table_name
RENAME COLUMN old_column_name TO new_column_name;

Powered By 
78. How do you delete a column from a table?
Using the ALTER TABLE statement in combination with DROP COLUMN. The syntax is:

ALTER TABLE table_name
DROP COLUMN column_name;

Powered By 
79. How do you select all even or all odd records in a table?
By checking the remainder of the division by 2. In some SQL versions (e.g., PostgreSQL and My SQL), we use the MOD function, in the others (Microsoft SQL Server and SQLite) – the modulo operator (%). To select all even records using MOD:

SELECT * FROM table_name
WHERE MOD(ID_column, 2) = 0;

Powered By 
To select all even records using %:

SELECT * FROM table_name 
WHERE ID_column % 2 = 0;

Powered By 
To select all odd records, the syntax is identical in both cases, only that we would use the inequality operator <> instead of =.

80. How to prevent duplicate records when making a query?
Using the DISTINCT statement in combination with SELECT or creating a unique key for that table.

81. How do you insert many rows in a table?
Using the INSERT INTO statement in combination with VALUES. The syntax is:

INSERT INTO table_name
VALUES (value_1, value_2, ...),
      (value_3, value_4, ...),
      (value_5, value_6, ...),
      ...;

Powered By 
82. How do you find the nth highest value in a column of a table?
Using window functions to handle ties correctly:

SELECT column_name
FROM (
  SELECT column_name, DENSE_RANK() OVER (ORDER BY column_name DESC) AS rnk
  FROM table_name
) t
WHERE rnk = :n;

Powered By 
For the nth row by order (ignoring ties): ORDER BY column_name DESC OFFSET n-1 ROWS FETCH NEXT 1 ROW ONLY.

83. How do you find the values in a text column of a table that start with a certain letter?
Using the LIKE operator in combination with the % and _ wildcards. For example, we need to find all surnames in a table that start with "A". The query is:

SELECT * FROM table_name
WHERE surname LIKE 'A_';

Powered By 
Here, we assume that a surname must contain at least two letters. Without this assumption (meaning that a surname can be just A), the query is as follows:

SELECT * FROM table_name
WHERE surname LIKE 'A%';

Powered By 
84. How do you find the last id in a table?
The simplest method is to use the aggregate function MAX().

SELECT MAX(id) AS highest_id
FROM table_name;

Powered By 
Using ORDER BY with LIMIT or TOP

SELECT id
FROM table_name
ORDER BY id DESC
LIMIT 1;

Powered By 
85. How to select random rows from a table?
Using the RAND() function in combination with ORDER BY and LIMIT. In some SQL flavors, such as PostgreSQL, it's called RANDOM(). For example, the following code will return five random rows from a table in MySQL:

SELECT * FROM table_name
ORDER BY RAND()
LIMIT 5;
SQL Interview Questions at a Glance
Throughout this article, we'll explore a range of SQL interview questions and answers for practitioners at the beginner and intermediate levels. If you're looking for an overview, we've compiled some top tips below:

For beginners
General questions. Expect questions about your experience, the SQL flavors you're familiar with, and your level of proficiency.
Technical questions. These will cover the basics of SQL, such as what it is, its applications, SQL statements, SQL commands, and types of SQL queries, among others.
For intermediate practitioners
Functions in SQL. You should know about aggregate and scalar functions, as well as built-in and user-defined functions.

Advanced commands. Questions may cover topics like joins, primary and foreign keys, indexes, and SQL relationships.

Database design. Expect questions on normalization, denormalization, and the differences between various SQL statements like DELETE, TRUNCATE, and DROP.

Advanced queries. You may be asked about subqueries, both nested and correlated, as well as how to perform specific tasks like finding the nth highest value in a column.

General SQL Interview Questions for Beginners
Before asking you technical questions, your interviewer may ask you some general questions about your overall experience with SQL. You can expect the following questions:

What SQL flavors are you familiar with?
How can you estimate your level of proficiency in SQL?
For how long have you been working in SQL?
While this information can be mentioned in your resume, be ready to talk about it. Naturally, there are no "right" answers to such questions, and there is no need to make up things when answering them.

Don't worry if your experience in SQL is limited: this is something your interviewer, most probably, already knows from your resume. Since they are interested in talking to you anyway, your profile was considered a good fit for their company.

Also, it's perfectly fine if you have only worked with one SQL flavor. Remember that all SQL dialects are fairly similar. Therefore, being familiar with only one of them is a solid basis for you to learn any others.

Technical SQL Interview Questions for Beginners 
Now, let's move on to the technical SQL interview questions and some potential answers to them.

When answering technical questions, the best strategy is to give as precise answers as possible. It may look like an attempt to deviate from the main topic. In addition, it may provoke additional questions about which you can feel less confident.

1. What is SQL?
It stands for Structured Query Language, and it's a programming language used for interaction with relational database management systems (RDBMS). This includes fetching, updating, inserting, and removing data from tables.

2. What are SQL dialects? Give some examples.
The various versions of SQL, both free and paid, are also called SQL dialects. All the flavors of SQL have a very similar syntax and vary insignificantly only in additional functionality. Some examples are Microsoft SQL Server, PostgreSQL, MySQL, SQLite, T-SQL, and Oracle.

3. What are the main applications of SQL?
Using SQL, we can:

create, delete, and update tables in a database
access, manipulate, and modify data in a table
retrieve and summarize the necessary information from a table or several tables
add or remove certain rows or columns from a table
All in all, SQL allows querying a database in multiple ways. In addition, SQL easily integrates with other programming languages, such as Python or R, so we can use their combined power.

4. What is an SQL statement? Give some examples.
Also known as an SQL command. It's a string of characters interpreted by the SQL engine as a legal command and executed accordingly. Some examples of SQL statements are SELECT, CREATE, DELETE, DROP, REVOKE, and so on.

5. What types of SQL commands (or SQL subsets) do you know?
Data Definition Language (DDL) – to define and modify the structure of a database.
Data Manipulation Language (DML) – to access, manipulate, and modify data in a database.
Data Control Language (DCL) – to control user access to the data in the database and give or revoke privileges to a specific user or a group of users.
Transaction Control Language (TCL) – to control transactions in a database.
Data Query Language (DQL) – to perform queries on the data in a database to retrieve the necessary information from it.
6. Give some examples of common SQL commands of each type.
DDL: CREATE, ALTER TABLE, DROP, TRUNCATE, and ADD COLUMN

DML: UPDATE, DELETE, and INSERT

DCL: GRANT and REVOKE

TCL: COMMIT, SET TRANSACTION, ROLLBACK, and SAVEPOINT

DQL: – SELECT

7. What is a database?
A structured storage space where the data is kept in many tables and organized so that the necessary information can be easily fetched, manipulated, and summarized.

8. What is DBMS, and what types of DBMS do you know?
It stands for Database Management System, a software package used to perform various operations on the data stored in a database, such as accessing, updating, wrangling, inserting, and removing data. There are various types of DBMS, such as relational, hierarchical, network, graph, or object-oriented. These types are based on the way the data is organized, structured, and stored in the system.

9. What is RDBMS? Give some examples of RDBMS.
It stands for Relational Database Management System. It's the most common type of DBMS used for working with data stored in multiple tables related to each other by means of shared keys. The SQL programming language is designed to interact with RDBMS. Some examples of RDBMS are MySQL, PostgreSQL, Oracle, MariaDB, etc.

10. What are tables and fields in SQL?
A table is an organized set of related data stored in a tabular form, i.e., in rows and columns. A field is another term for a column of a table.

11. What is an SQL query, and what types of queries do you know?
A query is a piece of code written in SQL to access or modify data from a database.

There are two types of SQL queries: select and action queries. The first ones are used to retrieve the necessary data (this also includes limiting, grouping, ordering the data, extracting the data from multiple tables, etc.), while the second ones are used to create, add, delete, update, rename the data, etc.

12. What is a subquery?
Also called an inner query, a query placed inside another query, or an outer query. A subquery may occur in the clauses such as SELECT, FROM, WHERE, UPDATE, etc. It's also possible to have a subquery inside another subquery. The innermost subquery is run first, and its result is passed to the containing query (or subquery).

13. What types of SQL subqueries do you know?
Single-row – returns at most one row.
Multi-row – returns at least two rows.
Multi-column – returns at least two columns.
Correlated – a subquery related to the information from the outer query.
Nested – a subquery inside another subquery.
14. What is a constraint, and why use constraints?
A set of conditions defining the type of data that can be input into each column of a table. Constraints ensure data integrity in a table and block undesired actions.

15. What SQL constraints do you know?
DEFAULT – provides a default value for a column.

UNIQUE – allows only unique values.

NOT NULL – allows only non-null values.

PRIMARY KEY – allows only unique and strictly non-null values (NOT NULL and UNIQUE).

FOREIGN KEY – provides shared keys between two or more tables.

16. What is a join?
A clause used to combine and retrieve records from two or multiple tables. SQL tables can be joined based on the relationship between the columns of those tables. Check out our SQL joins tutorial for more context, plus our dedicated guide to SQL joins interview questions. 

17. What types of joins do you know?
(INNER) JOIN – returns only those records that satisfy a defined join condition in both (or all) tables. It's a default SQL join.

LEFT (OUTER) JOIN – returns all records from the left table and those records from the right table that satisfy a defined join condition.

RIGHT (OUTER) JOIN – returns all records from the right table and those records from the left table that satisfy a defined join condition.

FULL (OUTER) JOIN – returns all records from both (or all) tables. It can be considered as a combination of left and right joins.

Note: FULL OUTER JOIN is supported by PostgreSQL, SQL Server, Oracle, and MySQL 8.0 and above, but MySQL only allows it through UNION patterns; meanwhile, SQLite does not support RIGHT JOIN, which can be emulated using LEFT JOIN in combination with UNION.

18. What is a primary key?
A column (or multiple columns) of a table to which the PRIMARY KEY constraint was imposed to ensure unique and non-null values in that column. In other words, a primary key is a combination of the NOT NULL and UNIQUE constraints. The primary key uniquely identifies each record of the table. Each table can define at most one PRIMARY KEY (which may be composite). A PRIMARY KEY  is strongly recommended but not strictly required by all engines.

19. What is a unique key?
A column (or multiple columns) of a table to which the UNIQUE constraint was imposed to ensure unique values in that column, including a possible NULL value (the only one).

Note: 

SQL Server: Only one NULL allowed unless you use a filtered index.
PostgreSQL / Oracle / MySQL: Multiple NULLs allowed because NULL <> NULL.
20. What is a foreign key? 
A column (or multiple columns) of a table to which the FOREIGN KEY (or UNIQUE key) constraint was imposed to link this column to the primary key in another table (or several tables). The purpose of foreign keys is to keep connected various tables of a database.

21. What is an index?
A special data structure related to a database table and used for storing its important parts and enabling faster data search and retrieval. Indexes are especially efficient for large databases, where they significantly enhance query performance.

22. What types of indexes do you know?
Unique index – doesn't allow duplicates in a table column and hence helps maintain data integrity.
Clustered index – defines the physical order of records of a database table and performs data searching based on the key values. A table can have only one clustered index.
Non-clustered index – keeps the order of the table records that don't match the physical order of the actual data on the disk. It means that the data is stored in one place and a non-clustered index – in another one. A table can have multiple non-clustered indexes.
23. What is a schema?
A collection of database structural elements such as tables, stored procedures, indexes, functions, and triggers. It shows the overall database architecture, specifies the relationships between various objects of a database, and defines different access permissions for them. Read our database schema guide for a deeper understanding.

24. What is a SQL comment?
A human-readable clarification of what a particular piece of code does. SQL code comments can be single-line (preceded by a double dash --) or span over multiple lines (as follows: /*comment_text*/). When the SQL engine runs, it ignores code comments. The purpose of adding SQL code comments is to make the code more comprehensive for those people who will read it in the future.

25. What is a SQL operator?
A reserved character, a combination of characters, or a keyword used in SQL queries to perform a specific operation. SQL operators are commonly used with the WHERE clause to set a condition (or conditions) for filtering the data.

26. What types of SQL operators do you know?
Arithmetic (+, -, *, /, etc.)

Comparison (>, <, =, >=, etc.)

Compound (+=, -=, *=, /=, etc.)

Logical (AND, OR, NOT, BETWEEN, etc.)

String (%, _, +, ^, etc.)

Set (UNION, UNION ALL, INTERSECT, and MINUS (or EXCEPT))

27. What is an alias?
A temporary name given to a table (or a column in a table) while executing a certain SQL query. Aliases are used to improve the code readability and make the code more compact. An alias is introduced with the AS keyword:

SELECT col_1 AS column
FROM table_name;

Powered By 
28. What is a clause?
A condition imposed on a SQL query to filter the data to obtain the desired result. Some examples are WHERE, LIMIT, HAVING, LIKE, AND, OR, ORDER BY, etc.

29. What are some common statements used with the SELECT query?
The most common ones are FROM, GROUP BY, JOIN, WHERE, ORDER BY, LIMIT, and HAVING.

30. How to create a table?
Using the CREATE TABLE statement. For example, to create a table with three columns of predefined datatypes, we apply the following syntax:

CREATE TABLE table_name (col_1 datatype,
                         col_2 datatype,
                         col_3 datatype);

Powered By 
31. How to update a table?
Using the UPDATE statement. The syntax is:

UPDATE table_name
SET col_1 = value_1, col_2 = value_2
WHERE condition;

Powered By 
32. How to delete a table from a database?
Using the DROP TABLE statement. The syntax is: DROP TABLE table_name;.

33. How to get the count of records in a table?
Using the COUNT() aggregate function with the asterisk passed as its argument: SELECT COUNT(*) FROM table_name;.

34. How to sort records in a table?
Using the ORDER BY statement:

SELECT * FROM table_name
ORDER BY col_1;

Powered By 
We can specify that we need a descending order using the DESC keyword; otherwise, the order will be ascending by default. Also, we can sort by more than one column and specify for each one, ascending or descending order separately. For example:

SELECT * FROM table_name
ORDER BY col_1 DESC, col_3, col_6 DESC;

Powered By 
35. How to select all columns from a table?
Using the asterisk * with the SELECT statement. The syntax is: SELECT * FROM table_name;.

36. How to select common records from two tables?
Using the INTERSECT statement:

SELECT col1, col2 FROM table_1
INTERSECT
SELECT col1, col2 FROM table_2;

Powered By 
Note: INTERSECT requires the same number of columns and compatible types.

37. What is the DISTINCT statement and how do you use it?
This statement is used with the SELECT statement to filter out duplicates and return only unique values from a column of a table. The syntax is:

SELECT DISTINCT col_1
FROM table_name;

Powered By 
38. What are relationships? Give some examples.
Relationships are the connections and correlations between entities, basically meaning how two or more tables of a database are related to one another. For example, we can find an ID of the same client in a table on sales data and in a customer table.

39. What is a NULL value? How is it different from zero or a blank space?
A NULL value indicates the absence of data for a certain cell of a table. Instead, zero is a valid numeric value, and an empty string is a legal string of zero length.

40. What is the difference between SQL and NoSQL?
SQL databases are relational, structured, and use tables with predefined schemas, while NoSQL databases are non-relational, schema-less, and designed to handle unstructured or semi-structured data.

41. What are some common challenges when working with SQL databases?
Challenges include performance tuning for large datasets, managing indexing strategies, ensuring data integrity with constraints, handling concurrent transactions, and optimizing query execution.

Intermediate SQL Interview Questions
In this section, we take a look at the most popular intermediate SQL questions and answers so that you'll know what to expect from your interviewer.

42. What is a function in SQL, and why use functions?
A database object representing a set of SQL statements frequently used for a certain task. A function takes in some input parameters, performs calculations or other manipulations on them, and returns the result. Functions help improve code readability and avoid repetition of the same code snippets.

43. What types of SQL functions do you know?
Aggregate functions – work on multiple, usually grouped records for the provided columns of a table, and return a single value (usually by group).
Scalar functions – work on each individual value and return a single value.
On the other hand, SQL functions can be built-in (defined by the system) or user-defined (created by the user for their specific needs).

44. What aggregate functions do you know?
AVG() – returns the average value

SUM() – returns the sum of values

MIN() – returns the minimum value

MAX() – returns the maximum value

COUNT() – returns the number of rows, including those with null values

FIRST() – returns the first value from a column

LAST()– returns the last value from a column

45. What scalar functions do you know?
LEN() (in other SQL flavors – LENGTH()) – returns the length of a string, including the blank spaces

UCASE() (in other SQL flavors – UPPER()) – returns a string converted to the upper case

LCASE() (in other SQL flavors – LOWER()) – returns a string converted to the lower case

INITCAP() – returns a string converted to the title case (i.e., each word of the string starts from a capital letter)

MID() (in other SQL flavors – SUBSTR()) – extracts a substring from a string

ROUND() – returns the numerical value rounded to a specified number of decimals

NOW() – returns the current date and time

46. What are case manipulation functions? Give some examples.
Case manipulation functions represent a subset of character functions, and they're used to change the case of the text data. With these functions, we can convert the data into the upper, lower, or title case.

UCASE() (in other SQL flavors – UPPER()) – returns a string converted to the upper case

LCASE() (in other SQL flavors – LOWER()) – returns a string converted to the lower case

INITCAP() – returns a string converted to the title case (i.e., each word of the string starts from a capital letter)

47. What are character manipulation functions? Give some examples.
Character manipulation functions represent a subset of character functions, and they're used to modify the text data.

CONCAT() – joins two or more string values appending the second string to the end of the first one

SUBSTRING()/SUBSTR() – returns a part of a string satisfying the provided start and end points

LENGTH() (in other SQL flavors – LEN()) – returns the length of a string, including the blank spaces

REPLACE() – replaces all occurrences of a defined substring in a provided string with another substring

INSTR() – returns the numeric position of a defined substring in a provided string

LPAD() and RPAD() – return the padding of the left-side/right-side character for right-justified/left-justified value

TRIM() – removes all the defined characters, as well as white spaces, from the left, right, or both ends of a provided string

48. What is the difference between local and global variables?
Local variables can be accessed only inside the function in which they were declared. Instead, global variables, being declared outside any function, are stored in fixed memory structures and can be used throughout the entire program.

49. What is the difference between SQL and PL/SQL?
SQL is a standard language for querying and managing relational databases, primarily used for data manipulation and retrieval. PL/SQL (Procedural Language/SQL) is an extension of SQL used in Oracle databases that includes procedural programming constructs like loops, conditions, and exception handling, allowing for complex business logic to be implemented within the database. We have another article on the Top 20 PL/SQL Interview Questions and Answers which is a good review if you know you will be asked about your knowledge of Oracle. 

50. What is the difference between LEFT JOIN and LEFT OUTER JOIN?
There is no difference between LEFT JOIN and LEFT OUTER JOIN. They are interchangeable. SQL allows the OUTER keyword to be optional, so LEFT JOIN is simply a shorthand for LEFT OUTER JOIN. Both return all records from the left table and the matching records from the right table.

51. What is indexing in SQL, and how does it improve performance?
Indexing creates a special data structure that speeds up data retrieval by allowing the database to find rows more efficiently. It works like an optimized lookup table, reducing the need for full table scans. However, excessive indexing can slow down insert, update, and delete operations due to the need for index maintenance.

52. What is a stored procedure, and how is it different from a function?
A stored procedure is a precompiled set of SQL statements executed as a unit to perform a task. Procedures can modify data or schema objects, manage transactions, and return zero or more result sets. Functions, on the other hand, are typically used in SQL expressions, must return a value (scalar or table-valued), and in many databases are restricted from side effects. Exact behavior differs by DB (e.g., T-SQL has scalar and table-valued functions; PostgreSQL distinguishes between functions and procedures).

53. What is the default data ordering with the ORDER BY statement, and how do you change it?
Default is ascending (NULLS FIRST/LAST varies by DB). Use ASC/DESC per column keyword as follows:

SELECT * FROM table_name 
ORDER BY col_1 DESC;

Powered By 
54. What set operators do you know?
UNION – returns the records obtained by at least one of two queries (excluding duplicates)

UNION ALL – returns the records obtained by at least one of two queries (including duplicates)

INTERSECT – returns the records obtained by both queries

EXCEPT (called MINUS in MySQL and Oracle) – returns only the records obtained by the first query but not the second one

55. What operator is used in the query for pattern matching?
The LIKE operator in combination with the % and _ wildcards. The % wildcard represents any number of characters including zero, while _ – strictly one character.

56. What is the difference between a primary key and a unique key?
While both types of keys ensure unique values in a column of a table, the first one uniquely identifies each record of the table, and the second one prevents duplicates in that column.

57. What is a composite primary key?
The primary key of a table, based on multiple columns.

58. What is the order of appearance of the common statements in the SELECT query?
SELECT – FROM – JOIN – ON – WHERE – GROUP BY – HAVING – ORDER BY – LIMIT

59. In which order does the interpreter execute the common statements in the SELECT query?
Here is the SQL order of execution: 

FROM → ON → JOIN → WHERE → GROUP BY → HAVING → SELECT → ORDER BY → LIMIT/OFFSET (FETCH)

60. What is a view, and why use it?
A virtual table containing a subset of data retrieved from one or more database tables (or other views). Standard views store no data; materialized views do. Views can simplify queries, encapsulate logic, and, combined with privileges, restrict columns/rows. They can join/aggregate data from multiple tables.

61. Can we create a view based on another view?
Yes. This is also known as nested views. However, we should avoid nesting multiple views since the code becomes difficult to read and debug.

62. Can we still use a view if the original table is deleted?
No. Any views based on that table will become invalid after deleting the base table. If we try to use such a view anyway, we'll receive an error message.

63. What types of SQL relationships do you know?
One-to-one – each record in one table corresponds to only one record in another table
One-to-many – each record in one table corresponds to several records in another table
Many-to-many – each record in both tables corresponds to several records in another table
64. What are the possible values of a BOOLEAN data field?
In some SQL flavors, such as PostgreSQL, the BOOLEAN data type exists explicitly and takes values TRUE, FALSE, or NULL. In other flavors, such as Microsoft SQL Server, the BIT datatype is used to store Boolean values as integers 1 (true) or 0 (false).

65. What is normalization in SQL, and why use it?
Normalization is a process of database design that includes organizing and restructuring data in a way to reduce data redundancy, dependency, duplication, and inconsistency. This leads to enhanced data integrity, more tables within the database, more efficient data access and security control, and greater query flexibility.

66. What is denormalization in SQL, and why use it?
Denormalization is the process opposite of normalization: it introduces data redundancy and combines data from multiple tables. Denormalization optimizes the performance of the database infrastructure in situations when read operations are more important than write operations since it helps avoid complex joins and reduces the time of query running.

67. What is the difference between renaming a column and giving an alias to it?
Renaming a column means permanently changing its actual name in the original table. Giving an alias to a column means giving it a temporary name while executing an SQL query, with the purpose to make the code more readable and compact.

68. What is the difference between nested and correlated subqueries?
A correlated subquery is an inner query nested in a bigger (outer) query that refers to the values from the outer query for its execution, meaning that a correlated subquery depends on its outer query. Instead, a non-correlated subquery doesn't rely on the data from the outer query and can be run independently of it.

69. What is the difference between clustered and non-clustered indexes?
While a clustered index defines the physical order of records of a table and performs data searching based on the key values, a non-clustered index keeps the order of records that do not match the physical order of the actual data on the disk. A table can have only one clustered index but many non-clustered ones.

70. What is the CASE() function?
The way to implement the if-then-else logic in SQL. This function sequentially checks the provided conditions in the WHEN clauses and returns the value from the corresponding THEN clause when the first condition is satisfied. If none of the conditions is satisfied, the function returns the value from the ELSE clause in case it's provided, otherwise, it returns NULL. The syntax is:

CASE
    WHEN condition_1 THEN value_1
    WHEN condition_2 THEN value_2
    WHEN condition_3 THEN value_3
    ...
    ELSE value
END;

Powered By 
71. What is the difference between the DELETE and TRUNCATE statements?
DELETE is a DML (Data Manipulation Language) command used to remove one or more rows from a table based on a condition in the WHERE clause. It is reversible only if wrapped in a transaction (via ROLLBACK).

TRUNCATE is a DDL (Data Definition Language) command that removes all rows from a table by deallocating pages. It is faster but generally irreversible, and cannot be used on tables that are referenced by a foreign key.

72. What is the difference between the DROP and TRUNCATE statements?
DROP deletes a table from the database completely, including the table structure and all the associated constraints, relationships with other tables, and access privileges. TRUNCATE deletes all rows from a table without affecting the table structure and constraints.  Both are DDL. DROP removes the table and its metadata; TRUNCATE deletes all rows but keeps the table definition. Performance and transactional behavior depend on the DB engine.

73. What is the difference between the HAVING and WHERE statements?
The first one works on aggregated data after they are grouped, while the second one checks each row individually. If both statements are present in a query, they appear in the following order: WHERE – GROUP BY – HAVING. The SQL engine interprets them also in the same order.

74. How do you add a record to a table?
Using the INSERT INTO statement in combination with VALUES. The syntax is:

INSERT INTO table_name
VALUES (value_1, value_2, ...);

Powered By 
75. How do you delete a record from a table?
Using the DELETE statement. The syntax is:

DELETE FROM table_name
WHERE condition;

Powered By 
In this way, we can also delete multiple records if they satisfy the provided condition.

76. How do you add a column to a table?
Using the ALTER TABLE statement in combination with ADD. The syntax is:

ALTER TABLE table_name
ADD column_name datatype;

Powered By 
77. How do you rename a column of a table?
Using the ALTER TABLE statement in combination with RENAME COLUMN ... TO ... The syntax is:

ALTER TABLE table_name
RENAME COLUMN old_column_name TO new_column_name;

Powered By 
78. How do you delete a column from a table?
Using the ALTER TABLE statement in combination with DROP COLUMN. The syntax is:

ALTER TABLE table_name
DROP COLUMN column_name;

Powered By 
79. How do you select all even or all odd records in a table?
By checking the remainder of the division by 2. In some SQL versions (e.g., PostgreSQL and My SQL), we use the MOD function, in the others (Microsoft SQL Server and SQLite) – the modulo operator (%). To select all even records using MOD:

SELECT * FROM table_name
WHERE MOD(ID_column, 2) = 0;

Powered By 
To select all even records using %:

SELECT * FROM table_name 
WHERE ID_column % 2 = 0;

Powered By 
To select all odd records, the syntax is identical in both cases, only that we would use the inequality operator <> instead of =.

80. How to prevent duplicate records when making a query?
Using the DISTINCT statement in combination with SELECT or creating a unique key for that table.

81. How do you insert many rows in a table?
Using the INSERT INTO statement in combination with VALUES. The syntax is:

INSERT INTO table_name
VALUES (value_1, value_2, ...),
      (value_3, value_4, ...),
      (value_5, value_6, ...),
      ...;

Powered By 
82. How do you find the nth highest value in a column of a table?
Using window functions to handle ties correctly:

SELECT column_name
FROM (
  SELECT column_name, DENSE_RANK() OVER (ORDER BY column_name DESC) AS rnk
  FROM table_name
) t
WHERE rnk = :n;

Powered By 
For the nth row by order (ignoring ties): ORDER BY column_name DESC OFFSET n-1 ROWS FETCH NEXT 1 ROW ONLY.

83. How do you find the values in a text column of a table that start with a certain letter?
Using the LIKE operator in combination with the % and _ wildcards. For example, we need to find all surnames in a table that start with "A". The query is:

SELECT * FROM table_name
WHERE surname LIKE 'A_';

Powered By 
Here, we assume that a surname must contain at least two letters. Without this assumption (meaning that a surname can be just A), the query is as follows:

SELECT * FROM table_name
WHERE surname LIKE 'A%';

Powered By 
84. How do you find the last id in a table?
The simplest method is to use the aggregate function MAX().

SELECT MAX(id) AS highest_id
FROM table_name;

Powered By 
Using ORDER BY with LIMIT or TOP

SELECT id
FROM table_name
ORDER BY id DESC
LIMIT 1;

Powered By 
85. How to select random rows from a table?
Using the RAND() function in combination with ORDER BY and LIMIT. In some SQL flavors, such as PostgreSQL, it's called RANDOM(). For example, the following code will return five random rows from a table in MySQL:

SELECT * FROM table_name
ORDER BY RAND()
LIMIT 5;

Powered By 
Upskilling Your Team with SQL
While SQL interview preparation is crucial for job seekers and hiring managers, it’s equally important for businesses to invest in continuous SQL training for their teams. Being able to work with data is more important than ever, so ensuring that your employees have strong SQL skills can be a game-changer for your company's success.

If you’re a team leader or business owner looking to ensure your whole team is proficient in SQL, DataCamp for Business offers tailored training programs that can help your employees master SQL skills, from the basics to advanced concepts. We can provide:

Targeted learning paths: Customizable to your team’s current skill level and business needs.
Hands-on practice: Real-world scenarios and exercises that reinforce learning and improve retention.
Progress tracking: Tools to monitor and assess your team’s progress, ensuring they achieve their learning goals.
Investing in SQL upskilling through platforms like DataCamp not only enhances your team’s capabilities but also provides your business with a strategic advantage, enabling you to stay competitive and deliver results. Talk to one of our team and request a demo today. 
SQL interview questions often diverge from typical daily tasks, focusing on deeper comprehension and mastery of the language’s intricacies. Recognizing and mastering specific SQL patterns is crucial, as interviews frequently test for edge cases and complex scenarios not always encountered in routine work. Familiarity with these patterns ensures candidates can handle not just common tasks but also intricate and unexpected challenges that might arise in real-world data engineering roles.

— — — — — — — — — — — — — — — — — — — — —

You can crush the data engineering interview by learning the tips and tricks in my book, Ace The Data Engineering Interview, on kindle and paperback.

— — — — — — — — — — — — — — — — — — — — —

Understanding CTEs and Rolling Windows

A Common Table Expression (CTE) is a temporary result set that you can refer to within a SELECT, INSERT, UPDATE, or DELETE statement. It provides better readability and can be thought of as defining a temporary view for just one query. When used in combination with BETWEEN, it’s a powerful tool to create rolling windows.

In the given SQL, we’re attempting to calculate the average daily active users (DAU) over a rolling window of 3 days.

WITH cte_dau
AS (
  SELECT event_date AS "day",
  COUNT(DISTINCT userid) as dau
  FROM user_activity_log
  GROUP BY event_date
  ORDER BY 1
)
SELECT 
  a.dy,
  AVG(b.dau) AS "3_day_dau_avg"
FROM cte_dau a
INNER JOIN cte_dau b
ON b.dy BETWEEN DATEADD(day, -3, a.dy) AND a.dy
GROUP BY a.dy
ORDER BY a.dy
Breaking Down the Solution

1. CTE Definition (`cte_dau`)

The CTE (cte_dau) selects the event_date as the “day” and calculates the distinct count of userid as dau (Daily Active Users) for each day.
This is a simple aggregation of how many unique users were active each day from the user_activity_log table.
2. Using the CTE

Now that we have our CTE, we’re doing a self-join on it. We alias these as `a` and `b`.
The INNER JOIN condition with BETWEEN is crucial. For each day in `a`, we’re looking for rows in `b` that fall within a 3-day window including and preceding the day in `a`.
The function DATEADD(day, -3, a.dy) is used to get the starting day of our 3-day window for each day in `a`. The `BETWEEN` condition ensures that for each day in `a`, we’re considering that day and the previous 2 days from `b`.
3. Calculating the Rolling Average

The AVG(b.dau) computes the average DAU over the rolling window. The GROUP BY ensures we’re calculating this average for each distinct day.
4. Output:

The final result shows the day and the corresponding 3-day average DAU for that day.
In essence, for each day, this SQL provides the average of daily active users for that day and the two preceding days, giving a rolling window average.
Understanding How to Use SUM with CASE:
Using SUM with a CASE statement allows you to create conditional aggregations. Essentially, you’re telling the SQL engine to sum up values only when certain conditions are met.

The aim is to calculate the percentage of sales that happened on the first and last day of the promotion.

SELECT Round(Sum(
  CASE WHEN Min(start_date) = transaction_date THEN 1
       WHEN Max(end_date) = transaction_date THEN 1
       ELSE 0
  END) 100/ Count(), 2)
FROM sales s
JOIN promotions p
ON s.promotion_id = p.promotion_id
Breaking Down the Query:

1: The CASE Statement:

The CASE statement checks each transaction’s date (transaction_date).
WHEN Min(start_date) = transaction_date THEN 1: This checks if the transaction date is the same as the start date of the promotion. If true, it returns 1.
ELSE 0: If neither of the above conditions are met, it returns 0.
The result of the CASE statement is that each transaction on the first or last day of the promotion is counted as 1, and all other transactions are counted as 0.
2: SUM and COUNT:

SUM(CASE … END) will sum up all the 1s and 0s from the CASE statement. This gives the total number of transactions on the first and last days.
COUNT() will count all transactions irrespective of their date, giving the total number of transactions.
3: Percentage Calculation:

The calculated sum (from the CASE statement) is then multiplied by 100 and divided by the count of all transactions. This gives the percentage of transactions that happened on the first and last days.
4: ROUND:

The Round( … , 2) function ensures the percentage is rounded to 2 decimal places for a cleaner output.
5: FROM and JOIN:

The data is sourced from the sales table and joined with the promotions table on promotion_id.
Using SUM with a CASE statement for the numerator:
The combination of SUM with a CASE statement and COUNT(*) is frequently used in SQL for deriving proportions or percentages based on specific criteria. We can use this pattern to solve a problem such as What is the the proportion of valid sales that occurred on Jan 1 2021. Let’s break down how this works:

SELECT

 -- Numerator: Count of valid sales on '2021-01-01'
SUM(CASE WHEN transaction_date = '2021-01-01' THEN 1 ELSE 0 END)
/

 -- Denominator: Total count of valid sales
COUNT(*) AS proportion
FROM sales
WHERE is_valid = true; 
1. The CASE Statement:

The CASE statement inspects each row’s transaction_date.
WHEN transaction_date = ‘2021-01-01’ THEN 1: This checks if the transaction_date is the specified date. If it’s true, it returns 1; otherwise, nothing is returned (implicitly null).
By summing up the result of this CASE statement, we obtain a total count of transactions that occurred on the specified date.
2. Using COUNT(*) for the denominator:

COUNT(*) counts all rows in the result set. Since the query is already filtered by the WHERE clause to only include valid sales, this count will give us the total number of valid sales, regardless of their date.
3. Calculating the Proportion:

The numerator (result of the SUM(CASE…END)) is divided by the denominator (result of the COUNT(*)). This gives the proportion of valid sales that happened on the specified date.
4. The WHERE Clause:

The WHERE filter limits both the numerator and denominator to rows where the sale is valid using WHERE is_valid = true.
To summarize, this SQL query calculates the proportion of valid sales that occurred on a particular date (2021–01–01 in our example). The numerator is the count of sales on that date, and the denominator is the total count of valid sales. The result is the proportion of sales on the specified date relative to all valid sales.

Complex Ordering of Aggregates
SQL allows users to perform complex sorting by using aggregate functions combined with arithmetic operations in the ORDER BY clause. Let’s break down an example query and understand how this works:

We want to identify the top 3 product_class_ids based on the total sales value.

SELECT product_class_id
FROM product
JOIN sales ON product.product_id = sales.product_id
ORDER BY SUM(units * store_cost) DESC
LIMIT 3;
How the Query Works

1. JOIN Operation:

The query performs an inner join between the product and sales tables based on a common column, presumably product_id.
2. Calculating Sales Value for Each Product:

Within the ORDER BY clause, the product’s units from the sales table is multiplied by store_cost from the same or another table (it’s not explicitly mentioned in the provided query). This gives the total sales value for each product in the dataset.
3. Ordering Using Aggregate Function:

The sum(units * store_cost) part aggregates (sums up) the total sales value for each product_class_id.
The ORDER BY … DESC sorts the product_class_ids based on this aggregated sales value in descending order, so the product_class_id with the highest sales value will appear first.
4. Limiting Results:

Finally, the LIMIT 3 ensures that only the top 3 rows (i.e., the top 3 product_class_ids with the highest sales values) are returned.
The beauty of using ORDER BY with aggregate and arithmetic operations like in this query is that it enables complex sorting criteria. Rather than just sorting by a single column, you can create custom sorting metrics based on the data’s properties and your business requirements.
SQL Interview Questions
1. What is Pattern Matching in SQL?
SQL pattern matching provides for pattern search in data if you have no clue as to what that word should be. This kind of SQL query uses wildcards to match a string pattern, rather than writing the exact word. The LIKE operator is used in conjunction with SQL Wildcards to fetch the required information.

Using the % wildcard to perform a simple search
The % wildcard matches zero or more characters of any type and can be used to define wildcards both before and after the pattern. Search a student in your database with first name beginning with the letter K:

SELECT *
FROM students
WHERE first_name LIKE 'K%'
Omitting the patterns using the NOT keyword
Use the NOT keyword to select records that don't match the pattern. This query returns all students whose first name does not begin with K.

SELECT *
FROM students
WHERE first_name NOT LIKE 'K%'
Matching a pattern anywhere using the % wildcard twice
Search for a student in the database where he/she has a K in his/her first name.

SELECT *
FROM students
WHERE first_name LIKE '%Q%'
Using the _ wildcard to match pattern at a specific position
The _ wildcard matches exactly one character of any type. It can be used in conjunction with % wildcard. This query fetches all students with letter K at the third position in their first name.

SELECT *
FROM students
WHERE first_name LIKE '__K%'
Matching patterns for a specific length
The _ wildcard plays an important role as a limitation when it matches exactly one character. It limits the length and position of the matched results. For example - 

SELECT *   /* Matches first names with three or more letters */
FROM students
WHERE first_name LIKE '___%'

SELECT *   /* Matches first names with exactly four characters */
FROM students
WHERE first_name LIKE '____'
2. How to create empty tables with the same structure as another table?
Creating empty tables with the same structure can be done smartly by fetching the records of one table into a new table using the INTO operator while fixing a WHERE clause to be false for all records. Hence, SQL prepares the new table with a duplicate structure to accept the fetched records but since no records get fetched due to the WHERE clause in action, nothing is inserted into the new table.

SELECT * INTO Students_copy
FROM Students WHERE 1 = 2;
3. What is a Recursive Stored Procedure?
A stored procedure that calls itself until a boundary condition is reached, is called a recursive stored procedure. This recursive function helps the programmers to deploy the same set of code several times as and when required. Some SQL programming languages limit the recursion depth to prevent an infinite loop of procedure calls from causing a stack overflow, which slows down the system and may lead to system crashes.

DELIMITER $$     /* Set a new delimiter => $$ */
CREATE PROCEDURE calctotal( /* Create the procedure */
   IN number INT,   /* Set Input and Ouput variables */
   OUT total INT
) BEGIN
DECLARE score INT DEFAULT NULL;   /* Set the default value => "score" */
SELECT awards FROM achievements   /* Update "score" via SELECT query */
WHERE id = number INTO score;
IF score IS NULL THEN SET total = 0;   /* Termination condition */
ELSE
CALL calctotal(number+1);   /* Recursive call */
SET total = total + score;   /* Action after recursion */
END IF;
END $$     /* End of procedure */
DELIMITER ;     /* Reset the delimiter */
4. What is a Stored Procedure?
A stored procedure is a subroutine available to applications that access a relational database management system (RDBMS). Such procedures are stored in the database data dictionary. The sole disadvantage of stored procedure is that it can be executed nowhere except in the database and occupies more memory in the database server. It also provides a sense of security and functionality as users who can't access the data directly can be granted access via stored procedures.

DELIMITER $$
CREATE PROCEDURE FetchAllStudents()
BEGIN
SELECT *  FROM myDB.students;
END $$
DELIMITER ;

5. What is Collation? What are the different types of Collation Sensitivity?
Collation refers to a set of rules that determine how data is sorted and compared. Rules defining the correct character sequence are used to sort the character data. It incorporates options for specifying case sensitivity, accent marks, kana character types, and character width. Below are the different types of collation sensitivity:

Case sensitivity: A and a are treated differently.
Accent sensitivity: a and á are treated differently.
Kana sensitivity: Japanese kana characters Hiragana and Katakana are treated differently.
Width sensitivity: Same character represented in single-byte (half-width) and double-byte (full-width) are treated differently.
6. What are the differences between OLTP and OLAP?
OLTP stands for Online Transaction Processing, is a class of software applications capable of supporting transaction-oriented programs. An important attribute of an OLTP system is its ability to maintain concurrency. OLTP systems often follow a decentralized architecture to avoid single points of failure. These systems are generally designed for a large audience of end-users who conduct short transactions. Queries involved in such databases are generally simple, need fast response times, and return relatively few records. A number of transactions per second acts as an effective measure for such systems.

OLAP stands for Online Analytical Processing, a class of software programs that are characterized by the relatively low frequency of online transactions. Queries are often too complex and involve a bunch of aggregations. For OLAP systems, the effectiveness measure relies highly on response time. Such systems are widely used for data mining or maintaining aggregated, historical data, usually in multi-dimensional schemas.


7. What is OLTP?
OLTP stands for Online Transaction Processing, is a class of software applications capable of supporting transaction-oriented programs. An essential attribute of an OLTP system is its ability to maintain concurrency. To avoid single points of failure, OLTP systems are often decentralized. These systems are usually designed for a large number of users who conduct short transactions. Database queries are usually simple, require sub-second response times, and return relatively few records. Here is an insight into the working of an OLTP system [ Note - The figure is not important for interviews ] -

8. What is User-defined function? What are its various types?
The user-defined functions in SQL are like functions in any other programming language that accept parameters, perform complex calculations, and return a value. They are written to use the logic repetitively whenever required. There are two types of SQL user-defined functions:

Scalar Function: As explained earlier, user-defined scalar functions return a single scalar value.
Table-Valued Functions: User-defined table-valued functions return a table as output.
Inline: returns a table data type based on a single SELECT statement.
Multi-statement: returns a tabular result-set but, unlike inline, multiple SELECT statements can be used inside the function body.
9. What is a UNIQUE constraint?
A UNIQUE constraint ensures that all values in a column are different. This provides uniqueness for the column(s) and helps identify each row uniquely. Unlike primary key, there can be multiple unique constraints defined per table. The code syntax for UNIQUE is quite similar to that of PRIMARY KEY and can be used interchangeably.

CREATE TABLE Students (   /* Create table with a single field as unique */
   ID INT NOT NULL UNIQUE
   Name VARCHAR(255)
);

CREATE TABLE Students (   /* Create table with multiple fields as unique */
   ID INT NOT NULL
   LastName VARCHAR(255)
   FirstName VARCHAR(255) NOT NULL
   CONSTRAINT PK_Student
   UNIQUE (ID, FirstName)
);

ALTER TABLE Students   /* Set a column as unique */
ADD UNIQUE (ID);
ALTER TABLE Students   /* Set multiple columns as unique */
ADD CONSTRAINT PK_Student   /* Naming a unique constraint */
UNIQUE (ID, FirstName);
10. What is a Query?
A query is a request for data or information from a database table or combination of tables. A database query can be either a select query or an action query.

SELECT fname, lname    /* select query */
FROM myDb.students
WHERE student_id = 1;
UPDATE myDB.students    /* action query */
SET fname = 'Captain', lname = 'America'
WHERE student_id = 1;
11. What is Data Integrity?
Data Integrity is the assurance of accuracy and consistency of data over its entire life-cycle and is a critical aspect of the design, implementation, and usage of any system which stores, processes, or retrieves data. It also defines integrity constraints to enforce business rules on the data when it is entered into an application or a database.

logo
Practice Problems
Solve these problems to ace this concept
Engineers Joined
Easy
25.11 Mins
Solve

Job Offer
Hard
24.23 Mins
Solve

12. What is the difference between Clustered and Non-clustered index?
As explained above, the differences can be broken down into three small factors -

Clustered index modifies the way records are stored in a database based on the indexed column. A non-clustered index creates a separate entity within the table which references the original table.
Clustered index is used for easy and speedy retrieval of data from the database, whereas, fetching records from the non-clustered index is relatively slower.
In SQL, a table can have a single clustered index whereas it can have multiple non-clustered indexes.
13. What is an Index? Explain its different types.
A database index is a data structure that provides a quick lookup of data in a column or columns of a table. It enhances the speed of operations accessing data from a database table at the cost of additional writes and memory to maintain the index data structure.

CREATE INDEX index_name   /* Create Index */
ON table_name (column_1, column_2);
DROP INDEX index_name;   /* Drop Index */
There are different types of indexes that can be created for different purposes:

Unique and Non-Unique Index:
Unique indexes are indexes that help maintain data integrity by ensuring that no two rows of data in a table have identical key values. Once a unique index has been defined for a table, uniqueness is enforced whenever keys are added or changed within the index.

CREATE UNIQUE INDEX myIndex
ON students (enroll_no);
Non-unique indexes, on the other hand, are not used to enforce constraints on the tables with which they are associated. Instead, non-unique indexes are used solely to improve query performance by maintaining a sorted order of data values that are used frequently.

Clustered and Non-Clustered Index:
Clustered indexes are indexes whose order of the rows in the database corresponds to the order of the rows in the index. This is why only one clustered index can exist in a given table, whereas, multiple non-clustered indexes can exist in the table.

The only difference between clustered and non-clustered indexes is that the database manager attempts to keep the data in the database in the same order as the corresponding keys appear in the clustered index.

Clustering indexes can improve the performance of most query operations because they provide a linear-access path to data stored in the database.

14. What is a Cross-Join?
Cross join can be defined as a cartesian product of the two tables included in the join. The table after join contains the same number of rows as in the cross-product of the number of rows in the two tables. If a WHERE clause is used in cross join then the query will work like an INNER JOIN.

SELECT stu.name, sub.subject 
FROM students AS stu
CROSS JOIN subjects AS sub;
15. What is a Self-Join?
A self JOIN is a case of regular join where a table is joined to itself based on some relation between its own column(s). Self-join uses the INNER JOIN or LEFT JOIN clause and a table alias is used to assign different names to the table within the query.

SELECT A.emp_id AS "Emp_ID",A.emp_name AS "Employee",
B.emp_id AS "Sup_ID",B.emp_name AS "Supervisor"
FROM employee A, employee B
WHERE A.emp_sup = B.emp_id;
16. What is a Join? List its different types.
The SQL Join clause is used to combine records (rows) from two or more tables in a SQL database based on a related column between the two.

There are four different types of JOINs in SQL:

(INNER) JOIN: Retrieves records that have matching values in both tables involved in the join. This is the widely used join for queries.
SELECT *
FROM Table_A
JOIN Table_B;
SELECT *
FROM Table_A
INNER JOIN Table_B;
LEFT (OUTER) JOIN: Retrieves all the records/rows from the left and the matched records/rows from the right table.
SELECT *
FROM Table_A A
LEFT JOIN Table_B B
ON A.col = B.col;
RIGHT (OUTER) JOIN: Retrieves all the records/rows from the right and the matched records/rows from the left table.
SELECT *
FROM Table_A A
RIGHT JOIN Table_B B
ON A.col = B.col;
FULL (OUTER) JOIN: Retrieves all the records where there is a match in either the left or right table.
SELECT *
FROM Table_A A
FULL JOIN Table_B B
ON A.col = B.col;
17. What is a Foreign Key?
A FOREIGN KEY comprises of single or collection of fields in a table that essentially refers to the PRIMARY KEY in another table. Foreign key constraint ensures referential integrity in the relation between two tables.
The table with the foreign key constraint is labeled as the child table, and the table containing the candidate key is labeled as the referenced or parent table.

CREATE TABLE Students (   /* Create table with foreign key - Way 1 */
   ID INT NOT NULL
   Name VARCHAR(255)
   LibraryID INT
   PRIMARY KEY (ID)
   FOREIGN KEY (Library_ID) REFERENCES Library(LibraryID)
);

CREATE TABLE Students (   /* Create table with foreign key - Way 2 */
   ID INT NOT NULL PRIMARY KEY
   Name VARCHAR(255)
   LibraryID INT FOREIGN KEY (Library_ID) REFERENCES Library(LibraryID)
);

ALTER TABLE Students   /* Add a new foreign key */
ADD FOREIGN KEY (LibraryID)
REFERENCES Library (LibraryID);
18. What is a Subquery? What are its types?
A subquery is a query within another query, also known as a nested query or inner query. It is used to restrict or enhance the data to be queried by the main query, thus restricting or enhancing the output of the main query respectively. For example, here we fetch the contact information for students who have enrolled for the maths subject:

SELECT name, email, mob, address
FROM myDb.contacts
WHERE roll_no IN (
 SELECT roll_no
 FROM myDb.students
 WHERE subject = 'Maths');
There are two types of subquery - Correlated and Non-Correlated.

A correlated subquery cannot be considered as an independent query, but it can refer to the column in a table listed in the FROM of the main query.
A non-correlated subquery can be considered as an independent query and the output of the subquery is substituted in the main query.
logo
Practice Problems
Solve these problems to ace this concept
Study Selection
Medium
9.23 Mins
Solve

Job Offers 2.0
Hard
25.46 Mins
Solve

19. What is a Primary Key?
The PRIMARY KEY constraint uniquely identifies each row in a table. It must contain UNIQUE values and has an implicit NOT NULL constraint.
A table in SQL is strictly restricted to have one and only one primary key, which is comprised of single or multiple fields (columns).

CREATE TABLE Students (   /* Create table with a single field as primary key */
   ID INT NOT NULL
   Name VARCHAR(255)
   PRIMARY KEY (ID)
);

CREATE TABLE Students (   /* Create table with multiple fields as primary key */
   ID INT NOT NULL
   LastName VARCHAR(255)
   FirstName VARCHAR(255) NOT NULL,
   CONSTRAINT PK_Student
   PRIMARY KEY (ID, FirstName)
);

ALTER TABLE Students   /* Set a column as primary key */
ADD PRIMARY KEY (ID);
ALTER TABLE Students   /* Set multiple columns as primary key */
ADD CONSTRAINT PK_Student   /*Naming a Primary Key*/
PRIMARY KEY (ID, FirstName);
logo
Practice Problems
Solve these problems to ace this concept
Student Query
Easy
8.10 Mins
Solve

Country Filtration
Easy
6.8 Mins
Solve

20. What are Constraints in SQL?
Constraints are used to specify the rules concerning data in the table. It can be applied for single or multiple fields in an SQL table during the creation of the table or after creating using the ALTER TABLE command. The constraints are:

NOT NULL - Restricts NULL value from being inserted into a column.
CHECK - Verifies that all values in a field satisfy a condition.
DEFAULT - Automatically assigns a default value if no value has been specified for the field.
UNIQUE - Ensures unique values to be inserted into the field.
INDEX - Indexes a field providing faster retrieval of records.
PRIMARY KEY - Uniquely identifies each record in a table.
FOREIGN KEY - Ensures referential integrity for a record in another table.
21. What are Tables and Fields?
A table is an organized collection of data stored in the form of rows and columns. Columns can be categorized as vertical and rows as horizontal. The columns in a table are called fields while the rows can be referred to as records.

22. What is the difference between SQL and MySQL?
SQL is a standard language for retrieving and manipulating structured databases. On the contrary, MySQL is a relational database management system, like SQL Server, Oracle or IBM DB2, that is used to manage SQL databases.

23. What is SQL?
SQL stands for Structured Query Language. It is the standard language for relational database management systems. It is especially useful in handling organized data comprised of entities (variables) and relations between different entities of the data.

24. What is RDBMS? How is it different from DBMS?
RDBMS stands for Relational Database Management System. The key difference here, compared to DBMS, is that RDBMS stores data in the form of a collection of tables, and relations can be defined between the common fields of these tables. Most modern database management systems like MySQL, Microsoft SQL Server, Oracle, IBM DB2, and Amazon Redshift are based on RDBMS.

25. What is DBMS?
DBMS stands for Database Management System. DBMS is a system software responsible for the creation, retrieval, updation, and management of the database. It ensures that our data is consistent, organized, and is easily accessible by serving as an interface between the database and its end-users or application software.

26. What is Database?
A database is an organized collection of data, stored and retrieved digitally from a remote or local computer system. Databases can be vast and complex, and such databases are developed using fixed design and modeling approaches.

27. What is the SELECT statement?
SELECT operator in SQL is used to select data from a database. The data returned is stored in a result table, called the result-set.

SELECT * FROM myDB.students;
28. What are some common clauses used with SELECT query in SQL?
Some common SQL clauses used in conjuction with a SELECT query are as follows:

WHERE clause in SQL is used to filter records that are necessary, based on specific conditions.
ORDER BY clause in SQL is used to sort the records based on some field(s) in ascending (ASC) or descending order (DESC).
SELECT *
FROM myDB.students
WHERE graduation_year = 2019
ORDER BY studentID DESC;
GROUP BY clause in SQL is used to group records with identical data and can be used in conjunction with some aggregation functions to produce summarized results from the database.
HAVING clause in SQL is used to filter records in combination with the GROUP BY clause. It is different from WHERE, since the WHERE clause cannot filter aggregated records.
SELECT COUNT(studentId), country
FROM myDB.students
WHERE country != "INDIA"
GROUP BY country
HAVING COUNT(studentID) > 5;
29. What are UNION, MINUS and INTERSECT commands?
The UNION operator combines and returns the result-set retrieved by two or more SELECT statements.
The MINUS operator in SQL is used to remove duplicates from the result-set obtained by the second SELECT query from the result-set obtained by the first SELECT query and then return the filtered results from the first.
The INTERSECT clause in SQL combines the result-set fetched by the two SELECT statements where records from one match the other and then returns this intersection of result-sets.

Certain conditions need to be met before executing either of the above statements in SQL -

Each SELECT statement within the clause must have the same number of columns
The columns must also have similar data types
The columns in each SELECT statement should necessarily have the same order
SELECT name FROM Students   /* Fetch the union of queries */
UNION
SELECT name FROM Contacts;
SELECT name FROM Students   /* Fetch the union of queries with duplicates*/
UNION ALL
SELECT name FROM Contacts;
SELECT name FROM Students   /* Fetch names from students */
MINUS     /* that aren't present in contacts */
SELECT name FROM Contacts;
SELECT name FROM Students   /* Fetch names from students */
INTERSECT    /* that are present in contacts as well */
SELECT name FROM Contacts;
30. What is Cursor? How to use a Cursor?
A database cursor is a control structure that allows for the traversal of records in a database. Cursors, in addition, facilitates processing after traversal, such as retrieval, addition, and deletion of database records. They can be viewed as a pointer to one row in a set of rows.

Working with SQL Cursor:

DECLARE a cursor after any variable declaration. The cursor declaration must always be associated with a SELECT Statement.
Open cursor to initialize the result set. The OPEN statement must be called before fetching rows from the result set.
FETCH statement to retrieve and move to the next row in the result set.
Call the CLOSE statement to deactivate the cursor.
Finally use the DEALLOCATE statement to delete the cursor definition and release the associated resources.
DECLARE @name VARCHAR(50)   /* Declare All Required Variables */
DECLARE db_cursor CURSOR FOR   /* Declare Cursor Name*/
SELECT name
FROM myDB.students
WHERE parent_name IN ('Sara', 'Ansh')
OPEN db_cursor   /* Open cursor and Fetch data into @name */ 
FETCH next
FROM db_cursor
INTO @name
CLOSE db_cursor   /* Close the cursor and deallocate the resources */
DEALLOCATE db_cursor
31. What are Entities and Relationships?
Entity: An entity can be a real-world object, either tangible or intangible, that can be easily identifiable. For example, in a college database, students, professors, workers, departments, and projects can be referred to as entities. Each entity has some associated properties that provide it an identity.

Relationships: Relations or links between entities that have something to do with each other. For example - The employee's table in a company's database can be associated with the salary table in the same database.

32. List the different types of relationships in SQL.
One-to-One - This can be defined as the relationship between two tables where each record in one table is associated with the maximum of one record in the other table.
One-to-Many & Many-to-One - This is the most commonly used relationship where a record in a table is associated with multiple records in the other table.
Many-to-Many - This is used in cases when multiple instances on both sides are needed for defining a relationship.
Self-Referencing Relationships - This is used when a table needs to define a relationship with itself.
33. What is an Alias in SQL?
An alias is a feature of SQL that is supported by most, if not all, RDBMSs. It is a temporary name assigned to the table or table column for the purpose of a particular SQL query. In addition, aliasing can be employed as an obfuscation technique to secure the real names of database fields. A table alias is also called a correlation name.

An alias is represented explicitly by the AS keyword but in some cases, the same can be performed without it as well. Nevertheless, using the AS keyword is always a good practice.

SELECT A.emp_name AS "Employee"  /* Alias using AS keyword */
B.emp_name AS "Supervisor"
FROM employee A, employee B   /* Alias without AS keyword */
WHERE A.emp_sup = B.emp_id;
34. What is a View?
A view in SQL is a virtual table based on the result-set of an SQL statement. A view contains rows and columns, just like a real table. The fields in a view are fields from one or more real tables in the database.

35. What is Normalization?
Normalization represents the way of organizing structured data in the database efficiently. It includes the creation of tables, establishing relationships between them, and defining rules for those relationships. Inconsistency and redundancy can be kept in check based on these rules, hence, adding flexibility to the database.

36. What is Denormalization?
Denormalization is the inverse process of normalization, where the normalized schema is converted into a schema that has redundant information. The performance is improved by using redundancy and keeping the redundant data consistent. The reason for performing denormalization is the overheads produced in the query processor by an over-normalized structure.

37. What are the various forms of Normalization?
Normal Forms are used to eliminate or reduce redundancy in database tables. The different forms are as follows:

First Normal Form:
A relation is in first normal form if every attribute in that relation is a single-valued attribute. If a relation contains a composite or multi-valued attribute, it violates the first normal form. Let's consider the following students table. Each student in the table, has a name, his/her address, and the books they issued from the public library -
Students Table

Student 	Address 	Books Issued 	Salutation
Sara 	Amanora Park Town 94 	Until the Day I Die (Emily Carpenter), Inception (Christopher Nolan)	Ms.
Ansh	62nd Sector A-10 	The Alchemist (Paulo Coelho), Inferno (Dan Brown) 	Mr.
Sara 	24th Street Park Avenue 	Beautiful Bad (Annie Ward), Woman 99 (Greer Macallister)	Mrs.
Ansh 	Windsor Street 777 	Dracula (Bram Stoker)	Mr.
As we can observe, the Books Issued field has more than one value per record, and to convert it into 1NF, this has to be resolved into separate individual records for each book issued. Check the following table in 1NF form -

Students Table (1st Normal Form)

Student 	Address 	Books Issued 	Salutation
Sara	Amanora Park Town 94	Until the Day I Die (Emily Carpenter) 	Ms.
Sara	Amanora Park Town 94	Inception (Christopher Nolan) 	Ms.
Ansh	62nd Sector A-10	The Alchemist (Paulo Coelho) 	Mr.
Ansh	62nd Sector A-10	Inferno (Dan Brown) 	Mr.
Sara	24th Street Park Avenue	Beautiful Bad (Annie Ward) 	Mrs.
Sara	24th Street Park Avenue	Woman 99 (Greer Macallister) 	Mrs.
Ansh	Windsor Street 777	Dracula (Bram Stoker) 	Mr.
Second Normal Form:
A relation is in second normal form if it satisfies the conditions for the first normal form and does not contain any partial dependency. A relation in 2NF has no partial dependency, i.e., it has no non-prime attribute that depends on any proper subset of any candidate key of the table. Often, specifying a single column Primary Key is the solution to the problem. Examples -

Example 1 - Consider the above example. As we can observe, the Students Table in the 1NF form has a candidate key in the form of [Student, Address] that can uniquely identify all records in the table. The field Books Issued (non-prime attribute) depends partially on the Student field. Hence, the table is not in 2NF. To convert it into the 2nd Normal Form, we will partition the tables into two while specifying a new Primary Key attribute to identify the individual records in the Students table. The Foreign Key constraint will be set on the other table to ensure referential integrity.

Students Table (2nd Normal Form)

Student_ID 	Student	Address 	Salutation
1	Sara	Amanora Park Town 94 	Ms.
2	Ansh	62nd Sector A-10 	Mr.
3	Sara	24th Street Park Avenue 	Mrs.
4	Ansh	Windsor Street 777 	Mr.
Books Table (2nd Normal Form)

Student_ID 	Book Issued
1	Until the Day I Die (Emily Carpenter)
1	Inception (Christopher Nolan)
2	The Alchemist (Paulo Coelho)
2	Inferno (Dan Brown)
3	Beautiful Bad (Annie Ward)
3	Woman 99 (Greer Macallister)
4	Dracula (Bram Stoker)
Example 2 - Consider the following dependencies in relation to R(W,X,Y,Z)

 WX -> Y    [W and X together determine Y] 
 XY -> Z    [X and Y together determine Z] 
Here, WX is the only candidate key and there is no partial dependency, i.e., any proper subset of WX doesn’t determine any non-prime attribute in the relation.

Third Normal Form
A relation is said to be in the third normal form, if it satisfies the conditions for the second normal form and there is no transitive dependency between the non-prime attributes, i.e., all non-prime attributes are determined only by the candidate keys of the relation and not by any other non-prime attribute.

Example 1 - Consider the Students Table in the above example. As we can observe, the Students Table in the 2NF form has a single candidate key Student_ID (primary key) that can uniquely identify all records in the table. The field Salutation (non-prime attribute), however, depends on the Student Field rather than the candidate key. Hence, the table is not in 3NF. To convert it into the 3rd Normal Form, we will once again partition the tables into two while specifying a new Foreign Key constraint to identify the salutations for individual records in the Students table. The Primary Key constraint for the same will be set on the Salutations table to identify each record uniquely.

Students Table (3rd Normal Form)

Student_ID 	Student 	Address 	Salutation_ID
1	Sara	Amanora Park Town 94 	1
2	Ansh	62nd Sector A-10 	2
3	Sara	24th Street Park Avenue 	3
4	Ansh	Windsor Street 777 	1
Books Table (3rd Normal Form)

Student_ID	Book Issued
1	Until the Day I Die (Emily Carpenter)
1	Inception (Christopher Nolan)
2	The Alchemist (Paulo Coelho)
2	Inferno (Dan Brown)
3	Beautiful Bad (Annie Ward)
3	Woman 99 (Greer Macallister)
4	Dracula (Bram Stoker)
Salutations Table (3rd Normal Form)

Salutation_ID	Salutation
1	Ms.
2	Mr.
3	Mrs.
Example 2 - Consider the following dependencies in relation to R(P,Q,R,S,T)

 P -> QR     [P together determine C] 
 RS -> T     [B and C together determine D] 
 Q -> S 
 T -> P 
For the above relation to exist in 3NF, all possible candidate keys in the above relation should be {P, RS, QR, T}.

Boyce-Codd Normal Form
A relation is in Boyce-Codd Normal Form if satisfies the conditions for third normal form and for every functional dependency, Left-Hand-Side is super key. In other words, a relation in BCNF has non-trivial functional dependencies in form X –> Y, such that X is always a super key. For example - In the above example, Student_ID serves as the sole unique identifier for the Students Table and Salutation_ID for the Salutations Table, thus these tables exist in BCNF. The same cannot be said for the Books Table and there can be several books with common Book Names and the same Student_ID.

38. What are the TRUNCATE, DELETE and DROP statements?
DELETE statement is used to delete rows from a table.

DELETE FROM Candidates
WHERE CandidateId > 1000;
TRUNCATE command is used to delete all the rows from the table and free the space containing the table.

TRUNCATE TABLE Candidates;
DROP command is used to remove an object from the database. If you drop a table, all the rows in the table are deleted and the table structure is removed from the database.

DROP TABLE Candidates;
39. What is the difference between DROP and TRUNCATE statements?
If a table is dropped, all things associated with the tables are dropped as well. This includes - the relationships defined on the table with other tables, the integrity checks and constraints, access privileges and other grants that the table has. To create and use the table again in its original form, all these relations, checks, constraints, privileges and relationships need to be redefined. However, if a table is truncated, none of the above problems exist and the table retains its original structure.

40. What is the difference between DELETE and TRUNCATE statements?
The TRUNCATE command is used to delete all the rows from the table and free the space containing the table.
The DELETE command deletes only the rows from the table based on the condition given in the where clause or deletes all the rows from the table if no condition is specified. But it does not free the space containing the table.

41. What are Aggregate and Scalar functions?
An aggregate function performs operations on a collection of values to return a single scalar value. Aggregate functions are often used with the GROUP BY and HAVING clauses of the SELECT statement. Following are the widely used SQL aggregate functions:

AVG() - Calculates the mean of a collection of values.
COUNT() - Counts the total number of records in a specific table or view.
MIN() - Calculates the minimum of a collection of values.
MAX() - Calculates the maximum of a collection of values.
SUM() - Calculates the sum of a collection of values.
FIRST() - Fetches the first element in a collection of values.
LAST() - Fetches the last element in a collection of values.
Note: All aggregate functions described above ignore NULL values except for the COUNT function.

A scalar function returns a single value based on the input value. Following are the widely used SQL scalar functions:

LEN() - Calculates the total length of the given field (column).
UCASE() - Converts a collection of string values to uppercase characters.
LCASE() - Converts a collection of string values to lowercase characters.
MID() - Extracts substrings from a collection of string values in a table.
CONCAT() - Concatenates two or more strings.
RAND() - Generates a random collection of numbers of a given length.
ROUND() - Calculates the round-off integer value for a numeric field (or decimal point values).
NOW() - Returns the current date & time.
FORMAT() - Sets the format to display a collection of values.
PostgreSQL Interview Questions
42. What is PostgreSQL?
PostgreSQL was first called Postgres and was developed by a team led by Computer Science Professor Michael Stonebraker in 1986. It was developed to help developers build enterprise-level applications by upholding data integrity by making systems fault-tolerant. PostgreSQL is therefore an enterprise-level, flexible, robust, open-source, and object-relational DBMS that supports flexible workloads along with handling concurrent users. It has been consistently supported by the global developer community. Due to its fault-tolerant nature, PostgreSQL has gained widespread popularity among developers.

43. What is the capacity of a table in PostgreSQL?
The maximum size of PostgreSQL is 32TB.

44. What is the importance of the TRUNCATE statement?
TRUNCATE TABLE name_of_table statement removes the data efficiently and quickly from the table.
The truncate statement can also be used to reset values of the identity columns along with data cleanup as shown below:

TRUNCATE TABLE name_of_table 
RESTART IDENTITY;
We can also use the statement for removing data from multiple tables all at once by mentioning the table names separated by comma as shown below:

TRUNCATE TABLE 
   table_1, 
   table_2,
   table_3;
45. Define tokens in PostgreSQL?
A token in PostgreSQL is either a keyword, identifier, literal, constant, quotes identifier, or any symbol that has a distinctive personality. They may or may not be separated using a space, newline or a tab. If the tokens are keywords, they are usually commands with useful meanings. Tokens are known as building blocks of any PostgreSQL code.

46. What are partitioned tables called in PostgreSQL?
Partitioned tables are logical structures that are used for dividing large tables into smaller structures that are called partitions. This approach is used for effectively increasing the query performance while dealing with large database tables. To create a partition, a key called partition key which is usually a table column or an expression, and a partitioning method needs to be defined. There are three types of inbuilt partitioning methods provided by Postgres:

Range Partitioning: This method is done by partitioning based on a range of values. This method is most commonly used upon date fields to get monthly, weekly or yearly data. In the case of corner cases like value belonging to the end of the range, for example: if the range of partition 1 is 10-20 and the range of partition 2 is 20-30, and the given value is 10, then 10 belongs to the second partition and not the first.
List Partitioning: This method is used to partition based on a list of known values. Most commonly used when we have a key with a categorical value. For example, getting sales data based on regions divided as countries, cities, or states.
Hash Partitioning: This method utilizes a hash function upon the partition key. This is done when there are no specific requirements for data division and is used to access data individually. For example, you want to access data based on a specific product, then using hash partition would result in the dataset that we require.
The type of partition key and the type of method used for partitioning determines how positive the performance and the level of manageability of the partitioned table are.

47. How can we start, restart and stop the PostgreSQL server?
To start the PostgreSQL server, we run:
service postgresql start
Once the server is successfully started, we get the below message:
Starting PostgreSQL: ok
To restart the PostgreSQL server, we run:
service postgresql restart
Once the server is successfully restarted, we get the message:

Restarting PostgreSQL: server stopped
ok
To stop the server, we run the command:
service postgresql stop
Once stopped successfully, we get the message:

Stopping PostgreSQL: server stopped
ok
48. What is the command used for creating a database in PostgreSQL?
The first step of using PostgreSQL is to create a database. This is done by using the createdb command as shown below: createdb db_name
After running the above command, if the database creation was successful, then the below message is shown:

CREATE DATABASE
49. How will you change the datatype of a column?
This can be done by using the ALTER TABLE statement as shown below:

Syntax:

ALTER TABLE tname
ALTER COLUMN col_name [SET DATA] TYPE new_data_type;
50. How do you define Indexes in PostgreSQL?
Indexes are the inbuilt functions in PostgreSQL which are used by the queries to perform search more efficiently on a table in the database. Consider that you have a table with thousands of records and you have the below query that only a few records can satisfy the condition, then it will take a lot of time to search and return those rows that abide by this condition as the engine has to perform the search operation on every single to check this condition. This is undoubtedly inefficient for a system dealing with huge data. Now if this system had an index on the column where we are applying search, it can use an efficient method for identifying matching rows by walking through only a few levels. This is called indexing.

Select * from some_table where table_col=120
51. Define sequence.
A sequence is a schema-bound, user-defined object which aids to generate a sequence of integers. This is most commonly used to generate values to identity columns in a table. We can create a sequence by using the CREATE SEQUENCE statement as shown below:

CREATE SEQUENCE serial_num START 100;
To get the next number 101 from the sequence, we use the nextval() method as shown below:

SELECT nextval('serial_num');
We can also use this sequence while inserting new records using the INSERT command:

INSERT INTO ib_table_name VALUES (nextval('serial_num'), 'interviewbit');
52. What are string constants in PostgreSQL?
They are character sequences bound within single quotes. These are using during data insertion or updation to characters in the database.
There are special string constants that are quoted in dollars. Syntax: $tag$<string_constant>$tag$ The tag in the constant is optional and when we are not specifying the tag, the constant is called a double-dollar string literal.

53. How can you get a list of all databases in PostgreSQL?
This can be done by using the command \l -> backslash followed by the lower-case letter L.

54. How can you delete a database in PostgreSQL?
This can be done by using the DROP DATABASE command as shown in the syntax below:

DROP DATABASE database_name;
If the database has been deleted successfully, then the following message would be shown:

DROP DATABASE
55. What are ACID properties? Is PostgreSQL compliant with ACID?
ACID stands for Atomicity, Consistency, Isolation, Durability. They are database transaction properties which are used for guaranteeing data validity in case of errors and failures.

Atomicity: This property ensures that the transaction is completed in all-or-nothing way.
Consistency: This ensures that updates made to the database is valid and follows rules and restrictions.
Isolation: This property ensures integrity of transaction that are visible to all other transactions.
Durability: This property ensures that the committed transactions are stored permanently in the database.
PostgreSQL is compliant with ACID properties.

56. Can you explain the architecture of PostgreSQL?
The architecture of PostgreSQL follows the client-server model.
The server side comprises of background process manager, query processer, utilities and shared memory space which work together to build PostgreSQL’s instance that has access to the data. The client application does the task of connecting to this instance and requests data processing to the services. The client can either be GUI (Graphical User Interface) or a web application. The most commonly used client for PostgreSQL is pgAdmin.
57. What do you understand by multi-version concurrency control?
MVCC or Multi-version concurrency control is used for avoiding unnecessary database locks when 2 or more requests tries to access or modify the data at the same time. This ensures that the time lag for a user to log in to the database is avoided. The transactions are recorded when anyone tries to access the content.

For more information regarding this, you can refer here.

58. What do you understand by command enable-debug?
The command enable-debug is used for enabling the compilation of all libraries and applications. When this is enabled, the system processes get hindered and generally also increases the size of the binary file. Hence, it is not recommended to switch this on in the production environment. This is most commonly used by developers to debug the bugs in their scripts and help them spot the issues. For more information regarding how to debug, you can refer here.

59. How do you check the rows affected as part of previous transactions?
SQL standards state that the following three phenomena should be prevented whilst concurrent transactions. SQL standards define 4 levels of transaction isolations to deal with these phenomena.

Dirty reads: If a transaction reads data that is written due to concurrent uncommitted transaction, these reads are called dirty reads.
Phantom reads: This occurs when two same queries when executed separately return different rows. For example, if transaction A retrieves some set of rows matching search criteria. Assume another transaction B retrieves new rows in addition to the rows obtained earlier for the same search criteria. The results are different.
Non-repeatable reads: This occurs when a transaction tries to read the same row multiple times and gets different values each time due to concurrency. This happens when another transaction updates that data and our current transaction fetches that updated data, resulting in different values.
To tackle these, there are 4 standard isolation levels defined by SQL standards. They are as follows:

Read Uncommitted – The lowest level of the isolations. Here, the transactions are not isolated and can read data that are not committed by other transactions resulting in dirty reads.
Read Committed – This level ensures that the data read is committed at any instant of read time. Hence, dirty reads are avoided here. This level makes use of read/write lock on the current rows which prevents read/write/update/delete of that row when the current transaction is being operated on.
Repeatable Read – The most restrictive level of isolation. This holds read and write locks for all rows it operates on. Due to this, non-repeatable reads are avoided as other transactions cannot read, write, update or delete the rows.
Serializable – The highest of all isolation levels. This guarantees that the execution is serializable where execution of any concurrent operations are guaranteed to be appeared as executing serially.
The following table clearly explains which type of unwanted reads the levels avoid:

Isolation levels 	Dirty Reads 	Phantom Reads 	Non-repeatable reads
Read Uncommitted 	Might occur	Might occur	Might occur
Read Committed 	Won’t occur	Might occur	Might occur
Repeatable Read	Won’t occur	Might occur	Won’t occur
Serializable	Won’t occur	Won’t occur	Won’t occur
60. What can you tell about WAL (Write Ahead Logging)?
Write Ahead Logging is a feature that increases the database reliability by logging changes before any changes are done to the database. This ensures that we have enough information when a database crash occurs by helping to pinpoint to what point the work has been complete and gives a starting point from the point where it was discontinued.

For more information, you can refer here.

61. What is the main disadvantage of deleting data from an existing table using the DROP TABLE command?
DROP TABLE command deletes complete data from the table along with removing the complete table structure too. In case our requirement entails just remove the data, then we would need to recreate the table to store data in it. In such cases, it is advised to use the TRUNCATE command.

62. How do you perform case-insensitive searches using regular expressions in PostgreSQL?
To perform case insensitive matches using a regular expression, we can use POSIX (~*) expression from pattern matching operators. For example:

'interviewbit' ~* '.*INTervIewBit.*'
63. How will you take backup of the database in PostgreSQL?
We can achieve this by using the pg_dump tool for dumping all object contents in the database into a single file. The steps are as follows:

Step 1: Navigate to the bin folder of the PostgreSQL installation path.

C:\>cd C:\Program Files\PostgreSQL\10.0\bin
Step 2: Execute pg_dump program to take the dump of data to a .tar folder as shown below:

pg_dump -U postgres -W -F t sample_data > C:\Users\admin\pgbackup\sample_data.tar
The database dump will be stored in the sample_data.tar file on the location specified.

64. Does PostgreSQL support full text search?
Full-Text Search is the method of searching single or collection of documents stored on a computer in a full-text based database. This is mostly supported in advanced database systems like SOLR or ElasticSearch. However, the feature is present but is pretty basic in PostgreSQL.

65. What are parallel queries in PostgreSQL?
Parallel Queries support is a feature provided in PostgreSQL for devising query plans capable of exploiting multiple CPU processors to execute the queries faster.

66. Differentiate between commit and checkpoint.
The commit action ensures that the data consistency of the transaction is maintained and it ends the current transaction in the section. Commit adds a new record in the log that describes the COMMIT to the memory. Whereas, a checkpoint is used for writing all changes that were committed to disk up to SCN which would be kept in datafile headers and control files.

Conclusion:
SQL is a language for the database. It has a vast scope and robust capability of creating and manipulating a variety of database objects using commands like CREATE, ALTER, DROP, etc, and also in loading the database objects using commands like INSERT. It also provides options for Data Manipulation using commands like DELETE, TRUNCATE and also does effective retrieval of data using cursor commands like FETCH, SELECT, etc. There are many such commands which provide a large amount of control to the programmer to interact with the database in an efficient way without wasting many resources. The popularity of SQL has grown so much that almost every programmer relies on this to implement their application's storage functionalities thereby making it an exciting language to learn. Learning this provides the developer a benefit of understanding the data structures used for storing the organization's data and giving an additional level of control and in-depth understanding of the application.

PostgreSQL being an open-source database system having extremely robust and sophisticated ACID, Indexing, and Transaction supports has found widespread popularity among the developer community. 

References and Resources:

PostgreSQL Download
PostgreSQL Tutorial
SQL Guide
SQL Server Interview Questions
SQL Query Interview Questions and Answers
SQL Interview Questions for Data Science
MySQL Interview Questions
DBMS Interview Questions
PL SQL Interview Questions
MongoDB Interview Questions
Database Testing Interview Questions
SQL Vs MySQL
PostgreSQL vs MySQL
Difference Between SQL and PLSQL
Difference between RDBMS and DBMS
SQL Vs NoSQL
SQL IDE
SQL Projects
MySQL Commands
SQL Books
OLTP vs OLAP
SQL MCQ
1.
An SQL query to delete a table from the database and memory while keeping the structure of the table intact?


DROP TABLE table_name;

DROP FROM TABLE table_name;

DELETE FROM TABLE table_name;

TRUNCATE TABLE table_name;
2.
What is a pre-requisite for creating a database in PostgreSQL?To create a database in PostgreSQL, you must have the special CREATEDB privilege or


Super user privilege or CREATEDB privilege

Admin privilege

CREATEDBL privilege and Super user privilege

Just run the script
3.
Which of the following is known as a virtual table in SQL?


SELF JOIN

INNER JOIN

VIEW

NONE
4.
What is the main advantage of a clustered index over a non-clustered index?


It is easier to create and manipulate.

It requires extra memory but allows for speedy retrieval of records.

It does not require additional memory and allows for speedy retrieval of records.

None of the above.
5.
SQL query used to fetch unique values from a field?


SELECT UNIQUE column_name FROM table_name;

SELECT DISTINCT column_name FROM table_name;

SELECT column_name FROM table_name WHERE COUNT(column_name) = 1;

SELECT UNIQUE column_name FROM table_name WHERE COUNT(column_name) = 1;
6.
Which statement is used to update data in the database?


MODIFY

UPDATE

ALTER TABLE

SAVE AS
7.
Which statement is false for the ORDER BY statement?


Requires a ASC or DESC keyword explicitly to sort the result set.

Sorts the result set in descending order using the DESC keyword.

Can sort based on multiple columns

None of the above.
8.
What statement is used for adding data to PostgreSQL?


UPDATE

ADD

APPEND

INSERT
9.
Normalization which has neither composite values nor partial dependencies?


Second Normal Formal

Third Normal Form

Boyce-Codd Normal Form

All of the above
10.
What does SQL stand for?


Structured Question Language

Strong Query Language

Structured Query Language

Strong Question Language
11.
Which statement is true for a PRIMARY KEY constraint?


Primary key defines a realtionship between two tables.

A table in SQL must have a primary key associated with it to uniquely identify its records.

A table in SQL is indexed by default based on its primary key.

Primary key may or may not be unique but can be comprised of multiple fields.
12.
What is the order of results shown by default if the ASC or DESC parameter is not specified with the ORDER BY command?


Results are shown in descending order

Results are shown in ascending order

Results display is random

Results are shown in ascending and descending order alternately.
13.
What allows us to define how various tables are related to each other formally in a database?


Views

Foreign Key Constraints

Primary Key Constraints

Database manager
14.
What is the name of the component that requests data to the PostgreSQL server?


Client

Thin Client

Workstation

Interface
15.
What languages are supported by PostgreSQL?


PL/pgSQL, PL/Tcl, PL/Perl and PL/Python

PL/pgSQL, PL/Pcl, PL/Ruby and PL/Java

PL/Perl, PL/Dcl, PL/Dtl and PL/Dml

Only SQL
16.
What command is used for restoring the backup of PostgreSQL which was created using pg_dump?


psql -R db_dump.psql db_name

psql -r db_dump.psql db_name

psql -f db_dump.psql db_name

psql -F db_dump.psql db_name
17.
Query to select all records with "bar" in their name?


SELECT * FROM people WHERE name = "%bar%";

SELECT * FROM people WHERE name LIKE "%bar%";

SELECT * FROM people WHERE name IN ("bar");

SELECT * FROM people WHERE name = "_bar_"
18.
Which command is used to tell PostgreSQL to make all changes made to the database permanent?


Submit

Execute

Apply

Commit
19.
Which statement is false for a FOREIGN KEY constraint?


Foreign key defines a relationship between two tables.

Foreign Key is automatically created when two tables are joined.

Foreign Key uniquely identifies all the records in the referenced table.

Foreign key may or may not be unique but can be comprised of multiple fields.
20.
What is a Query?


A SELECT or UPDATE statement in SQL.

A request for data from a table in the database.

A request to input data from the user.

A request for data from single or multiple tables in the database.
SQL Interview Questions
What is the difference between SQL and MySQL?
What are the different subsets of SQL?
What do you mean by DBMS? What are its different types?
What do you mean by table and field in SQL?
What are joins in SQL?
What is the difference between CHAR and VARCHAR2 datatype in SQL?
What is the Primary key?
What are Constraints?
What is the difference between DELETE and TRUNCATE statements?
What is a Unique key?
Basic SQL Interview Questions
 

Q1. What is the difference between SQL and MySQL?
SQL vs MySQL

SQL	MySQL
SQL is a standard language which stands for Structured Query Language based on the English language	MySQL is a database management system.
SQL is the core of the relational database which is used for accessing and managing database	
MySQL is an RDMS (Relational Database Management System) such as SQL Server, Informix etc.

Q2. What are the different subsets of SQL?
Data Definition Language (DDL) – It allows you to perform various operations on the database such as CREATE, ALTER, and DELETE objects.
Data Manipulation Language(DML) – It allows you to access and manipulate data. It helps you to insert, update, delete and retrieve data from the database.
Data Control Language(DCL) – It allows you to control access to the database. Example – Grant and Revoke access permissions.
Q3. What do you mean by DBMS? What are its different types?

Database - SQL Interview Questions - EdurekaA Database Management System (DBMS) is a software application that interacts with the user, applications, and the database itself to capture and analyze data. A database is a structured collection of data. 

A DBMS allows a user to interact with the database. The data stored in the database can be modified, retrieved and deleted and can be of any type like strings, numbers, images, etc.

There are two types of DBMS:

Relational Database Management System: The data is stored in relations (tables). Example – MySQL.
Non-Relational Database Management System: There is no concept of relations, tuples and attributes.  Example – MongoDB
Let’s move to the next question in this SQL Interview Questions.


Q4. What is RDBMS? How is it different from DBMS?
A relational database management system (RDBMS) is a set of applications and features that allow IT professionals and others to develop, edit, administer, and interact with relational databases. Most commercial relational database management systems use Structured Query Language (SQL) to access the database, which is stored in the form of tables.
RDBMS is the most widely used database system in businesses all over the world. It offers a stable means of storing and retrieving massive amounts of data.

Databases, in general, hold collections of data that may be accessed and used in other applications. The development, administration, and use of database platforms are all supported by a database management system.

A relational database management system (RDBMS) is a type of database management system (DBMS) that stores data in a row-based table structure that links related data components. An RDBMS contains functions that ensure the data’s security, accuracy, integrity, and consistency. This is not the same as the file storage utilized by a database management system.

The following are some further distinctions between database management systems and relational database management systems:

Number of users who are permitted to utilise the system
DBMS can only handle one user at a time, whereas an RDBMS can handle numerous users.
Hardware and software specifications
In comparison to an RDBMS, a DBMS requires less software and hardware.
Amount of information
RDBMSes can handle any quantity of data, from tiny to enormous, whereas DBMSes are limited to small amounts.
The structure of the database
Data is stored in a hierarchical format in a DBMS, whereas an RDBMS uses a table with headers that serve as column names and rows that hold the associated values.
Implementation of the ACID principle
The atomicity, consistency, isolation, and durability (ACID) concept is not used by DBMSs for data storage. RDBMSes, on the other hand, use the ACID model to organize their data and ensure consistency.
Databases that are distributed
DBMS will not provide complete support for distributed databases, whereas RDBMS will.
Programs that are managed
DBMS focuses on keeping databases that are present within the computer network and system hard discs, whereas an RDBMS helps manage relationships between its incorporated tables of data.
Normalization of databases is supported
RDBMS can be normalized , but a DBMS cannot be normalized.

Q5. What is a Self-Join?
Self-join is a type of join that can be used to connect two tables. As a result, it is a unary relationship. Each row of the table is attached to itself and all other rows of the same table in a self-join. As a result, self-joining is mostly used to combine and compare rows from the same database table.

Q6. What is the SELECT statement?
The SELECT command gets zero or more rows from one or more database tables or views. SELECT the most frequent data manipulation language (DML) command in most applications. SELECT queries define a result set, but not how to calculate it, because SQL is a declarative programming language.

Q7. What are some common clauses used with SELECT query in SQL?
The following are some frequent SQL clauses used in conjunction with a SELECT query:

WHERE clause: In SQL, the WHERE clause is used to filter records that are required depending on certain criteria.
ORDER BY clause: The ORDER BY clause in SQL is used to sort data in ascending (ASC) or descending (DESC) order depending on specified field(s) (DESC).
GROUP BY clause: GROUP BY clause in SQL is used to group entries with identical data and may be used with aggregation methods to obtain summarised database results.
HAVING clause in SQL is used to filter records in combination with the GROUP BY clause. It is different from WHERE, since the WHERE clause cannot filter aggregated records.

Q8. What are UNION, MINUS and INTERSECT commands?
The UNION operator is used to combine the results of two tables while also removing duplicate entries.
The MINUS operator is used to return rows from the first query but not from the second query.
The INTERSECT operator is used to combine the results of both queries into a single row.
Before running either of the above SQL statements, certain requirements must be satisfied –
Within the clause, each SELECT query must have the same number of columns.
The data types in the columns must also be comparable.
In each SELECT statement, the columns must be in the same order.

Q9. What is Cursor? How to use a Cursor?
After any variable declaration, DECLARE a cursor. A SELECT Statement must always be coupled with the cursor definition.

To start the result set, move the cursor over it. Before obtaining rows from the result set, the OPEN statement must be executed.

To retrieve it and go to the next row in the result set, use the FETCH command.

To disable the cursor, use the CLOSE command.

Finally, use the DEALLOCATE command to remove the cursor definition and free up the resources connected to it.

Q10. List the different types of relationships in SQL.
There are different types of relations in the database:

One-to-One – This is a connection between two tables in which each record in one table corresponds to the maximum of one record in the other.

One-to-Many and Many-to-One – This is the most frequent connection, in which a record in one table is linked to several records in another.

Many-to-Many – This is used when defining a relationship that requires several instances on each side.

Self-Referencing Relationships – When a table has to declare a connection with itself, this is the method to employ.

Q12. What is OLTP?
OLTP, or online transactional processing, allows huge groups of people to execute massive amounts of database transactions in real time, usually via the internet. A database transaction occurs when data in a database is changed, inserted, deleted, or queried.

Q13. What are the differences between OLTP and OLAP?
OLTP stands for online transaction processing, whereas OLAP stands for online analytical processing. OLTP is an online database modification system, whereas OLAP is an online database query response system.

Q14. How do I create empty tables with the same structure as another table?
To create empty tables:
Using the INTO operator to fetch the records of one table into a new table while setting a WHERE clause to false for all entries, it is possible to create empty tables with the same structure. As a result, SQL creates a new table with a duplicate structure to accept the fetched entries, but nothing is stored into the new table since the WHERE clause is active.

Q15. What is PostgreSQL?
In 1986, a team lead by Computer Science Professor Michael Stonebraker created PostgreSQL under the name Postgres. It was created to aid developers in the development of enterprise-level applications by ensuring data integrity and fault tolerance in systems. PostgreSQL is an enterprise-level, versatile, resilient, open-source, object-relational database management system that supports variable workloads and concurrent users. The international developer community has constantly backed it. PostgreSQL has achieved significant appeal among developers because to its fault-tolerant characteristics.
It’s a very reliable database management system, with more than two decades of community work to thank for its high levels of resiliency, integrity, and accuracy. Many online, mobile, geospatial, and analytics applications utilise PostgreSQL as their primary data storage or data warehouse.

SQL Interview Questions and Answers for Freshers
 

Q16. What are SQL comments?
SQL Comments are used to clarify portions of SQL statements and to prevent SQL statements from being executed. Comments are quite important in many programming languages. The comments are not supported by a Microsoft Access database. As a result, the Microsoft Access database is used in the examples in Mozilla Firefox and Microsoft Edge.
Single Line Comments: It starts with two consecutive hyphens (–).
Multi-line Comments: It starts with /* and ends with */.

Q17. What is the usage of the NVL() function?
You may use the NVL function to replace null values with a default value. The function returns the value of the second parameter if the first parameter is null. If the first parameter is anything other than null, it is left alone.

This function is used in Oracle, not in SQL and MySQL. Instead of NVL() function, MySQL have IFNULL() and SQL Server have ISNULL() function.

Let’s move to the next question in this SQL Interview Questions.

Q18. Explain character-manipulation functions? Explains its different types in SQL.
Change, extract, and edit the character string using character manipulation routines. The function will do its action on the input strings and return the result when one or more characters and words are supplied into it.

The character manipulation functions in SQL are as follows:

A) CONCAT (joining two or more values): This function is used to join two or more values together. The second string is always appended to the end of the first string.

B) SUBSTR: This function returns a segment of a string from a given start point to a given endpoint.

C) LENGTH: This function returns the length of the string in numerical form, including blank spaces.

D) INSTR: This function calculates the precise numeric location of a character or word in a string.

E) LPAD: For right-justified values, it returns the padding of the left-side character value.

F) RPAD: For a left-justified value, it returns the padding of the right-side character value.

G) TRIM: This function removes all defined characters from the beginning, end, or both ends of a string. It also reduced the amount of wasted space.

H) REPLACE: This function replaces all instances of a word or a section of a string (substring) with the other string value specified.

Q19. Write the SQL query to get the third maximum salary of an employee from a table named employees.
Employee table

employee_name	salary
A	24000
C	34000
D	55000
E	75000
F	21000
G	40000
H	50000
 

SELECT * FROM(

SELECT employee_name, salary, DENSE_RANK() 

OVER(ORDER BY salary DESC)r FROM Employee) 

WHERE r=&n;

To find 3rd highest salary set n = 3

Q20. What is the difference between the RANK() and DENSE_RANK() functions?
The RANK() function in the result set defines the rank of each row within your ordered partition. If both rows have the same rank, the next number in the ranking will be the previous rank plus a number of duplicates. If we have three records at rank 4, for example, the next level indicated is 7.

The DENSE_RANK() function assigns a distinct rank to each row within a partition based on the provided column value, with no gaps. It always indicates a ranking in order of precedence. This function will assign the same rank to the two rows if they have the same rank, with the next rank being the next consecutive number. If we have three records at rank 4, for example, the next level indicated is 5.

Q21. What are Tables and Fields?

A table is a collection of data components organized in rows and columns in a relational database. A table can also be thought of as a useful representation of relationships. The most basic form of data storage is the table. An example of an Employee table is shown below.

ID	Name	Department	Salary
1	Rahul	Sales	24000
2	Rohini	Marketing	34000
3	Shylesh	Sales	24000
4	Tarun	Analytics	30000
 

A Record or Row is a single entry in a table. In a table, a record represents a collection of connected data. The Employee table, for example, has four records.

A table is made up of numerous records (rows), each of which can be split down into smaller units called Fields(columns). ID, Name, Department, and Salary are the four fields in the Employee table above.

Q22. What is a UNIQUE constraint?
The UNIQUE Constraint prevents identical values in a column from appearing in two records. The UNIQUE constraint guarantees that every value in a column is unique.

Q23. What is a Self-Join?
Self-join is a type of join that can be used to connect two tables. As a result, it is a unary relationship. Each row of the table is attached to itself and all other rows of the same table in a self-join. As a result, a self-join is mostly used to combine and compare rows from the same database table.

Q24. What is the SELECT statement?
A SELECT command gets zero or more rows from one or more database tables or views. SELECT the most frequent data manipulation language (DML) command is SELECT in most applications. SELECT queries define a result set, but not how to calculate it, because SQL is a declarative programming language.

Q25. What are some common clauses used with SELECT query in SQL?
The following are some frequent SQL clauses used in conjunction with a SELECT query:

WHERE clause: In SQL, the WHERE clause is used to filter records that are required depending on certain criteria.
ORDER BY clause: The ORDER BY clause in SQL is used to sort data in ascending (ASC) or descending (DESC) order depending on specified field(s) (DESC).
GROUP BY clause: GROUP BY clause in SQL is used to group entries with identical data and may be used with aggregation methods to obtain summarised database results.
HAVING clause in SQL is used to filter records in combination with the GROUP BY clause. It is different from WHERE, since the WHERE clause cannot filter aggregated records.

Q26. What are UNION, MINUS and INTERSECT commands?
The UNION operator is used to combine the results of two tables while also removing duplicate entries. 

The MINUS operator is used to return rows from the first query but not from the second query. 

The INTERSECT operator is used to combine the results of both queries into a single row.
Before running either of the above SQL statements, certain requirements must be satisfied –

Within the clause, each SELECT query must have the same amount of columns.

The data types in the columns must also be comparable.

In each SELECT statement, the columns must be in the same order.

Let’s move to the next question in this SQL Interview Questions.

Q27. What is Cursor? How to use a Cursor?
After any variable declaration, DECLARE a cursor. A SELECT Statement must always be coupled with the cursor definition.

To start the result set, move the cursor over it. Before obtaining rows from the result set, the OPEN statement must be executed.

To retrieve and go to the next row in the result set, use the FETCH command.

To disable the cursor, use the CLOSE command.

Finally, use the DEALLOCATE command to remove the cursor definition and free up the resources connected with it.

Q28. List the different types of relationships in SQL.
There are different types of relations in the database:
One-to-One – This is a connection between two tables in which each record in one table corresponds to the maximum of one record in the other.
One-to-Many and Many-to-One – This is the most frequent connection, in which a record in one table is linked to several records in another.
Many-to-Many – This is used when defining a relationship that requires several instances on each sides.
Self-Referencing Relationships – When a table has to declare a connection with itself, this is the method to employ.

Q29. What is SQL example?
SQL is a database query language that allows you to edit, remove, and request data from databases. The following statements are a few examples of SQL statements:

SELECT 
INSERT 
UPDATE
DELETE
CREATE DATABASE
ALTER DATABASE
Q30. What are basic SQL skills?
SQL skills aid data analysts in the creation, maintenance, and retrieval of data from relational databases, which divide data into columns and rows. It also enables users to efficiently retrieve, update, manipulate, insert, and alter data.

The most fundamental abilities that a SQL expert should possess are:

Database Management
Structuring a Database
Creating SQL clauses and statements
SQL System SKills like MYSQL, PostgreSQL
PHP expertise is useful.
Analyze SQL data
Using WAMP with SQL to create a database
OLAP Skills
Q31. What is schema in SQL Server?

A schema is a visual representation of the database that is logical. It builds and specifies the relationships among the database’s numerous entities. It refers to the several kinds of constraints that may be applied to a database. It also describes the various data kinds. It may also be used on Tables and Views.
Schemas come in a variety of shapes and sizes. Star schema and Snowflake schema are two of the most popular. The entities in a star schema are represented in a star form, whereas those in a snowflake schema are shown in a snowflake shape.
Any database architecture is built on the foundation of schemas.


Q32. How to create a temp table in SQL Server?
Temporary tables are created in TempDB and are erased automatically after the last connection is closed. We may use Temporary Tables to store and process interim results. When we need to store temporary data, temporary tables come in handy.

The following is the syntax for creating a Temporary Table:

CREATE TABLE #Employee (id INT, name VARCHAR(25))
INSERT INTO #Employee VALUES (01, ‘Ashish’), (02, ‘Atul’)

Let’s move to the next question in this SQL Interview Questions.


Q33. How to install SQL Server in Windows 11?

Install SQL Server Management Studio In Windows 11

Step 1: Click on SSMS, which will take you to the SQL Server Management Studio page.

Step 2: Moreover, click on the SQL Server Management Studio link and tap on Save File. 

Step 3: Save this file to your local drive and go to the folder.

Step 4: The setup window will appear, and here you can choose the location where you want to save the file.
Step 5: Click on Install.
Step 6: Close the window after the installation is complete.
Step 7: Furthermore, go back to your Start Menu and search for SQL server management studio.

Step 8: Furthermore, double-click on it, and the login page will appear once it shows up.

Step 9: You should be able to see your server name. However, if that’s not visible, click on the drop-down arrow on the server and tap on Browse.

Step 10: Choose your SQL server and click on Connect.

After that, the SQL server will connect, and Windows 11 will run well.

Q34. What is the case when in SQL Server?
The CASE statement is used to construct logic in which one column’s value is determined by the values of other columns.

At least one set of WHEN and THEN commands makes up the SQL Server CASE Statement. The condition to be tested is specified by the WHEN statement. If the WHEN the condition returns TRUE, the THEN sentence explains what to do.

When none of the WHEN conditions return true, the ELSE statement is executed. The END keyword brings the CASE statement to a close.

CASE
WHEN condition1 THEN result1
WHEN condition2 THEN result2
WHEN conditionN THEN resultN
ELSE results
END;

Q35. NoSQL vs SQL

In summary, the following are the five major distinctions between SQL and NoSQL:

Relational databases are SQL, while non-relational databases are NoSQL.

SQL databases have a specified schema and employ structured query language. For unstructured data, NoSQL databases use dynamic schemas.

SQL databases scale vertically, but NoSQL databases scale horizontally.

NoSQL databases are document, key-value, graph, or wide-column stores, whereas SQL databases are table-based.

SQL databases excel in multi-row transactions, while NoSQL excels at unstructured data such as documents and JSON.

 

Q36. What is the difference between NOW() and CURRENT_DATE()?
NOW() returns a constant time that indicates the time at which the statement began to execute. (Within a stored function or trigger, NOW() returns the time at which the function or triggering statement began to execute.
The simple difference between NOW() and CURRENT_DATE() is that NOW() will fetch the current date and time both in format ‘YYYY-MM_DD HH:MM:SS’ while CURRENT_DATE() will fetch the date of the current day ‘YYYY-MM_DD’.

Let’s move to the next question in this SQL Interview Questions.

Q37. What is BLOB and TEXT in MySQL?

BLOB stands for Binary Huge Objects and can be used to store binary data, whereas TEXT may be used to store a large number of strings. BLOB may be used to store binary data, which includes images, movies, audio, and applications.
BLOB values function similarly to byte strings, and they lack a character set. As a result, bytes’ numeric values are completely dependent on comparison and sorting.
    TEXT values behave similarly to a character string or a non-binary string. The comparison/sorting of TEXT is completely dependent on the character set collection.

SQL Interview Questions and Answers for Experienced
 

Q38. How to remove duplicate rows in SQL?

If the SQL table has duplicate rows, the duplicate rows must be removed.

Let’s assume the following table for our dataset:

 

ID	Name	Age
1	A	21
2	B	23
2	B	23
4	D	22
5	E	25
6	G	26
5	E	25
The following SQL query removes the duplicate ids from the table:

DELETE FROM table WHERE ID IN (
SELECT 
ID, COUNT(ID) 
FROM   table
GROUP BY  ID
HAVING 
COUNT (ID) > 1); 

Q39. How to create a stored procedure using SQL Server?

A stored procedure is a piece of prepared SQL code that you can save and reuse again and over.
So, if you have a SQL query that you create frequently, save it as a stored procedure and then call it to run it.
You may also supply parameters to a stored procedure so that it can act based on the value(s) of the parameter(s) given.

Stored Procedure Syntax

CREATE PROCEDURE procedure_name

AS

sql_statement

GO;

Execute a Stored Procedure

EXEC procedure_name;


Q40. What is Database Black Box Testing?

Black Box Testing is a software testing approach that involves testing the functions of software applications without knowing the internal code structure, implementation details, or internal routes. Black Box Testing is a type of software testing that focuses on the input and output of software applications and is totally driven by software requirements and specifications. Behavioral testing is another name for it.


Q41. What are the different types of SQL sandbox?

SQL Sandbox is a secure environment within SQL Server where untrusted programmes can be run. There are three different types of SQL sandboxes:

Safe Access Sandbox: In this environment, a user may execute SQL activities like as building stored procedures, triggers, and so on, but they can’t access the memory or create files.

Sandbox for External Access: Users can access files without having the ability to alter memory allocation.

Unsafe Access Sandbox: This contains untrustworthy code that allows a user to access memory.

Let’s move to the next question in this SQL Interview Questions.

Q42. Where MyISAM table is stored?

Prior to the introduction of MySQL 5.5 in December 2009, MyISAM was the default storage engine for MySQL relational database management system versions.  It’s based on the older ISAM code, but it comes with a lot of extra features. Each MyISAM table is split into three files on disc (if it is not partitioned). The file names start with the table name and end with an extension that indicates the file type. The table definition is stored in a.frm file, however this file is not part of the MyISAM engine; instead, it is part of the server. The data file suffix is.MYD (MYData). The index file extension is.MYI (MYIndex). If you lose your index file, you may always restore it by recreating indexes.

Q43. How to find the nth highest salary in SQL?
The most typical interview question is to find the Nth highest pay in a table. This work can be accomplished using the dense rank() function.
Employee table

employee_name	salary
A	24000
C	34000
D	55000
E	75000
F	21000
G	40000
H	50000
 

SELECT * FROM(

SELECT employee_name, salary, DENSE_RANK() 

OVER(ORDER BY salary DESC)r FROM Employee) 

WHERE r=&n;

To find to the 2nd highest salary set n = 2

To find 3rd highest salary set n = 3 and so on.

Q44. What do you mean by table and field in SQL?
A table refers to a collection of data in an organised manner in form of rows and columns. A field refers to the number of columns in a table. For example:

Table: StudentInformation
Field: Stu Id, Stu Name, Stu Marks

Q45. What are joins in SQL?
A JOIN clause is used to combine rows from two or more tables, based on a related column between them. It is used to merge two tables or retrieve data from there. There are 4 types of joins, as you can refer to below:



Inner join: Inner Join in SQL is the most common type of join. It is used to return all the rows from multiple tables where the join condition is satisfied. 
Left Join: Left Join in SQL is used to return all the rows from the left table but only the matching rows from the right table where the join condition is fulfilled.

Right Join: Right Join in SQL is used to return all the rows from the right table but only the matching rows from the left table where the join condition is fulfilled.

Full Join: Full join returns all the records when there is a match in any of the tables. Therefore, it returns all the rows from the left-hand side table and all the rows from the right-hand side table.

Let’s move to the next question in this SQL Interview Questions.

Q46. What is the difference between CHAR and VARCHAR2 datatype in SQL?
Both Char and Varchar2 are used for character datatype but varchar2 is used for character strings of variable length whereas Char is used for strings of fixed length. For example, char(10) can only store 10 characters and will not be able to store a string of any other length whereas varchar2(10) can store any length i.e 6,8,2 in this variable.

Q47. What is a Primary key?

A Primary key in SQL is a column (or collection of columns) or a set of columns that uniquely identifies each row in the table.
Uniquely identifies a single row in the table
Null values not allowed
Example- In the Student table, Stu_ID is the primary key.

Q48. What are Constraints?
Constraints in SQL are used to specify the limit on the data type of the table. It can be specified while creating or altering the table statement. The sample of constraints are:

NOT NULL
CHECK
DEFAULT
UNIQUE
PRIMARY KEY
FOREIGN KEY
Q49. What is the difference between DELETE and TRUNCATE statements?
DELETE vs TRUNCATE

DELETE	TRUNCATE
Delete command is used to delete a row in a table.	Truncate is used to delete all the rows from a table.
You can rollback data after using delete statement.	You cannot rollback data.
It is a DML command.	It is a DDL command.
It is slower than truncate statement.	It is faster.
Q50. What is a Unique key?
Uniquely identifies a single row in the table.
Multiple values allowed per table.
Null values allowed.
Apart from this SQL Interview Questions blog, if you want to get trained by professionals on this technology, you can opt for structured training from edureka! 

Q51. What is a Foreign key in SQL?
Foreign key maintains referential integrity by enforcing a link between the data in two tables.
The foreign key in the child table references the primary key in the parent table.
The foreign key constraint prevents actions that would destroy links between the child and parent tables.
Q52. What do you mean by data integrity? 
Data Integrity defines the accuracy as well as the consistency of the data stored in a database. It also defines integrity constraints to enforce business rules on the data when it is entered into an application or a database.

Q53. What is the difference between clustered and non-clustered index in SQL?
The differences between the clustered and non clustered index in SQL are :

Clustered index is used for easy retrieval of data from the database and its faster whereas reading from non clustered index is relatively slower.
Clustered index alters the way records are stored in a database as it sorts out rows by the column which is set to be clustered index whereas in a non clustered index, it does not alter the way it was stored but it creates a separate object within a table which points back to the original table rows after searching.
One table can only have one clustered index whereas it can have many non clustered indexes.

Q54. Write a SQL query to display the current date?
In SQL, there is a built-in function called GetDate() which helps to return the current timestamp/date.

Q55.What do you understand from query optimization?
The phase that identifies a plan for the evaluation query with the least estimated cost is known as query optimization.

The advantages of query optimization are as follows:

The output is provided faster
A larger number of queries can be executed in less time
Reduces time and space complexity
Denormalization refers to a technique which is used to access data from higher to lower forms of a database. It helps database managers to increase the performance of the entire infrastructure as it introduces redundancy into a table. It adds the redundant data into a table by incorporating database queries that combine data from various tables into a single table.

Q57. What are Entities and Relationships?
Entities:  A person, place, or thing in the real world about which data can be stored in a database. Tables store data that represents one type of entity. For example – A bank database has a customer table to store customer information. The customer table stores this information as a set of attributes (columns within the table) for each customer.

Relationships: Relation or links between entities that have something to do with each other. For example – The customer’s name is related to the customer account number and contact information, which might be in the same table. There can also be relationships between separate tables (for example, customer to accounts).

Let’s move to the next question in this SQL Interview Questions.


Q58. What is an Index?
An index refers to a performance tuning method of allowing faster retrieval of records from the table. An index creates an entry for each value and hence it will be faster to retrieve data.

Q59. Explain different types of index in SQL.
There are three types of index in SQL namely:

Unique Index:
This index does not allow the field to have duplicate values if the column is unique indexed. If a primary key is defined, a unique index can be applied automatically.

Clustered Index:
This index reorders the physical order of the table and searches based on the basis of key values. Each table can only have one clustered index.

Non-Clustered Index:
Non-Clustered Index does not alter the physical order of the table and maintains a logical order of the data. Each table can have many nonclustered indexes.

SQL Interview Questions for Data Analyst
 

Q60. What is Normalization and what are the advantages of it?
Normalization in SQL is the process of organizing data to avoid duplication and redundancy. Some of the advantages are:

Better Database organization
More Tables with smaller rows
Efficient data access
Greater Flexibility for Queries
Quickly find the information
Easier to implement Security
Allows easy modification
Reduction of redundant and duplicate data
More Compact Database
Ensure Consistent data after modification
Apart from this SQL Interview Questions Blog, if you want to get trained by professionals on this technology, you can opt for structured training from edureka! 

Q61. What is the difference between DROP and TRUNCATE commands?
DROP command removes a table and it cannot be rolled back from the database whereas TRUNCATE command removes all the rows from the table.

Q62. Explain the different types of Normalization.
There are many successive levels of normalization. These are called normal forms. Each consecutive normal form depends on the previous one.The first three normal forms are usually adequate.

Normal Forms are used in database tables to remove or decrease duplication. The following are the many forms:

First Normal Form:
When every attribute in a relationship is a single-valued attribute, it is said to be in the first normal form. The first normal form is broken when a relation has a composite or multi-valued property.

Second Normal Form:

A relationship is in a second normal form if it meets the first normal form’s requirements and does not contain any partial dependencies. In 2NF, a relation has no partial dependence, which means it has no non-prime attribute that is dependent on any suitable subset of any table candidate key. Often, the problem may be solved by setting a single column Primary Key.

Third Normal Form:
If a relation meets the requirements for the second normal form and there is no transitive dependency, it is said to be in the third normal form.


Q63. What is OLTP?

OLTP, or online transactional processing, allows huge groups of people to execute massive amounts of database transactions in real time, usually via the internet. A database transaction occurs when data in a database is changed, inserted, deleted, or queried.

What are the differences between OLTP and OLAP?

OLTP stands for online transaction processing, whereas OLAP stands for online analytical processing. OLTP is an online database modification system, whereas OLAP is an online database query response system.

Q64. How to create empty tables with the same structure as another table?

To create empty tables:

Using the INTO operator to fetch the records of one table into a new table while setting a WHERE clause to false for all entries, it is possible to create empty tables with the same structure. As a result, SQL creates a new table with a duplicate structure to accept the fetched entries, but nothing is stored into the new table since the WHERE clause is active.

Q65. What is PostgreSQL?

In 1986, a team lead by Computer Science Professor Michael Stonebraker created PostgreSQL under the name Postgres. It was created to aid developers in the development of enterprise-level applications by ensuring data integrity and fault tolerance in systems. PostgreSQL is an enterprise-level, versatile, resilient, open-source, object-relational database management system that supports variable workloads and concurrent users. The international developer community has consistently backed it. PostgreSQL has achieved significant appeal among developers because of its fault-tolerant characteristics.
It’s a very reliable database management system, with more than two decades of community work to thank for its high levels of resiliency, integrity, and accuracy. Many online, mobile, geospatial, and analytics applications utilise PostgreSQL as their primary data storage or data warehouse.

Q66. What are SQL comments?

SQL Comments are used to clarify portions of SQL statements and to prevent SQL statements from being executed. Comments are quite important in many programming languages. These comments are not supported by the Microsoft Access database. As a result, the Microsoft Access database is used in the examples in Mozilla Firefox and Microsoft Edge.
Single Line Comments: It starts with two consecutive hyphens (–).
Multi-line Comments: It starts with /* and ends with */.

Let’s move to the next question in this SQL Interview Questions.

Q67. What is the difference between the RANK() and DENSE_RANK() functions?

The RANK() function in the result set defines the rank of each row within your ordered partition. If both rows have the same rank, the next number in the ranking will be the previous rank plus a number of duplicates. If we have three records at rank 4, for example, the next level indicated is 7.

The DENSE_RANK() function assigns a distinct rank to each row within a partition based on the provided column value, with no gaps. It always indicates a ranking in order of precedence. This function will assign the same rank to the two rows if they have the same rank, with the next rank being the next consecutive number. If we have three records at rank 4, for example, the next level indicated is 5.

Q68. What is SQL Injection?

SQL injection is a sort of flaw in website and web app code that allows attackers to take control of back-end processes and access, retrieve, and delete sensitive data stored in databases. In this approach, malicious SQL statements are entered into a database entry field, and the database becomes exposed to an attacker once they are executed. By utilising data-driven apps, this strategy is widely utilised to get access to sensitive data and execute administrative tasks on databases. SQLi attack is another name for it.

The following are some examples of SQL injection:

Getting access to secret data in order to change a SQL query to acquire the desired results.
UNION attacks are designed to steal data from several database tables.
Examine the database to get information about the database’s version and structure

Q69. How many Aggregate functions are available in SQL?

SQL aggregate functions provide information about a database’s data. AVG, for example, returns the average of a database column’s values.

SQL provides seven (7) aggregate functions, which are given below:

AVG(): returns the average value from the specified columns.
COUNT(): returns the number of table rows, including rows with null values.
MAX(): returns the largest value among the group.
MIN(): returns the smallest value among the group.
SUM(): returns the total summed values(non-null) of the specified column.
FIRST(): returns the first value of an expression.
LAST(): returns the last value of an expression.

Q70. What is the default ordering of data using the ORDER BY clause? How could it be changed?

The ORDER BY clause in MySQL can be used without the ASC or DESC modifiers. The sort order is preset to ASC or ascending order when this attribute is absent from the ORDER BY clause.


Q71. How do we use the DISTINCT statement? What is its use?

The SQL DISTINCT keyword is combined with the SELECT query to remove all duplicate records and return only unique records. There may be times when a table has several duplicate records.
The DISTINCT clause in SQL is used to eliminate duplicates from a SELECT statement’s result set.

Q72. What are the syntax and use of the COALESCE function?

From a succession of expressions, the COALESCE function returns the first non-NULL value. The expressions are evaluated in the order that they are supplied, and the function’s result is the first non-null value. Only if all of the inputs are null does the COALESCE method return NULL.

The syntax of COALESCE function is COALESCE (exp1, exp2, …. expn) 

 

Q73. What is the ACID property in a database?
ACID stands for Atomicity, Consistency, Isolation, Durability. It is used to ensure that the data transactions are processed reliably in a database system. 

Atomicity: Atomicity refers to the transactions that are completely done or failed where transaction refers to a single logical operation of a data. It means if one part of any transaction fails, the entire transaction fails and the database state is left unchanged.
Consistency: Consistency ensures that the data must meet all the validation rules. In simple words, you can say that your transaction never leaves the database without completing its state.
Isolation: The main goal of isolation is concurrency control.
Durability: Durability means that if a transaction has been committed, it will occur whatever may come in between such as power loss, crash or any sort of error.
Top 10 Trending Technologies to Learn in 2025 | Edureka



This video talks about the Top 10 Trending Technologies in 2025 that you must learn.
 

Q74. What do you mean by “Trigger” in SQL?
Trigger in SQL is are a special type of stored procedures that are defined to execute automatically in place or after data modifications. It allows you to execute a batch of code when an insert, update or any other query is executed against a specific table.

Q75. What are the different operators available in SQL?
There are three operators available in SQL, namely:

Arithmetic Operators
Logical Operators
Comparison Operators
Apart from this SQL Interview Questions blog, if you want to get trained by professionals on this technology, you can opt for structured training from Edureka! 

Q76.  Are NULL values the same as that of zero or a blank space? 
A NULL value is not at all the same as that of zero or a blank space. NULL values represent a value that is unavailable, unknown, assigned or not applicable whereas a zero is a number and a blank space is a character.

Q77. What is the difference between cross join and natural join?
The cross join produces the cross product or Cartesian product of two tables whereas the natural join is based on all the columns having the same name and data types in both the tables.

Q78. What is subquery in SQL?
A subquery is a query inside another query where a query is defined to retrieve data or information back from the database. In a subquery, the outer query is called as the main query whereas the inner query is called subquery. Subqueries are always executed first and the result of the subquery is passed on to the main query. It can be nested inside a SELECT, UPDATE or any other query. A subquery can also use any comparison operators such as >,< or =.

Q79. What are the different types of a subquery?
There are two types of subquery namely, Correlated and Non-Correlated.

Correlated subquery: These are queries which select the data from a table referenced in the outer query. This is not considered an independent query as it refers to another table and refers to the column in a table.

Non-Correlated subquery: This query is an independent query where the output of subquery is substituted in the main query.

Let’s move to the next question in this SQL Interview Questions.

Q80. List the ways to get the count of records in a table?
To count the number of records in a table in SQL, you can use the below commands:

SELECT * FROM table1

SELECT COUNT(*) FROM table1

SELECT rows FROM sysindexes WHERE id = OBJECT_ID(table1) AND indid < 2

Apart from this SQL Interview Questions Blog, if you want to get trained by professionals on this technology, you can opt for structured training from edureka! 

Interview Questions for SQL Developer
 

Q81. Write a SQL query to find the names of employees that begin with ‘A’?
To display name of the employees that begin with ‘A’, type in the below command:

SELECT * FROM Table_name WHERE EmpName like 'A%'
Q82. Write a SQL query to get the third-highest salary of an employee from employee_table?
SELECT TOP 1 salary
FROM(
SELECT TOP 3 salary
FROM employee_table
ORDER BY salary DESC) AS emp
ORDER BY salary ASC;
Q83. What is the need for group functions in SQL? 
Group functions work on the set of rows and return one result per group. Some of the commonly used group functions are: AVG, COUNT, MAX, MIN, SUM, VARIANCE.

Q84. What is a Relationship and what are they?
Relation or links are between entities that have something to do with each other. Relationships are defined as the connection between the tables in a database. There are various relationships, namely:

One to One Relationship.
One to Many Relationship.
Many to One Relationship.
Self-Referencing Relationship.
Q85.  How can you insert NULL values in a column while inserting the data?
NULL values in SQL can be inserted in the following ways:

Implicitly by omitting column from column list.
Explicitly by specifying NULL keyword in the VALUES clause
Q86. What is the main difference between ‘BETWEEN’ and ‘IN’ condition operators?
BETWEEN operator is used to display rows based on a range of values in a row whereas the IN condition operator is used to check for values contained in a specific set of values.

 Example of BETWEEN:
SELECT * FROM Students where ROLL_NO BETWEEN 10 AND 50;
Example of IN:
SELECT * FROM students where ROLL_NO IN (8,15,25);

Get More SQL query interview questions
Q87. Why are SQL functions used?
SQL functions are used for the following purposes:

To perform some calculations on the data
To modify individual data items
To manipulate the output
To format dates and numbers
To convert the data types
Q88. What is the need for MERGE statement?
This statement allows conditional update or insertion of data into a table. It performs an UPDATE if a row exists, or an INSERT if the row does not exist.

Q89. What do you mean by recursive stored procedure?
Recursive stored procedure refers to a stored procedure which calls by itself until it reaches some boundary condition. This recursive function or procedure helps the programmers to use the same set of code n number of times.

Q90. What is CLAUSE in SQL?
SQL clause helps to limit the result set by providing a condition to the query. A clause helps to filter the rows from the entire set of records.

For example – WHERE, HAVING clause.

Apart from this SQL Interview Questions Blog, if you want to get trained by professionals on this technology, you can opt for a structured training from edureka! Click below to learn more.

Q91. What is the difference between ‘HAVING’ CLAUSE and a “where” clause?
HAVING clause can only be used only with SELECT statement. It is usually used in a GROUP BY clause and whenever GROUP BY is not used, HAVING behaves like a WHERE clause.
Having Clause is only used with the GROUP BY function in a query whereas WHERE Clause is applied to each row before they are a part of the GROUP BY function in a query.

Q92. List the ways in which Dynamic SQL can be executed?
Following are the ways in which dynamic SQL can be executed:

Write a query with parameters.
Using EXEC.
Using sp_executesql.
Q93. What are the various levels of constraints?
Constraints are the representation of a column to enforce data entity and consistency. There are two levels of a constraint, namely:

column level constraint
table level constraint
Q94. How can you fetch common records from two tables?
You can fetch common records from two tables using INTERSECT. For example:


Select studentID from student. <strong>INTERSECT </strong> Select StudentID from Exam

Q95. List some case manipulation functions in SQL?
There are three case manipulation functions in SQL, namely:

LOWER: This function returns the string in lowercase. It takes a string as an argument and returns it by converting it into lower case. Syntax:
LOWER(‘string’)
UPPER: This function returns the string in uppercase. It takes a string as an argument and returns it by converting it into uppercase. Syntax:
UPPER(‘string’)
INITCAP: This function returns the string with the first letter in uppercase and rest of the letters in lowercase. Syntax:
INITCAP(‘string’)

Apart from this SQL Interview Questions blog, if you want to get trained by professionals on this technology, you can opt for a structured training from edureka! Click below to learn more.

 Q96. What are the different set operators available in SQL?
Some of the available set operators are – Union, Intersect or Minus operators.

Q97. What is an ALIAS command?
ALIAS command in SQL is the name that can be given to any table or a column. The alias name can be referred in WHERE clause to identify a particular table or column.

For example-

Select emp.empID, dept.Result from employee emp, department as dept where emp.empID=dept.empID
In the above example, emp refers to alias name for employee table and dept refers to alias name for department table.

Let’s move to the next question in this SQL Interview Questions.

Q98. What are aggregate and scalar functions?
Aggregate functions are used to evaluate mathematical calculation and returns a single value. These calculations are done from the columns in a table. For example- max(),count() are calculated with respect to numeric.

Scalar functions return a single value based on the input value. For example – UCASE(), NOW() are calculated with respect to string.

Let’s move to the next question in this SQL Interview Questions.

Q99. How can you fetch alternate records from a table?
You can fetch alternate records i.e both odd and even row numbers. For example- To display even numbers, use the following command:

Select studentId from (Select rowno, studentId from student) where mod(rowno,2)=0

Now, to display odd numbers:

Select studentId from (Select rowno, studentId from student) where mod(rowno,2)=1
Q100. Name the operator which is used in the query for pattern matching?
LIKE an operator is used for pattern matching, and it can be used as -.

% – It matches zero or more characters.
For example- select * from students where studentname like ‘a%’

_ (Underscore) – it matches exactly one character.
For example- select * from student where studentname like ‘abc_’

Apart from this SQL Interview Questions Blog, if you want to get trained from professionals on this technology, you can opt for structured training from edureka! 

Q101. How can you select unique records from a table?
You can select unique records from a table by using the DISTINCT keyword.

Select DISTINCT studentID from Student
Using this command, it will print unique student id from the table Student.

Q102. How can you fetch the first 5 characters of the string?
There are a lot of ways to fetch characters from a string. For example:

Select SUBSTRING(StudentName,1,5) as studentname from student

Q103. What is the main difference between SQL and PL/SQL?
SQL is a query language that allows you to issue a single query or execute a single insert/update/delete whereas PL/SQL is Oracle’s “Procedural Language” SQL, which allows you to write a full program (loops, variables, etc.) to accomplish multiple operations such as selects/inserts/updates/deletes. 

Q104. What is a View?
A view is a virtual table which consists of a subset of data contained in a table. Since views are not present, it takes less space to store. View can have data from one or more tables combined and it depends on the relationship.

Let’s move to the next question in this SQL Interview Questions.

Q105. What are Views used for?
A view refers to a logical snapshot based on a table or another view. It is used for the following reasons:

Restricting access to data.
Making complex queries simple.
Ensuring data independence.
Providing different views of the same data.
Q106. What is a Stored Procedure?
A Stored Procedure is a function which consists of many SQL statements to access the database system. Several SQL statements are consolidated into a stored procedure and execute them whenever and wherever required which saves time and avoids writing code again and again.

Q107. List some advantages and disadvantages of Stored Procedure?
Advantages:
A Stored Procedure can be used as a modular programming which means to create once, store and call several times whenever it is required. This supports faster execution. It also reduces network traffic and provides better security to the data.

Disadvantage:
The only disadvantage of Stored Procedure is that it can be executed only in the database and utilizes more memory in the database server.

Q108. List all the types of user-defined functions?
There are three types of user-defined functions, namely:

Scalar Functions
Inline Table-valued functions
Multi-statement valued functions
Scalar returns the unit, variant defined the return clause. Other two types of defined functions return table.

Let’s move to the next question in this SQL Interview Questions.

Q109. What do you mean by Collation?
Collation is defined as a set of rules that determine how data can be sorted as well as compared. Character data is sorted using the rules that define the correct character sequence along with options for specifying case-sensitivity, character width etc.

Let’s move to the next question in this SQL Interview Questions.

Q110. What are the different types of Collation Sensitivity?
Following are the different types of collation sensitivity:

Case Sensitivity: A and a and B and b.
Kana Sensitivity: Japanese Kana characters.
Width Sensitivity: Single byte character and double-byte character.
Accent Sensitivity.
Apart from this SQL Interview Questions Blog, if you want to get trained by professionals on this technology, you can opt for structured training from Edureka! 

Q111. What are the Local and Global variables?
Local variables:
These variables can be used or exist only inside the function. These variables are not used or referred to by any other function.

Global variables:
These variables are the variables which can be accessed throughout the program. Global variables cannot be created whenever that function is called.

Q112. What is Auto Increment in SQL?
Autoincrement keyword allows the user to create a unique number to get generated whenever a new record is inserted into the table.
This keyword is usually required whenever PRIMARY KEY in SQL is used.

AUTO INCREMENT keyword can be used in Oracle and IDENTITY keyword can be used in SQL SERVER.

Q113. What is a Datawarehouse?
Datawarehouse refers to a central repository of data where the data is assembled from multiple sources of information. Those data are consolidated, transformed and made available for mining as well as online processing. Warehouse data also has a subset of data called Data Marts.

Q114. What are the different authentication modes in SQL Server? How can it be changed?
Windows mode and Mixed Mode – SQL and Windows. You can go to the steps below to change authentication mode in SQL Server:

Click Start> Programs> Microsoft SQL Server and click SQL Enterprise Manager to run SQL Enterprise Manager from the Microsoft SQL Server program group.
Then select the server from the Tools menu.
Select SQL Server Configuration Properties and choose the Security page.
Q115. What are STUFF and REPLACE function?
STUFF Function: This function is used to overwrite existing character or inserts a string into another string. Syntax:
STUFF(string_expression,start, length, replacement_characters)
where,
string_expression: it is the string that will have characters substituted
start: This refers to the starting position
length: It refers to the number of characters in the string which are substituted.
replacement_string: They are the new characters which are injected in the string.
 

REPLACE function: This function is used to replace the existing characters of all the occurrences. Syntax:
REPLACE (string_expression, search_string, replacement_string)
Here every search_string in the string_expression will be replaced with the replacement_string.
Q116. How do you add email validation using only one query?
You can use a regular expression within a SQL query to validate an email format. For example, in MySQL, you can do it like this:

SELECT email

FROM your_table_name

WHERE email REGEXP ‘^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+.[a-zA-Z]{2,}$’;

This query selects the email column from your_table_name where the email matches the specified regular expression pattern for a valid email format.

Q117. Write a query to get the last record from a table.
To get the last record from a table, you can use a query like this:

SELECT * FROM your_table_name ORDER BY your_primary_key_column DESC LIMIT 1;

Replace your_table_name with the name of your table and your_primary_key_column with the primary key column that defines the order of records. The DESC keyword sorts the records in descending order, and LIMIT 1 ensures you only get the last record.

So this brings us to the end of the SQL interview questions blog. I hope this set of SQL Interview Questions will help you ace your job interview. All the best for your interview!

Apart from this SQL Interview Questions Blog, if you want to get trained from professionals on SQL, you can opt for a structured training from edureka! Click below to know more.
SQL Live Interview Playbook — Sr. 
Manager, Advanced Analytics (Marketing) 
Use this as a rapid reference during live SQL rounds. The idioms are 
BigQuery‑flavored but portable. All snippets fit within ~80 chars/line. 
0) How to use this 
● Skim Section 1 to structure your approach under time pressure. 
● Use Section 2 to map the prompt → pattern quickly. 
● Paste skeletons from Section 3–4 and adapt with table/column names. 
● Run the Section 5 checklist before executing. 
● Drill with Section 6. Start with P1–P8 solutions, then the rest. 
1) 8‑step approach under time pressure 
Step What to do 
1 
Key checks & phrases 
Restate goal Output grain, columns, sort, tie‑breaks, NULL policy 
2 
3 
4 
5 
Identify data 
Define grain 
Edge cases 
Plan query 
Table names, keys, types, time zone, partitions 
e.g., user‑day, user‑session, order, cohort‑week 
Dups, late events, backfills, bots, test traffic 
CTE stages: filter → join → enrich → agg → present 
6 
7 
8 
Write 
Verify 
Finalize 
Use WITH CTEs, small test filters, determinism 
Row counts, sample rows, invariants (e.g., totals) 
Remove test filters, add order/limit, comments 
Ask explicitly: time window (inclusive/exclusive), time zone, attribution 
rule (first/last, click>view), dedupe rule (latest by ts, max version), and 
whether to count unique users or events. 
2) Prompt → Pattern decision matrix 
Clues in prompt 
"latest record", "one per 
user" 
Pattern to use 
Dedupe by ranking 
Core idiom 
QUALIFY ROW_NUMBER() 
"top N per group" 
"first/last touch" 
"within 30 mins" 
"consecutive days" 
"retention by cohort" 
"rolling 7 days" 
"funnel step 1→n" 
Greatest‑N per group QUALIFY ROW_NUMBER<=N 
Attribution window 
Sessionization 
Gaps & islands 
Cohort cube 
Moving window 
Step gating 
JOIN ... ON ts BETWEEN ... + 
rank 
LAG + DIF > 30m run‑sum groups 
LAG + (date!=lag+1) group 
labels 
First date per user → DATEDIFF 
bucket 
SUM(...) OVER win ROWS/RANGE 
Semi‑joins or MAX(step_k) method 
"percentile/median" Quantiles APPROX_QUANTILES(x,100)[OFFS
 ET(p)] 
"users not in ..." Anti‑join LEFT JOIN ... WHERE right IS 
NULL 
"second highest" Rank/Distinct DENSE_RANK or LIMIT/OFFSET 
"missing days" Date spine GENERATE_DATE_ARRAY left join 
 
3) Reusable pattern library (BigQuery‑friendly) 
3.1 Dedupe: latest record per key (deterministic) 
WITH ranked AS ( 
  SELECT t.*, ROW_NUMBER() OVER ( 
    PARTITION BY user_id ORDER BY event_ts DESC, event_id DESC 
  ) AS rn 
  FROM `proj.ds.events` t 
) 
SELECT * FROM ranked QUALIFY rn = 1; 
 
3.2 Top‑N per group (ties broken deterministically) 
WITH ranked AS ( 
  SELECT p.product_id, p.user_id, p.revenue, 
         ROW_NUMBER() OVER ( 
           PARTITION BY user_id ORDER BY revenue DESC, product_id 
         ) AS rk 
  FROM `proj.ds.purchases` p 
) 
SELECT * FROM ranked QUALIFY rk <= 3; 
 
3.3 Gaps & islands: consecutive‑day streaks per user 
WITH d AS ( 
  SELECT user_id, DATE(event_ts) AS d 
  FROM `proj.ds.events` 
  GROUP BY user_id, d 
), x AS ( 
  SELECT *, DATE_DIFF(d, LAG(d) OVER (PARTITION BY user_id ORDER BY d), 
                     DAY) AS diff 
  FROM d 
), y AS ( 
  SELECT *, SUM(CASE WHEN diff = 1 THEN 0 ELSE 1 END) OVER ( 
                PARTITION BY user_id ORDER BY d) AS grp 
  FROM x 
) 
SELECT user_id, MIN(d) AS start_d, MAX(d) AS end_d, COUNT(*) AS days 
FROM y 
GROUP BY user_id, grp; 
 
3.4 Date spine (fill missing days) 
WITH spine AS ( 
  SELECT d AS dt 
  FROM UNNEST(GENERATE_DATE_ARRAY('2025-01-01','2025-01-31')) AS d 
), agg AS ( 
  SELECT DATE(event_ts) AS dt, COUNT(*) AS cnt 
  FROM `proj.ds.events` 
  WHERE event_ts >= '2025-01-01' AND event_ts < '2025-02-01' 
  GROUP BY dt 
) 
SELECT s.dt, COALESCE(a.cnt, 0) AS cnt 
FROM spine s 
LEFT JOIN agg a USING (dt) 
ORDER BY s.dt; 
 
3.5 Rolling 7‑day sum per user 
SELECT user_id, DATE(event_ts) AS d, 
       SUM(value) OVER ( 
         PARTITION BY user_id ORDER BY d 
         RANGE BETWEEN INTERVAL 6 DAY PRECEDING AND CURRENT ROW 
       ) AS val_7d 
FROM `proj.ds.events`; 
 
3.6 Funnel (step gating; 1→N in order) 
WITH s AS ( 
  SELECT user_id, 
         MIN(CASE WHEN step='view' THEN event_ts END) AS ts1, 
         MIN(CASE WHEN step='add' THEN event_ts END) AS ts2, 
         MIN(CASE WHEN step='pay' THEN event_ts END) AS ts3 
  FROM `proj.ds.funnel_events` 
  GROUP BY user_id 
) 
SELECT 
  COUNTIF(ts1 IS NOT NULL) AS view_users, 
  COUNTIF(ts2 IS NOT NULL AND ts2 >= ts1) AS add_users, 
  COUNTIF(ts3 IS NOT NULL AND ts3 >= ts2 AND ts2 >= ts1) AS pay_users; 
 
3.7 Sessionization (30‑minute inactivity) 
WITH e AS ( 
  SELECT user_id, event_ts, 
         IF(TIMESTAMP_DIFF(event_ts, 
                           LAG(event_ts) OVER ( 
                             PARTITION BY user_id ORDER BY event_ts), 
                           MINUTE) > 30, 1, 0) AS new_sess 
  FROM `proj.ds.events` 
), s AS ( 
  SELECT *, SUM(new_sess) OVER ( 
           PARTITION BY user_id ORDER BY event_ts) AS sess_id 
  FROM e 
) 
SELECT user_id, sess_id, 
       MIN(event_ts) AS session_start, 
       MAX(event_ts) AS session_end, 
       COUNT(*) AS events 
FROM s 
GROUP BY user_id, sess_id; 
 
3.8 Cohort retention (weekly, user first‑purchase cohort) 
WITH firsts AS ( 
  SELECT user_id, 
         DATE_TRUNC(MIN(order_date), WEEK(MONDAY)) AS cohort_w 
  FROM `proj.ds.orders` 
  GROUP BY user_id 
), weeks AS ( 
  SELECT user_id, DATE_TRUNC(order_date, WEEK(MONDAY)) AS active_w 
  FROM `proj.ds.orders` 
  GROUP BY user_id, active_w 
) 
SELECT f.cohort_w, 
       DATE_DIFF(w.active_w, f.cohort_w, WEEK) AS wk, 
       COUNT(DISTINCT w.user_id) AS active_users 
FROM firsts f 
JOIN weeks w USING (user_id) 
GROUP BY cohort_w, wk 
ORDER BY cohort_w, wk; 
 
3.9 Ads attribution: click over view within 7 days -- clicks c(user_id, click_ts, campaign_id) -- views  v(user_id, view_ts, campaign_id) -- conv   k(user_id, conv_ts) 
WITH cand AS ( 
  SELECT k.user_id, k.conv_ts, 
         c.campaign_id AS camp, 
         c.click_ts AS at_ts, 
         1 AS pri 
  FROM k 
  JOIN c 
    ON c.user_id = k.user_id 
   AND c.click_ts BETWEEN TIMESTAMP_SUB(k.conv_ts, INTERVAL 7 DAY) 
                      AND k.conv_ts 
  UNION ALL 
  SELECT k.user_id, k.conv_ts, 
         v.campaign_id, v.view_ts, 2 
  FROM k 
  JOIN v 
    ON v.user_id = k.user_id 
   AND v.view_ts BETWEEN TIMESTAMP_SUB(k.conv_ts, INTERVAL 7 DAY) 
                     AND k.conv_ts 
), picked AS ( 
  SELECT *, ROW_NUMBER() OVER ( 
    PARTITION BY user_id, conv_ts ORDER BY pri, at_ts DESC 
  ) AS rn 
  FROM cand 
) 
SELECT user_id, conv_ts, camp 
FROM picked 
QUALIFY rn = 1; 
 
3.10 Percentiles / median (approximate is fine live) 
SELECT APPROX_QUANTILES(metric, 100)[OFFSET(50)] AS p50 
FROM `proj.ds.table`; 
 
3.11 Anti‑join / semi‑join -- Users who purchased but never installed app 
SELECT DISTINCT p.user_id 
FROM `proj.ds.purchases` p 
LEFT JOIN `proj.ds.installs` i USING (user_id) 
WHERE i.user_id IS NULL; 
 
3.12 UTM parsing (lowercase, safe) 
SELECT user_id, 
       LOWER(REGEXP_EXTRACT(url, r"[?&]utm_source=([^&#]+)")) AS src, 
       LOWER(REGEXP_EXTRACT(url, r"[?&]utm_medium=([^&#]+)")) AS med, 
       LOWER(REGEXP_EXTRACT(url, r"[?&]utm_campaign=([^&#]+)")) AS cmp 
FROM `proj.ds.pageviews`; 
 
 
4) Marketing analytics staples (ready‑to‑run) 
4.1 DAU / WAU / MAU and stickiness (DAU/MAU) 
WITH u AS ( 
  SELECT DATE(event_ts) AS d, user_id 
  FROM `proj.ds.events` 
  GROUP BY d, user_id 
) 
SELECT 
  (SELECT COUNT(DISTINCT user_id) 
   FROM u WHERE d = '2025-08-28') AS dau, 
  (SELECT COUNT(DISTINCT user_id) 
   FROM u WHERE d BETWEEN '2025-08-22' AND '2025-08-28') AS wau, 
  (SELECT COUNT(DISTINCT user_id) 
   FROM u WHERE d BETWEEN '2025-07-30' AND '2025-08-28') AS mau; -- stickiness = dau / mau 
 
4.2 CAC by channel per week -- spend(channel, dt, cost) -- signups(user_id, dt, channel) 
WITH s AS ( 
  SELECT DATE_TRUNC(dt, WEEK(MONDAY)) AS wk, channel, 
         SUM(cost) AS spend 
  FROM `proj.ds.spend` 
  GROUP BY wk, channel 
), u AS ( 
  SELECT DATE_TRUNC(dt, WEEK(MONDAY)) AS wk, channel, 
         COUNT(DISTINCT user_id) AS users 
  FROM `proj.ds.signups` 
  GROUP BY wk, channel 
) 
SELECT s.wk, s.channel, spend, users, SAFE_DIVIDE(spend, users) AS cac 
FROM s JOIN u USING (wk, channel) 
ORDER BY wk, channel; 
 
4.3 LTV within 90 days of signup -- orders(user_id, order_ts, net_rev) -- signups(user_id, signup_ts) 
WITH joined AS ( 
  SELECT s.user_id, s.signup_ts, o.order_ts, o.net_rev 
  FROM `proj.ds.signups` s 
  LEFT JOIN `proj.ds.orders` o 
    ON o.user_id = s.user_id 
   AND o.order_ts BETWEEN s.signup_ts AND 
       TIMESTAMP_ADD(s.signup_ts, INTERVAL 90 DAY) 
) 
SELECT DATE(s.signup_ts) AS signup_d, 
       AVG(net_rev) AS ltv_90d 
FROM joined s 
GROUP BY signup_d 
ORDER BY signup_d; 
 
4.4 Reactivation rate (churned ≥30d then active) 
WITH act AS ( 
  SELECT user_id, DATE(event_ts) AS d 
  FROM `proj.ds.events` 
  GROUP BY user_id, d 
), gap AS ( 
  SELECT user_id, d, 
         LAG(d) OVER (PARTITION BY user_id ORDER BY d) AS prev_d 
  FROM act 
), reac AS ( 
  SELECT user_id, d 
  FROM gap 
  WHERE prev_d IS NOT NULL AND DATE_DIFF(d, prev_d, DAY) >= 30 
) 
SELECT d AS reactivation_d, COUNT(*) AS users 
FROM reac 
GROUP BY reactivation_d 
ORDER BY reactivation_d; 
 
4.5 A/B test metric (mean revenue/user; guardrails) -- events contain (user_id, variant, revenue) 
WITH by_u AS ( 
SELECT variant, user_id, SUM(revenue) AS r 
FROM `proj.ds.exp_events` 
GROUP BY variant, user_id 
) 
SELECT variant, 
COUNT(*) AS users, 
AVG(r) AS arpu, 
STDDEV_SAMP(r) AS sd 
FROM by_u 
GROUP BY variant; -- Compute diff, CI in Python or use APPROX_QUANTILES for quick sanity. 
5) BigQuery correctness & performance checklist 
Area 
Filters 
Rule 
Always filter by _PARTITIONTIME/date on large tables 
Select 
Joins 
Dups 
Nulls 
Time 
Avoid SELECT *. Project only needed columns 
Pre‑aggregate before many‑to‑many joins; add join keys list 
Make ranking/tie‑breakers deterministic (secondary ORDER BY) 
Use COALESCE, SAFE_ casts, and decide inclusion early 
Fix time zone; define window boundaries precisely 
Windows Prefer QUALIFY to prune after window ranking 
Arrays 
Approx 
Costs 
UNNEST deliberately; pre‑filter to avoid explosion 
APPROX_* for speed in interviews if exactness not needed 
LIMIT in dev; remove before final, or justify keeping it 
6) Practice problems (20) 
ID 
Topic 
P1 Dedupe 
Prompt 
Latest paid order per user in last 60 days 
P2 Top‑N 
P3 Funnel 
P4 Rolling 
P5 Attribution 
P6 Cohort 
P7 Gaps 
P8 Anti‑join 
P9 Date spine 
P10 Percentile 
P11 Session 
P12 UTM 
Top 3 cities per country by bookings (ties deterministic) 
view→wishlist→book conversion counts & rates 
7‑day rolling cancellations per market 
Assign each booking to click>view within 7 days 
Weekly retention for hosts after first listing 
Longest consecutive active streak per guest 
Guests who booked but never messaged a host 
Fill missing days of bookings per market 
Median nightly price per city per month 
Sessionize guest web events (30‑min gap) 
CAC by utm_source 
P13 WAU/MAU Compute DAU/WAU/MAU + stickiness 
P14 Reactivation Guests inactive 60d who returned 
P15 AB 
P16 String 
P17 Join 
P18 Window 
P19 Quality 
P20 Cleanup 
ARPU by variant with sanity guardrails 
Extract room type from title via regex 
Hosts with ≥3 listings and ≥2 bookings each 
2nd highest booking value per host 
Flag suspected test traffic by rules 
Remove duplicates by (id, ts) keep max version 
Selected solutions & skeletons 
P1 Latest paid order per user (60 days) 
WITH f AS ( 
SELECT * 
FROM `proj.ds.orders` 
WHERE status = 'paid' 
AND order_ts >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 60 DAY) 
), r AS ( 
SELECT *, ROW_NUMBER() OVER ( 
PARTITION BY user_id ORDER BY order_ts DESC, order_id DESC 
) AS rn 
FROM f 
) 
SELECT * FROM r WHERE rn = 1; 
P2 Top 3 cities per country by bookings 
WITH agg AS ( 
  SELECT country, city, COUNT(*) AS b 
  FROM `proj.ds.bookings` 
  GROUP BY country, city 
), r AS ( 
  SELECT *, ROW_NUMBER() OVER ( 
    PARTITION BY country ORDER BY b DESC, city 
  ) AS rk 
  FROM agg 
) 
SELECT country, city, b FROM r WHERE rk <= 3 ORDER BY country, rk; 
 
P3 Funnel view→wishlist→book 
WITH s AS ( 
  SELECT user_id, 
    MIN(CASE WHEN ev='view'     THEN ts END) AS t1, 
    MIN(CASE WHEN ev='wishlist' THEN ts END) AS t2, 
    MIN(CASE WHEN ev='book'     THEN ts END) AS t3 
  FROM `proj.ds.site_events` 
  GROUP BY user_id 
) 
SELECT 
  COUNTIF(t1 IS NOT NULL) AS v, 
  COUNTIF(t2 IS NOT NULL AND t2 >= t1) AS w, 
  COUNTIF(t3 IS NOT NULL AND t3 >= t2 AND t2 >= t1) AS b, 
  SAFE_DIVIDE(COUNTIF(t2 IS NOT NULL AND t2 >= t1), 
              COUNTIF(t1 IS NOT NULL)) AS v2w, 
  SAFE_DIVIDE(COUNTIF(t3 IS NOT NULL AND t3 >= t2 AND t2 >= t1), 
              COUNTIF(t1 IS NOT NULL)) AS v2b; 
 
P4 7‑day rolling cancellations per market 
WITH d AS ( 
  SELECT market, DATE(cancel_ts) AS d, COUNT(*) AS c 
  FROM `proj.ds.cancels` 
  GROUP BY market, d 
) 
SELECT market, d, 
  SUM(c) OVER ( 
    PARTITION BY market ORDER BY d 
    RANGE BETWEEN INTERVAL 6 DAY PRECEDING AND CURRENT ROW 
  ) AS c_7d 
FROM d; 
 
P5 Booking attribution click>view -- Use 3.9; replace tables with bookings(conv) and ads clicks/views. 
 
P6 Weekly retention for hosts after first listing 
WITH firsts AS ( 
  SELECT host_id, 
         DATE_TRUNC(MIN(list_ts), WEEK(MONDAY)) AS cohort_w 
  FROM `proj.ds.listings` 
  GROUP BY host_id 
), act AS ( 
SELECT host_id, DATE_TRUNC(event_ts, WEEK(MONDAY)) AS w 
FROM `proj.ds.host_events` 
GROUP BY host_id, w 
) 
SELECT cohort_w, DATE_DIFF(w, cohort_w, WEEK) AS wk, 
COUNT(DISTINCT host_id) AS hosts 
FROM firsts JOIN act USING (host_id) 
GROUP BY cohort_w, wk 
ORDER BY cohort_w, wk; 
P7 Longest active streak per guest -- Use 3.3 and then take MAX(days) per user. 
P8 Guests booked but never messaged 
SELECT DISTINCT b.user_id 
FROM `proj.ds.bookings` b 
LEFT JOIN `proj.ds.messages` m USING (user_id) 
WHERE m.user_id IS NULL; 
(For P9–P20, use the matching patterns from Section 3.) 
7) Speed tactics for live rounds 
● Write CTEs in vertical slices. Name CTEs by purpose: filt, agg, rnk. 
● Enforce determinism in every ORDER BY used for ranking. 
● Develop with a narrow date filter, then widen. Keep the filter commented. 
● Print sample rows for sanity: ORDER BY ... LIMIT 10. 
● Prefer QUALIFY over re‑wrapping ranked CTEs when simple. 
8) Debugging checklist 
● Row count growth across CTEs as expected? Any blowups after joins? 
● Keys unique at the correct grain before joining? If not, pre‑agg. 
● Off‑by‑one dates? Inclusive/exclusive windows consistent? 
● NULL‑safety: SAFE_ casts, COALESCE defaults. 
● Time zone alignment: pick one and convert at inputs. 
9) Minimal BigQuery function glossary 
● QUALIFY, GENERATE_DATE_ARRAY, APPROX_QUANTILES, SAFE_CAST 
● DATE_TRUNC, TIMESTAMP_ADD/SUB, DATE_DIFF, IFNULL/COALESCE 
● LAG/LEAD, ROW_NUMBER, DENSE_RANK, COUNTIF 
● RANGE‑based window frames for true time windows 
SQL Performance Tuning : Advanced Techniques with Examples
Code Crafter
Published : 29 July 2025
Updated : 29 July 2025
80 Views
SQLMySQLOptimizationPerformance
SQL performance tuning plays a crucial role in database management by enhancing the speed and efficiency of SQL queries. A well-optimized queries not only run faster but also reduce server load and improve overall application performance.

In this article, we will explore proven techniques, tools and best practices for SQL performance tuning. Whether you're a developer or DBA, mastering these strategies will help you write high-performing queries and ensure your database operates at peak efficiency.

What is SQL Performance Tuning?
SQL performance tuning is the process of optimizing SQL queries to improve execution speed, minimize system resource usage (CPU, memory, disk I/O) and maximize the overall efficiency and scalability of the database system. It is an essential practice for ensuring fast application performance, especially in high-traffic or data-heavy environments.

Unoptimized queries can dramatically slow a database (like lead to full table scans), increase CPU usage and even cause downtime. By improving execution plans, indexing and resource usage, DBAs keep systems responsive and scalable as data volumes grow.

Why SQL Query Optimization Matters
Optimizing SQL queries isn't just a best practice. it's essential for building fast, reliable and scalable database systems. Here's some points why it matters

✅ Reduces Query Execution Time and Improves Performance : Speeds up response time by eliminating unnecessary computations and improving data access paths.
✅ Minimizes Resource Consumption: Lowers CPU usage, memory load and I/O overhead, which helps maintain server health and performance.
✅ Improves Concurrency and High Availability: Enables smooth operation for multiple users by reducing lock contention and resource bottlenecks.
✅ Boosts Throughput: Enhances performance in both OLTP (Online Transaction Processing) and OLAP (Online Analytical Processing) workloads.
✅ Prevents Downtime and SLA Violations: Slow queries can lead to timeout errors, degraded performance or even downtime. Ensures your production systems remain stable, responsive and compliant with service level agreements.
✅ Improves Developer Productivity : Developers spend less time debugging or rewriting slow queries and easier maintenance and debugging of well-structured queries.
✅ Better User Experience : End-users experience faster load times, especially in data-intensive applications (e.g., reporting dashboards, analytics portals).
✅ Enables Complex Business Logic : Complex reports and analytics are only feasible if underlying queries are well-optimized.
Key Factors Affecting SQL Query Performance
Several factors can significantly influence how efficiently your SQL queries run. Understanding these helps in identifying performance bottlenecks and applying the right optimizations. Some of the major factors that influence the computation and execution time of query in SQL are

Factor	Impact on Performance
Table Size	Large tables increase scan times and slow down joins, especially without proper indexing or partitioning.
Joins	Complex or unindexed joins can cause heavy CPU usage and I/O, leading to major performance issues.
Aggregations	Performing aggregations (e.g., SUM(), COUNT(), GROUP BY) on large datasets without optimization leads to high processing time.
Concurrency	Multiple users accessing the same data concurrently can cause contention, blocking or deadlocks if not managed properly.
Indexes	Missing or poorly maintained indexes can severely degrade query speed on the flip side, too many indexes can slow down writes.
Query Plan	An inefficient execution plan may result in full table scans, nested loops or other costly operations that hurt performance.
How to Identify Slow SQL Queries
Identifying slow or poorly performing SQL queries is the first step in database performance tuning. Below are effective techniques used by developers and DBAs to diagnose bottlenecks and take corrective action.

1. Analyze SQL Query Execution Plan
Execution plans (a.k.a. query plans) show how the SQL engine processes your query.

1. Using Query
You can run below query to see execution plan of your query

-- SQL Server
SET STATISTICS PROFILE ON;
GO
SELECT * FROM Orders WHERE CustomerID = 123;
GO
SET STATISTICS PROFILE OFF;

-- MySQL
EXPLAIN SELECT * FROM Orders WHERE CustomerID = 123;
2. Using SQL Server Management Studio (SSMS)
It provides a graphical breakdown of how SQL Server executes a query, including operations like index seeks, table scans, joins and sorts.

Steps to View Execution Plan
Open SQL Server Management Studio (SSMS)
Write your query
Click on "Include Actual Execution Plan" or press Ctrl + M
Run the query (F5)
Examine the execution steps for inefficiencies
What to Look For
Table Scan – Indicates a missing index or unfiltered query.
Index Seek – A sign of efficient index usage
Nested Loops vs. Hash Join – Evaluate if the join method is optimal for the data size
2. Monitoring System Resources
Monitoring system metrics helps identify performance bottlenecks caused by hardware limitations or excessive query loads.

Tools like Windows Performance Monitor, SQL Server Activity Monitor or SysInternals help you identify CPU spikes, Memory pressure, Disk I/O bottlenecks, Blocking sessions etc.

Tip : Monitor Batch Requests/sec, Page Life Expectancy and SQL Compilations/sec.

3. Using DMVs (Dynamic Management Views)
DMVs give direct insight into query execution statistics, cached plans, CPU usage and I/O patterns.

Query Syntax Example : Helpful DMV Query to Spot Slow Queries

SELECT TOP 10
    qs.total_elapsed_time / qs.execution_count AS AvgElapsedTime,
    qs.execution_count,
    qs.total_worker_time,
    qt.text
FROM sys.dm_exec_query_stats qs
CROSS APPLY sys.dm_exec_sql_text(qs.sql_handle) qt
ORDER BY AvgElapsedTime DESC;
What This Shows
Average execution time
Query execution frequency
CPU time usage
Query text for slowest queries
4. Monitor Slow Query Logs
Most modern relational databases offer built-in mechanisms to log queries that exceed a defined execution time. These slow query logs are essential for pinpointing expensive queries in production environments.

MySQL
Enable the slow query log and define a time threshold to log queries that exceed the limit.

-- Enable slow query logging
SET GLOBAL slow_query_log = 'ON';

-- Log queries taking longer than 1 second
SET GLOBAL long_query_time = 1;
Note : Logs are typically stored in a file like /var/log/mysql/mysql-slow.log or /var/lib/mysql/hostname-slow.log, which can be analyzed manually or with tools like pt-query-digest.

How to Check Current Log Path
SHOW VARIABLES LIKE 'slow_query_log_file';
PostgreSQL
PostgreSQL logs slow statements using the log_min_duration_statement setting

-- Log queries taking longer than 1000 milliseconds (1 second)
SET log_min_duration_statement = 1000;
You can set this in postgresql.conf or on a per-session basis. The logs will appear in the standard PostgreSQL log file.

5. Query Performance Tools
Modern relational databases come with built-in utilities or support third-party tools that help monitor and analyze SQL query performance. These tools provide critical insights into execution time, resource usage, indexing issues and more.

Popular Tools by Database System
DBMS	Query Performance Tools
SQL Server	- Query Store: Tracks historical query performance and plan changes
- SQL Profiler: Captures real-time query events and durations
- Extended Events: Lightweight event monitoring for detailed diagnostics
MySQL	- SHOW PROCESSLIST: Displays currently running queries and their status
- Performance Schema: Advanced monitoring of query execution, waits, and resource usage
PostgreSQL	- pg_stat_statements: Tracks execution stats like call count, total time, etc.
- auto_explain: Logs query plans for slow queries automatically
Oracle	- AWR Reports (Automatic Workload Repository): Historical performance analysis
- SQL Trace & TKPROF: Detailed tracing of SQL statements and execution
Tip : Combine these tools with slow query logs and execution plans to get a complete picture of performance bottlenecks.

6. Check Wait Stats
Wait statistics reveal where SQL Server is spending time when executing queries , whether it's waiting on CPU, disk I/O, memory, locks or other system resources. Analyzing these stats helps uncover performance bottlenecks beyond just slow queries.

SQL Server Example: Get Wait Stats
SELECT * 
FROM sys.dm_os_wait_stats
ORDER BY wait_time_ms DESC;
7. Check Blocking and Locks
Locking and blocking are common causes of poor performance in SQL databases. A single long-running or inefficient query can block other queries, causing timeouts, delays and reduced system concurrency.

Monitoring locks in real time helps identify the root cause of contention and take corrective action.

MySQL: Check Locks and Blocking
Use the InnoDB engine status to examine current locks and blocked transactions

SHOW ENGINE INNODB STATUS\G
Look in the output for sections like
LATEST DETECTED DEADLOCK
TRANSACTIONS
WAITING FOR THIS LOCK TO BE GRANTED
For more structured insights

SELECT * 
FROM performance_schema.data_locks;

SELECT * 
FROM performance_schema.metadata_locks;
SQL Server: View Active Locks
Use the following DMV to list current locks

SELECT * 
FROM sys.dm_tran_locks;
You can also join this with session and request views to see who is blocking whom

SELECT 
    blocking_session_id AS BlockerSession,
    session_id AS BlockedSession,
    wait_type,
    wait_time,
    wait_resource,
    status
FROM sys.dm_exec_requests
WHERE blocking_session_id <> 0;
SQL Query Optimization Techniques (with Examples)
Poorly written or inefficient SQL queries can place a heavy load on your production database, leading to high CPU usage, long response times and even blocking or disconnecting other users. To maintain high performance and scalability, query optimization is essential.

In this section, we will explore a variety of proven SQL performance tuning techniques from indexing strategies to query rewriting. Each technique is supported by practical examples to help you understand how and when to apply them.

These optimizations aim to

Reduce resource consumption (CPU, memory, I/O)
Improve query execution time
Enhance overall database responsiveness and user experience
1. Select Only Required Columns instead of using SELECT *
Using SELECT * retrieves all columns from a table regardless of whether you need them or not. This can result in increased memory usage, longer execution times and higher network traffic, especially for large tables or high-concurrency systems.

Inefficient
SELECT * FROM Orders;
Optimized
SELECT OrderID, CustomerName, OrderDate FROM Orders;
✅ Benefits of Selecting Specific Columns
Reduces processing overhead on both the server and client side.
Improves performance by enabling more efficient query plans.
Enhances maintainability and avoids potential issues when table structure changes.
Avoid exposing sensitive or unintended data to the response that was not meant to be served intentionally.
Fetch only required fields from the database to minimize memory consumption and improve performance.
Supports covering indexes, which can eliminate the need to access the full table.
Note : SELECT * can also break your app if the table structure changes (e.g., column added), so being explicit is safer and more maintainable.

2. Avoid SELECT DISTINCT
SELECT DISTINCT removes duplicate rows from the result set after retrieving all the data. It performs a sort or hashing operation to identify duplicates and both operations are CPU and memory-intensive, especially on large datasets.

Inefficient Query Example
SELECT DISTINCT FirstName, LastName, Mobile FROM Persons;
This forces the database to compare every row to every other row to find duplicates.
Costly in terms of CPU, memory and execution time.
Better Alternatives
1. Refine the WHERE clause to reduce duplication at the source:

SELECT FirstName, LastName FROM Persons WHERE Mobile IS NOT NULL;
Filters out rows that cause redundancy (in this case, NULL values).
Often reduces the result set before expensive de-duplication is needed.
2. Use GROUP BY if it makes logical sense

SELECT Mobile FROM Persons GROUP BY Mobile;
Faster than DISTINCT because it groups on indexed fields and can utilize aggregation logic efficiently.

Note
Use DISTINCT only when required, since it can negatively impact performance by forcing additional sorting or deduplication operations
If possible, restructure the query to avoid DISTINCT, especially on large datasets.
3. Use Explicit JOINs Instead of WHERE Clause Joins
Many developers still use the old-style join syntax by listing multiple tables in the FROM clause and applying join conditions in the WHERE clause. This approach is not only harder to read but can also lead to inefficient queries and even accidental Cartesian products if join conditions are missed.

Example – Inefficient (Comma Join / Implicit Join)
SELECT Orders.CustomerId, Customers.Name, Customers.Mobile
FROM Orders, Customers
WHERE Orders.CustomerId = Customers.CustomerId;
Solution: Use Explicit JOIN Syntax (INNER, LEFT, RIGHT etc.)
Modern SQL standards and best practices encourage using explicit JOIN syntax, which clearly defines the relationship between tables and improves readability, maintainability and performance.

Example – Efficient (Explicit INNER JOIN)
SELECT Orders.CustomerId, Customers.Name, Customers.Mobile
FROM Orders
INNER JOIN Customers ON Orders.CustomerID = Customers.CustomerID;
Tip : Always use explicit JOIN syntax for clarity, flexibility and performance. It’s a modern best practice that prevents mistakes, supports complex joins and helps the database engine optimize your queries more effectively.

4. Use WHERE Instead of HAVING for Pre-Aggregation Filtering
The HAVING clause is designed to filter groups after aggregation. However, Some developers often misuse it to apply filters that could be handled more efficiently by the WHERE clause. This mistake can increase processing time because filtering happens after all rows have been grouped.

Example – Inefficient (Using HAVING instead of WHERE)
SELECT COUNT(*) 
FROM Orders 
GROUP BY Region 
HAVING Region = 'East';
Solution: Use WHERE for Filtering Before Aggregation
The WHERE clause filters rows before the GROUP BY and aggregation steps. This reduces the number of rows processed and improving query performance.

Example – Efficient (Filter First with WHERE)
SELECT COUNT(*) 
FROM Orders 
WHERE Region = 'East' 
GROUP BY Region;
Real-World Example: Filter Sales by Year
Let’s say you want to count customers who had sales in 2025. Filtering with WHERE instead of HAVING results in a much more efficient query:

Inefficient
SELECT C.CustomerId, C.Name, MAX(O.LastSaleDate)
FROM Customers C
INNER JOIN Orders O ON C.CustomerId = O.CustomerId
GROUP BY C.CustomerId, C.Name
HAVING MAX(O.LastSaleDate) BETWEEN '2025-01-01' AND '2025-12-31';
Efficient
SELECT C.CustomerId, C.Name, MAX(O.LastSaleDate)
FROM Customers C
INNER JOIN Orders O ON C.CustomerId = O.CustomerId
WHERE O.LastSaleDate BETWEEN '2025-01-01' AND '2025-12-31'
GROUP BY C.CustomerId, C.Name;
✅ Tip : Use WHERE to filter data before aggregation and reserve HAVING for filtering on aggregate values like SUM(), COUNT(), or AVG()

5. Optimize LIKE and Wildcard Searches
Using the % wildcard at the beginning of a search pattern ('%value') disables index usage, forcing SQL to perform a full table scan. This leads to slower query performance, especially on large datasets.

Inefficient
-- SQL must scan all rows
SELECT * 
FROM Customers 
WHERE Name LIKE '%son%';
This query cannot use an index on the Name column effectively because the search term starts with %.

Efficient
-- SQL can use index on Name
SELECT * 
FROM Customers 
WHERE Name LIKE 'John%';
By placing the wildcard only at the end, SQL can utilize any available index on the Name column, speeding up the search significantly.

Leading Wildcard Blocks Index Use
Suffix Wildcard Allows Index Use
Use LIKE 'value%' to retain index optimization.
Avoid % at the start of the pattern unless full-text indexing is in place.
Consider Full-Text Search or inverted indexes for complex substring searches.
6. Use LIMIT or TOP for Sampling Query Results
When working with large datasets, full-table queries can be inefficient and potentially risky. To improve performance and reduce load, use LIMIT (or TOP in some SQL dialects) to retrieve a manageable subset of rows. which is extremely useful during

Query debugging or optimization
Data analysis
Paginating large result sets
Preventing accidental database strain
Limits rows returned → reduces memory and I/O
Prevents production risks → avoids scanning millions of rows by accident
Best Practice Example
SELECT Orders.CustomerId, Customers .Name, MAX(Orders.LastSaleDate)
FROM Orders
INNER JOIN Customers ON Orders.CustomerId = Customers .CustomerId
WHERE Orders.LastSaleDate BETWEEN '2025-01-01' AND '2025-12-31'
GROUP BY Orders.CustomerId, Customers .Name
LIMIT 10;
Benefits
Fetch a sample of n records(on limit n) matching specific conditions without querying the entire dataset.
Minimizes resource consumption: Limits memory, CPU and disk I/O.
Helps preview data structure: Quickly inspect output during development.
Avoid stressing the production database with a big query.
Speeds up development cycles: Using LIMIT (or TOP) helps developers iterate and debug SQL queries much faster.
7. Schedule Heavy Queries During Off-Peak Hours
Running resource-intensive queries, such as ETL processes, batch updates, analytical reports or large joins during peak business hours can negatively impact overall system performance. To minimize disruption, schedule these operations during off-peak hours (e.g., late at night or early morning) when system usage is low.

Best Practice
Monitor system activity to identify the best off-peak window.
Use SQL Agent Jobs (SQL Server), cron jobs (Linux-based systems) or database scheduler tools to automate execution on during Off-Peak Hours.
Avoid running multiple heavy tasks simultaneously, even during off-hours.
Batch or defer analytics when possible.
Example Use Cases
Daily report generation
Backfilling historical data
Data warehouse refresh or sync
Index maintenance or rebuild
8. Index Tuning Techniques for Optimal SQL Performance
Indexes are powerful tools for speeding up SQL query execution but ineffective or excessive indexing can degrade performance, especially for write-heavy workloads. Proper index tuning ensures fast data access while keeping storage and maintenance costs in check.

Indexing Best Practices
Technique	What It Does	Why It’s Important	Best For
Covering Index	Includes all columns used in the query	Avoids table access entirely; boosts read performance	Read-heavy queries with predictable patterns
Index Key Columns in WHERE / JOIN / ORDER BY	Speeds up search, joins, and sorting	Enables index seeks instead of table scans	Filtering, joining, sorting, aggregating
Avoid Over-Indexing	Limits excessive indexing	Reduces write overhead, improves maintainability, and saves storage	Write-heavy tables; large-scale systems
Filtered (Partial) Indexes	Indexes only a subset of rows	Optimizes queries on sparse/highly selective data; reduces index size	Columns with nulls, status flags, active rows
Rebuild or Reorganize Indexes	Fixes index fragmentation	Restores query performance degraded by frequent updates or deletes	Long-running databases; OLTP systems
What Is Index Tuning?
Index tuning is the process of strategically analyzing, designing and optimizing indexes to improve SQL query performance and reduce database overhead.

It’s a critical part of SQL performance tuning that ensures your indexes are helping, not hurting, your workload efficiency. It involves

Selecting appropriate indexes for frequent and expensive queries.
Modifying or removing unused indexes to reduce overhead.
Monitoring usage patterns to continuously adjust strategy.
Advantages of Proper Index Tuning in SQL
Effective index tuning can significantly transform your database performance. When done correctly, it optimizes how data is accessed and retrieved, especially under high-load or read-intensive environments.

Index tuning improves query and database performance by identifying how indexes can best support your actual query workload. It does this through

Improved Query Execution Speed
Index tuning significantly reduces the execution time of frequent and expensive queries by enabling proper indexing to the database. This enables database engine to use index seeks instead of full table scans, which can cut query execution time from seconds (or minutes) down to milliseconds.

Reduced CPU, Memory and Disk I/O Load
Well-tuned indexes help the query engine to avoid unnecessary reads and memory usage, Perform fewer CPU-intensive operations (like sorting or hashing) and Conserve disk access bandwidth.

Better Application Responsiveness
If query performance improves, so API response time, UI rendering speed and Report generation time also will improve. Proper index tuning boosts end-to-end performance across the stack.

Efficient Page Retrieval
Indexes help fetch only relevant rows, improving performance in filters, joins and sorting.

Intelligent Index Recommendations
Leverages the query optimizer and historical workload analysis to recommend the most efficient indexes for improving query performance.

Activity-Based Insights
Uses tools like SQL Profiler to record live database activity, helping uncover index usage trends and bottlenecks.

Reduced Trial-and-Error
Automated suggestions reduce the need for manual guesswork in performance tuning.

Top SQL Performance Tuning Tools
Here’s a breakdown of popular tuning tools that help monitor, analyze and optimize SQL queries

Tool	Vendor	Key Features
SQL Sentry	SolarWinds	Monitors server health, detects blocking/deadlocks, visualizes wait stats.
SQL Profiler	Microsoft	Captures query events, execution plans, and trace logs for in-depth review.
SQL Index Manager	Redgate	Detects index fragmentation, unused indexes, and recommends fixes.
SQL Diagnostic Manager	IDERA	Offers real-time monitoring, tuning advice, alerts, and diagnostics.
Database Tuning Advisor	Microsoft	Analyzes workloads and suggests indexes, partitions, and stats improvements.
EverSQL (Cloud)	EverSQL	AI-based query optimizer with index suggestions and rewrite recommendations.
pgBadger	PostgreSQL	Fast log analyzer for PostgreSQL, with visual reports and query stats.
Performance Schema	MySQL	Built-in MySQL tool for tracking low-level server performance metrics.
How These Tools Help
Capability	Benefit
Query Monitoring	Tracks long-running or resource-intensive queries.
Index Recommendations	Suggests covering, filtered or composite indexes.
Execution Plan Visualization	Helps identify costly operations like table scans or sorts.
Fragmentation Analysis	Detects and fixes fragmented indexes that slow down performance.
Real-Time Alerting	Notifies of blocking, deadlocks or CPU spikes before they affect users.
Workload Replay and Simulation	Allows testing query impact in dev/test environments before production.
Frequently Asked Questions (FAQs)
1. What is index tuning in SQL?
Index tuning is the process of analyzing, creating, modifying or removing indexes to improve the performance of SQL queries. It aims to speed up data retrieval while balancing storage, maintenance and write operation costs.

2. When should I create an index?
Create an index when

A column is frequently used in WHERE, JOIN or ORDER BY clauses.
Queries on a table become slow as data grows.
You need to enforce uniqueness (UNIQUE INDEX).
The same query pattern is executed repeatedly and needs optimization.
3. What is a covering index?
A covering index includes all the columns required by a query, allowing it to be served entirely from the index without accessing the table. This significantly improves performance.

4. What’s the difference between clustered and non-clustered indexes?
Clustered Index: Sorts and stores the data rows in the table based on the key. Only one per table.

Non-Clustered Index: Contains a copy of indexed columns with a reference to the actual data row. A table can have many non-clustered indexes.

5. Can too many indexes hurt performance?
Yes. While indexes improve read performance, they slow down write operations (INSERT, UPDATE, DELETE) and increase storage and maintenance overhead. Always index strategically.

6. How can I find unused indexes in SQL Server?
Use the dynamic management view

SELECT * 
FROM sys.dm_db_index_usage_stats 
WHERE user_seeks = 0 AND user_scans = 0 AND user_lookups = 0;
This identifies indexes that are never read.

7. What is index fragmentation and how does it impact performance?
Fragmentation occurs when the logical order of pages in an index does not match the physical order, leading to inefficient I/O. This slows down query performance. You should reorganize or rebuild fragmented indexes regularly.
SQL Question 1: Average Post Hiatus
Given a table of Facebook posts, for each user who posted at least twice in 2024, write a SQL query to find the number of days between each user’s first post of the year and last post of the year in the year 2024. Output the user and number of the days between each user's first and last post.

You can solve this SQL problem interactively and run your solution directly on DataLemur:

Average Post Hiatus Facebook/Meta SQL interview question

posts Table:
Column Name	Type
user_id	integer
post_id	integer
post_date	timestamp
post_content	text
posts Example Input:
user_id	post_id	post_date	post_content
151652	599415	07/10/2024 12:00:00	Need a hug
661093	624356	07/29/2024 13:00:00	Bed. Class 8-12. Work 12-3. Gym 3-5 or 6. Then class 6-10. Another day that's gonna fly by. I miss my girlfriend
004239	784254	07/04/2024 11:00:00	Happy 4th of July!
661093	442560	07/08/2024 14:00:00	Just going to cry myself to sleep after watching Marley and Me.
151652	111766	07/12/2024 19:00:00	I'm so done with covid - need traveling ASAP!
Example Output:
user_id	days_between
151652	2
661093	21
Answer:
First, we can use the MIN() and MAX() aggregate functions on the post_date column to retrieve the earliest and latest post dates, and substract one from another accordingly.

To calculate the difference for each user, we GROUP the results by user_id, and then filter for posts made in the year 2024. To do so, we use the DATE_PART() function to extract the year from the post_date column.

In the final step, to exclude users who have posted only once during the year, we apply the HAVING clause with a COUNT() condition greater than 1.

This yields the final solution:

SELECT 
	user_id, 
    MAX(post_date::DATE) - MIN(post_date::DATE) AS days_between
FROM posts
WHERE DATE_PART('year', post_date::DATE) = 2024 
GROUP BY user_id
HAVING COUNT(post_id) > 1;
p.s. DataLemur Premium users can attempt part 2 of the problem which is much harder.

SQL Question 2: Facebook Power Users
A Facebook power user is defined as someone who posts a ton, and gets a lot of reactions on their post. For the purpose of this question, consider a Facebook power user as someone who posts at least twice a day and receives an average of 150 comments and/or reactions per post.

Write a SQL query to return the IDs of all Facebook power users, along with the number of posts, and the average number of reactions per post.

Use the following tables "user_post" and "post_interactions":

user_post Example Input:
user_id	post_id	post_date
1	1001	2024-09-01
1	1002	2024-09-01
2	1003	2024-09-02
2	1004	2024-09-03
1	1005	2024-09-02
post_interactions Example Input:
post_id	comments	reactions
1001	75	200
1002	85	250
1004	60	90
1005	100	150
1003	50	70
Answer:
SELECT
    up.user_id,
    COUNT(DISTINCT up.post_id) AS no_of_posts,
    AVG(pi.comments + pi.reactions) AS avg_interaction_per_post
FROM
    user_post up
JOIN
    post_interactions pi on up.post_id = pi.post_id
GROUP BY
    up.user_id
HAVING
    COUNT(DISTINCT up.post_id) >= 2 AND AVG(pi.comments + pi.reactions) >= 150
Grandma Addicted to Social Media & Facebook Power User

SQL QUESTION 3: Can you explain the difference between WHERE and HAVING?
The HAVING clause works similarly to the WHERE clause, but it is used to filter the groups of rows created by the GROUP BY clause rather than the rows of the table themselves.

For example, say you were analyzing Facebook ads data:

SELECT region, SUM(sales)
FROM facebook_ads
WHERE date > '2024-01-01'
GROUP BY region
HAVING SUM(sales) > 500000;
This query retrieves the total sales from all ads in each region, and uses the WHERE clause to only sales made after January 1, 2024. The rows are then grouped by region and the HAVING clause filters the groups to include only those with total sales greater than $500k.

SQL Question 4: Active User Retention
Assume you're given a table containing information on Facebook user actions. Write a SQL query to obtain number of monthly active users (MAUs) in July 2022, including the month in numerical format "1, 2, 3".

Hint: An active user is defined as a user who has performed actions such as 'sign-in', 'like', or 'comment' in both the current month and the previous month.

You can type up + execute your SQL query interactively to this problem on DataLemur:

Active User Retention: Facebook SQL Interview Question

user_actions Table:
Column Name	Type
user_id	integer
event_id	integer
event_type	string ("sign-in, "like", "comment")
event_date	datetime
user_actionsExample Input:
user_id	event_id	event_type	event_date
445	7765	sign-in	06/31/2022 12:00:00
742	6458	sign-in	07/03/2022 12:00:00
445	3634	like	07/05/2022 12:00:00
742	1374	comment	07/05/2022 12:00:00
648	3124	like	07/18/2022 12:00:00
Example Output for June 2022:
month	monthly_active_users
6	1
In July 2022, there was only one monthly active user (MAU) with the user_id 445.

Answer
SELECT 
  EXTRACT(MONTH FROM curr_month.event_date) AS mth, 
  COUNT(DISTINCT curr_month.user_id) AS monthly_active_users 
FROM user_actions AS curr_month
WHERE EXISTS (
  SELECT last_month.user_id 
  FROM user_actions AS last_month
  WHERE last_month.user_id = curr_month.user_id
    AND EXTRACT(MONTH FROM last_month.event_date) =
    EXTRACT(MONTH FROM curr_month.event_date - interval '1 month')
)
  AND EXTRACT(MONTH FROM curr_month.event_date) = 7
  AND EXTRACT(YEAR FROM curr_month.event_date) = 2022
GROUP BY EXTRACT(MONTH FROM curr_month.event_date);
For a full step-by-step explanation of the problem, click here.

SQL QUESTION 5: What's the difference between a left and right join?
In SQL, a join generally retrieves rows from multiple tables and combines them into a single result set. For an example of the difference between a left vs. right join, suppose you had a table of Facebook users and Facebook posts.

A LEFT JOIN retrieves all rows from the left table (in this case, the users table) and any matching rows from the right table (the posts table). If there is no match in the right table, NULL values will be returned for the right table's columns.

A RIGHT JOIN combines all rows from the right table (in this case, the posts table) and any matching rows from the left table (the users table). If there is no match in the left table, NULL values will be displayed for the left table's columns.

Left vs. Right Join

SQL Question 6: Facebook Friend Recommendations
Facebook wants to recommend new friends to people who show interest in attending 2 or more of the same private Facebook events.

Notes:

A user interested in attending would have either 'going' or 'maybe' as their attendance status.
Friend recommendations are unidirectional, meaning if user x and user y should be recommended to each other, the result table should have both user x recommended to user y and user y recommended to user x.
The result should not contain duplicates (i.e., user y should not be recommended to user x multiple times).
friendship_status Table:
Column Name	Type
user_a_id	integer
user_b_id	integer
status	enum ('friends', 'not_friends')
Each row of this table indicates the status of the friendship between user_a_id and user_b_id.

friendship_status Example Input:
user_a_id	user_b_id	status
111	333	not_friends
222	333	not_friends
333	222	not_friends
222	111	friends
111	222	friends
333	111	not_friends
event_rsvp Table:
Column Name	Type
user_id	integer
event_id	integer
event_type	enum ('public', 'private')
attendance_status	enum ('going', 'maybe', 'not_going')
event_date	date
event_rsvp Example Input:
user_id	event_id	event_type	attendance_status	event_date
111	567	public	going	07/12/2022
222	789	private	going	07/15/2022
333	789	private	maybe	07/15/2022
111	234	private	not_going	07/18/2022
222	234	private	going	07/18/2022
333	234	private	going	07/18/2022
Example Output:
user_a_id	user_b_id
222	333
333	222
Users 222 and 333 who are not friends have shown interest in attending 2 or more of the same private events.

Answer:
To find pairs of friends to be recommended to each other if they're interested in attending 2 or more of the same private events we'll:

Find users who are interested in attending private events.
Join tables to compare the correct data
Find pairs of users who are not friends but are interested in 2 or more of the same private events.
This leads to the following query:

WITH private_events AS (
SELECT user_id, event_id
FROM event_rsvp
WHERE attendance_status IN ('going', 'maybe')
  AND event_type = 'private'
)

SELECT 
  friends.user_a_id, 
  friends.user_b_id
FROM private_events AS events_1
INNER JOIN private_events AS events_2
  ON events_1.user_id != events_2.user_id
  AND events_1.event_id = events_2.event_id
INNER JOIN friendship_status AS friends
  ON events_1.user_id = friends.user_a_id
  AND events_2.user_id = friends.user_b_id
WHERE friends.status = 'not_friends'
GROUP BY friends.user_a_id, friends.user_b_id
HAVING COUNT(*) >= 2;
For a full step-by-step solution, and to run the code yourself, subscribe to DataLemur premium to unlock this problem:

Facebook Friends Recommendation SQL Interview Question

Also, if you're a nerd for Machine Learning, you can learn more about Facebook's People-You-May-Know (PYMK) algorithm here.

SQL QUESTION 7: Can you explain the concept of a constraint in SQL?
Constraints are just rules for your DBMS to follow when updating/inserting/deleting data.

Say you had a table of Facebook employees, and their salaries, job titles, and performance review data. Here's some examples of SQL constraints you could implement:

NOT NULL: This constraint could be used to ensure that certain columns in the employee table, such as the employee's first and last name, cannot contain NULL values.

UNIQUE: This constraint could be used to ensure that the employee ID is unique. This would prevent duplicate entries in the employee table.

PRIMARY KEY: This constraint could be used to combine the NOT NULL and UNIQUE constraints to create a primary key for the employee table. The employee ID could serve as the primary key.

FOREIGN KEY: This constraint could be used to establish relationships between the employee table and other tables in the database. For example, you could use a foreign key to link the employee ID to the department ID in a department table to track which department each employee belongs to.

CHECK: This constraint could be used to ensure that certain data meets specific conditions. For example, you could use a CHECK constraint to ensure that salary values are always positive numbers.

DEFAULT: This constraint could be used to specify default values for certain columns. For example, you could use a DEFAULT constraint to set the employee hire date to the current date if no value is provided when a new employee is added to the database.

SQL Question 8: Average Number of Shares per Post
As a data analyst at Facebook, you are asked to find the average number of shares per post for each user.

In the user_posts table, each row represents a post by a user. Each user may have zero or more posts.

In the post_shares table, each row represents a share of a post. Each post may have zero or more shares.

Please write a SQL query to find the average number of shares per post for each user.

user_posts Example Input:
post_id	user_id	post_text	post_date
1	1	Hello world!	06/08/2022 00:00:00
2	2	What a beautiful day!	06/10/2022 00:00:00
3	1	Hope everyone is having a good day!	06/18/2022 00:00:00
4	3	Facebook is amazing!	07/26/2022 00:00:00
5	2	Enjoying a great meal!	07/05/2022 00:00:00
post_shares Example Input:
share_id	post_id	share_date
1	1	06/09/2022 00:00:00
2	2	06/11/2022 00:00:00
3	1	06/19/2022 00:00:00
4	1	06/29/2022 00:00:00
5	3	07/27/2022 00:00:00
Example Output:
user_id	avg_shares_per_post
1	1.67
2	0.50
3	0.00
Answer:
SELECT 
    U.user_id,
    COALESCE(AVG(S.shares), 0) as avg_shares_per_post
FROM 
    user_posts as U
    LEFT JOIN (
        SELECT 
            post_id, 
            COUNT(share_id) as shares
        FROM 
            post_shares
        GROUP BY 
            post_id
        ) as S on U.post_id = S.post_id
GROUP BY 
    U.user_id;
This query works by first finding the total number of shares for each post by grouping on post_id in the post_shares table. Then, it joins this with user_posts on post_id to get the user_id of the user who made each post. It finally averages the total number of shares for each user using the AVG function. Null values are replaced with zero so that users who have no shares do not return null for avg_shares_per_post.

In case this marketing problem was interesting, checkout how this blog on how marketing analysts use SQL.

SQL Question 9: Calculate Facebook Ad Click-Through Rate
Assume you have an events table on Facebook app analytics. Write a SQL query to calculate the click-through rate (CTR) for the app in 2022 and round the results to 2 decimal places.

Definition and note:

Percentage of click-through rate (CTR) = 100.0 * Number of clicks / Number of impressions
To avoid integer division, multiply the CTR by 100.0, not 100.
Before reading our solution, practice this Meta SQL question interactively:

Meta SQL interview question

events Table:
Column Name	Type
app_id	integer
event_type	string
timestamp	datetime
events Example Input:
app_id	event_type	timestamp
123	impression	07/18/2022 11:36:12
123	impression	07/18/2022 11:37:12
123	click	07/18/2022 11:37:42
234	impression	07/18/2022 14:15:12
234	click	07/18/2022 14:16:12
Example Output:
app_id	ctr
123	50.00
234	100.00
Answer:
We first find the number of clicks and impressions using the CASE statement to assign a value of 1 for 'click' events and 0 for other events.

Then, to calculate the percentage of click-through rate (CTR) we divide the number of clicks by the number of impressions and multiplying by 100.0, rounded to 2 decimal places using the ROUND() function.

This yields the following solution:

SELECT
  app_id,
  ROUND(100.0 *
    SUM(CASE WHEN event_type = 'click' THEN 1 ELSE 0 END) /
    SUM(CASE WHEN event_type = 'impression' THEN 1 ELSE 0 END), 2)  AS ctr_rate
FROM events
WHERE timestamp >= '2022-01-01' 
  AND timestamp < '2023-01-01'
GROUP BY app_id;
SQL Theory 
I. Fundamentals of SQL 
SQL (Structured Query Language) is a standard, domain-specific programming language for 
managing data in a relational database management system (RDBMS). Unlike imperative 
languages (e.g., Python), SQL is declarative, focusing on what data to retrieve rather than how. 
It is a core skill for a wide range of data-focused roles, including Data Analyst, Data Scientist, 
Data Engineer, and Business Intelligence Engineer. At a company like Airbnb, SQL is used daily 
for tasks like analyzing customer behavior to improve property recommendations and 
monitoring system performance. 
A. SQL Dialects and Big Data Engines 
While standard SQL provides the foundation, different database systems have their own 
implementations or "dialects". It is important to be aware of the specific dialect used by a 
potential employer, as syntax for certain functions can vary. 
● Common Dialects: T-SQL (Microsoft SQL Server), PL/SQL (Oracle), MySQL, and 
PostgreSQL are some of the most common. 
● Big Data SQL Engines (Used at Airbnb): For handling massive datasets, companies 
like Airbnb use specialized query engines like Hive and Presto. While their syntax is 
very similar to standard SQL, being aware of them is beneficial for interviews at such 
companies. 
B. Relational vs. Non-Relational Databases 
● Relational Databases (SQL): These databases store data in structured tables with 
rows and columns, enforcing data consistency through a predefined schema and ACID 
properties (Atomicity, Consistency, Isolation, and Durability). They are ideal for 
structured data and complex queries. 
● Non-Relational Databases (NoSQL): These offer more flexibility for unstructured or 
evolving data, using various models like document, key-value, and graph formats. 
While highly scalable, they may not guarantee full ACID compliance. 
C. Subsets of SQL Commands 
SQL commands are grouped into functional subsets: 
● Data Definition Language (DDL): Defines the database structure. Commands include 
CREATE, ALTER, DROP, TRUNCATE. 
● Data Manipulation Language (DML): Accesses and manipulates data. Commands 
include INSERT, UPDATE, DELETE. 
● Data Control Language (DCL): Manages user access. Commands include GRANT, 
REVOKE. 
● Transaction Control Language (TCL): Manages transactions. Commands include 
COMMIT, ROLLBACK, SAVEPOINT. 
● Data Query Language (DQL): Retrieves data. The primary command is SELECT. 
II. Core Commands and Clauses for 
Querying Data 
A solid mental model of the logical order in which SQL clauses are processed is fundamental 
to writing correct and efficient queries. The logical processing order is: FROM/JOIN, WHERE, 
GROUP BY, HAVING, SELECT, DISTINCT, ORDER BY, and finally LIMIT/OFFSET. 
● SELECT: Retrieves data from tables. It is best practice to name specific columns rather 
than using SELECT * to improve performance and clarity. 
● DISTINCT: Eliminates duplicate rows from the result set. 
● FROM: Specifies the source table(s). 
● WHERE: Filters individual rows before any grouping occurs. 
● GROUP BY: Groups rows with the same values into summary rows, often used with 
aggregate functions. 
● HAVING: Filters groups after aggregation has occurred. The difference between WHERE 
and HAVING is a very common interview question. 
● ORDER BY: Sorts the final result set in ascending (ASC) or descending (DESC) order. 
● CASE: Provides if-then-else logic, useful for creating new categories or conditional 
aggregations. 
III. Joining and Combining Data 
JOIN clauses are considered one of the most critical topics in SQL interviews across all 
experience levels. 
● Common JOIN Types: 
○ INNER JOIN: Returns records with matching values in both tables. 
○ LEFT JOIN (or LEFT OUTER JOIN): Returns all records from the left table and 
matched records from the right. This is useful for finding entities that may not 
have a corresponding entry in another table (an "anti-join" pattern when 
combined with WHERE right_table.key IS NULL). There's no functional 
difference between LEFT JOIN and LEFT OUTER JOIN. 
○ RIGHT JOIN: The inverse of a LEFT JOIN. Many developers avoid it by 
reordering the tables and using a LEFT JOIN instead. 
○ FULL OUTER JOIN: Returns all records when there is a match in either table. 
○ CROSS JOIN: Returns the Cartesian product of two tables. 
● SELF JOIN: A regular join where a table is joined to itself. This is a common pattern 
tested in interviews, especially for querying hierarchical data (e.g., employees and their 
managers) or analyzing pairs of items. 
● UNION vs. UNION ALL: Combines result sets from multiple SELECT statements. UNION 
removes duplicates, making it slower, while UNION ALL includes all rows and is more 
performant. 
IV. Advanced SQL Concepts 
For senior roles, interviewers expect proficiency in features that enable sophisticated analysis, 
with window functions and CTEs being the most frequently cited advanced topics. 
● Window Functions: These perform calculations across a set of rows related to the 
current row without collapsing them like GROUP BY does. They are crucial for ranking, 
running totals, and period-over-period analysis. 
○ Ranking Functions: A critical interview topic is the difference between RANK(), 
DENSE_RANK(), and ROW_NUMBER(). 
■ ROW_NUMBER() assigns a unique number to each row. 
■ RANK() assigns the same rank to ties but leaves gaps in the sequence. 
■ DENSE_RANK() assigns the same rank to ties without leaving gaps, 
which is often preferred for "Top N per group" problems. 
○ Navigation Functions: LAG() and LEAD() access data from previous or 
subsequent rows, respectively. They are invaluable for calculating 
period-over-period changes or for sessionization tasks. 
● Common Table Expressions (CTEs): A named, temporary result set created using the 
WITH clause. CTEs are often preferred over subqueries because they break down 
complex logic into readable, sequential steps. 
○ Recursive CTEs: A powerful feature for querying hierarchical data, such as 
organizational charts. 
● Subqueries (Nested Queries): A query nested inside another. They can be 
non-correlated (independent) or correlated (dependent on the outer query, executing 
once for each outer row). While powerful, they can often be rewritten as more readable 
CTEs or more efficient JOINs. 
V. Database Design and Data 
Integrity 
Understanding database structure is crucial, especially for senior roles. 
● Keys and Constraints: 
○ Primary Key: Uniquely identifies each record in a table; cannot be NULL. 
○ Foreign Key: Links two tables together, ensuring referential integrity. 
○ Unique Key: Ensures all values in a column are different, but can allow NULL 
values. 
● Normalization: The process of organizing data to reduce redundancy and improve data 
integrity, commonly up to Third Normal Form (3NF). 
● Denormalization: The intentional addition of redundancy to improve read performance 
by reducing the need for joins. It is often used in data warehousing (OLAP systems) 
where query speed is critical. 
VI. Performance and Optimization 
Senior-level interviews often include questions on how to diagnose and fix slow-running 
queries. 
● Indexes: The primary tool for improving query performance. A clustered index 
determines the physical order of data (only one per table), while a non-clustered index 
is a separate lookup structure (multiple allowed per table). 
● Query Optimization Techniques: 
○ Analyze the Execution Plan: Use EXPLAIN to understand how the database 
executes a query and find bottlenecks. 
○ Filter Early: Use the WHERE clause effectively to reduce the dataset size before 
joins or aggregations. 
○ Avoid SELECT *: Only retrieve the columns you need. 
○ Prefer UNION ALL over UNION if duplicates are acceptable, as it avoids a costly 
deduplication step. 
VII. SQL in Practice at Airbnb 
At Airbnb, SQL is a foundational tool for data-driven decision-making, particularly in 
marketing, product, and operations analytics. 
● Interview Process and Expectations: 
○ The process often includes a technical screen (e.g., on HackerRank or Coderpad) 
followed by an on-site or virtual loop with case studies. 
○ Interviewers often permit looking up syntax, emphasizing conceptual mastery 
and problem-solving over rote memorization. 
○ A key evaluation criterion is your ability to translate ambiguous business 
problems into structured SQL queries and clearly communicate your thought 
process. 
● Common Business Scenarios and Data Schemas: 
○ Interview questions frequently model real-world Airbnb scenarios using tables 
like listings, hosts, guests, bookings, reviews, and search_sessions. 
○ Product & Growth Analysis: Measuring the success of new features (e.g., a new 
search algorithm), analyzing user funnels (like search-to-book conversion), and 
performing cohort analysis to understand user retention are common tasks. 
○ Marketing Analytics: Typical problems include calculating customer acquisition 
cost (CAC), lifetime value (CLV), and marketing attribution to measure campaign 
effectiveness. 
○ Marketplace Analysis: Queries often involve calculating key metrics such as 
occupancy rates, pricing trends, and marketplace supply vs. demand. 
○ User Behavior Analysis: Common tasks include identifying repeat guests, 
f
 inding users with consecutive activity (a "gaps and islands" problem), and 
sessionizing user events. 
● Learning Resources and Practice: 
○ To prepare, sources consistently recommend practicing on platforms with 
real-world problems, such as LeetCode, HackerRank, DataLemur, and 
StrataScratch. For foundational learning, W3Schools and SQLZoo are also 
suggested. 
SQL Theory v2 
Comprehensive SQL Interview Preparation 
Guide 
Table of Contents 
1. SQL Fundamentals 
2. Data Types and Constraints 
3. Query Structure and Clauses 
4. Joins and Relationships 
5. Functions and Operations 
6. Advanced Topics 
7. Interview Questions by Difficulty 
8. Company-Specific Questions 
9. Practice Problems 
10. Performance and Optimization 
SQL Fundamentals 
What is SQL? 
SQL (Structured Query Language) is a standard programming language used to manage and 
manipulate relational databases. It is used for querying, inserting, updating, and deleting data 
in a database. 
SQL Command Categories 
Data Definition Language (DDL) 
● CREATE: Creates database objects (tables, indexes, etc.) 
● ALTER: Modifies existing database objects 
● DROP: Removes database objects 
● TRUNCATE: Removes all data from a table 
Data Manipulation Language (DML) 
● SELECT: Retrieves data from tables 
● INSERT: Adds new data to tables 
● UPDATE: Modifies existing data 
● DELETE: Removes specific data 
Data Control Language (DCL) 
● GRANT: Provides user access privileges 
● REVOKE: Removes user access privileges 
Transaction Control Language (TCL) 
● COMMIT: Saves transaction changes 
● ROLLBACK: Undoes transaction changes 
● SAVEPOINT: Sets transaction savepoints 
Data Types and Constraints 
Primary Data Types 
● CHAR: Fixed-length character strings 
● VARCHAR: Variable-length character strings 
● INT/INTEGER: Whole numbers 
● DECIMAL/NUMERIC: Decimal numbers 
● DATE: Date values 
● DATETIME/TIMESTAMP: Date and time values 
● BOOLEAN: True/false values 
Key Constraints 
Primary Key 
● Uniquely identifies each record in a table 
● Cannot contain NULL values 
● Only one per table 
CREATE TABLE employees ( 
employee_id INT PRIMARY KEY, 
name VARCHAR(100), 
email VARCHAR(100) 
); 
Foreign Key 
● Links two tables together 
● References primary key in another table 
● Maintains referential integrity 
CREATE TABLE orders ( 
order_id INT PRIMARY KEY, 
customer_id INT, 
FOREIGN KEY (customer_id) REFERENCES customers(customer_id) 
); 
Unique Key 
● Ensures all values in column are unique 
● Can accept NULL values 
● Multiple unique keys per table allowed 
Check Constraint 
● Ensures values meet specific conditions 
ALTER TABLE employees ADD CONSTRAINT chk_salary CHECK (salary > 0); 
Not NULL Constraint 
● Ensures column cannot contain NULL values 
Query Structure and Clauses 
Basic SELECT Structure 
SELECT column1, column2, ... 
FROM table_name 
WHERE condition 
GROUP BY column 
HAVING condition 
ORDER BY column ASC/DESC 
LIMIT number; 
Essential Clauses 
WHERE vs HAVING 
● WHERE: Filters rows before grouping 
● HAVING: Filters groups after GROUP BY -- WHERE example 
SELECT department, COUNT(*) 
FROM employees 
WHERE salary > 50000 
GROUP BY department; -- HAVING example 
SELECT department, COUNT(*) 
FROM employees 
GROUP BY department 
HAVING COUNT(*) > 10; 
ORDER BY 
● Sorts result set 
● ASC (ascending) is default 
● DESC for descending order 
GROUP BY 
● Groups rows with same values 
● Used with aggregate functions 
DISTINCT 
● Returns unique values only 
SELECT DISTINCT department FROM employees; 
Joins and Relationships 
Join Types 
INNER JOIN 
Returns only rows with matching values in both tables 
SELECT e.name, d.department_name 
FROM employees e 
INNER JOIN departments d ON e.department_id = d.id; 
LEFT JOIN (LEFT OUTER JOIN) 
Returns all rows from left table and matching rows from right table 
SELECT e.name, d.department_name 
FROM employees e 
LEFT JOIN departments d ON e.department_id = d.id; 
RIGHT JOIN (RIGHT OUTER JOIN) 
Returns all rows from right table and matching rows from left table 
SELECT e.name, d.department_name 
FROM employees e 
RIGHT JOIN departments d ON e.department_id = d.id; 
FULL OUTER JOIN 
Returns all rows from both tables 
SELECT e.name, d.department_name 
FROM employees e 
FULL OUTER JOIN departments d ON e.department_id = d.id; 
CROSS JOIN 
Returns Cartesian product of both tables 
SELECT e.name, d.department_name 
FROM employees e 
CROSS JOIN departments d; 
SELF JOIN 
Joins table to itself 
SELECT e1.name AS Employee, e2.name AS Manager 
FROM employees e1 
JOIN employees e2 ON e1.manager_id = e2.employee_id; 
Relationship Types 
● One-to-One: Single row in one table relates to single row in another 
● One-to-Many: Single row in one table relates to multiple rows in another 
● Many-to-Many: Multiple rows in each table relate to multiple rows in the other 
Functions and Operations 
Aggregate Functions 
● COUNT(): Counts rows 
● SUM(): Sums numeric values 
● AVG(): Calculates average 
● MAX(): Returns maximum value 
● MIN(): Returns minimum value 
SELECT  
COUNT(*) as total_employees, 
AVG(salary) as average_salary, 
MAX(salary) as highest_salary, 
MIN(salary) as lowest_salary 
FROM employees; 
String Functions 
● CONCAT(): Joins strings 
● SUBSTRING(): Extracts part of string 
● UPPER(): Converts to uppercase 
● LOWER(): Converts to lowercase 
● LENGTH(): Returns string length 
● TRIM(): Removes leading/trailing spaces 
Date Functions 
● NOW(): Current date and time 
● CURDATE(): Current date 
● DATEADD(): Adds interval to date 
● DATEDIFF(): Calculates difference between dates 
● EXTRACT(): Extracts part of date 
Mathematical Functions 
● ROUND(): Rounds numbers 
● CEIL(): Rounds up 
● FLOOR(): Rounds down 
● ABS(): Absolute value 
● MOD(): Modulo operation 
Window Functions 
● ROW_NUMBER(): Assigns row numbers 
● RANK(): Assigns ranks (with gaps for ties) 
● DENSE_RANK(): Assigns ranks (without gaps) 
● LAG(): Accesses previous row value 
● LEAD(): Accesses next row value 
SELECT  
name, 
salary, 
ROW_NUMBER() OVER (ORDER BY salary DESC) as row_num, 
RANK() OVER (ORDER BY salary DESC) as rank_num, 
DENSE_RANK() OVER (ORDER BY salary DESC) as dense_rank 
FROM employees; 
Advanced Topics 
Subqueries 
Queries nested inside other queries 
Types of Subqueries 
● Scalar: Returns single value 
● Row: Returns single row 
● Table: Returns multiple rows and columns -- Scalar subquery 
SELECT name FROM employees  
WHERE salary = (SELECT MAX(salary) FROM employees); -- Correlated subquery 
SELECT name FROM employees e1 
WHERE salary > (SELECT AVG(salary) FROM employees e2  
WHERE e1.department = e2.department); 
Common Table Expressions (CTE) 
Temporary result sets for complex queries 
WITH high_earners AS ( 
SELECT name, salary  
FROM employees  
WHERE salary > 80000 
) 
SELECT * FROM high_earners ORDER BY salary DESC; 
Views 
Virtual tables based on query results 
CREATE VIEW employee_summary AS 
SELECT department, COUNT(*) as emp_count, AVG(salary) as avg_salary 
FROM employees 
GROUP BY department; 
Stored Procedures 
Precompiled collections of SQL statements 
CREATE PROCEDURE GetEmployeesByDept(@dept_name VARCHAR(50)) 
AS 
BEGIN 
SELECT * FROM employees WHERE department = @dept_name; 
END; 
Triggers 
Special procedures that execute automatically 
CREATE TRIGGER salary_audit 
AFTER UPDATE ON employees 
FOR EACH ROW 
BEGIN 
INSERT INTO audit_log (employee_id, old_salary, new_salary, change_date) 
VALUES (NEW.employee_id, OLD.salary, NEW.salary, NOW()); 
END; 
Transactions and ACID Properties 
● Atomicity: All operations succeed or all fail 
● Consistency: Database remains in valid state 
● Isolation: Concurrent transactions don't interfere 
● Durability: Committed changes persist 
BEGIN TRANSACTION; 
UPDATE accounts SET balance = balance - 100 WHERE id = 1; 
UPDATE accounts SET balance = balance + 100 WHERE id = 2; 
COMMIT; 
Indexes 
Improve query performance by creating lookup structures -- Single column index 
CREATE INDEX idx_employee_name ON employees(name); -- Composite index 
CREATE INDEX idx_dept_salary ON employees(department, salary); -- Unique index 
CREATE UNIQUE INDEX idx_employee_email ON employees(email); 
Performance and Optimization 
Query Optimization Techniques 
Index Strategy 
● Create indexes on frequently queried columns 
● Use composite indexes for multi-column searches 
● Avoid over-indexing (impacts INSERT/UPDATE performance) 
Query Writing Best Practices 
● Use specific columns instead of SELECT * 
● Filter early with WHERE clauses 
● Use appropriate JOIN types 
● Avoid functions in WHERE clauses on indexed columns -- Bad: Function prevents index usage 
SELECT * FROM orders WHERE YEAR(order_date) = 2023; -- Good: Index can be used 
SELECT * FROM orders WHERE order_date >= '2023-01-01' AND order_date < '2024-01-01'; 
Common Performance Issues 
● N+1 Query Problem: Use JOINs instead of multiple queries 
● Cartesian Products: Ensure proper JOIN conditions 
● Subquery vs JOIN: JOINs often perform better than correlated subqueries 
Monitoring and Analysis 
● Use EXPLAIN plans to analyze query execution 
● Monitor query execution time 
● Identify and optimize slow queries 
● Regular database maintenance (statistics updates, index rebuilds) 
Key Interview Tips 
Technical Preparation 
1. Understand fundamentals: JOINs, subqueries, indexes 
2. Practice complex queries: Window functions, CTEs, recursive queries 
3. Know performance concepts: Execution plans, indexing strategies 
4. Understand database design: Normalization, relationships 
Common Question Types 
1. Query writing: Given requirements, write SQL 
2. Query optimization: Improve slow query performance 
3. Database design: Design schema for given requirements 
4. Troubleshooting: Debug problematic queries 
Best Practices During Interview 
● Ask clarifying questions about data structure and requirements 
● Explain your approach before writing code 
● Consider edge cases and data quality issues 
● Discuss performance implications of your solutions 
● Be familiar with multiple SQL dialects (MySQL, PostgreSQL, SQL Server) 
Conclusion 
This comprehensive guide covers essential SQL concepts for technical interviews. Practice 
regularly with real datasets, understand performance implications, and stay current with 
modern SQL features. Remember that interviews often focus on problem-solving approach as 
much as syntax knowledge. 
SQL tips and tricks
Stand With Ukraine

Ceasefire Now

A (somewhat opinionated) list of SQL tips and tricks that I've picked up over the years.

There's so much you can do with SQL but I've focused on what I find most useful in my day-to-day work as a data analyst and what I wish I had known when I first started writing SQL.

Please note that some of these tips might not be relevant for all RDBMSs.

Table of contents
Formatting/readability
Use a leading comma to separate fields
Use a dummy value in the WHERE clause
Indent your code
Consider CTEs when writing complex queries
Comment your code
Simplify joins with USING
Data wrangling
Anti-joins will return rows from one table that have no match in another table
Use QUALIFY to filter window functions
You can (but shouldn't always) GROUP BY column position
Create a grand total with GROUP BY ROLLUP
Use EXCEPT to find the difference between two tables
Performance
NOT EXISTS is faster than NOT IN if your column allows NULL
Implicit casting will slow down (or break)
Common mistakes
Be aware of how NOT IN behaves with NULL values
Avoid ambiguity when naming calculated fields
Always specify which column belongs to which table
Miscellaneous
Understand the order of execution
Read the documentation (in full)
Use descriptive names for your saved queries
Formatting/readability
Use a leading comma to separate fields
Use a leading comma to separate fields in the SELECT clause rather than a trailing comma.

Clearly defines that this is a new column vs code that's wrapped to multiple lines.

Visual cue to easily identify if the comma is missing or not. Varying line lengths makes it harder to determine.

SELECT
employee_id
, employee_name
, job
, salary
FROM employees
;
Also use a leading AND in the WHERE clause, for the same reasons (following tip demonstrates this).
Use a dummy value in the WHERE clause
Use a dummy value in the WHERE clause so you can easily comment out conditions when testing or tweaking a query.

/*
If I want to comment out the job
condition the following query
will break:
*/
SELECT *
FROM employees
WHERE
--job IN ('Clerk', 'Manager')
AND dept_no != 5
;

/*
With a dummy value there's no issue.
I can comment out all the conditions
and 1=1 will ensure the query still runs:
*/
SELECT *
FROM employees
WHERE 1=1
-- AND job IN ('Clerk', 'Manager')
AND dept_no != 5
;
Indent your code
Indent your code to make it more readable to colleagues and your future self.

Opinions will vary on what this looks like, so be sure to follow your company/team's guidelines or, if that doesn't exist, go with whatever works for you.

You can also use an online formatter like poorsql or a linter like sqlfluff.

SELECT
-- Bad:
vc.video_id
, CASE WHEN meta.GENRE IN ('Drama', 'Comedy') THEN 'Entertainment' ELSE meta.GENRE END as content_type
FROM video_content AS vc
INNER JOIN metadata ON vc.video_id = metadata.video_id
;

-- Good:
SELECT 
vc.video_id
, CASE 
	WHEN meta.GENRE IN ('Drama', 'Comedy') THEN 'Entertainment' 
	ELSE meta.GENRE 
END AS content_type
FROM video_content
INNER JOIN metadata 
	ON video_content.video_id = metadata.video_id
;
Consider CTEs when writing complex queries
For longer than I'd care to admit I would nest inline views, which would lead to queries that were hard to understand, particularly if revisited after a few weeks.

If you find yourself nesting inline views more than 2 or 3 levels deep, consider using common table expressions, which can help you keep your code more organised and readable.

-- Using inline views:
SELECT 
vhs.movie
, vhs.vhs_revenue
, cs.cinema_revenue
FROM 
    (
    SELECT
    movie_id
    , SUM(ticket_sales) AS cinema_revenue
    FROM tickets
    GROUP BY movie_id
    ) AS cs
    INNER JOIN 
        (
        SELECT 
        movie
        , movie_id
        , SUM(revenue) AS vhs_revenue
        FROM blockbuster
        GROUP BY movie, movie_id
        ) AS vhs
        ON cs.movie_id = vhs.movie_id
;

-- Using CTEs:
WITH cinema_sales AS 
    (
        SELECT 
        movie_id
        , SUM(ticket_sales) AS cinema_revenue
        FROM tickets
        GROUP BY movie_id
    ),
    vhs_sales AS
    (
        SELECT 
        movie
        , movie_id
        , SUM(revenue) AS vhs_revenue
        FROM blockbuster
        GROUP BY movie, movie_id
    )
SELECT 
vhs.movie
, vhs.vhs_revenue
, cs.cinema_revenue
FROM cinema_sales AS cs
    INNER JOIN vhs_sales AS vhs
    ON cs.movie_id = vhs.movie_id
;
Comment your code
While in the moment you know why you did something, if you revisit the code weeks, months or years later you might not remember.

In general you should strive to write comments that explain why you did something, not how.

Your colleagues and future self will thank you!

SELECT 
video_content.*
FROM video_content
    LEFT JOIN archive
    ON video_content.video_id = archive.video_id
WHERE 1=1
-- Need to filter out as new CMS cannot process archive video formats:
AND archive.video_id IS NULL
;
Simplify joins with USING
If you're joining using a column with the same name in two tables you can use USING to simplify your join:

-- USING:
SELECT * 
FROM album 
	INNER JOIN artist 
	USING (artistid)

-- Traditional ON clause:
SELECT * 
FROM album 
	INNER JOIN artist 
	ON album.artistid = artist.ArtistId 
The other benefit of USING is that the column in common between the two tables is deduplicated, with only one column returned in the result set.

This means that there is no ambiguity, unlike the following query which would throw a ambiguous column name error as the database would not be sure which column to which you are referring if you are using the ON clause:

SELECT ArtistId -- Which table column?
FROM album
	INNER JOIN artist 
	ON album.artistid = artist.ArtistId
Data wrangling
Anti-joins will return rows from one table that have no match in another table
Use anti-joins when you want to return rows from one table that don't have a match in another table.

For example, you only want video IDs of content that hasn't been archived.

There are multiple ways to do an anti-join:

-- Using a LEFT JOIN:
SELECT 
vc.video_id
FROM video_content AS vc
    LEFT JOIN archive
    ON vc.video_id = archive.video_id
WHERE 1=1
AND archive.video_id IS NULL -- Any rows with no match will have a NULL value.
;

-- Using NOT IN/subquery:
SELECT 
video_id
FROM video_content
WHERE 1=1
AND video_id NOT IN (SELECT video_id FROM archive) -- Be mindful of NULL values.

-- Using NOT EXISTS/correlated subquery:
SELECT 
video_id
FROM video_content AS vc
WHERE 1=1
AND NOT EXISTS (
        SELECT 1
        FROM archive AS a
        WHERE a.video_id = vc.video_id
        )
Note that I advise against using NOT IN - see this tip.

Use QUALIFY to filter window functions
QUALIFY lets you filter the results of a query based on a window function, meaning you don't need to use an inline view to filter your result set and thus reducing the number of lines of code.

For example, if I want to return the top 10 markets per product I can use QUALIFY rather than an inline view:

-- Using QUALIFY:
SELECT 
product
, market
, SUM(revenue) AS market_revenue 
FROM sales
GROUP BY product, market
QUALIFY DENSE_RANK() OVER (PARTITION BY product ORDER BY SUM(revenue) DESC)  <= 10
ORDER BY product, market_revenue
;

-- Without QUALIFY:
SELECT 
product
, market
, market_revenue 
FROM
(
SELECT 
product
, market
, SUM(revenue) AS market_revenue
, DENSE_RANK() OVER (PARTITION BY product ORDER BY SUM(revenue) DESC) AS market_rank
FROM sales
GROUP BY product, market
)
WHERE market_rank  <= 10
ORDER BY product, market_revenue
;
Unfortunately it looks like QUALIFY is only available in the big data warehouses (Snowflake, Amazon Redshift, Google BigQuery) but I had to include this because it's so useful.

You can (but shouldn't always) GROUP BY column position
Instead of using the column name, you can GROUP BY or ORDER BY using the column position.

This can be useful for ad-hoc/one-off queries, but for production code you should always refer to a column by its name.
SELECT 
dept_no
, SUM(salary) AS dept_salary
FROM employees
GROUP BY 1 -- dept_no is the first column in the SELECT clause.
ORDER BY 2 DESC
;
Create a grand total with GROUP BY ROLLUP
Creating a grand total (or sub-totals) is possible thanks to GROUP BY ROLLUP.

For example, if you've aggregated a company's employees salary per department you can use GROUP BY ROLLUP to create a grand total that sums up the aggregated dept_salary column.

SELECT 
COALESCE(dept_no, 'Total') AS dept_no
, SUM(salary) AS dept_salary
FROM employees
GROUP BY ROLLUP(dept_no)
ORDER BY dept_salary -- Be sure to order by this column to ensure the Total appears last/at the bottom of the result set.
;
Use EXCEPT to find the difference between two tables
EXCEPT returns rows from the first query's result set that don't appear in the second query's result set.

/*
Miles Davis will be returned from
this query
*/
SELECT artist_name
FROM artist
WHERE artist_name = 'Miles Davis'
EXCEPT 
SELECT artist_name
FROM artist
WHERE artist_name = 'Nirvana'
;

/*
Nothing will be returned from this
query as 'Miles Davis' appears in
both queries' result sets.
*/
SELECT artist_name
FROM artist
WHERE artist_name = 'Miles Davis'
EXCEPT 
SELECT artist_name
FROM artist
WHERE artist_name = 'Miles Davis'
;
You can also utilise EXCEPT with UNION ALL to verify whether two tables have the same data.

If no rows are returned the tables are identical - otherwise, what's returned are the rows causing the difference:

/* 
The first query will return rows from
employees that aren't present in
department.

The second query will return rows from
department that aren't present in employees.

The UNION ALL will ensure that the
final result set returned combines
all of these rows so you know
which rows are causing the difference.
*/
(
SELECT 
id
, employee_name
FROM employees
EXCEPT 
SELECT 
id
, employee_name
FROM department
)
UNION ALL 
(
SELECT 
id
, employee_name
FROM department
EXCEPT
SELECT 
id
, employee_name
FROM employees
)
;
Performance
NOT EXISTS is faster than NOT IN if your column allows NULL
NOT IN is usually slower than using NOT EXISTS, if the values/column you're comparing against allows NULL.

I've experienced this when using Snowflake and the PostgreSQL Wiki explicitly calls this out:

"...NOT IN (SELECT ...) does not optimize very well."

Aside from being slow, using NOT IN will not work as intended if there is a NULL in the values being compared against - see tip 11.

Why include this tip if NOT IN doesn't work with NULL values anyway?

Well just because a column allows NULL values does not mean there are any NULL values present and if you're working with a table that you cannot alter you'll want to use NOT EXISTS to speed up your query.

Implicit casting will slow down (or break) your query
If you specify a value with a different data type than a column's, your database may automatically (implicitly) convert the value.

For example, let's say the video_id column has a string data type and you specify an integer in the WHERE clause:

SELECT video_name
FROM video_content 
 -- Behind the scenes the database will implicitly attempt to convert the video_id column to an integer:
WHERE video_id = 200050
There's a couple of problems with relying on implicit casting:

An error may be thrown if the implicit conversion isn't possible - for example, if one of the video IDs has a string value of 'abc2000'

*Your query may be slower, due to the additional work of converting each value to the specified data type.

Instead, use the same data type as the column you're operating on (WHERE video_ID = '200050') or, to avoid errors, use a function like TRY_TO_NUMBER that will attempt the conversion but handle any errors:

SELECT video_name
FROM video_content 
 -- This won't result in an error:
WHERE TRY_TO_NUMBER(video_id) = 200050
* Note that this depends on the size of the dataset being operated on.

Common mistakes
Be aware of how NOT IN behaves with NULL values
NOT IN doesn't work if NULL is present in the values being checked against. As NULL represents Unknown the SQL engine can't verify that the value being checked is not present in the list.

Instead use NOT EXISTS.
INSERT INTO departments (id)
VALUES (1), (2), (NULL);

-- Doesn't work due to NULL:
SELECT * 
FROM employees 
WHERE department_id NOT IN (SELECT DISTINCT id from departments)
;

-- Solution.
SELECT * 
FROM employees e
WHERE NOT EXISTS (
    SELECT 1 
    FROM departments d 
    WHERE d.id = e.department_id
)
;
Avoid ambiguity when naming calculated fields
When creating a calculated field, naming it the same as an existing column can lead to unexpected behaviour.

Note Snowflake's documentation on the topic:

"If a GROUP BY clause contains a name that matches both a column name and an alias, then the GROUP BY clause uses the column name."

For example you might expect the following to return 2 rows but what's actually returned is 3 rows:

CREATE TABLE products (
    product VARCHAR(50) NOT NULL,
    revenue INT NOT NULL
)
;

INSERT INTO products (product, revenue)
VALUES 
    ('Shark', 100),
    ('Robot', 150),
    ('Racecar', 90);

SELECT 
LEFT(product, 1) AS product -- Returns the first letter of the product value.
, MAX(revenue) as max_revenue
FROM products
GROUP BY product
;
PRODUCT	MAX_REVENUE
S	100
R	150
R	90
What's happened is that the LEFT function has been applied after the product column has been grouped and aggregation applied.

The solution is to use a unique alias or be more explicit in the GROUP BY clause:

-- Solution option 1:
SELECT 
LEFT(product, 1) AS product_letter
, MAX(revenue) AS max_revenue
FROM products
GROUP BY product_letter
;

-- Solution option 2:
SELECT 
LEFT(product, 1) AS product,
, MAX(revenue) AS max_revenue
FROM products
GROUP BY LEFT(product, 1)
;
Result:

PRODUCT_LETTER	MAX_REVENUE
S	100
R	150
Assigning an alias to a calculated field can also be problematic when it comes to window functions.

In this example the CASE statement is being applied AFTER the window function has executed:

/*
The window function will rank the 'Robot' product as 1 when it should be 3.
*/
SELECT 
product
, CASE product WHEN 'Robot' THEN 0 ELSE revenue END AS revenue
, RANK() OVER (ORDER BY revenue DESC)
FROM products
;
Our earlier solutions apply:

/*
Solution option 1 (note this might not work in all RDBMS, in which case use the other solution):
*/
SELECT 
product
, CASE product WHEN 'Robot' THEN 0 ELSE revenue END AS updated_revenue
, RANK() OVER (ORDER BY updated_revenue DESC)
FROM products
;

-- Solution option 2:
SELECT 
product
, CASE product WHEN 'Robot' THEN 0 ELSE revenue END AS revenue
, RANK() OVER (ORDER BY CASE product WHEN 'Robot' THEN 0 ELSE revenue END DESC)
FROM products
;
My advice - use a unique alias when possible to avoid confusion.

Always specify which column belongs to which table
When you have complex queries with multiple joins, it pays to be able to trace back an issue with a value to its source.

Additionally, your RDBMS might raise an error if two tables share the same column name and you don't specify which column you are using.

SELECT 
vc.video_id
, vc.series_name
, metadata.season
, metadata.episode_number
FROM video_content AS vc 
    INNER JOIN video_metadata AS metadata
    ON vc.video_id = metadata.video_id
;
Miscellaneous
Understand the order of execution
If I had to give one piece of advice to someone learning SQL, it'd be to understand the order of execution (of clauses). It will completely change how you write queries. This blog post is a fantastic resource for learning.

Read the documentation (in full)
Using Snowflake I once needed to return the latest date from a list of columns and so I decided to use GREATEST().

What I didn't realise was that if one of the arguments is NULL then the function returns NULL.

If I'd read the documentation in full I'd have known! In many cases it can take just a minute or less to scan the documentation and it will save you the headache of having to work out why something isn't working the way you expected:

/*
If I'd read the documentation
further I'd also have realised
that my solution to the NULL
problem with GREATEST()... 
*/

SELECT COALESCE(GREATEST(signup_date, consumption_date), signup_date, consumption_date);

/*
... could have been solved with the
following function:
*/
SELECT GREATEST_IGNORE_NULLS(signup_date, consumption_date);
Use descriptive names for your saved queries
There's almost nothing worse than not being able to find a query you need to re-run/refer back to.

Use a descriptive name when saving your queries so you can easily find what you're looking for.

I usually will write the subject of the query, the month the query was ran and the name of the requester (if they exist). For example: Lapsed users analysis - 2023-09-01 - Olivia Roberts
SQL
Basics
DML vs DDL vs DQL

Query Processing Order
# a complete SQL query
SELECT DISTINCT column, AGG_FUNC(column_or_expression), ...
FROM mytable
    JOIN another_table
      ON mytable.column = another_table.column
    WHERE constraint_expression
    GROUP BY column
    HAVING constraint_expression
    ORDER BY column ASC/DESC
    LIMIT count OFFSET COUNT;
FROM/JOIN

WHERE

GROUP BY

HAVING

WINDOW Functions

SELECT

DISTINCT

ORDER BY

LIMIT/OFFSET

REFERENCE

REFERENCE

Window Functions
Note that window functions are performed on the result set after all JOIN, WHERE, GROUP BY, and HAVING clauses and before the ORDER BY, LIMIT and SELECT DISTINCT.

CTE vs Temporary Tables
Stack Overflow Answer

CTEs

Are unindexable (but can use existing indexes on referenced objects)

Cannot have constraints

Are essentially disposable VIEWs

Persist only until the next query is run

Can be recursive

Do not have dedicated stats (rely on stats on the underlying objects)

Temporary Tables

Are real materialized tables that exist in tempdb

Can be indexed

Can have constraints

Persist for the life of the current CONNECTION

Can be referenced by other queries or subprocedures

Have dedicated stats generated by the engine

As far as when to use each, they have very different use cases. If you will have a very large result set, or need to refer to it more than once, put it in a #temp table. If it needs to be recursive, is disposable, or is just to simplify something logically, a CTE is preferred.

Also, a CTE should never be used for performance. You will almost never speed things up by using a CTE, because, again, it's just a disposable view. You can do some neat things with them but speeding up a query isn't really one of them.

CASE WHEN. Shows up all the time.

Self Joins. Common in product.

DISTINCT and GROUP BY

Left vs outer joins.

UNION. Rarely discussed but frequent.

SUM and COUNT

Date-time manipulation

String formatting, substring

Window functions like rank and row

Subqueries

HAVING vs WHERE

In some cases LAG and LEAD

Understanding indexing

Running totals

MAX and MIN

CTEsa and Recursive CTEs

Sample Queries
window function with group by

student practice database

lead/lag first_value, nth_value

user journey

wins and losses
SQL & Interview Playbook — Marketing Analytics
1) Foundations & Environment (analytics-oriented)
●	Relational model & set semantics: primary/foreign keys, star schemas
 (facts for events/transactions; dims for entities like users/listings).

2) Query lifecycle & execution model
Logical evaluation order
 FROM/JOIN → WHERE → GROUP BY → HAVING → SELECT → DISTINCT → ORDER BY → LIMIT
Optimizer behavior (mental model)
●	Predicate pushdown / SARGability: keep partition/filter columns bare on
 LHS; avoid wrapping in functions (enables pruning).

●	Join strategy: pre-aggregate/filter before joins; avoid row-explosion by
 asserting key uniqueness.

●	Sorting is expensive: pair ORDER BY with LIMIT unless needed
 globally.

●	Window frames: prefer ROWS over RANGE for deterministic rolling.


________________________________________
3) Core query building blocks
●	Column discipline: avoid SELECT *; project only what you need.

●	Filtering: use half-open windows (e.g., dt >= '2025-08-01' AND dt < '2025-09-01').

●	CASE expressions: categorical logic; conditional aggregation.

●	DISTINCT vs GROUP BY: dedupe vs aggregation intent.

________________________________________
4) Joins & set operations (with correctness checks)
Need	Canonical idiom	Notes
Inner match	INNER JOIN ... ON ...	Assert key uniqueness upstream.
Keep left rows	LEFT JOIN ...	Anti-join via WHERE right.key IS NULL.
Self-join	table to itself	Adjacency, near-dupe patterns.
Union	UNION ALL	Prefer over UNION (dedupe cost).
Row-explosion guard (pre-check)
SELECT 'bookings' AS t, COUNT(*) AS n, COUNT(DISTINCT booking_id) AS d
FROM   bookings
UNION ALL
SELECT 'reviews', COUNT(*), COUNT(DISTINCT booking_id)
FROM   reviews;

Safer anti-join
SELECT g.guest_id
FROM   guests g
WHERE  NOT EXISTS (
  SELECT 1
  FROM   bookings b
  WHERE  b.guest_id = g.guest_id
     AND b.status   = 'confirmed'
);

________________________________________
5) Aggregations & grouping
●	Conditional aggregation

○	Presto/Trino: COUNT_IF(cond) or COUNT(*) FILTER (WHERE cond)

○	Hive: SUM(CASE WHEN cond THEN 1 ELSE 0 END)

●	Approximate aggregations (exploration)

○	Presto/Trino: approx_distinct(x), approx_percentile(x, p)

○	Hive: percentile_approx(x, p)

________________________________________
6) Window functions (analytic functions)
Ranking (tie policy)
●	ROW_NUMBER() — unique sequence; breaks ties deterministically.

●	RANK() — gaps on ties.

●	DENSE_RANK() — no gaps on ties.

Navigation
SELECT user_id,
       event_ts,
       LAG(event_ts) OVER (
         PARTITION BY user_id
         ORDER BY event_ts
       ) AS prev_ts
FROM   events;

Deterministic rolling (7 days)
SELECT d,
       SUM(bookings) OVER (
         ORDER BY d
         ROWS BETWEEN 6 PRECEDING AND CURRENT ROW
       ) AS roll7
FROM   daily_bookings;

Latest row per entity (portable)
WITH ranked AS (
  SELECT  e.*,
          ROW_NUMBER() OVER (
            PARTITION BY user_id
            ORDER BY event_ts DESC, event_id DESC
          ) AS rn
  FROM    events e
)
SELECT *
FROM   ranked
WHERE  rn = 1;

Greatest-N per group
●	Exactly K: ROW_NUMBER() ... WHERE rn <= K

●	Include ties: DENSE_RANK() ... WHERE r <= K

________________________________________
7) Subqueries & CTEs
●	Scalar/correlated subqueries: use cautiously; can be expensive.

●	CTEs (WITH): stage logic for clarity and testability.

●	Materialization: CTEs are often inlined; if reused, consider temp
 tables (workflow-level choice, not included here).

________________________________________
8) Time & date handling (analytics hygiene)
●	Normalize timestamps to UTC; convert only for presentation.

●	Half-open windows: >= start AND < end to avoid off-by-one.

●	Weekly alignment: pick ISO week consistently across pipelines.

●	DST pitfalls: avoid hourly compares across DST; work in UTC.

Presto/Trino date diff
SELECT date_diff('day', DATE '2025-08-01', DATE '2025-09-01') AS days;

Hive date diff
SELECT datediff('2025-09-01', '2025-08-01') AS days;

________________________________________
9) Semi-structured data (JSON, arrays)
Presto/Trino JSON
SELECT json_extract_scalar(metadata, '$.utm_source') AS utm_source
FROM   page_events
WHERE  metadata IS NOT NULL;

Hive JSON
SELECT get_json_object(metadata, '$.utm_source') AS utm_source
FROM   page_events
WHERE  metadata IS NOT NULL;

Arrays (Presto/Trino)
SELECT listing_id,
       cardinality(array_distinct(amenities)) AS uniq_amenities
FROM   listings_dim;

________________________________________
10) Canonical analytical patterns (marketing)
10.1 Sessionization (30-minute idle break)
WITH base AS (
  SELECT user_id,
         event_ts,
         LAG(event_ts) OVER (
           PARTITION BY user_id ORDER BY event_ts
         ) AS prev_ts
  FROM   events
),
gaps AS (
  SELECT user_id,
         event_ts,
         CASE
           WHEN prev_ts IS NULL THEN 1
           WHEN event_ts - prev_ts > INTERVAL '30' MINUTE THEN 1
           ELSE 0
         END AS is_new
  FROM   base
),
labeled AS (
  SELECT user_id,
         event_ts,
         SUM(is_new) OVER (
           PARTITION BY user_id ORDER BY event_ts
           ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW
         ) AS session_id
  FROM   gaps
)
SELECT user_id,
       session_id,
       MIN(event_ts) AS session_start,
       MAX(event_ts) AS session_end,
       COUNT(*)      AS events
FROM   labeled
GROUP  BY user_id, session_id;

10.2 Funnels & step gating
Presto/Trino (use COUNT_IF or filtered aggregates)
WITH steps AS (
  SELECT user_id,
         MAX(CASE WHEN step = 'search' THEN 1 END) AS s1,
         MAX(CASE WHEN step = 'view'   THEN 1 END) AS s2,
         MAX(CASE WHEN step = 'book'   THEN 1 END) AS s3
  FROM   user_events
  WHERE  event_dt >= DATE '2025-08-01'
     AND event_dt <  DATE '2025-09-01'
  GROUP  BY user_id
)
SELECT COUNT(*) AS users_seen,
       COUNT_IF(s1 = 1) AS step1,
       COUNT_IF(s1 = 1 AND s2 = 1) AS step2,
       COUNT_IF(s1 = 1 AND s2 = 1 AND s3 = 1) AS step3,
       CAST(step2 AS DOUBLE) / NULLIF(step1, 0) AS p_s1_to_s2,
       CAST(step3 AS DOUBLE) / NULLIF(step2, 0) AS p_s2_to_s3
FROM   steps;

Hive variant (no COUNT_IF)
WITH steps AS (
  SELECT user_id,
         MAX(CASE WHEN step = 'search' THEN 1 ELSE 0 END) AS s1,
         MAX(CASE WHEN step = 'view'   THEN 1 ELSE 0 END) AS s2,
         MAX(CASE WHEN step = 'book'   THEN 1 ELSE 0 END) AS s3
  FROM   user_events
  WHERE  event_dt >= '2025-08-01' AND event_dt < '2025-09-01'
  GROUP  BY user_id
)
SELECT COUNT(*) AS users_seen,
       SUM(CASE WHEN s1 = 1 THEN 1 ELSE 0 END) AS step1,
       SUM(CASE WHEN s1 = 1 AND s2 = 1 THEN 1 ELSE 0 END) AS step2,
       SUM(CASE WHEN s1 = 1 AND s2 = 1 AND s3 = 1
                THEN 1 ELSE 0 END) AS step3,
       CAST(step2 AS DOUBLE) / NULLIF(step1, 0) AS p_s1_to_s2,
       CAST(step3 AS DOUBLE) / NULLIF(step2, 0) AS p_s2_to_s3
FROM   steps;

Enforce ordering with session/timestamps if steps can be out of order.
10.3 Cohorts & retention
Presto/Trino
WITH first_seen AS (
  SELECT  user_id,
          MIN(CAST(event_ts AS DATE)) AS cohort_dt
  FROM    events
  GROUP   BY user_id
),
grid AS (
  SELECT  f.cohort_dt,
          CAST(e.event_ts AS DATE) AS act_dt,
          date_diff('day', f.cohort_dt, CAST(e.event_ts AS DATE)) AS d
  FROM    first_seen f
  JOIN    events e
    ON    e.user_id = f.user_id
)
SELECT cohort_dt,
       COUNT_IF(d BETWEEN 0 AND 0)   AS d0,
       COUNT_IF(d BETWEEN 1 AND 7)   AS d1_7,
       COUNT_IF(d BETWEEN 8 AND 14)  AS d8_14,
       COUNT_IF(d BETWEEN 15 AND 30) AS d15_30
FROM   grid
GROUP  BY cohort_dt
ORDER  BY cohort_dt;

Hive
WITH first_seen AS (
  SELECT  user_id,
          MIN(CAST(event_ts AS DATE)) AS cohort_dt
  FROM    events
  GROUP   BY user_id
),
grid AS (
  SELECT  f.cohort_dt,
          CAST(e.event_ts AS DATE) AS act_dt,
          datediff(CAST(e.event_ts AS DATE), f.cohort_dt) AS d
  FROM    first_seen f
  JOIN    events e
    ON    e.user_id = f.user_id
)
SELECT cohort_dt,
       SUM(CASE WHEN d BETWEEN 0 AND 0  THEN 1 ELSE 0 END) AS d0,
       SUM(CASE WHEN d BETWEEN 1 AND 7  THEN 1 ELSE 0 END) AS d1_7,
       SUM(CASE WHEN d BETWEEN 8 AND 14 THEN 1 ELSE 0 END) AS d8_14,
       SUM(CASE WHEN d BETWEEN 15 AND 30 THEN 1 ELSE 0 END) AS d15_30
FROM   grid
GROUP  BY cohort_dt
ORDER  BY cohort_dt;

10.4 Attribution & pathing (first-click in 7 days)
WITH first_click AS (
  SELECT  i.guest_id,
          i.campaign_id,
          MIN(c.click_dt) AS first_click_dt
  FROM    ad_impressions i
  JOIN    ad_clicks c
    ON    c.guest_id    = i.guest_id
   AND    c.campaign_id = i.campaign_id
   AND    c.click_dt BETWEEN i.imp_dt
                         AND i.imp_dt + INTERVAL '7' DAY
  GROUP   BY i.guest_id, i.campaign_id
)
SELECT  campaign_id,
        COUNT(*) AS imps_with_click
FROM    first_click
GROUP   BY campaign_id;

10.5 Weighted metrics (guardrail-safe)
SELECT CAST(SUM(r.rating * b.price_usd) AS DOUBLE)
       / NULLIF(SUM(b.price_usd), 0) AS w_avg_rating
FROM   reviews r
JOIN   bookings b
  ON   b.booking_id = r.booking_id
WHERE  b.status = 'confirmed';

________________________________________
11) Performance & scalability (query-level)
●	Pushdown & pruning: keep filters on raw columns (dt, event_date).

●	Join reduction: filter & aggregate before joining; broadcast small dims
 when feasible (Hive hint /*+ MAPJOIN(dim) */).

●	Sort cost: prefer partial orders with LIMIT; avoid global sorts.

●	Skew handling: identify heavy keys; conditional logic to isolate them.

________________________________________
12) Data quality, testing & validation (analytics)
Check	How
Null semantics	COUNT(*) vs COUNT(col); deliberate COALESCE.
Key uniqueness	ROW_NUMBER() ... = 1 under the key you expect unique.
Row conservation	Counts before/after joins; explain deltas.
Totals reconcile	Sum by dim vs grand total; fix leakage.
Time coverage	MIN/MAX(ts) after filters; no unintended gaps.
Outliers	IQR/percentiles (approx_percentile) for quick scans.
________________________________________
13) Interview strategy — Marketing Analytics Manager
What interviewers look for
Area	Expectation	Evidence
SQL depth	Windows, joins, rigor	Clean live coding
Causal sense	A/B, geo holdout, DiD	Clear assumptions
Marketing IQ	Funnels, LTV/ROAS, attr	Exact metric defs
Product sense	Guardrails, trade-offs	“So what” framing
Leadership	Prioritization, coaching	Decision memos, OKRs
Communication	Structure, brevity	Tables > prose
Live-coding talk-track (30–60s)
 “I’ll restate the goal, define the metric and cohort, choose the pattern
 (dedupe, Top-K, anti-join, windowed agg, session, funnel, cohort), write a
 readable CTE sequence with correctness checks (key uniqueness, row counts),
 and call out trade-offs (accuracy vs cost).”
Case-study checklist (paste-ready)
●	Unit of analysis & randomization unit (user/session/geo).

●	Principal metric & exact SQL definition; guardrails (cancels, CS contacts).

●	Eligibility/exposure; assignment integrity; pre-period balance.

●	Design: A/B, geo holdout, DiD; power/MDE planning.

●	Analysis plan: CUPED/covariates, variance, heterogeneity by device/market.

●	Risks: seasonality, interference; mitigation plan.

●	Decision rule & follow-ups (ship/iterate/rollback).

Behavioral (prepare 5–7 STAR stories)
Theme	Prompt seed
Prioritization	“Trade-off: lift vs guardrails on latency/cancels.”
Conflict	“Metric definition dispute with PM; resolution path.”
Failure	“Wrong conclusion; how detected, corrected, prevented.”
Leadership	“Coaching analyst to window-function mastery.”
Influence	“Finance sign-off on MMM; changed forecast policy.”
Ambiguity	“No A/B allowed; credible quasi-exp design.”
________________________________________
14) Engine-specific quick reference (Hive vs Presto/Trino)
Topic	Presto/Trino	Hive
Conditional count	COUNT_IF(cond) or COUNT(*) FILTER (WHERE cond)	SUM(CASE WHEN cond THEN 1 ELSE 0 END)
Date diff	date_diff('day', a, b)	datediff(b, a)
JSON	json_extract_scalar(json, path)	get_json_object(json, path)
Broadcast hint	Session/auto, depends env	/*+ MAPJOIN(dim) */
QUALIFY	Not available (use subquery)	Not available (use subquery)
________________________________________
15) Anti-patterns & edge cases
●	NOT IN with NULLs ⇒ empty result. Prefer NOT EXISTS.

●	Casting timestamp to date in WHERE on partition col breaks pruning.

●	Global ORDER BY without LIMIT = expensive; avoid unless necessary.

●	RANGE frames with duplicates can bloat windows; use ROWS.

●	Non-unique join keys silently multiply rows; assert uniqueness.

●	UNION when UNION ALL is acceptable wastes time on dedup.

________________________________________
16) Reusable snippet library (copy-ready)
Latest row per entity
WITH ranked AS (
  SELECT  t.*,
          ROW_NUMBER() OVER (
            PARTITION BY entity_id
            ORDER BY event_ts DESC, event_id DESC
          ) AS rn
  FROM    t
)
SELECT *
FROM   ranked
WHERE  rn = 1;

Top-K per group (include ties)
WITH ranked AS (
  SELECT  g.*,
          DENSE_RANK() OVER (
            PARTITION BY group_key
            ORDER BY metric DESC, tie_breaker
          ) AS r
  FROM    g
)
SELECT *
FROM   ranked
WHERE  r <= 3;

Anti-join (users not converted)
SELECT u.user_id
FROM   users u
WHERE  NOT EXISTS (
  SELECT 1
  FROM   conversions c
  WHERE  c.user_id = u.user_id
);

Sessionization (30-min)
-- See §10.1 for full version; gist below:
... LAG(ts) ... CASE gap>30m THEN 1 ELSE 0 END AS is_new
... SUM(is_new) OVER (PARTITION BY user ORDER BY ts ROWS UNBOUNDED PRECEDING)

Funnel gating (Hive)
SUM(CASE WHEN s1=1              THEN 1 ELSE 0 END) AS step1,
SUM(CASE WHEN s1=1 AND s2=1     THEN 1 ELSE 0 END) AS step2,
SUM(CASE WHEN s1=1 AND s2=1
          AND s3=1              THEN 1 ELSE 0 END) AS step3

Cohort grid (daily buckets)
-- Pick Presto/Hive variant from §10.3

Attribution: first click in 7 days
-- See §10.4

Weighted average (guardrail-safe)
CAST(SUM(x * w) AS DOUBLE) / NULLIF(SUM(w), 0) AS weighted_avg

________________________________________
17) Practice index (marketing-centric)
1.	Monthly bookings & GMV by channel/geo (half-open windows).

2.	Top-3 listings per city by nights & revenue (tie-breaks deterministic).

3.	Repeat-guest share (≥2 priors) in last 180 days (window count).

4.	Near-repeat trips (pairs within 7 days) via self-join.

5.	Running daily bookings by channel (30-day cumulative).

6.	Impression→click→booking windows (first click in 7d).

7.	Re-engagement email lift with cancels/CS guardrails.

8.	Search ranking tweak: CTR vs bookings vs cancellations (guardrails).

9.	Experiences cross-sell attach rate & net incremental GMV.

10.	Geo promo: holdout/DiD set-up with cluster-robust SEs (SQL prep).

________________________________________
Final tip
In interviews, state the decision each query supports, name the pattern
 you’re using (e.g., “Top-K per group with deterministic ties”), and call out
 validation checks (key uniqueness, row counts, totals reconcile). This
 signals senior-level rigor aligned to Marketing impact.



SQL 
Below is a compact Airbnb-style schema we’ll use for all questions, followed by 30 SQL 
prompts that mirror what’s typically tested (joins, aggregations, filters, subqueries, self-joins, 
window functions, pivoting/conditional agg, date ops, CTEs, semi/anti joins, etc.). I’ve kept each 
question self-contained and tagged the target concepts. 
Working schema (columns you can assume exist) 
● listings(listing_id, host_id, city, country, room_type, 
n_bedrooms, created_at, is_active) 
● hosts(host_id, host_since, superhost_flag, country) 
● guests(guest_id, signup_dt, country) 
● bookings(booking_id, listing_id, guest_id, checkin_dt, 
checkout_dt, booked_dt, status, nights, guests_count, price_usd) 
● reviews(review_id, booking_id, listing_id, guest_id, review_dt, 
rating, review_text) 
● calendar(listing_id, dt, available_flag, nightly_price_usd) 
● search_sessions(session_id, guest_id, search_dt, device, country, 
origin_channel) 
● search_results(session_id, listing_id, position, clicked_flag, 
booked_flag) 
● marketing_campaigns(campaign_id, channel, campaign_name, 
start_dt, end_dt, country) 
● ad_impressions(imp_id, campaign_id, guest_id, imp_dt) 
● ad_clicks(click_id, campaign_id, guest_id, click_dt) 
● product_events(event_dt, guest_id, listing_id, event_name, 
session_id) 
● countries(country, region) 
30 SQL interview-style prompts (Airbnb context) 
# 
Scenario / Prompt 
1 Monthly bookings and GMV. For each 
YYYY-MM, compute total confirmed 
bookings and GMV (sum(price_usd)) for 
active listings only. 
Tables 
bookings, listings 
2 Host-level performance. Top 10 hosts by 
GMV last 90 days; include bookings count, 
distinct guests, and share of bookings with 
nights >= 7. 
3 Occupancy rate. For the last full calendar 
month, compute occupancy rate per city: 
booked_nights / total_nights using 
calendar.available_flag and 
bookings (confirmed only). 
4 Search-to-book funnel. By channel, last 30 
days: sessions, clicks, bookings, and 
CTR/BTB rates. 
5 Repeat guests. % of bookings in last 180 
days from guests with ≥2 prior confirmed 
bookings (any time before the booking’s 
booked_dt). 
bookings, listings, hosts 
calendar, bookings, 
listings 
search_sessions, 
search_results 
bookings 
Concepts 
Tested 
Filter, join, 
group by month 
Join, filters, agg 
w/ conditional 
sums 
Date math, 
joins, 
aggregation 
Semi-joins, 
conditional agg 
Correlated 
subquery or 
window 
lag/count 
6 Review coverage. For each city: share of 
bookings that received a review within 30 
days of checkout. 
7 Superhost uplift. Compare average rating 
and GMV for superhosts vs non-superhosts 
in the last quarter; return group, bookings, 
avg_rating, avg_price, GMV. 
8 Cancellations. Weekly cancellation rate 
(status = 'canceled') by room_type; 
include 3-week moving average. 
9 Search ranking effect. Compute conversion 
to booking by position bucket (1–3, 4–10, 
>10) controlling for device and country. 
10 Longest stays. For each country, list the 
single longest nights stay in last year with 
listing & host details (ties keep highest 
price_usd). 
11 Guest lifecycle. For each guest, report 
f
 irst_booking_dt, days_to_second_booking, 
and whether they churned (no bookings last 
365d). 
12 Activation cohort. Monthly host cohorts by 
host_since and their month-2 GMV (GMV 
in cohort_month+1). 
13 Price dispersion. For active listings in a city, 
compute median nightly price over last 60 
days and IQR from calendar. 
bookings, reviews, 
listings 
hosts, bookings, reviews, 
listings 
bookings, listings 
search_results, 
search_sessions 
bookings, listings, hosts 
bookings, guests 
hosts, bookings, listings 
calendar, listings 
Left join, date 
diff, conditional 
agg 
Joins, grouping, 
window alt: 
none 
Window 
(moving avg), 
date binning 
Case/pivot, 
multi-dim group 
Row_number 
partition, top-1 
per group 
Window 
(min/lead), 
derived flags 
Cohorts via date 
trunc + joins 
Percentiles 
(approx or 
window), filter 
14 Calendar integrity check. Find listings that 
were unavailable (available_flag=0) on 
any date where a booking overlapped but 
nights were priced at 0 or NULL. 
15 Attribution (last touch). Within a 7-day 
attribution window ending at booked_dt, 
assign each booking to the most recent ad 
click per guest; output channel-level GMV. 
16 Marketing reach & frequency. For a date 
range, compute unique guests reached and 
average frequency by campaign. 
17 Cross-sell signal. Among guests who 
booked ≥2 cities in the last year, top 10 city 
pairs by count (unordered pairs). 
18 Supply growth. 3-month rolling new 
listings by region (countries), and % 
change vs prior 3-month window. 
19 Time to first review. For each listing created 
in the last year, days from created_at to 
f
 irst review_dt; return P50 and P90 by 
city. 
20 Click quality. For sessions with ≥1 click, 
compute probability a clicked listing gets 
booked within 48h; return by device. 
21 Room-type mix. For each city, share of 
active supply by room_type and Herfindahl 
index (HHI) of room types. 
calendar, bookings 
bookings, ad_clicks, 
marketing_campaigns 
ad_impressions 
bookings, listings 
listings, countries 
listings, reviews 
search_sessions, 
search_results 
listings 
Interval overlap, 
data QA 
Window over 
time, last-touch 
join 
Distinct count, 
ratio 
Self-join or set 
ops, 
combinatorics 
Rolling sums, 
joins, pct 
change 
First-value via 
min, percentiles 
Filters, time 
window, 
conditional agg 
Pivot via 
conditional 
sums, HHI calc 
22 Price vs rating elasticity proxy. Correlate 
listing’s median price (last 90d) with 
average rating (last 365d) by city; return 
Pearson r. 
23 Host multi-listing behavior. Flag hosts 
with ≥N active listings and compute their 
share of city GMV last quarter. 
24 Cancellation window abuse. Identify 
guests with ≥2 cancellations within 24h of 
booked_dt across different listings in past 
180d. 
25 Underpriced blackout. Find top 50 listings 
where calendar shows 
available_flag=1 but 
nightly_price_usd is below the city’s 
10th percentile in the next 14 days. 
26 Session to search gap. Compute median 
minutes from first 
product_events.event_name='sessio
 n_start' to first search per session. 
27 Geo expansion. For new countries launched 
in last 18 months (first listing), report 
month-over-month bookings for first 6 
months post-launch. 
28 Review bias. Compare average rating for 
stays ≤2 nights vs ≥7 nights, controlling for 
city (return city-level deltas). 
29 Seasonality index. For each city, compute 
monthly seasonality index = month_GMV / 
avg_month_GMV using 24 months of 
history. 
calendar, reviews, 
listings 
listings, bookings 
bookings 
calendar, listings 
product_events 
listings, bookings 
bookings, reviews, 
listings 
bookings, listings 
Aggregation to 
features, corr 
formula 
Threshold 
f
 iltering, share 
of total 
Self-join on 
guest, time diff 
Percentiles by 
group, date 
f
 ilter 
Window by 
session, time 
diff 
Launch 
detection, 
aligned 
windows 
Join, group, diff 
calculations 
Window avg 
over partition 
30 Booking lead time distribution. For last 
quarter, output deciles of lead time 
(booked_dt→checkin_dt) by room_type. 
bookings, listings 
Date diff, 
percentiles by 
group 
Notes on expected approaches (best practices) 
● Prefer date_trunc('month', …) or DATE_TRUNC('month', …) depending on 
Presto/Hive. 
● Use CTEs for readability, especially when deriving features (e.g., lead time, cohorts). 
● For percentiles in Presto: approx_percentile(x, 0.5); in Hive: 
percentile_approx. 
● For rolling windows, use window frames: 
avg(x) over (partition by k order by d rows between 2 preceding 
and current row). 
● For interval overlaps: GREATEST(start1, start2) < LEAST(end1, end2). 
● For attribution: row_number() over (partition by booking_id order by 
click_dt desc) 
after joining clicks ≤ booked_dt and within window. 
● For distinct counts at scale, approximate functions (HLL/approx_distinct) may be 
acceptable if the engine supports them. 
If you’d like, I can provide answer keys for any subset (e.g., 1–10) tailored to Presto/Hive syntax 
and optimized for readability and performance. 
Below is a compact, interview-style set of 30 Airbnb-context SQL questions spanning the 
usual segments (joins, aggregation, filtering, subqueries, self-joins, windowing, date/time, 
arrays/maps, set ops, funnels/retention, and experiment/marketing measurement). They are 
framed for Presto/Hive-style SQL (Airbnb commonly uses SQL engines in that family) and 
align with the role’s emphasis on experimentation/causal methods, product & growth metrics, 
and building robust reporting/metrics. (Careers at Airbnb) 
Starter schema (assumed) 
Table 
users 
listings 
bookings 
reviews 
calendar 
search_events 
sessions 
marketing_impre
 ssions 
marketing_click
 s 
payments 
refunds 
experiments 
Key columns (subset) 
user_id (PK), created_at (ts), country, device_type 
listing_id (PK), host_id, city, room_type, created_at 
booking_id (PK), user_id, listing_id, checkin_date (date), 
checkout_date (date), status (‘confirmed’, ‘cancelled’), 
booked_at (ts), total_price (decimal), coupon_code 
review_id, booking_id, user_id, listing_id, rating (int), 
reviewed_at (ts) 
listing_id, dt (date), is_available (bool), price (decimal) 
event_ts (ts), user_id, query, city, result_count, 
device_type 
session_id, user_id, session_start (ts), session_end (ts), 
source (utm_source), medium (utm_medium) 
imp_id, user_id, campaign_id, imp_ts (ts), channel, cost 
click_id, user_id, campaign_id, click_ts (ts), channel 
payment_id, booking_id, paid_at (ts), amount, method 
refund_id, booking_id, refunded_at (ts), amount 
exp_name, user_id, variant (‘control’, ‘treatment’), 
assigned_ts 
hosts 
messages 
host_id (PK), joined_at (ts), country 
message_id, booking_id, from_user_id, to_user_id, 
sent_at (ts) 
Use these columns as needed; not every column will be used per question. 
30 interview-style SQL questions (Airbnb 
themed) 
# 
Segment 
1 Filtering + 
Agg 
2 Joins 
Scenario 
New guest 
conversion 
Prompt (what to return) 
Among users created in last 90 
days, % with ≥1 confirmed 
booking within 30 days of 
signup. 
Search→booking By device, weekly 
search→booking conversion: 
users with a search this week 
who booked within 7 days after 
f
 irst search. 
3 Window fn Repeat bookers Top 10 cities by share of repeat 
guests in last 12 months (repeat 
= ≥2 confirmed bookings). 
4 Self-join 
Re-engagement 
Users inactive ≥180 days who 
returned to book; compute days 
since prior booking at return 
time. 
Key concepts tested 
Date filters, 
conditional agg 
Join on user & time 
window 
Grouping, window 
percent 
Self-join by user, 
LAG 
5 Subquery 
Listing quality 
6 Window fn Host activation 
funnel 
7 Anti-join 
For listings with ≥10 reviews, 
output avg rating and count; 
keep top decile by avg rating per 
city. 
For new hosts (joined last 
quarter), compute time from join 
to first booking; report P50/P90 
by host country. 
Unreviewed stays Bookings completed 14+ days 
ago with no review yet; list 
booking_id, user_id, listing_id. 
8 String/date 
Peak pricing 
For calendar, find 
month/day-of-week combos 
with highest median price per 
city (last year). 
9 Joins + CASE Cancellation rate By room_type and month, 
cancellation rate = cancelled / 
(cancelled+confirmed). 
10 Window fn Greatest-N per 
group 
11 CTE + 
Window 
Sessionization 
sanity 
12 Arrays/Maps Query analysis 
For each city, top 3 listings by 
nights booked in the last 90 
days. Break ties by total 
revenue. 
Compute avg 
sessions/user/week and median 
session duration by source for 
past 8 weeks. 
From search_events.query, 
count searches containing any of 
['pool','pet','wifi'] by device, 
week. 
Aggregation, 
percentile filter 
DATEDIFF, 
percentiles 
LEFT/ANTI join 
DATE_TRUNC, 
extract DOW 
Conditional sums 
ROW_NUMBER, 
ORDER BY multi 
TIMESTAMP math, 
pctls 
Simple 
substring/regex 
13 Dedup 
14 Funnel 
(semi-joins) 
15 Retention 
16 Marketing 
join 
17 MMM prep 
(SQL) 
18 CUPED 
inputs 
19 Guardrails 
20 Percentiles 
Identity stitching 
Booking funnel 
Clicks table may have dup 
events. Dedup by (user_id, 
campaign_id, minute(click_ts)). 
Count unique clicks/day. 
Build 3-step funnel: search → 
listing view (assume sessions 
page events) → booking; output 
step rates by city. 
8-week retention Weekly cohort retention for 
guests by signup week; return 
CohortWeek, WeekK, 
RetainedUsers. 
CPA by channel 
Feature table 
Join marketing_clicks to first 
booking within 7 days post-click; 
compute CPA and CVR by 
channel. 
Create weekly panel with 
bookings, revenue, and ad spend 
by channel 
(sum(marketing_impression
 s.cost)). 
Pre-period metric For an experiment on new 
search UI, compute each user’s 
pre-period mean daily searches 
(baseline covariate). 
Abuse spike 
Price dispersion 
In experiment, report 
week-over-week % change in 
cancellation rate and refund rate 
by variant. 
For each city, P10, P50, P90 of 
nightly price (from calendar) 
restricted to available nights. 
QUALIFY 
ROW_NUMBER 
Semi-join, distinct 
users 
Cohort labeling, 
joins 
Attribution, first 
touch 
Time bucketing, 
panel 
Pre window agg 
Joins, ratios, WoW 
approx_percentile 
21 Multi-table 
join 
22 Set ops 
23 Rollups 
24 QA outliers 
Host response 
Supply vs 
demand 
KPI cube 
Price sanity 
25 Window fn RFM proxy 
26 Time zones 
Booking hour 
27 Joins + CASE Net revenue 
For completed bookings, 
compute median host response 
time (first message after guest 
message). 
Cities with demand growth 
(bookings +20% YoY) but supply 
growth (active listings) <5% 
YoY. 
Produce a cube of 
bookings/revenue by (city, 
room_type, device_type), 
including subtotals and grand 
total. 
Identify listings whose median 
price is >5× city median in a 
month (potential data issues). 
For each user, compute recency 
(days since last booking), 
frequency (bookings last 365d), 
and monetary (sum revenue). 
Distribution of booked_at by 
local listing time zone hour; 
assume listing city→TZ mapping 
table city_tz. 
Net revenue = payments − 
refunds; report by month and 
room_type for past 12 months. 
Self join by time, 
MIN 
Intersections, YoY 
GROUPING 
SETS/ROLLUP 
Median by group, 
compare 
MAX over user, sums 
TZ conversion 
Joins, arithmetic 
28 Self-join 
Cannibalization 
29 Window fn SLA breaches 
30 UDF/Regex 
Coupon lift 
For users in experiment with 
new pricing, compute if first 
booking post-assignment was 
cheaper than their prior booking 
(same city). 
For support messages during a 
booking, % of first host replies > 
12 hours; breakdown by city. 
For bookings with 
coupon_code, estimate 
incremental lift proxy: avg 
revenue with coupon vs similar 
listings without (same city, 
room_type, month). 
Pairwise compare 
FIRST_VALUE/MIN, 
CASE 
Matched grouping 
Notes for practicing these: 
● Favor CTEs to keep logic modular; many prompts are naturally multi-step (e.g., build 
user cohorts, then join to outcomes). 
● Prefer window functions for “first/last/next” (e.g., ROW_NUMBER for “first booking 
after X”, LAG/LEAD for diffs). 
● For Presto/Hive percentiles use approx_percentile(x, 0.5) or 
percentile_approx depending on engine. 
● When attribution windows are involved (e.g., Q16), explicitly define lookback/forward 
windows (7 days here) and choose first or last touch consistently. 
● When you need top-N per group (Q10), use ROW_NUMBER() OVER (PARTITION BY 
group ORDER BY metric DESC) + WHERE rn <= N. 
● Guardrail metrics (Q19) should be pre-specified and computed alongside primary 
outcomes to avoid p-hacking. 
If you want, I can provide worked solutions next (Presto/Hive-style), one block per question 
with readable line widths and short comments, or prioritize the areas you want to drill (e.g., 
window functions + A/B). 
SQL 
I'll help you prepare for your Airbnb SQL interview by creating 10 relevant practice questions 
that cover the key concepts mentioned. These questions progressively increase in complexity 
and reflect real Airbnb business scenarios. 
SQL Practice Questions for Airbnb Interview 
Question 1: Basic Filtering and Aggregation 
Scenario: Find top performing listings in a specific city -- Given tables: listings (listing_id, host_id, city, price_per_night, property_type, created_date) -- Write a query to find all listings in Paris with price > $100 per night,  -- showing the average price by property_type, ordered by average price descending 
Question 2: Simple JOIN 
Scenario: Match hosts with their booking performance -- Given tables:  -- hosts (host_id, host_name, join_date, host_status) -- bookings (booking_id, listing_id, host_id, guest_id, check_in_date, check_out_date, 
total_price) -- Find all hosts who joined in 2024 and their total booking revenue 
Question 3: Multiple JOINs with Aggregation 
Scenario: Analyze guest booking patterns and satisfaction -- Given tables: -- bookings (booking_id, guest_id, listing_id, check_in_date, nights_stayed, total_paid) -- reviews (review_id, booking_id, rating, review_date) 
-- guests (guest_id, signup_date, country) -- Find the average rating by guest country for bookings in the last 90 days 
Question 4: Subquery - Identifying High-Value Guests 
Scenario: Find guests who spend above average -- Using the bookings table, identify guests whose average booking value  -- is higher than the overall average booking value -- Show guest_id and their average booking value 
Question 5: Self-JOIN - Finding Repeat Guests 
Scenario: Identify guests who rebooked within 30 days -- Given bookings table, find all instances where the same guest  -- made another booking within 30 days of their previous checkout -- Show both booking IDs and the gap in days 
Question 6: Window Functions - Ranking 
Scenario: Rank hosts by revenue within each city -- Given bookings and listings tables, rank hosts by their total revenue  -- within each city using ROW_NUMBER() or RANK() -- Show only the top 3 hosts per city 
Question 7: Window Functions - Running Totals 
Scenario: Calculate cumulative bookings for marketing campaign analysis -- Given tables: -- bookings (booking_id, booking_date, channel_source) 
-- marketing_campaigns (campaign_id, channel_source, start_date, end_date, budget) -- Calculate daily bookings and cumulative bookings by channel_source  -- for the last 30 days using window functions 
Question 8: Complex Subquery with EXISTS 
Scenario: Find hosts who've never received a bad review -- Find all hosts who have at least 10 bookings but have never  -- received a review with rating < 4 -- Use EXISTS or NOT EXISTS in your solution 
Question 9: Advanced Window Function - Lead/Lag 
Scenario: Analyze pricing changes and occupancy -- Given tables: -- listing_calendar (listing_id, date, price, available) -- Calculate the price change from previous day for each listing -- and identify dates where price increased by more than 20% -- Also show the occupancy rate (7-day moving average) using window functions 
Question 10: Comprehensive Business Question 
Scenario: Marketing Attribution and Customer Lifetime Value -- Given tables: -- users (user_id, signup_date, acquisition_channel, first_booking_date) -- bookings (booking_id, user_id, booking_date, total_value, booking_status) -- marketing_spend (date, channel, daily_spend) -- Calculate: 
-- 1. Customer acquisition cost (CAC) by channel for Q1 2024 -- 2. Average customer lifetime value (CLV) by acquisition channel -- 3. The ratio of CLV to CAC by channel -- 4. Identify channels where CLV/CAC > 3 -- Use CTEs or subqueries to build this analysis step by step 
Key SQL Concepts to Review: 
Essential Functions: 
● GROUP BY with HAVING 
● CASE WHEN statements 
● Date functions (DATE_DIFF, DATE_ADD, DATE_TRUNC) 
● String functions (CONCAT, SUBSTRING, REGEXP) 
Window Functions: 
● ROW_NUMBER(), RANK(), DENSE_RANK() 
● LAG(), LEAD() 
● SUM() OVER(), AVG() OVER() 
● Partition and ordering clauses 
Performance Considerations: 
● When to use CTEs vs subqueries 
● Index usage implications 
● Query optimization basics 
Tips for the Interview: 
1. Think out loud - Explain your approach before writing SQL 
2. Start simple - Build complexity incrementally 
3. Consider edge cases - NULLs, duplicates, date boundaries 
4. Validate assumptions - Ask about data quality, table relationships 
5. Business context - Connect your queries to business impact 
I'll provide detailed solutions for all 10 questions with explanations and alternative approaches 
where relevant. 
SQL Solutions for Airbnb Interview Practice 
Question 1: Basic Filtering and Aggregation -- Solution: 
SELECT  
property_type, 
AVG(price_per_night) as avg_price, 
COUNT(*) as listing_count 
FROM listings 
WHERE city = 'Paris'  
AND price_per_night > 100 
GROUP BY property_type 
ORDER BY avg_price DESC; -- Alternative with ROUND for cleaner output: 
SELECT  
property_type, 
ROUND(AVG(price_per_night), 2) as avg_price, 
COUNT(*) as listing_count, 
MIN(price_per_night) as min_price, 
MAX(price_per_night) as max_price 
FROM listings 
WHERE city = 'Paris'  
AND price_per_night > 100 
GROUP BY property_type 
HAVING COUNT(*) >= 5  -- Only show property types with at least 5 listings 
ORDER BY avg_price DESC; 
 
Question 2: Simple JOIN -- Solution: 
SELECT  
    h.host_id, 
    h.host_name, 
    h.join_date, 
    COUNT(DISTINCT b.booking_id) as total_bookings, 
    SUM(b.total_price) as total_revenue 
FROM hosts h 
LEFT JOIN bookings b  
    ON h.host_id = b.host_id 
WHERE YEAR(h.join_date) = 2024 
GROUP BY h.host_id, h.host_name, h.join_date 
ORDER BY total_revenue DESC; 
 -- Note: Using LEFT JOIN to include hosts with no bookings yet -- Could use COALESCE to handle NULL values: 
SELECT  
    h.host_id, 
    h.host_name, 
    h.join_date, 
    COALESCE(COUNT(DISTINCT b.booking_id), 0) as total_bookings, 
    COALESCE(SUM(b.total_price), 0) as total_revenue 
FROM hosts h 
LEFT JOIN bookings b  
    ON h.host_id = b.host_id 
WHERE YEAR(h.join_date) = 2024 
GROUP BY h.host_id, h.host_name, h.join_date 
ORDER BY total_revenue DESC; 
 
Question 3: Multiple JOINs with Aggregation -- Solution: 
SELECT  
    g.country, 
    COUNT(DISTINCT b.booking_id) as total_bookings, 
    AVG(r.rating) as avg_rating, 
    COUNT(DISTINCT r.review_id) as total_reviews 
FROM bookings b 
INNER JOIN reviews r  
    ON b.booking_id = r.booking_id 
INNER JOIN guests g  
    ON b.guest_id = g.guest_id 
WHERE b.check_in_date >= CURRENT_DATE - INTERVAL '90 days' 
    -- Or: WHERE b.check_in_date >= DATE_SUB(CURRENT_DATE, INTERVAL 90 DAY) 
GROUP BY g.country 
HAVING COUNT(DISTINCT b.booking_id) >= 10  -- Countries with meaningful sample size 
ORDER BY avg_rating DESC; 
 -- Alternative with review completion rate: 
SELECT  
    g.country, 
    COUNT(DISTINCT b.booking_id) as total_bookings, 
    ROUND(AVG(r.rating), 2) as avg_rating, 
    COUNT(DISTINCT r.review_id) as total_reviews, 
ROUND(100.0 * COUNT(DISTINCT r.review_id) / COUNT(DISTINCT b.booking_id), 1) as 
review_rate 
FROM bookings b 
LEFT JOIN reviews r  
ON b.booking_id = r.booking_id 
INNER JOIN guests g  
ON b.guest_id = g.guest_id 
WHERE b.check_in_date >= CURRENT_DATE - INTERVAL '90 days' 
GROUP BY g.country 
ORDER BY avg_rating DESC; 
Question 4: Subquery - Identifying High-Value Guests -- Solution using subquery in WHERE: 
SELECT  
guest_id, 
AVG(total_paid) as avg_booking_value, 
COUNT(*) as booking_count 
FROM bookings 
GROUP BY guest_id 
HAVING AVG(total_paid) > ( 
SELECT AVG(total_paid)  
FROM bookings 
) 
ORDER BY avg_booking_value DESC; -- Alternative using CTE (often clearer): 
WITH overall_metrics AS ( 
SELECT AVG(total_paid) as overall_avg 
    FROM bookings 
) 
SELECT  
    b.guest_id, 
    AVG(b.total_paid) as avg_booking_value, 
    COUNT(*) as booking_count, 
    om.overall_avg, 
    ROUND(AVG(b.total_paid) - om.overall_avg, 2) as diff_from_avg 
FROM bookings b 
CROSS JOIN overall_metrics om 
GROUP BY b.guest_id, om.overall_avg 
HAVING AVG(b.total_paid) > om.overall_avg 
ORDER BY avg_booking_value DESC; 
 
Question 5: Self-JOIN - Finding Repeat Guests -- Solution: 
SELECT  
    b1.guest_id, 
    b1.booking_id as first_booking, 
    b2.booking_id as repeat_booking, 
    b1.check_in_date as first_checkin, 
    b2.check_in_date as repeat_checkin, 
    DATEDIFF(b2.check_in_date, DATE_ADD(b1.check_in_date, INTERVAL b1.nights_stayed 
DAY)) as days_between 
FROM bookings b1 
INNER JOIN bookings b2 
    ON b1.guest_id = b2.guest_id 
    AND b1.booking_id < b2.booking_id  -- Ensure we don't duplicate pairs 
    AND b2.check_in_date <= DATE_ADD(b1.check_in_date, INTERVAL b1.nights_stayed + 30 
DAY) 
    AND b2.check_in_date > DATE_ADD(b1.check_in_date, INTERVAL b1.nights_stayed DAY) 
ORDER BY b1.guest_id, b1.check_in_date; 
 -- Alternative with more details: 
SELECT  
    b1.guest_id, 
    b1.booking_id as first_booking, 
    b2.booking_id as repeat_booking, 
    b1.check_in_date as first_checkin, 
    DATE_ADD(b1.check_in_date, INTERVAL b1.nights_stayed DAY) as first_checkout, 
    b2.check_in_date as repeat_checkin, 
    DATEDIFF(b2.check_in_date, DATE_ADD(b1.check_in_date, INTERVAL b1.nights_stayed 
DAY)) as gap_days 
FROM bookings b1 
INNER JOIN bookings b2 
    ON b1.guest_id = b2.guest_id 
WHERE b2.check_in_date BETWEEN  
    DATE_ADD(b1.check_in_date, INTERVAL b1.nights_stayed DAY) 
    AND DATE_ADD(b1.check_in_date, INTERVAL b1.nights_stayed + 30 DAY) 
    AND b1.booking_id < b2.booking_id 
ORDER BY gap_days ASC; 
 
Question 6: Window Functions - Ranking -- Solution: 
WITH host_revenue AS ( 
    SELECT  
        l.city, 
        b.host_id, 
        SUM(b.total_paid) as total_revenue, 
        COUNT(DISTINCT b.booking_id) as total_bookings 
    FROM bookings b 
    INNER JOIN listings l  
        ON b.listing_id = l.listing_id 
    GROUP BY l.city, b.host_id 
), 
ranked_hosts AS ( 
    SELECT  
        city, 
        host_id, 
        total_revenue, 
        total_bookings, 
        ROW_NUMBER() OVER (PARTITION BY city ORDER BY total_revenue DESC) as 
revenue_rank, 
        RANK() OVER (PARTITION BY city ORDER BY total_revenue DESC) as 
revenue_rank_with_ties, 
        DENSE_RANK() OVER (PARTITION BY city ORDER BY total_revenue DESC) as 
dense_revenue_rank 
    FROM host_revenue 
) 
SELECT  
    city, 
    host_id, 
    total_revenue, 
    total_bookings, 
    revenue_rank 
FROM ranked_hosts 
WHERE revenue_rank <= 3 
ORDER BY city, revenue_rank; 
 -- Simpler version without CTE: 
SELECT * FROM ( 
    SELECT  
        l.city, 
        b.host_id, 
        SUM(b.total_paid) as total_revenue, 
        COUNT(*) as booking_count, 
        ROW_NUMBER() OVER (PARTITION BY l.city ORDER BY SUM(b.total_paid) DESC) as 
rank 
    FROM bookings b 
    INNER JOIN listings l ON b.listing_id = l.listing_id 
    GROUP BY l.city, b.host_id 
) ranked 
WHERE rank <= 3 
ORDER BY city, rank; 
 
Question 7: Window Functions - Running Totals -- Solution: 
WITH daily_bookings AS ( 
    SELECT  
        DATE(booking_date) as booking_day, 
        channel_source, 
        COUNT(*) as daily_count 
    FROM bookings 
    WHERE booking_date >= CURRENT_DATE - INTERVAL '30 days' 
    GROUP BY DATE(booking_date), channel_source 
) 
SELECT  
    booking_day, 
    channel_source, 
    daily_count, 
    SUM(daily_count) OVER ( 
        PARTITION BY channel_source  
        ORDER BY booking_day  
        ROWS UNBOUNDED PRECEDING 
    ) as cumulative_bookings, 
    AVG(daily_count) OVER ( 
        PARTITION BY channel_source  
        ORDER BY booking_day  
        ROWS BETWEEN 6 PRECEDING AND CURRENT ROW 
    ) as moving_avg_7day 
FROM daily_bookings 
ORDER BY channel_source, booking_day; 
 -- Alternative with campaign budget utilization: 
WITH daily_metrics AS ( 
    SELECT  
        DATE(b.booking_date) as booking_day, 
        b.channel_source, 
        COUNT(*) as daily_bookings, 
        SUM(b.total_paid) as daily_revenue, 
        MAX(mc.budget) as campaign_budget 
    FROM bookings b 
    LEFT JOIN marketing_campaigns mc 
        ON b.channel_source = mc.channel_source 
        AND DATE(b.booking_date) BETWEEN mc.start_date AND mc.end_date 
    WHERE b.booking_date >= CURRENT_DATE - INTERVAL '30 days' 
    GROUP BY DATE(b.booking_date), b.channel_source 
) 
SELECT  
    booking_day, 
    channel_source, 
    daily_bookings, 
    daily_revenue, 
    SUM(daily_bookings) OVER (PARTITION BY channel_source ORDER BY booking_day) as 
cumulative_bookings, 
    SUM(daily_revenue) OVER (PARTITION BY channel_source ORDER BY booking_day) as 
cumulative_revenue, 
    campaign_budget, 
    ROUND(100.0 * SUM(daily_revenue) OVER (PARTITION BY channel_source ORDER BY 
booking_day) / campaign_budget, 2) as budget_utilization_pct 
FROM daily_metrics 
ORDER BY channel_source, booking_day; 
 
Question 8: Complex Subquery with EXISTS -- Solution using NOT EXISTS: 
SELECT  
    h.host_id, 
    COUNT(DISTINCT b.booking_id) as total_bookings, 
    AVG(r.rating) as avg_rating 
FROM hosts h 
INNER JOIN bookings b ON h.host_id = b.host_id 
LEFT JOIN reviews r ON b.booking_id = r.booking_id 
WHERE NOT EXISTS ( 
    SELECT 1 
    FROM bookings b2 
    INNER JOIN reviews r2 ON b2.booking_id = r2.booking_id 
    WHERE b2.host_id = h.host_id 
        AND r2.rating < 4 
) 
GROUP BY h.host_id 
HAVING COUNT(DISTINCT b.booking_id) >= 10 
ORDER BY avg_rating DESC, total_bookings DESC; 
 -- Alternative using subquery in WHERE: 
SELECT  
    host_id, 
    total_bookings, 
    avg_rating, 
    min_rating 
FROM ( 
    SELECT  
        b.host_id, 
        COUNT(DISTINCT b.booking_id) as total_bookings, 
        AVG(r.rating) as avg_rating, 
        MIN(r.rating) as min_rating 
    FROM bookings b 
    LEFT JOIN reviews r ON b.booking_id = r.booking_id 
    GROUP BY b.host_id 
    HAVING COUNT(DISTINCT b.booking_id) >= 10 
) host_stats 
WHERE min_rating >= 4 OR min_rating IS NULL  -- Include hosts with no reviews yet 
ORDER BY avg_rating DESC, total_bookings DESC; 
 
Question 9: Advanced Window Function - Lead/Lag -- Solution: 
WITH pricing_analysis AS ( 
    SELECT  
        listing_id, 
        date, 
        price, 
        available, 
        LAG(price, 1) OVER (PARTITION BY listing_id ORDER BY date) as prev_price, 
        LEAD(price, 1) OVER (PARTITION BY listing_id ORDER BY date) as next_price, 
        -- Calculate 7-day occupancy rate 
        AVG(CASE WHEN available = FALSE THEN 1 ELSE 0 END) OVER ( 
            PARTITION BY listing_id  
            ORDER BY date  
            ROWS BETWEEN 3 PRECEDING AND 3 FOLLOWING 
        ) as occupancy_rate_7day 
    FROM listing_calendar 
) 
SELECT  
    listing_id, 
    date, 
    price, 
    prev_price, 
    ROUND((price - prev_price) * 100.0 / prev_price, 2) as price_change_pct, 
    ROUND(occupancy_rate_7day * 100, 2) as occupancy_pct, 
    CASE  
        WHEN (price - prev_price) * 100.0 / prev_price > 20 THEN 'Surge Pricing' 
        WHEN (price - prev_price) * 100.0 / prev_price < -20 THEN 'Deep Discount' 
        ELSE 'Normal' 
    END as pricing_strategy 
FROM pricing_analysis 
WHERE prev_price IS NOT NULL 
    AND (price - prev_price) * 100.0 / prev_price > 20  -- Filter for >20% increases 
ORDER BY listing_id, date; 
 -- Alternative with more insights: 
WITH calendar_metrics AS ( 
    SELECT  
        listing_id, 
        date, 
        price, 
        available, 
        LAG(price, 1) OVER (PARTITION BY listing_id ORDER BY date) as prev_day_price, 
        LAG(price, 7) OVER (PARTITION BY listing_id ORDER BY date) as prev_week_price, 
        AVG(price) OVER ( 
            PARTITION BY listing_id  
            ORDER BY date  
            ROWS BETWEEN 6 PRECEDING AND CURRENT ROW 
        ) as avg_price_7day, 
        SUM(CASE WHEN available = FALSE THEN 1 ELSE 0 END) OVER ( 
            PARTITION BY listing_id  
            ORDER BY date  
            ROWS BETWEEN 6 PRECEDING AND CURRENT ROW 
        ) * 100.0 / 7 as occupancy_rate_7day 
    FROM listing_calendar 
) 
SELECT  
    listing_id, 
    date, 
    price, 
    prev_day_price, 
    ROUND((price - prev_day_price) * 100.0 / NULLIF(prev_day_price, 0), 2) as 
daily_price_change_pct, 
    ROUND((price - prev_week_price) * 100.0 / NULLIF(prev_week_price, 0), 2) as 
weekly_price_change_pct, 
    ROUND(price - avg_price_7day, 2) as diff_from_7day_avg, 
    ROUND(occupancy_rate_7day, 1) as occupancy_pct 
FROM calendar_metrics 
WHERE date >= CURRENT_DATE - INTERVAL '30 days' 
    AND prev_day_price IS NOT NULL 
ORDER BY listing_id, date DESC; 
 
Question 10: Comprehensive Business Question -- Solution: 
WITH user_cohorts AS ( 
    -- Get users who signed up in Q1 2024 
    SELECT  
        user_id, 
        acquisition_channel, 
        signup_date, 
        first_booking_date 
    FROM users 
    WHERE signup_date BETWEEN '2024-01-01' AND '2024-03-31' 
), 
channel_acquisitions AS ( 
    -- Count acquisitions by channel 
    SELECT  
        acquisition_channel, 
        COUNT(DISTINCT user_id) as users_acquired, 
        COUNT(DISTINCT CASE WHEN first_booking_date IS NOT NULL THEN user_id END) as 
users_with_bookings 
    FROM user_cohorts 
    GROUP BY acquisition_channel 
), 
channel_spending AS ( 
    -- Calculate total spend by channel for Q1 2024 
    SELECT  
        channel, 
        SUM(daily_spend) as total_spend 
    FROM marketing_spend 
    WHERE date BETWEEN '2024-01-01' AND '2024-03-31' 
    GROUP BY channel 
), 
user_ltv AS ( 
    -- Calculate LTV for each user 
    SELECT  
        u.user_id, 
        u.acquisition_channel, 
        COUNT(b.booking_id) as total_bookings, 
        SUM(b.total_value) as lifetime_value 
    FROM user_cohorts u 
    LEFT JOIN bookings b 
        ON u.user_id = b.user_id 
        AND b.booking_status = 'completed' 
    GROUP BY u.user_id, u.acquisition_channel 
), 
channel_ltv AS ( 
    -- Average LTV by channel 
    SELECT  
        acquisition_channel, 
        AVG(lifetime_value) as avg_ltv, 
        PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY lifetime_value) as median_ltv, 
        AVG(total_bookings) as avg_bookings_per_user 
    FROM user_ltv 
    GROUP BY acquisition_channel 
), 
final_metrics AS ( 
    -- Combine all metrics 
    SELECT  
        ca.acquisition_channel, 
        ca.users_acquired, 
        ca.users_with_bookings, 
        ROUND(100.0 * ca.users_with_bookings / ca.users_acquired, 2) as conversion_rate, 
        cs.total_spend, 
        ROUND(cs.total_spend / ca.users_acquired, 2) as cac, 
        ROUND(cl.avg_ltv, 2) as avg_ltv, 
        ROUND(cl.median_ltv, 2) as median_ltv, 
        ROUND(cl.avg_ltv / NULLIF(cs.total_spend / ca.users_acquired, 0), 2) as ltv_cac_ratio, 
        cl.avg_bookings_per_user 
    FROM channel_acquisitions ca 
    LEFT JOIN channel_spending cs ON ca.acquisition_channel = cs.channel 
    LEFT JOIN channel_ltv cl ON ca.acquisition_channel = cl.acquisition_channel 
) -- Final output with channels where LTV/CAC > 3 
SELECT  
    acquisition_channel, 
    users_acquired, 
    conversion_rate, 
    cac, 
    avg_ltv, 
    median_ltv, 
    ltv_cac_ratio, 
    CASE  
        WHEN ltv_cac_ratio > 3 THEN 'High Performance' 
        WHEN ltv_cac_ratio > 2 THEN 'Good Performance' 
        WHEN ltv_cac_ratio > 1 THEN 'Breaking Even' 
        ELSE 'Underperforming' 
    END as channel_status 
FROM final_metrics 
WHERE ltv_cac_ratio IS NOT NULL 
ORDER BY ltv_cac_ratio DESC; 
 -- Simpler version focusing on key metrics: 
WITH q1_metrics AS ( 
    SELECT  
        u.acquisition_channel, 
        COUNT(DISTINCT u.user_id) as users_acquired, 
        SUM(ms.daily_spend) as total_spend, 
        SUM(b.total_value) as total_revenue 
    FROM users u 
    LEFT JOIN bookings b  
        ON u.user_id = b.user_id  
        AND b.booking_status = 'completed' 
    LEFT JOIN marketing_spend ms  
        ON u.acquisition_channel = ms.channel  
        AND ms.date BETWEEN '2024-01-01' AND '2024-03-31' 
    WHERE u.signup_date BETWEEN '2024-01-01' AND '2024-03-31' 
    GROUP BY u.acquisition_channel 
) 
SELECT  
    acquisition_channel, 
    users_acquired, 
    ROUND(total_spend / NULLIF(users_acquired, 0), 2) as cac, 
    ROUND(total_revenue / NULLIF(users_acquired, 0), 2) as avg_ltv, 
    ROUND(total_revenue / NULLIF(total_spend, 0), 2) as roi 
FROM q1_metrics 
WHERE total_revenue / NULLIF(total_spend, 0) > 3 
ORDER BY roi DESC; 
 
Key Takeaways and Tips: 
1. Always handle NULLs - Use COALESCE, NULLIF, or explicit NULL checks 
2. Consider performance - CTEs are often more readable than nested subqueries 
3. Window functions - Know when to use ROWS vs RANGE in window frames 
4. Business context - Always think about what the metric means for Airbnb's business 
5. Edge cases - Consider scenarios like new users with no bookings, hosts with no 
reviews, etc. 
Remember during the interview: 
● Start with a simple version, then add complexity 
● Explain your assumptions about the data 
● Mention alternative approaches when relevant 
● Think about query performance for large datasets 
Comprehensive SQL Interview Questions 
Guide 
Table of Contents 
1. Sample Database Schema 
2. Basic SELECT Queries 
3. Filtering and Conditional Logic 
4. JOIN Operations 
5. Aggregations and GROUP BY 
6. Subqueries 
7. Window Functions 
8. Common Table Expressions (CTEs) 
9. Data Modification 
10. Advanced Patterns 
Sample Database Schema 
Let's work with a company database containing employees, departments, projects, and sales 
data: 
-- Employees table 
CREATE TABLE employees ( 
employee_id INT PRIMARY KEY, 
f
 irst_name VARCHAR(50), 
last_name VARCHAR(50), 
email VARCHAR(100), 
phone_number VARCHAR(20), 
hire_date DATE, 
job_title VARCHAR(50), 
salary DECIMAL(10, 2), 
manager_id INT, 
department_id INT 
); -- Sample data 
INSERT INTO employees VALUES 
(1, 'John', 'Smith', 'john.smith@company.com', '555-0101', '2020-01-15', 'Software Engineer', 
75000, 5, 1), 
(2, 'Sarah', 'Johnson', 'sarah.j@company.com', '555-0102', '2019-03-22', 'Senior Software 
Engineer', 95000, 5, 1), 
(3, 'Michael', 'Brown', 'michael.b@company.com', '555-0103', '2021-06-01', 'Data Analyst', 
65000, 6, 2), 
(4, 'Emily', 'Davis', 'emily.d@company.com', '555-0104', '2020-09-15', 'Marketing Manager', 
80000, 7, 3), 
(5, 'David', 'Wilson', 'david.w@company.com', '555-0105', '2018-02-01', 'Engineering 
Manager', 110000, NULL, 1), 
(6, 'Lisa', 'Anderson', 'lisa.a@company.com', '555-0106', '2019-07-10', 'Analytics Manager', 
95000, NULL, 2), 
(7, 'James', 'Taylor', 'james.t@company.com', '555-0107', '2017-05-15', 'Marketing Director', 
120000, NULL, 3), 
(8, 'Jennifer', 'Martinez', 'jennifer.m@company.com', '555-0108', '2022-01-10', 'Junior 
Developer', 60000, 2, 1), 
(9, 'Robert', 'Lee', 'robert.l@company.com', '555-0109', '2021-11-01', 'Sales Representative', 
55000, 10, 4), 
(10, 'Maria', 'Garcia', 'maria.g@company.com', '555-0110', '2018-08-20', 'Sales Manager', 
90000, NULL, 4); -- Departments table 
CREATE TABLE departments ( 
department_id INT PRIMARY KEY, 
department_name VARCHAR(50), 
location VARCHAR(100) 
); 
INSERT INTO departments VALUES 
(1, 'Engineering', 'San Francisco'), 
(2, 'Analytics', 'New York'), 
(3, 'Marketing', 'Los Angeles'), 
(4, 'Sales', 'Chicago'), 
(5, 'HR', 'Boston'); -- Projects table 
CREATE TABLE projects ( 
project_id INT PRIMARY KEY, 
project_name VARCHAR(100), 
start_date DATE, 
end_date DATE, 
budget DECIMAL(12, 2) 
); 
INSERT INTO projects VALUES 
(1, 'Mobile App Development', '2023-01-01', '2023-06-30', 500000), 
(2, 'Data Warehouse Migration', '2023-03-15', '2023-09-15', 750000), 
(3, 'Marketing Campaign Q2', '2023-04-01', '2023-06-30', 200000), 
(4, 'Customer Portal', '2023-02-01', '2023-08-31', 600000); -- Employee_Projects table (many-to-many relationship) 
CREATE TABLE employee_projects ( 
employee_id INT, 
project_id INT, 
role VARCHAR(50), 
hours_allocated INT, 
PRIMARY KEY (employee_id, project_id) 
); 
INSERT INTO employee_projects VALUES 
(1, 1, 'Developer', 500), 
(1, 4, 'Developer', 300), 
(2, 1, 'Lead Developer', 600), 
(2, 2, 'Consultant', 200), 
(3, 2, 'Data Analyst', 800), 
(4, 3, 'Project Manager', 400), 
(5, 1, 'Technical Lead', 300), 
(8, 4, 'Developer', 700); -- Sales table 
CREATE TABLE sales ( 
sale_id INT PRIMARY KEY, 
employee_id INT, 
sale_date DATE, 
amount DECIMAL(10, 2), 
product_category VARCHAR(50) 
); 
INSERT INTO sales VALUES 
(1, 9, '2023-01-15', 15000, 'Software'), 
(2, 9, '2023-01-20', 8000, 'Services'), 
(3, 10, '2023-01-22', 25000, 'Software'), 
(4, 9, '2023-02-10', 12000, 'Hardware'), 
(5, 10, '2023-02-15', 30000, 'Software'), 
(6, 9, '2023-03-01', 18000, 'Services'), 
(7, 10, '2023-03-10', 22000, 'Hardware'), 
(8, 9, '2023-03-15', 9000, 'Software'); 
Basic SELECT Queries 
Question 1: Select all employees 
Question: Retrieve all employee information from the employees table. 
Answer: 
SELECT * FROM employees; 
Question 2: Select specific columns 
Question: Get the first name, last name, and salary of all employees. 
Answer: 
SELECT first_name, last_name, salary  
FROM employees; 
Question 3: Using aliases 
Question: Display employee names with their annual salary (use meaningful column aliases). 
Answer: 
SELECT  
f
 irst_name + ' ' + last_name AS full_name, 
salary AS monthly_salary, 
salary * 12 AS annual_salary 
FROM employees; -- For MySQL/PostgreSQL use CONCAT: 
SELECT  
CONCAT(first_name, ' ', last_name) AS full_name, 
salary AS monthly_salary, 
salary * 12 AS annual_salary 
FROM employees; 
Question 4: DISTINCT values 
Question: Find all unique job titles in the company. 
Answer: 
SELECT DISTINCT job_title  
FROM employees 
ORDER BY job_title; 
Filtering and Conditional Logic 
Question 5: WHERE clause 
Question: Find all employees earning more than $80,000. 
Answer: 
SELECT first_name, last_name, salary 
FROM employees 
WHERE salary > 80000 
ORDER BY salary DESC; 
Question 6: Multiple conditions 
Question: Find employees in the Engineering department (id=1) earning between $70,000 and 
$100,000. 
Answer: 
SELECT first_name, last_name, salary, department_id 
FROM employees 
WHERE department_id = 1  
AND salary BETWEEN 70000 AND 100000 
ORDER BY salary; 
Question 7: Pattern matching with LIKE 
Question: Find all employees whose email starts with 'j'. 
Answer: 
SELECT first_name, last_name, email 
FROM employees 
WHERE email LIKE 'j%'; 
Question 8: IN operator 
Question: Find employees who are either managers or directors. 
Answer: 
SELECT first_name, last_name, job_title 
FROM employees 
WHERE job_title IN ('Engineering Manager', 'Marketing Director', 'Sales Manager', 'Analytics 
Manager') 
ORDER BY job_title; 
Question 9: NULL handling 
Question: Find all employees who don't have a manager (top-level employees). 
Answer: 
SELECT first_name, last_name, job_title 
FROM employees 
WHERE manager_id IS NULL; 
Question 10: CASE statements 
Question: Categorize employees by salary ranges. 
Answer: 
SELECT  
f
 irst_name, 
last_name, 
salary, 
    CASE  
        WHEN salary < 60000 THEN 'Entry Level' 
        WHEN salary BETWEEN 60000 AND 80000 THEN 'Mid Level' 
        WHEN salary BETWEEN 80001 AND 100000 THEN 'Senior Level' 
        ELSE 'Executive Level' 
    END AS salary_category 
FROM employees 
ORDER BY salary; 
 
 
JOIN Operations 
Question 11: INNER JOIN 
Question: List all employees with their department names. 
Answer: 
SELECT  
    e.first_name, 
    e.last_name, 
    e.job_title, 
    d.department_name, 
    d.location 
FROM employees e 
INNER JOIN departments d ON e.department_id = d.department_id 
ORDER BY d.department_name, e.last_name; 
 
Question 12: LEFT JOIN 
Question: Show all departments and their employees (including departments with no 
employees). 
Answer: 
SELECT  
d.department_name, 
d.location, 
e.first_name, 
e.last_name 
FROM departments d 
LEFT JOIN employees e ON d.department_id = e.department_id 
ORDER BY d.department_name, e.last_name; 
Question 13: Self JOIN 
Question: List all employees with their manager's name. 
Answer: 
SELECT  
e.first_name + ' ' + e.last_name AS employee_name, 
e.job_title, 
m.first_name + ' ' + m.last_name AS manager_name 
FROM employees e 
LEFT JOIN employees m ON e.manager_id = m.employee_id 
ORDER BY manager_name, employee_name; 
Question 14: Multiple JOINs 
Question: Show employees with their department and project information. 
Answer: 
SELECT  
    e.first_name, 
    e.last_name, 
    d.department_name, 
    p.project_name, 
    ep.role, 
    ep.hours_allocated 
FROM employees e 
INNER JOIN departments d ON e.department_id = d.department_id 
INNER JOIN employee_projects ep ON e.employee_id = ep.employee_id 
INNER JOIN projects p ON ep.project_id = p.project_id 
ORDER BY e.last_name, p.project_name; 
 
Question 15: FULL OUTER JOIN (or simulation) 
Question: Find all employees and all departments, showing matches where they exist. 
Answer: -- For databases supporting FULL OUTER JOIN: 
SELECT  
    e.first_name, 
    e.last_name, 
    d.department_name 
FROM employees e 
FULL OUTER JOIN departments d ON e.department_id = d.department_id; 
 -- For MySQL (doesn't support FULL OUTER JOIN): 
SELECT  
    e.first_name, 
e.last_name, 
d.department_name 
FROM employees e 
LEFT JOIN departments d ON e.department_id = d.department_id 
UNION 
SELECT  
e.first_name, 
e.last_name, 
d.department_name 
FROM employees e 
RIGHT JOIN departments d ON e.department_id = d.department_id; 
Aggregations and GROUP BY 
Question 16: Basic aggregation 
Question: Find the average, minimum, and maximum salary in the company. 
Answer: 
SELECT  
AVG(salary) AS avg_salary, 
MIN(salary) AS min_salary, 
MAX(salary) AS max_salary, 
COUNT(*) AS total_employees 
FROM employees; 
Question 17: GROUP BY with COUNT 
Question: Count the number of employees in each department. 
Answer: 
SELECT  
d.department_name, 
COUNT(e.employee_id) AS employee_count 
FROM departments d 
LEFT JOIN employees e ON d.department_id = e.department_id 
GROUP BY d.department_id, d.department_name 
ORDER BY employee_count DESC; 
Question 18: GROUP BY with HAVING 
Question: Find departments with average salary above $80,000. 
Answer: 
SELECT  
d.department_name, 
AVG(e.salary) AS avg_salary, 
COUNT(e.employee_id) AS employee_count 
FROM departments d 
INNER JOIN employees e ON d.department_id = e.department_id 
GROUP BY d.department_id, d.department_name 
HAVING AVG(e.salary) > 80000 
ORDER BY avg_salary DESC; 
Question 19: Multiple aggregations 
Question: For each department, show total salary expense and average salary. 
Answer: 
SELECT  
d.department_name, 
COUNT(e.employee_id) AS num_employees, 
SUM(e.salary) AS total_salary_expense, 
AVG(e.salary) AS average_salary, 
MIN(e.salary) AS min_salary, 
MAX(e.salary) AS max_salary 
FROM departments d 
LEFT JOIN employees e ON d.department_id = e.department_id 
GROUP BY d.department_id, d.department_name 
ORDER BY total_salary_expense DESC; 
Question 20: GROUP BY with multiple columns 
Question: Find the number of employees by department and job title. 
Answer: 
SELECT  
d.department_name, 
e.job_title, 
COUNT(*) AS employee_count, 
AVG(e.salary) AS avg_salary 
FROM employees e 
INNER JOIN departments d ON e.department_id = d.department_id 
GROUP BY d.department_name, e.job_title 
ORDER BY d.department_name, employee_count DESC; 
Subqueries 
Question 21: Subquery in WHERE clause 
Question: Find employees who earn more than the average salary. 
Answer: 
SELECT  
    first_name, 
    last_name, 
    salary 
FROM employees 
WHERE salary > (SELECT AVG(salary) FROM employees) 
ORDER BY salary DESC; 
 
Question 22: Subquery with IN 
Question: Find employees who work on projects with budget over $600,000. 
Answer: 
SELECT DISTINCT 
    e.first_name, 
    e.last_name 
FROM employees e 
WHERE e.employee_id IN ( 
    SELECT ep.employee_id 
    FROM employee_projects ep 
    INNER JOIN projects p ON ep.project_id = p.project_id 
    WHERE p.budget > 600000 
) 
ORDER BY e.last_name; 
 
Question 23: Correlated subquery 
Question: Find employees who earn more than the average salary in their department. 
Answer: 
SELECT  
    e1.first_name, 
    e1.last_name, 
    e1.salary, 
    e1.department_id 
FROM employees e1 
WHERE e1.salary > ( 
    SELECT AVG(e2.salary) 
    FROM employees e2 
    WHERE e2.department_id = e1.department_id 
) 
ORDER BY e1.department_id, e1.salary DESC; 
 
Question 24: EXISTS clause 
Question: Find departments that have at least one employee earning over $100,000. 
Answer: 
SELECT  
    d.department_name, 
    d.location 
FROM departments d 
WHERE EXISTS ( 
    SELECT 1 
    FROM employees e 
    WHERE e.department_id = d.department_id 
      AND e.salary > 100000 
); 
 
Question 25: Subquery in SELECT clause 
Question: Show each employee with their department's average salary. 
Answer: 
SELECT  
    e.first_name, 
    e.last_name, 
    e.salary, 
    (SELECT AVG(salary)  
     FROM employees e2  
     WHERE e2.department_id = e.department_id) AS dept_avg_salary, 
    e.salary - (SELECT AVG(salary)  
                FROM employees e2  
                WHERE e2.department_id = e.department_id) AS salary_diff 
FROM employees e 
ORDER BY e.department_id, salary_diff DESC; 
 
 
Window Functions 
Question 26: ROW_NUMBER() 
Question: Rank employees by salary within each department. 
Answer: 
SELECT  
    first_name, 
    last_name, 
    department_id, 
    salary, 
    ROW_NUMBER() OVER (PARTITION BY department_id ORDER BY salary DESC) AS 
salary_rank 
FROM employees 
ORDER BY department_id, salary_rank; 
 
Question 27: RANK() vs DENSE_RANK() 
Question: Show the difference between RANK and DENSE_RANK for employee salaries. 
Answer: 
SELECT  
    first_name, 
    last_name, 
    salary, 
    RANK() OVER (ORDER BY salary DESC) AS rank, 
    DENSE_RANK() OVER (ORDER BY salary DESC) AS dense_rank, 
    ROW_NUMBER() OVER (ORDER BY salary DESC) AS row_num 
FROM employees 
ORDER BY salary DESC; 
 
Question 28: Running totals 
Question: Calculate running total of sales by date. 
Answer: 
SELECT  
    sale_date, 
    employee_id, 
    amount, 
    SUM(amount) OVER (ORDER BY sale_date) AS running_total, 
    SUM(amount) OVER (PARTITION BY employee_id ORDER BY sale_date) AS 
running_total_by_employee 
FROM sales 
ORDER BY sale_date; 
 
Question 29: LAG and LEAD 
Question: Compare each sale with the previous and next sale. 
Answer: 
SELECT  
    sale_id, 
    sale_date, 
    amount, 
    LAG(amount, 1) OVER (ORDER BY sale_date) AS previous_sale, 
    LEAD(amount, 1) OVER (ORDER BY sale_date) AS next_sale, 
    amount - LAG(amount, 1) OVER (ORDER BY sale_date) AS diff_from_previous 
FROM sales 
ORDER BY sale_date; 
 
Question 30: Moving averages 
Question: Calculate 3-month moving average of sales. 
Answer: 
SELECT  
    sale_date, 
    amount, 
    AVG(amount) OVER ( 
        ORDER BY sale_date  
        ROWS BETWEEN 2 PRECEDING AND CURRENT ROW 
    ) AS moving_avg_3 
FROM sales 
ORDER BY sale_date; 
 
Question 31: NTILE 
Question: Divide employees into salary quartiles. 
Answer: 
SELECT  
    first_name, 
    last_name, 
    salary, 
    NTILE(4) OVER (ORDER BY salary) AS salary_quartile 
FROM employees 
ORDER BY salary; 
 
 
Common Table Expressions (CTEs) 
Question 32: Basic CTE 
Question: Use a CTE to find high-earning employees (top 25%). 
Answer: 
WITH salary_stats AS ( 
    SELECT  
        PERCENTILE_CONT(0.75) WITHIN GROUP (ORDER BY salary) AS percentile_75 
    FROM employees 
) 
SELECT  
    e.first_name, 
    e.last_name, 
    e.salary 
FROM employees e, salary_stats s 
WHERE e.salary >= s.percentile_75 
ORDER BY e.salary DESC; 
 -- Alternative using subquery for databases without PERCENTILE_CONT: 
WITH salary_ranked AS ( 
    SELECT  
        first_name, 
        last_name, 
        salary, 
        NTILE(4) OVER (ORDER BY salary DESC) AS quartile 
    FROM employees 
) 
SELECT  
    first_name, 
    last_name, 
    salary 
FROM salary_ranked 
WHERE quartile = 1 
ORDER BY salary DESC; 
 
Question 33: Multiple CTEs 
Question: Calculate department statistics and identify departments above average. 
Answer: 
WITH dept_stats AS ( 
    SELECT  
        department_id, 
        COUNT(*) AS employee_count, 
        AVG(salary) AS avg_salary, 
        SUM(salary) AS total_salary 
    FROM employees 
    GROUP BY department_id 
), 
company_avg AS ( 
    SELECT AVG(avg_salary) AS overall_avg_salary 
    FROM dept_stats 
) 
SELECT  
    d.department_name, 
    ds.employee_count, 
    ds.avg_salary, 
    ca.overall_avg_salary, 
    ds.avg_salary - ca.overall_avg_salary AS diff_from_company_avg 
FROM dept_stats ds 
CROSS JOIN company_avg ca 
INNER JOIN departments d ON ds.department_id = d.department_id 
WHERE ds.avg_salary > ca.overall_avg_salary 
ORDER BY diff_from_company_avg DESC; 
 
Question 34: Recursive CTE 
Question: Build an employee hierarchy tree showing reporting structure. 
Answer: 
WITH RECURSIVE emp_hierarchy AS ( 
    -- Anchor: top-level employees 
    SELECT  
        employee_id, 
        first_name, 
        last_name, 
        manager_id, 
        job_title, 
        0 AS level, 
        CAST(employee_id AS VARCHAR(200)) AS path 
    FROM employees 
    WHERE manager_id IS NULL 
     
    UNION ALL 
     
    -- Recursive part 
    SELECT  
        e.employee_id, 
        e.first_name, 
        e.last_name, 
        e.manager_id, 
        e.job_title, 
        h.level + 1, 
        CONCAT(h.path, '->', e.employee_id) 
    FROM employees e 
    INNER JOIN emp_hierarchy h ON e.manager_id = h.employee_id 
) 
SELECT  
    level, 
    REPEAT('  ', level) || first_name || ' ' || last_name AS employee_name, 
    job_title, 
    path 
FROM emp_hierarchy 
ORDER BY path; 
 
 
Data Modification 
Question 35: INSERT with SELECT 
Question: Create a backup of high-salary employees. 
Answer: -- First create the backup table 
CREATE TABLE high_earners_backup AS 
SELECT * FROM employees WHERE 1=0; -- Structure only 
 -- Insert high earners 
INSERT INTO high_earners_backup 
SELECT *  
FROM employees 
WHERE salary > 90000; 
Question 36: UPDATE with JOIN 
Question: Give a 10% raise to all employees in the Engineering department. 
Answer: 
UPDATE employees e 
SET e.salary = e.salary * 1.10 
WHERE e.department_id IN ( 
SELECT department_id  
FROM departments  
WHERE department_name = 'Engineering' 
); -- Alternative with JOIN (MySQL syntax): 
UPDATE employees e 
INNER JOIN departments d ON e.department_id = d.department_id 
SET e.salary = e.salary * 1.10 
WHERE d.department_name = 'Engineering'; 
Question 37: DELETE with subquery 
Question: Delete employees who haven't been assigned to any projects. 
Answer: 
DELETE FROM employees 
WHERE employee_id NOT IN ( 
SELECT DISTINCT employee_id  
FROM employee_projects 
); 
 -- Alternative using NOT EXISTS: 
DELETE FROM employees e 
WHERE NOT EXISTS ( 
    SELECT 1  
    FROM employee_projects ep  
    WHERE ep.employee_id = e.employee_id 
); 
 
Question 38: MERGE/UPSERT 
Question: Update or insert employee records based on a staging table. 
Answer: -- SQL Server / Oracle MERGE syntax: 
MERGE employees AS target 
USING staging_employees AS source 
ON target.employee_id = source.employee_id 
WHEN MATCHED THEN 
    UPDATE SET  
        target.salary = source.salary, 
        target.job_title = source.job_title 
WHEN NOT MATCHED THEN 
    INSERT (employee_id, first_name, last_name, salary, job_title) 
    VALUES (source.employee_id, source.first_name, source.last_name,  
            source.salary, source.job_title); 
 -- MySQL UPSERT: 
INSERT INTO employees (employee_id, first_name, last_name, salary) 
VALUES (1, 'John', 'Smith', 80000) 
ON DUPLICATE KEY UPDATE 
salary = VALUES(salary); -- PostgreSQL UPSERT: 
INSERT INTO employees (employee_id, first_name, last_name, salary) 
VALUES (1, 'John', 'Smith', 80000) 
ON CONFLICT (employee_id) DO UPDATE 
SET salary = EXCLUDED.salary; 
Advanced Patterns 
Question 39: Finding duplicates 
Question: Find duplicate email addresses in the employees table. 
Answer: -- Method 1: Using GROUP BY and HAVING 
SELECT  
email, 
COUNT(*) AS duplicate_count 
FROM employees 
GROUP BY email 
HAVING COUNT(*) > 1; -- Method 2: Using Window Functions 
WITH email_counts AS ( 
    SELECT  
        *, 
        COUNT(*) OVER (PARTITION BY email) AS email_count 
    FROM employees 
) 
SELECT * 
FROM email_counts 
WHERE email_count > 1 
ORDER BY email, employee_id; 
 
Question 40: Gaps and Islands 
Question: Find gaps in employee IDs. 
Answer: 
WITH id_gaps AS ( 
    SELECT  
        employee_id, 
        LEAD(employee_id) OVER (ORDER BY employee_id) AS next_id 
    FROM employees 
) 
SELECT  
    employee_id + 1 AS gap_start, 
    next_id - 1 AS gap_end 
FROM id_gaps 
WHERE next_id - employee_id > 1; 
 
Question 41: Pivot table 
Question: Create a pivot table showing employee count by department and job title. 
Answer: -- Using CASE statements (works in all databases): 
SELECT  
    job_title, 
    SUM(CASE WHEN department_id = 1 THEN 1 ELSE 0 END) AS Engineering, 
    SUM(CASE WHEN department_id = 2 THEN 1 ELSE 0 END) AS Analytics, 
    SUM(CASE WHEN department_id = 3 THEN 1 ELSE 0 END) AS Marketing, 
    SUM(CASE WHEN department_id = 4 THEN 1 ELSE 0 END) AS Sales 
FROM employees 
GROUP BY job_title; 
 -- SQL Server PIVOT syntax: 
SELECT * 
FROM ( 
    SELECT job_title, department_id 
    FROM employees 
) AS source_table 
PIVOT ( 
    COUNT(department_id) 
    FOR department_id IN ([1], [2], [3], [4]) 
) AS pivot_table; 
 
Question 42: Cumulative percentage 
Question: Calculate cumulative percentage of total salary by employee. 
Answer: 
WITH salary_ordered AS ( 
    SELECT  
        first_name, 
        last_name, 
        salary, 
        SUM(salary) OVER (ORDER BY salary DESC) AS cumulative_salary, 
        SUM(salary) OVER () AS total_salary 
    FROM employees 
) 
SELECT  
    first_name, 
    last_name, 
    salary, 
    cumulative_salary, 
    ROUND(100.0 * cumulative_salary / total_salary, 2) AS cumulative_percentage 
FROM salary_ordered 
ORDER BY salary DESC; 
 
Question 43: Top N per group 
Question: Find the top 2 highest-paid employees in each department. 
Answer: -- Using ROW_NUMBER() 
WITH ranked_employees AS ( 
    SELECT  
        e.*, 
        d.department_name, 
        ROW_NUMBER() OVER (PARTITION BY e.department_id ORDER BY e.salary DESC) AS 
rank 
    FROM employees e 
    INNER JOIN departments d ON e.department_id = d.department_id 
) 
SELECT  
    department_name, 
    first_name, 
    last_name, 
    salary, 
    rank 
FROM ranked_employees 
WHERE rank <= 2 
ORDER BY department_name, rank; 
 
Question 44: Year-over-Year comparison 
Question: Compare monthly sales with the same month from the previous year. 
Answer: 
WITH monthly_sales AS ( 
    SELECT  
        DATE_FORMAT(sale_date, '%Y-%m') AS month, 
        SUM(amount) AS total_sales 
    FROM sales 
    GROUP BY DATE_FORMAT(sale_date, '%Y-%m') 
), 
yoy_comparison AS ( 
    SELECT  
        month, 
        total_sales, 
        LAG(total_sales, 12) OVER (ORDER BY month) AS prev_year_sales 
    FROM monthly_sales 
) 
SELECT  
    month, 
    total_sales, 
    prev_year_sales, 
    ROUND(100.0 * (total_sales - prev_year_sales) / prev_year_sales, 2) AS yoy_growth_percent 
FROM yoy_comparison 
WHERE prev_year_sales IS NOT NULL; 
 
Question 45: Finding consecutive records 
Question: Find employees hired on consecutive days. 
Answer: 
WITH consecutive_hires AS ( 
    SELECT  
        e1.first_name AS emp1_first, 
        e1.last_name AS emp1_last, 
        e1.hire_date AS emp1_hire_date, 
        e2.first_name AS emp2_first, 
        e2.last_name AS emp2_last, 
        e2.hire_date AS emp2_hire_date 
    FROM employees e1 
    INNER JOIN employees e2  
        ON e2.hire_date = DATE_ADD(e1.hire_date, INTERVAL 1 DAY) 
) 
SELECT * FROM consecutive_hires 
ORDER BY emp1_hire_date; 
 
Question 46: Hierarchical queries 
Question: Show the complete reporting chain for each employee. 
Answer: 
WITH RECURSIVE reporting_chain AS ( 
    SELECT  
        e.employee_id, 
        e.first_name, 
        e.last_name, 
        e.manager_id, 
        e.first_name || ' ' || e.last_name AS reporting_chain, 
        1 AS chain_level 
    FROM employees e 
     
    UNION ALL 
     
    SELECT  
        rc.employee_id, 
        rc.first_name, 
        rc.last_name, 
        e.manager_id, 
        e.first_name || ' ' || e.last_name || ' -> ' || rc.reporting_chain AS reporting_chain, 
        rc.chain_level + 1 
    FROM reporting_chain rc 
    INNER JOIN employees e ON rc.manager_id = e.employee_id 
    WHERE e.manager_id IS NOT NULL 
) 
SELECT  
    employee_id, 
    first_name || ' ' || last_name AS employee_name, 
    reporting_chain, 
    chain_level 
FROM reporting_chain 
WHERE manager_id IS NULL 
ORDER BY employee_id; 
 
Question 47: Data validation queries 
Question: Find data quality issues in the employees table. 
Answer: -- Check for various data quality issues 
WITH data_issues AS ( 
    -- Check for null values in required fields 
    SELECT 'NULL in required field' AS issue_type, COUNT(*) AS issue_count 
    FROM employees 
    WHERE first_name IS NULL OR last_name IS NULL OR email IS NULL 
     
    UNION ALL 
     
    -- Check for duplicate emails 
    SELECT 'Duplicate email' AS issue_type, COUNT(*) - COUNT(DISTINCT email) AS 
issue_count 
    FROM employees 
     
    UNION ALL 
     
    -- Check for invalid salary values 
    SELECT 'Invalid salary' AS issue_type, COUNT(*) AS issue_count 
    FROM employees 
    WHERE salary <= 0 OR salary > 1000000 
     
    UNION ALL 
     
    -- Check for future hire dates 
    SELECT 'Future hire date' AS issue_type, COUNT(*) AS issue_count 
    FROM employees 
    WHERE hire_date > CURRENT_DATE 
     
    UNION ALL 
     
    -- Check for employees reporting to themselves 
    SELECT 'Self-reporting' AS issue_type, COUNT(*) AS issue_count 
    FROM employees 
    WHERE employee_id = manager_id 
) 
SELECT * FROM data_issues 
WHERE issue_count > 0; 
 
Question 48: Complex date calculations 
Question: Calculate employee tenure and categorize by experience level. 
Answer: 
SELECT  
    first_name, 
    last_name, 
    hire_date, 
    DATEDIFF(CURRENT_DATE, hire_date) AS days_employed, 
    FLOOR(DATEDIFF(CURRENT_DATE, hire_date) / 365.25) AS years_employed, 
    FLOOR((DATEDIFF(CURRENT_DATE, hire_date) % 365.25) / 30) AS months_employed, 
    CASE  
        WHEN DATEDIFF(CURRENT_DATE, hire_date) < 365 THEN 'New Employee (<1 year)' 
        WHEN DATEDIFF(CURRENT_DATE, hire_date) < 365 * 3 THEN 'Junior (1-3 years)' 
        WHEN DATEDIFF(CURRENT_DATE, hire_date) < 365 * 5 THEN 'Mid-level (3-5 years)' 
        ELSE 'Senior (5+ years)' 
    END AS experience_level 
FROM employees 
ORDER BY days_employed DESC; 
 
Question 49: String manipulation 
Question: Parse and clean employee email addresses. 
Answer: 
SELECT  
    email, 
    SUBSTRING_INDEX(email, '@', 1) AS username, 
    SUBSTRING_INDEX(email, '@', -1) AS domain, 
    UPPER(LEFT(first_name, 1)) || LOWER(SUBSTRING(first_name, 2)) AS proper_first_name, 
    LENGTH(email) - LENGTH(REPLACE(email, '.', '')) AS dot_count, 
    CASE  
        WHEN email REGEXP '^[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Za-z]{2,}$'  
        THEN 'Valid'  
ELSE 'Invalid'  
END AS email_validation 
FROM employees; 
Question 50: Performance optimization patterns 
Question: Rewrite this inefficient query to find employees not assigned to projects. 
Inefficient Query: 
SELECT * FROM employees  
WHERE employee_id NOT IN (SELECT employee_id FROM employee_projects); 
Optimized Answer: -- Method 1: Using NOT EXISTS (typically fastest) 
SELECT e.* 
FROM employees e 
WHERE NOT EXISTS ( 
SELECT 1  
FROM employee_projects ep  
WHERE ep.employee_id = e.employee_id 
); -- Method 2: Using LEFT JOIN 
SELECT e.* 
FROM employees e 
LEFT JOIN employee_projects ep ON e.employee_id = ep.employee_id 
WHERE ep.employee_id IS NULL; -- Method 3: Using EXCEPT (PostgreSQL/SQL Server) 
SELECT employee_id, first_name, last_name 
FROM employees 
EXCEPT 
SELECT e.employee_id, e.first_name, e.last_name 
FROM employees e 
INNER JOIN employee_projects ep ON e.employee_id = ep.employee_id; 
Common Interview Patterns to Remember 
1. Ranking Patterns 
● ROW_NUMBER(): Unique ranking 
● RANK(): Allows ties, skips numbers 
● DENSE_RANK(): Allows ties, no gaps 
● NTILE(): Divides into buckets 
2. Comparison Patterns 
● Self-joins for comparing rows 
● LAG/LEAD for sequential comparisons 
● Correlated subqueries for group comparisons 
3. Aggregation Patterns 
● GROUP BY for summaries 
● Window functions for running totals 
● CTEs for multi-level aggregations 
4. Data Quality Patterns 
● Finding duplicates 
● Identifying NULL values 
● Data validation checks 
● Consistency checks across tables 
5. Performance Patterns 
● EXISTS vs IN for better performance 
● Proper indexing considerations 
● Avoiding SELECT * 
● Using appropriate JOIN types 
6. Date/Time Patterns 
● Date arithmetic 
● Extracting date parts 
● Period-over-period comparisons 
● Finding gaps in date sequences 
Key Tips for Interviews: 
1. Always clarify requirements before writing SQL 
2. Consider edge cases (NULLs, duplicates, empty results) 
3. Think about performance for large datasets 
4. Use CTEs for readability in complex queries 
5. Test with sample data mentally as you write 
6. Know the differences between database systems (MySQL vs PostgreSQL vs SQL 
Server) 
7. Practice explaining your approach while coding 
118	Daily bookings in the US  airbnb	hard	
5%
3.2 
119	Top 2 countries by bookings  airbnb	hard	
10%
4.2 
120	First ever booking  airbnb	hard	
10%
4.2 
121	Week over week change of first ever bookings  airbnb	hard	
14%
4.4 
122	Top country by wow growth  airbnb	hard	
13%
4.2 
123	Top listing in the United States, United Kingdom and Canada  airbnb	medium	
10%
4.2 

Status	Title	Solution	Difficulty
156. Meta SQL Question: Pinpoint User Engagement		Medium
157. Meta SQL Question: Orders By Registered Users		Easy
151. Google SQL Question: How many orders were placed by each registered customer?		Easy
152. Google SQL Question: Identify Users with Highest Post Engagement		Medium
153. Google SQL Question: Display highest and lowest amounts per product, sorted alphabetically by type and descending by amount.		Hard
155. Google SQL Question: City Delivery & Takeout Tally		Medium
154. Google SQL Question: Identify most purchased product pairs, showing names and purchase count, sorted by frequency.		Hard
142. Microsoft SQL Question: Unconfirmed Numbers		Easy
140. Apple SQL Question: Calculate the overall sales rankings for all employees		Easy
150. Microsoft SQL Question: Employee with Highest Clock-ins in Each Office		Easy
148. Microsoft SQL Question: Assign a unique number to each entry		Easy
149. Microsoft SQL Question: Rank employees by number of clock-ins		Easy
141. Microsoft SQL Question: Remove the duplicate logs		Easy
143. Microsoft SQL Question: Find the caller and receiver details		Medium
144. Meta SQL Question: Find the right clients		Easy
138. Apple SQL Question: Find the Top Sales Performers for Each Sales Date		Easy
139. Google SQL Question: Find, for Each Product the Customer Who Provided the Highest Review Score		Medium
15. LinkedIn SQL Question: Conversation threads		Hard
10. Meta SQL Question: Two different products		Easy
91. Salesforce SQL Question: Find the managers		Easy
58. Uber SQL Question: Find the valid entries		Hard
60. UpWork SQL Question: Total expenditure by item		Easy
64. Google SQL Question: Monthly cohorts based on join date		Medium
121. Google SQL Question: Total revenue generated by each genre in the year 2022		Easy
49. Amazon SQL Question: Find how many bids were yesterday		Easy
73. Robinhood SQL Question: Number of seats available		Medium
42. Amazon SQL Question: Number of times solved		Hard
30. Meta SQL Question: Get the percentage of interactions		Medium
137. Telsa SQL Question: Find the total revenue rounded to the second decimal place		Easy
132. Meta SQL Question: Find the most common word in a text column		Medium
11. Meta SQL Question: Promotion effectiveness		Hard
133. DoorDash SQL Question: Find the longest consecutive sequence of events in a time-series dataset		Medium
62. UpWork SQL Question: Find the employees and managers		Medium
89. Salesforce SQL Question: Top 3 most profitable customers		Easy
116. Apple SQL Question: Top 3 best selling products in each category for the month of January 2022		Medium
27. Meta SQL Question: Calculate ROI		Medium
88. Walmart SQL Question: Second most sold item		Medium
75. DoorDash SQL Question: Bottom 50% of dashers		Medium
85. Tik Tok SQL Question: Second and third level managers		Hard
48. Amazon SQL Question: Most common toys per factory		Hard
12. Meta SQL Question: Top 3 products		Hard
45. Amazon SQL Question: Cubic feet of volume		Medium
59. UpWork SQL Question: Find the total expenditures		Easy
43. Amazon SQL Question: Unique toys in inventory		Easy
26. Meta SQL Question: Calculate the ROI for ads		Easy
50. Amazon SQL Question: Bids by category in the last 7		Easy
72. Uber SQL Question: Time elapsed between trips		Hard
41. Amazon SQL Question: Time spent working out		Hard
117. Apple SQL Question: Total sales and average price per product category		Easy
34. DoorDash SQL Question: Find the top performing merchants by day of week		Hard
Statistics and Mathematics (SMS)




CORE TOPICS






Probability & Distributions


Descriptive Statistics


Inferential Statistics


Regression Analysis


Probability & Distributions (PDS)




BROAD TOPIC
SUB-TOPICS
DETAILED TOPICS
Probability
Fundamentals
- Sample spaces and events
- Probability rules (addition, multiplication)
- Conditional probability
- Bayes’ Theorem


Probability Distributions
- Discrete distributions (Binomial, Poisson)
- Continuous distributions (Normal, Exponential)
- Expected value and variance
- Central Limit Theorem




Descriptive Statistics





Descriptive Statistics
Measures of Central Tendency
- Mean, median, mode
- Weighted averages


Measures of Dispersion
- Variance, standard deviation
- Range, interquartile range
- Skewness and kurtosis



Inferential Statistics





Inferential Statistics
Hypothesis Testing
- Null vs. alternative hypothesis
- P-values and significance levels
- Type I and Type II errors
- T-tests, ANOVA


Confidence Intervals
- Estimation and interpretation
- Margin of error
- Confidence levels (e.g., 95%)


Regression Analysis





Regression Analysis
Linear Regression
- Simple and multiple linear regression
- Least squares method
- R-squared and adjusted R-squared


Logistic Regression
- Binary and multinomial logistic regression
- Odds ratios
- Model evaluation (ROC, AUC)














Regression Modeling w/ Cases
1. Linear Regression
Use: To measure the impact of one or more independent variables (e.g., ad spend, campaign impressions) on a dependent variable (e.g., sales or revenue).
Adobe Example:
Situation: Adobe’s Creative Cloud sales team needed to understand the relationship between marketing spend across channels and overall subscription revenue.
Task: Develop a model to identify which channels were driving the most impact on sales.
Action: Used linear regression to analyze historical spend data across channels, including TV, social media, and search. The model identified key contributors to sales and diminishing returns for overspending on certain channels.
Result: Reallocated 15% of the budget to underperforming but high-potential channels, resulting in a 20% increase in ROI within one quarter.
2. Logistic Regression
Use: To predict binary outcomes (e.g., purchase vs. no purchase, churn vs. retention) based on independent variables.
Credit Karma Example:
Situation: Credit Karma aimed to reduce churn among users who frequently accessed their credit reports but didn’t engage with other financial tools.
Task: Predict the likelihood of churn based on user behaviors and engagement metrics.
Action: Built a logistic regression model using features like frequency of logins, product usage patterns, and email engagement rates. Identified high-risk users and implemented targeted outreach campaigns.
Result: Reduced churn by 18% in the identified high-risk cohort, contributing to a 12% improvement in overall retention rates.
3. Ridge and Lasso Regression
Use: To handle multicollinearity in data by regularizing coefficients (Ridge) or performing feature selection (Lasso).
Adobe Example:
Situation: Adobe marketing managers faced multicollinearity in campaign data across overlapping channels (e.g., digital ads and social media).
Task: Build a robust model to identify key performance drivers without overfitting.
Action: Applied Ridge regression to reduce the impact of multicollinearity and Lasso regression to select the most impactful variables.
Result: Improved model accuracy by 25%, enabling the team to confidently reallocate budgets and focus on high-performing channels, driving a 15% sales uplift.
4. Time Series Regression
Use: To forecast outcomes based on temporal data, accounting for trends, seasonality, and cycles.
Credit Karma Example:
Situation: Credit Karma needed to predict daily site traffic during the tax season to optimize marketing efforts.
Task: Build a time series model to forecast traffic and allocate campaign budgets accordingly.
Action: Used an ARIMA model to forecast traffic, accounting for seasonal spikes in tax-related queries. Recommended scaling ad spend during high-traffic days.
Result: Increased site traffic by 25% during peak days while maintaining a cost-per-click 10% below the industry benchmark.
5. Logistic Regression with Interaction Terms
Use: To understand how combinations of variables influence a binary outcome.
Adobe Example:
Situation: Adobe wanted to understand how email engagement and webinar attendance together impacted trial-to-paid conversion rates.
Task: Build a model to identify the combined effects of user engagement behaviors.
Action: Built a logistic regression model with interaction terms for email opens and webinar participation. Insights revealed that users who attended webinars and opened emails were 3x more likely to convert.
Result: Focused resources on email follow-ups for webinar attendees, boosting trial-to-paid conversions by 30%.
6. Multivariate Regression
Use: To assess the impact of multiple dependent variables simultaneously.
Credit Karma Example:
Situation: Credit Karma wanted to evaluate the simultaneous effects of marketing campaigns on user engagement and revenue.
Task: Develop a model to analyze multiple outcomes.
Action: Built a multivariate regression model to measure how variations in campaign spending impacted revenue and engagement metrics like time on site and click-through rates.
Result: Revealed optimal budget thresholds for maximizing both engagement and revenue, increasing average session duration by 20% and revenue by 10%.
7. Stepwise Regression
Use: To identify the most statistically significant predictors in large datasets.
Adobe Example:
Situation: Adobe needed to identify the most critical drivers of customer satisfaction from a survey dataset with over 50 variables.
Task: Develop a model to highlight significant predictors while reducing noise.
Action: Used stepwise regression to iteratively add and remove predictors based on statistical significance. The analysis identified three key drivers of satisfaction: ease of use, feature breadth, and customer support.
Result: Informed product and support improvements, increasing customer satisfaction scores by 15% in six months.








Technical Skills & Utilities (TSU)




CORE TOPICS






















Coding, Data Engineering, & Testing






BROAD TOPIC
SUB-TOPICS
DETAILED TOPICS
SQL
SELECT, JOIN
Window Functions, CTEs, Recrusions
Python
Pandas, Numpy
…
Statistics
[Refer to: Statistics and Mathematics (SMS)]
Machine Learning
Scikit-Learn
XGBoost, Deep Learning, Feature Stores
Data Engineering
Airflow
Dragster (Prefect), dbt, macros, CI/CD
Visualization
Tableau, Looker, Amplitude
Custom d3.js, Dashboard Parameters (Filtering)
Cloud & DevOps
BigQuery, AWS
Terraform, Kubernetes
Experimentation
T-Tests
Sequential Testing, CUPED, Heterogeneous-Effect Models











Layer
2025-leading Options
Warehouses / Lakehouses
BigQuery, Snowflake, Databricks, Microsoft Fabric OneLake
Batch & Stream Ingest
Fivetran, Airbyte, Kafka, Google Dataflow
Transformation (ELT)
dbt, pyspark-on-Databricks, SQLMesh
Orchestration
Apache Airflow, Dagster, Prefect
Languages
SQL, Python, R, Julia; Polars & DuckDB for local analytics
BI / Viz
Looker, Power BI, Tableau, Superset; streamlit/Plotly for apps
ML & MLOps
scikit-learn, XGBoost, PyTorch/TensorFlow, MLflow, Vertex AI
Experimentation Platforms
Optimizely, Eppo, VWO, in-house CUPED frameworks
Reverse-ETL / Activation
Census, Hightouch, RudderStack
Data Observability
Monte Carlo, Soda, Databand
Collaboration & Versioning
GitHub, GitLab, JupyterLab, Hex, Deepnote




Competency
Skill Level & Key Concepts
Core Tools
Application in a Project (Use Case)
SQL
Foundational: SELECT, WHERE, GROUP BY, ORDER BY, INNER/LEFT JOINs. <br> Advanced: Window Functions (ROW_NUMBER, LEAD), Common Table Expressions (CTEs), Subqueries, performance tuning.
PostgreSQL, MySQL, SQL Server, BigQuery, Redshift
Extracting, segmenting, and aggregating customer subscription and product usage data from the company's main database.
Spreadsheets
Foundational: VLOOKUP/XLOOKUP, INDEX-MATCH, Pivot Tables, data filters. <br> Advanced: Power Query (Get & Transform), creating data models, complex nested formulas.
Excel, Google Sheets
Performing quick, ad-hoc analysis on a smaller data export; validating data quality; creating simple charts for a team meeting.
Python
Foundational: Data structures, functions, loops, using Jupyter Notebooks.
 Advanced: Mastery of key libraries:
 • Pandas: DataFrames for cleaning, transforming, merging, & grouping.
 • NumPy: Numerical operations.
 • Matplotlib/Seaborn: Data visualization.
 • Scikit-learn: Building predictive models (e.g., regression).
 • Statsmodels: Rigorous statistical testing.
Jupyter, VS Code, Python, Pandas, Scikit-learn
Writing a script to clean a 500k-row dataset, perform exploratory data analysis, and build a logistic regression model to predict customer churn.
BI Platforms
Foundational: Connecting to data sources (CSVs, databases), building standard charts (bar, line, scatter).
 Advanced: Creating interactive dashboards with filters and actions, using calculated fields, data storytelling.
Tableau, Power BI, Looker Studio
Creating a C-level interactive dashboard that reports on key churn drivers and allows stakeholders to drill down into specific customer segments.
Statistics
Foundational: Descriptive Statistics (mean, median, standard deviation), basic probability.
 Advanced: Inferential Statistics (Hypothesis/A/B testing, t-tests), Regression Analysis (Linear, Logistic).
Python (Statsmodels, Scikit-learn), Excel
Designing and analyzing an A/B test to determine if a new app feature significantly reduced customer churn compared to the old version.



Top Technologies & Tools by Frequency
Rank
Technology/Tool
Count
Category
1
SQL
27
Programming Language
2
Python
23
Programming Language
3
R
11
Programming Language
4
Tableau
10
Data Visualization
5
Looker
9
Business Intelligence
6
dbt/DBT
14
Data Engineering
7
AWS
5
Cloud Platform
8
Salesforce
5
CRM Platform
9
Excel
5
Spreadsheet Tool
10
DOMO
4
Analytics Platform
11
Power BI
3
Business Intelligence
12
Snowflake
3
Cloud Data Platform
13
Databricks
3
Analytics Platform
14
BigQuery
3
Cloud Database
15
Google Analytics
3
Web Analytics



SQL (27 mentions) - Most critical technical skill
Python (23 mentions) - Primary programming language
R (11 mentions) - Statistical analysis focus
Tableau (10 mentions) - Leading visualization platform
dbt (14 combined mentions) - Modern data transformation
Modern Data Stack:
Cloud-First: AWS (5), Snowflake (3), BigQuery (3), Databricks (3)
Data Engineering: dbt (14), Airflow (1), ETL (2)
Self-Service Analytics: Looker (9), Tableau (10), Power BI (3)
Programming-Heavy: Python (23), R (11), SQL (27)
Traditional vs Modern Tools:
Traditional: Excel (5), PowerPoint (implied)
Modern BI: Tableau (10), Looker (9), DOMO (4)
Cloud Analytics: Snowflake (3), BigQuery (3), Databricks (3)



Technical and Analytical Questions
Gemini
Tool Proficiency: What tools and technologies are you most comfortable with for marketing analytics? (e.g., Google Analytics, Tableau, SQL, Python, R)
Data Modeling: How do you design predictive models for customer lifetime value (CLV), churn, or lead scoring?
Attribution Models: Can you explain the different types of marketing attribution models and when to use each?
Data Quality: How do you ensure the accuracy and consistency of marketing data across multiple sources?
Advanced Analytics: Describe a situation where you used advanced statistical methods to solve a marketing problem.
Grok
Can you explain how you would measure the ROI of a marketing campaign? 
This tests your understanding of key performance indicators (KPIs), attribution models, and financial metrics.
What analytics tools are you proficient in? 
Expect to discuss software like Google Analytics, Adobe Analytics, Tableau, SQL, Python, or R for data manipulation and visualization.
How do you ensure data quality in your analytics work?
This can involve discussing data cleaning processes, validation checks, and maintaining data integrity.
Describe an instance where you used predictive analytics in marketing.
They'll want to see your ability to use data to forecast trends or consumer behavior.


The role of a Meta Data Scientist Product analytics
Data scientist, Product analytics is a vital role at Meta. It focuses heavily on business problems. As a DS, Product Analytics you will work to bring out the best product and market analysis to help Meta make data-driven business decisions. For related roles at similarly levelled companies, check out the Lyft Data Scientist and Microsoft Data Scientist guides.

The Data Scientist Product Analytics role has work across the following four areas:

Product Operations: Forecasting and setting product team goals. Designing and evaluating experiments. Monitoring key product metrics. Understanding the root causes of changes in metrics. Building and analyzing dashboards and reports. Building key data sets to empower operational and exploratory analysis. Evaluating and defining metrics
Exploratory Analysis: Proposing what to build in the next roadmap. Understanding ecosystems, user behaviors, and long-term trends. Identifying new levers to help move key metrics. Building models of user behaviors for analysis or to power production systems
Product Leadership: Influencing product teams through the presentation of data-based recommendations. Communicating state of business, experiment results, etc. to product teams. Spreading best practices to analytics and product teams
Data Infrastructure: Working in Hadoop and Hive primarily, sometimes MySQL, Oracle, and Vertica. Automating analyses and authoring pipelines via SQL and Python-based ETL framework
For additional insights, refer to the Roblox Data Scientist guide.

Meta Data Scientist Product analytics Interview Guide
The initial interview will be a 45-minute video conference with a Meta data scientist. 

The interview will include questions and discussion around both product interpretation 

and applied data, as well as a few minutes for your questions at the end.

Skills the interviewer is looking for:

Framing: Can you structure and see data to answer a fairly open-ended question?
Operationalization: Can you translate the concepts generated into specific actions?
Analytical Understanding: Can you translate between numbers and words (i.e. prove to your interviewer that product “X” should be built through data resulting in analytical proof)?
Hypothesis Driven: Can you identify reasonable hypotheses and apply basic logic to support those hypotheses? Can you identify hypotheses, and do you understand how to look at data to confirm or refute a product insight?
As mentioned earlier the initial screener has questions from 2 parts which are:

Product Interpretation-
This part of the interview is a product case study focused on interpreting user behavior using data and metrics. It focuses broadly on how you translate user behavior into product ideas and insights using data and metrics. A sample question might be positioned as: “How would you evaluate YouTube’s video recommendations?”The interviewer will be assessing your ability to:
Understand hypotheses for launching new features: “How can I improve a product?”
Consider and quantify tradeoffs of a feature in terms of metrics.
Design experiments to test these hypotheses.
 Interpret results of experiments.
Communicate decision-making via metrics.
Applied Data-
The applied data part of your interview focuses more on the technical side of solving a problem 

using data, for example: “How do you frame a problem, from selecting the most suitable data sets all the way down to execution?” Or, “How would you evaluate YouTube’s video recommendations?” 

It would be worth your while to go through Meta's core products and also engage with each of their core products, trying to reverse-engineer in your mind how these products came to be, what metrics, and what testing and experimentation were involved.

Interview tips:

1.Think out loud. 

Narrate your approach to the problem/question asked as you go through the problem so that the interviewer has insight into your thought process.

2. Deconstruct problems. 

Follow the modular thinking approach to big ambiguous problems, breaking them into smaller groups, and combining the groups for a solution.

3. Hints.

Resort to mid answer course correction if your interviewer prompts you that you’re heading in the wrong direction.

4. Clarification.

Ask clarifying questions during the interview.

5. Prepare an answer to the cliched "Why Meta?" question.

Meta interviewers like to see people who know about the company's environment, projects, challenges, etc.

6. Questions. 

If time permits you may pop in a few questions yourself, say about Meta and analytics.

The Meta DS Product Analytics interview might be a challenging one to crack, but if your preparation is on the lines of the guide we have prepared, we believe you are definitely going to come off with flying colors.
The Ultimate SQL Interview Guide Covers:

Common SQL Commands Used in Interviews
SQL Questions from Tesla, Microsoft, TikTok, and Uber (with solutions)
Join SQL Interview Questions
Window Functions Used in SQL Interviews
Common Database Design Interview Questions
6-Step Process to Solve Hard SQL Questions
3 Best Books for SQL Interviews
Best Site to Practice SQL Interview Questions
How to Approach Open-Ended Take-Home SQL Challenges
4 Real Take-Home SQL Interview Challenges
Before we go into the exact topics SQL interviews cover, we need to get into the interviewer’s head and truly understand WHY tech companies ask SQL questions during interviews.

Why Do Data Interviews Ask SQL Questions?
While SQL might not be as glamorous and sexy as Python or R, SQL is an important language to master for Data Analysts and Data Scientists because your data lives in a database, and that’s where cleaning, filtering, and joining of large datasets can be done in a performant way. You don’t want to pull all the data into a Pandas or R dataframe, and crash your laptop, when you can efficiently shape and filter datasets thanks to SQL.

Just Use SQL Bellcurve Meme

That's why SQL is listed in 61% of data analytics jobs posted on Indeed, according to research done by DataQuest.io. In order to see if you can do the day-to-day work, hiring managers typically send candidates a SQL assessment during the Data Analyst or Data Science interview process.

But you might say: "Nick, I got into this field to do Deep Learning with Tensorflow, are you seriously telling me Data Science interviews cover boring old SQL?"

YES that's exactly what I'm saying!!

SQL?! I thought Data Science was about Neural Networks in Python?
Even at companies like Amazon and Facebook, which have massive amounts of data, most Data Scientists still spend most of their time writing SQL queries to answer business questions like "What are the top-selling products?", or "How do we increase ad click-through rates?".

For more insight into the importance of SQL for Data Scientists, you can read this infamous article "No, you don't need ML/AI – You need SQL" which concretely shows you how so many ML problems can just be solved in a fraction of the time with some heuristics and a SQL query.

What version of SQL is used in interviews?
SQL comes in a variety of versions (also known as flavors), like MySQL, SQL Server, Oracle SQL, and PostgreSQL. Because the SQL versions are pretty similar, most data job interviews don't require you to use a specific version of SQL during the interview. We recommend aspiring Data Analysts and Data Scientists practice their SQL interview questions in PostgreSQL, because it’s the most standards-compliant version of SQL out there, and one of the most popular flavors of SQL in the data industry.

However, if you are strongest in another flavor of SQL, it usually shouldn’t be a problem for SQL interviews. That’s because interviewers are more-so seeing if you understand how to write SQL queries and problem-solve – they know on the job you can just learn the version of SQL the company uses in a few days. As such, during live SQL interviews, a good interviewer won’t stress about minor syntactical errors or differences between different SQL versions.

What Do SQL Interviews Cover?
SQL interviews typically cover five main topics:

basic SQL commands
SQL joins
window functions
database design concepts
your ability to write SQL queries to answer business questions
While most other SQL interview question lists cover SQL trivia, like “What does DBMS stand for?” this guide focuses on what FAANG companies like Amazon and Google ask during interviews. I need to emphasize this point, because the first result on Google for "SQL interview questions" is a pop-up riddled website claiming "What is database?" is a legit interview question 😂.

Fake List of Questions on InterviewBit

Instead of asking conceptual questions, top Silicon Valley technology companies put you on the spot, and ask you to write a SQL query to answer a realistic business questions like "Find me the number of companies who accidentally posted duplicate job listings on LinkedIn?"

LinkedIn SQL Interview Question: Find Duplicate Job Listings

Before we can learn to apply SQL to these scenario-based questions, we need to cover some foundational SQL concepts like the most common SQL commands you need to know for interviews, what kinds of joins show up, and the most popular window functions for SQL interviews.

What are the most common SQL commands used in interviews?
Here’s the top 7 most common SQL commands tested during SQL interviews:

SELECT - used to select specific columns from a table
FROM - used to specify the table that contains the columns you are SELECT’ing
WHERE - used to specify which rows to pick
GROUP BY - used to group rows with similar values together
HAVING - used to specify which groups to include, that were formed by the GROUP BY clause.
ORDER BY - used to order the rows in the result set, either in ascending or descending order
LIMIT - used to limit the number of rows returned
However, 99% of Data Science & Data Analyst interviews at competitive companies won't just straight up ask you "What does GROUP BY do?". Instead you'll have to write a query that actually uses GROUP BY to solve a real-world problem. Check out the next section to see what we mean.

Group By Example: Tesla SQL Interview Question
In this real Tesla SQL Interview question, a Data Analyst was given the table called parts_assembly and asked to "Write a SQL query that determines which parts have begun the assembly process but are not yet finished?".

Tesla Data Analyst SQL Interview Question

To solve the question, realize that parts that are not yet finished can be found by filtering for rows with no data present in the finish_date column. This can be done using the SQL snippet:

WHERE finish_date IS NULL
Because some parts might be represented multiple times in the query data because they have several assembly steps that are not yet complete, we can GROUP BY to obtain only the unique parts.

Thus, the final answer to this Tesla SQL Interview question is:

SELECT part
FROM parts_assembly
WHERE finish_date IS NULL
GROUP BY part;
Hopefully, you've understood how just memorizing what WHERE or GROUP BY isn't going to cut it, and that to solve beginner SQL interview questions you still have to creatively apply the basic commands. To practice this Tesla SQL question yourself, click the image below:

Tesla SQL Question: Unfinished Parts

Now, let's cover another fundamental topic that's often combined with basic SQL commands: aggregate functions like COUNT() and SUM().

Aggregate Functions Used In SQL Interviews
Aggregate functions allow you to summarize information about a group of rows. For example, say you worked at JPMorgan Chase, in their Credit Card analytics department, and had access to a table called monthly_cards_issued. This table has data on how many credit cards were issued per month, for each different type of credit card that Chase offered.

JPMorgan Chase SQL Interview Question Data

To answer a question like “How many total cards were issued for each credit card” you’d use the SUM() aggregate function:

SELECT card_name,
       SUM(issued_amount)
FROM   monthly_cards_issued
GROUP  BY card_name; 
Entering this query on DataLemur yields the following output:

SUM() PostgreSQL Interview Question Example

Similarly, if you wanted to count the total number of rows, you could use the aggregate function COUNT(). To play around with this dataset, open the SQL sandbox for the JPMorgan SQL Interview Question.

JPMorgan SQL Interview Questions: Cards Issued Difference

While PostgreSQL technically has dozens of aggregate functions, 99% of the time you'll just be using the big five functions covered below.

What are the most common SQL aggregate functions?
The 5 most common aggregate functions used in SQL interviews are:

AVG() - Returns the average value
COUNT() - Returns the number of rows
MAX() - Returns the largest value
MIN() - Returns the smallest value
SUM() - Returns the sum
While array_agg() and string_agg() aggregate functions may show up in advanced SQL interviews, they are extremely rare. To learn more about these uncommon commands, visit the PostgreSQL documentation.

SQL Interview Questions On Joins
In real-world data science & data analytics, you don't just use aggregate functions on one table at a time. Because your data lives in multiple SQL tables, as an analyst you're constantly writing SQL joins to analyze all the data together in one go. As such, hiring managers frequently ask both conceptual questions about SQL joins, as well as give you practical scenarios and then ask you to write a SQL query to join two tables.

Microsoft SQL Interview Question Using JOIN
For a concrete example of how joins show up during SQL interviews, checkout this real SQL interview Question asked by Microsoft:

“Which Azure customer buys at least 1 Azure product from each product category?”

The data needed to answer this would be in two tables – a customer_contracts table, which details which companies buy which products, and a table of Azure products, which has details about what product category each Azure service belongs too.

Microsoft SQL Interview Question Dataset

To solve this question, you'd need to combine the customer_contracts and products tables with a SQL join, which is what the following SQL snippet does:

SELECT *
FROM customer_contracts
LEFT JOIN products 
    ON customers.product_id = products.product_id
To solve this real Microsoft Data Analyst SQL question yourself, and see the full solution give it a try on DataLemur:

Microsoft Join SQL Interview Question

What are the 4 different joins tested in SQL assessments?
There are four main ways to join two database tables, and one of the most frequently asked SQL interview questions is to distinguish between each kind:

INNER JOIN - combines rows from two tables that have matching values
LEFT JOIN - combines rows from the left table, even if there are no matching values in the right table
RIGHT JOIN - combines rows from the right table, even if there are no matching values in the left table
FULL JOIN - combines rows from both tables, regardless of whether there are matching values
Because a picture is worth a thousand words, checkout this neat infographic from DataSchool that explains joins visually:

SQL Joins Explained Visually

6 Most Common SQL Join Interview Questions
Besides having to write queries which use JOIN commands, you might also encounter the following commonly asked conceptual interview questions about SQL joins:

What is a self-join, and when would you use it?
What is an anti-join, and when would you use it?
What are the performance considerations of SQL join queries?
How do you optimize a slow join query?
How do you join more than two tables?
Does a join always have to be on two rows sharing the same value (non-equi joins)?
Many of these conceptual join questions closely relate to how databases are organized, and the costs and benefits of normalizing your tables. If you're interviewing for a Data Engineering, this topic is a must-know!

Do I need to know date/time functions for SQL assessments?
While it’s good to be familiar with date and time functions when preparing for a SQL interview, it isn’t absolutely mandatory to memorize the exact syntax for date/time functions because they differ greatly between SQL flavors. For example, SQL Server and MySQL have a DATEADD function, but PostgreSQL uses the keyword INTERVAL to get the same results.

Because of the varying syntax, interviewers often give you some leeway and allow you to look up the exact date/time SQL commands mid-interview, especially if you are interviewing in a version of SQL you aren’t accustomed to.

Most Common Date/Time Functions Used in SQL Interviews
The most common date/time functions to know for SQL interviews are:

NOW(): returns the current date and time
CURRENT_DATE(): returns the current date
INTERVAL: adds a specified time interval to a date
DATEDIFF: calculates the difference between two dates
EXTRACT: extracts a specific part of a date (e.g., month, day, year)
You should also know the following date/time operators:

+: adds a time interval to a date/time value
-: subtracts a time interval from a date/time value
||: concatenates two date/time values
Before a SQL assessment, it's also useful to be familiar with the various date/time types available in PostgreSQL, such as DATE, TIME, and TIMESTAMP.

Using Date/Time Functions In A TikTok SQL Assessment
To see PostgreSQL date/time operators in action, let’s solve this TikTok SQL Assessment Question called 2nd-day confirmation which gives you a table of text message and email signup data. You’re asked to write a query to display the ids of the users who confirmed their phone number via text message on the day AFTER they signed up (aka their 2nd day on Tik-Tok).

TikTok SQL Assessment: 2nd Day Confirmation

In the example data above, email_id 433 has a signup_date of 7/9/2022 and a confirmed action date of 7/10/2022. Hence, the user had a 1-day delay between the two events.

The answer to this TikTok SQL question utilizes the date/time operator INTERVAL to identify the 1-day gap between signup and confirmation. The snippet looks like this:

WHERE texts.action_date = emails.signup_date + INTERVAL '1 day'
The full solution also requires us to join the texts and emails table, and also filter down to text messages that were confirmed. Hence, the final solution is:

SELECT DISTINCT user_id
FROM emails 
INNER JOIN texts
  ON emails.email_id = texts.email_id
WHERE texts.action_date = emails.signup_date + INTERVAL '1 day'
  AND texts.signup_action = 'Confirmed';
Hard Date/Time SQL Interview Question From Stripe
If your up for a challenging date/time SQL interview question, try this very hard Stripe SQL Interview question asked in a final-round Data Science interview. The problem requires you to EXTRACT the EPOCH from a transaction timestamp.

Practice Problem
Stripe SQL Question: Write a SQL query to identify any payments made with the same credit card for the same amount within 10 minutes of each other.


If you have no idea how to solve this question, and reading the solution doesn't help, you probably need a refresher on window functions like LAG, conveniently covered up next!

Window Functions In SQL Interviews
Window functions are tricky, and hence show up constantly in advanced SQL interview questions to separate the beginners from the more experienced data analysts & data scientists.

At a high-level, a window function performs calculation across a set of rows that are related to the current row. This is similar to an aggregate function like SUM() or COUNT(), but unlike an aggregate function, a window function does not cause rows to become grouped into a single output row. Instead, you have control over the window (subset) of rows which are being acted upon.

ROW_NUMBER() Example From Google SQL Interview
For example the window function ROW_NUMBER() ranks selected rows in ascending order, but resets the ranks for each window. To demo this, let's analyze data from a real Google SQL Interview Question.

Google SQL Interview Question Odd Even

In the problem, you are given the measurements table which has data from an IoT sensor that collects multiple measurements per day:

measurements Example Input:
measurement_id	measurement_value	measurement_time
131233	1109.51	07/10/2022 09:00:00
135211	1662.74	07/10/2022 11:00:00
523542	1246.24	07/10/2022 13:15:00
143562	1124.50	07/11/2022 15:00:00
346462	1234.14	07/11/2022 16:45:00
You are asked to find the sum of the odd-numbered and even-numbered sensor measurements for each day. Before we start worrying about the odd measurements (1st, 3rd, 5th measurement of the day, etc.) and even measurements, we need to just understand what was the 1st, 2nd, 3rd, 4th, measurement of the day.

To do this we use ROW_NUMBER() to rank the rows BUT make the window only one-day wide. That means at the end of every day, the ranks reset back to 1. This is achieved with the following window function:

ROW_NUMBER() OVER (
    PARTITION BY CAST(measurement_time AS DATE) 
    ORDER BY measurement_time) AS measurement_num
When we run the code, you'll see at the end of each day the measurement number resets:Row_Number() Window Function Example.

From here, to get odd and even measurements, we just need to divide the measurement_num by 2 and check the remainder, but we'll leave it up to you to implement inside the SQL code sandbox for this Google Data Analyst SQL question.

For another example, let's dive into a practical exercise from an Uber Data Science assessment which also uses the ROW_NUMBER() window function.

Uber Window Function SQL Interview Question
Take for example this Uber SQL Interview Question about selecting a user's 3rd transaction made on the Uber platform.Uber SQL Interview Question: User's 3rd Transaction

At the core of this SQL question is the window function ROW_NUMBER() which assigns a number to each row within the partition. Essentially, we want to group/partition all the Uber transactions together based on which user_id made the transaction, and then order these transactions by when they occured (transaction_date), so that we can label the order in which they occured using ROW_NUMBER():

    ROW_NUMBER() OVER (
      PARTITION BY user_id ORDER BY transaction_date) AS row_num
    FROM transactions) AS trans_num 
Finally, using the output from the window function, we want to filter our results to only get the 3rd transaction for every user:

WHERE row_num = 3
This yields us the final solution:

SELECT 
  user_id,
  spend,
  transaction_date
FROM (
  SELECT 
    user_id, 
    spend, 
    transaction_date, 
    ROW_NUMBER() OVER (
      PARTITION BY user_id ORDER BY transaction_date) AS row_num
  FROM transactions) AS trans_num 
WHERE row_num = 3;
For more practice with SQL interview questions that use window functions select the 'Window Functions' filter on the DataLemur SQL interview questions.

Window Function SQL Interview Questions

What are the most common window functions for SQL interviews?
The top window functions used in SQL interviews are:

RANK() - gives a rank to each row in a partition based on a specified column or value
DENSE_RANK() - gives a rank to each row, but DOESN'T skip rank values
ROW_NUMBER() - gives a unique integer to each row in a partition based on the order of the rows
NTILE() - divides a partition into a specified number of groups, and gives a group number to each row
LAG() - retrieves a value from a previous row in a partition based on a specified column or expression
LEAD() - retrieves a value from a subsequent row in a partition based on a specified column or expression
NTH_VALUE() - retrieves the nth value in a partition
To understand each window function in more detail, check out Mode's SQL tutorial on Window Functions.

Now that you know the basic SQL commands that come up in interviews, along with intermediate SQL interview topics like joins and window functions, we're ready to cover database design and data modeling interview questions.

Database Design & Data Modeling Interview Questions
Database design and data modeling interview questions test you on how well you understand the inner-workings of databases, along with how to design your data warehouse. If you're preparing for a Data Engineering or Analytics Engineering interview, this section is just as important as being able to write SQL queries. However, we still think it’s an important topic for Data Analysts and Data Scientists to briefly cover too, especially if interviewing for a smaller startup where you’ll likely wear multiple hats and end up doing some Data Engineering work too.

Common Database Design Interview Questions
What is an index, and why does it speed up queries?
What are the dis-advantages of using indexes?
How do you troubleshoot a slow SQL query?
How do you CREATE, READ, UPDATE, and DELETE in SQL?
What is a stored procedure, and when do we use them?
What is normalization? Why might we want to also de-normalize some tables?
What is ACID, and how does a database enforce atomicity, consistency, isolation, durability?
What’s the difference between Star schema and Snowflake schema?
What are the different types of dimensions (e.g. junk dimensions, conformed dimensions, mini dimensions, shrunken dimensions)?
If you had to make a simple news feed, similar to the Facebook or LinkedIn feed, what are the main tables you’d have? Can you whiteboard a quick ER Diagram for it?
What is database sharding?
What are the advantages and disadvantages of relational vs. NoSQL databases?
How To Prep For Database Design Interview Questions
If these database design interview questions look super tough, I recommend reading the classic book Database Design for Mere Mortals because it covers topics like translating business needs into design specifications, how to determine what tables you need and their relationships, how to anticipate and mitigate performance bottlenecks, and how to ensure data integrity via field specifications and constraints.
These days it’s very common for an interview cycle for Data Analytics or Data Science roles to include a technical round that covers writing SQL code. It might seem very straightforward — you’re presented with a hypothetical or real data table and asked a series of questions from which you’ll write SQL queries that will get the correct answer.

However, it can be very stressful to actually write the code live while someone is watching. Additionally, even if you write SQL queries all day, being asked to write a query for a data table you’ve never worked with or even looked at, for a business problem you’ve never thought about, for an industry you may not be familiar with, with the clock ticking against a time limit … this can be very challenging.

In addition to doing practice problems in advance on sites like Interview Query, Dataford, or StrataScratch, it’s important to have a framework that you use during these interviews.

I use the overall framework — keep reading for more details for each step.

Understand the question and the data.
Ask clarifying questions.
Decide on the structure for my approach.
Write my code in an iterative way.
Explain my code.
Share my assumptions, trade-offs, or edge cases.
Understand the question and the data
Before jumping in, make sure you understand what you need to do and what data you are working with.

If you’re working in a pair programming environment (like Coder Pad) where you both can see what the other person writes, often the interviewer will write out the schema for the data tables you’re using as well as each question you need to answer.

I always keep a Google Doc open during interviews, so if they don’t write out the question for me, I will write it out for myself in my Doc.

Before I start writing anything, I take a minute to review both the question and the schema to make sure I am clear on the task at hand.

Ask clarifying questions
While you are reviewing the question and data and before you start to solve the problem, clarify your assumptions and ask questions about anything that is not clear to you.

This is something you should do when you are on the job, so you need to demonstrate that it is part of your process. Even if everything seems clear to you, restate in your own words what the interviewer is asking, and ask if your understanding is correct.

Things I typically want to check in the schema and/or clarify— which values are unique? What are the foreign and primary keys (if it’s not already identified)? Should I expect NULLs in any columns? If data types aren’t listed, state your assumptions and verify.

Decide on the structure for my approach
Before writing any code, it is usually a good idea to think through the structure of your query. If you’re joining tables, think about what columns you’ll join on and the type of join. Think about what data you need from each table. Think about how you want to aggregate the data and how you’ll group it. Think about any calculated columns you need to create. Double-check that the data types are usable for your approach.

Often, I like to summarize my approach out loud and ask the interviewer if they think that sounds good. This is especially a good idea if they stress that this is a pair programming exercise or meant to be collaborative —demonstrate that you can collaborate.

Write my code in an iterative way
Finally, it’s time to start writing my code. Because I’ve already laid out my structure, this step should be a little less intimidating.

If I know that I am in an environment where I can actually run my query and get a result, then I might use that during my process, to check my data or iterate along the way. I talk through my process so the interviewer understands that is what I’m doing.

If I get stuck , I ask for help or ask if I can look something up (via Google/Stackoverflow/etc). In some cases, they have structured the interview to anticipate common questions and have hints or code snippets ready should you need them — especially if this is meant to be pair programming or collaborative. Otherwise, usually, they are fine with you looking up something, after all, this is normal to do on the job, and they want to see how you would solve problems on the job.

Explain my code
I typically code out loud during interviews— I explain what I’m doing while writing my code.

For example:

“I’m doing a left/inner/whatever join because …” — they’ll often ask you why you used the join you did anyway.
While writing your WHERE clause: “I’m only including data that meets X criteria because …”
“I’m creating a column that is X divided by Y because …”
When I am done writing my query, I’ll announce that I’m done and summarize what my query does, and then if we’re in a live environment, run my code.

Share assumptions, trade-offs, or edge cases
You can also continue to ask questions while writing your code, including verifying more assumptions that come to mind.

You should also clarify any edge cases or trade-offs you’re making while writing your code. Often, follow-up questions from the interviewer include these, so it is good to show that you are already thinking about them.
This is a complementary post that goes with this video. The video covers three types of SQL questions that you need to know for data science interviews and a general approach for solving them, and this post will provide you with some practice questions to test your skills in each of those categories.


Before we get to the practice problems, I also mentioned a few resources in the video for learning the fundamentals of SQL. You will need to know the basics of SQL before you can work on the practice problems. If you need to learn the fundamentals, the following resources will be helpful:

Transact-SQL documentation by Microsoft
W3 School SQL tutorial
MySQL documentation
Once you know your SQL basics, you are ready to move on to the three categories of problems that the video described:

Computing Ratios
Data Categorization
Cumulative Sums
Of course, the best way to make sure that you have these 3 categories down is to practice. This post has a list of practice SQL problems to help you do just that. They are broken down into the three categories discussed in the video.

Computing Ratios
LeetCode 1174 Immediate Food Delivery II
LeetCode 1322 Ads Performance
LeetCode 1211 Queries Quality and Percentage
Data Categorization
LeetCode 1393. Capital Gain Loss
LeetCode 1907. Count Salary Categories
LeetCode 1468. Calculate Salaries
LeetCode 1212. Team Scores in Football Tournament
Cumulative Sums
LeetCode 1097. Game Play Analysis V
LeetCode 1454. Active Users
LeetCode 579. Find cumulative salary of an employee
For even more practice, the following problems will help you practice the self join which you can use to solve cumulative sum problems.

Self Join
LeetCode 1270 All People Report to the Given Manager
LeetCode 1811 Find Interview Candidates
LeetCode 1285 Find the Start and End Number of Continuous Ranges
These practice problems will help you get started in having a solid and comprehensive knowledge of SQL. Good luck practicing!
This story delves into advanced SQL techniques that will be useful for data science practitioners. In this piece, I will provide a detailed exploration of expert-grade SQL queries I use daily in my analytics projects. SQL, along with modern data warehouses, forms the backbone of data science. It is an indispensable tool for data manipulation and user behaviour analytics. The techniques I am going to talk about are designed to be practical and beneficial from the data science perspective. Mastery of SQL is a valuable skill, crucial for a wide range of projects, and these techniques have significantly streamlined my daily work. I hope it will be useful for you as well.

Given that SQL is the primary language used by data warehouse and business intelligence professionals, it’s an ideal choice for sharing data across data platforms. Its robust features facilitate seamless data modelling and visualization. It remains the most popular means of communication for any data team and nearly every data platform available in the market.

We will use BigQuery’s standard SQL dialect. It’s free and easy to run the queries I wrote and provided below.

Recursive CTEs
Similarly, we would use Python’s faker library, we can mock test data using recursive CTEs in SQL.

    WITH RECURSIVE
    CTE_1 AS (
        (SELECT 0 AS iteration)
        UNION ALL
        SELECT iteration + 1 AS iteration FROM CTE_1 WHERE iteration < 3
    )
    SELECT iteration FROM CTE_1
    ORDER BY 1 ASC
The output would be this:

Image by author.
Image by author.
In BigQuery and many other data warehouse solutions CTEs can be either non-recursive or recursive. The RECURSIVE keyword allows for recursion within the WITH clause (e.g., WITH RECURSIVE).

Recursive CTEs continue to execute until no new results are produced, making them well-suited for querying hierarchical and graph data. In our case execution will stop is defined by the where clause: FROM CTE_1 WHERE iteration < 3

In contrast, non-recursive CTEs execute only once.

The main difference is that a non-recursive CTE can only reference preceding CTEs and cannot reference itself, whereas a recursive CTE can reference itself, as well as preceding or subsequent CTEs.

Working with graphs
Using recursive CTE to work with graph data is very handy. In the data science world graphs are a pretty neat concept used almost everywhere. In Data engineering I use dependency graphs a lot to demonstrate data lineage in my data pipeline.

We can use recursive SQL techniques to evaluate reachability in graphs. In the code snippet below we will find nodes that can be reached from node table_5 in a graph called SampleGraph

WITH RECURSIVE
  SampleGraph AS (
    --      table_1               table_5
    --      /                      / 
    --  table_2 - table_3    table_6   table_7
    --      |               /
    --   table_4       table_8
    SELECT 'table_1' AS from_node, 'table_2' AS to_node UNION ALL
    SELECT 'table_1', 'table_3' UNION ALL
    SELECT 'table_2', 'table_3' UNION ALL
    SELECT 'table_3', 'table_4' UNION ALL
    SELECT 'table_5', 'table_6' UNION ALL
    SELECT 'table_5', 'table_7' UNION ALL
    SELECT 'table_6', 'table_8' UNION ALL
    SELECT 'table_7', 'table_8'
  ),
  R AS (
    (SELECT 'table_5' AS node)
    UNION ALL
    (
      SELECT SampleGraph.to_node AS node
      FROM R
      INNER JOIN SampleGraph
        ON (R.node = SampleGraph.from_node)
    )
  )
SELECT DISTINCT node FROM R ORDER BY node;
Output:

image by author
image by author
Recursive CTEs are quite expensive and we would want to make sure they are used for the intended purpose. If your query doesn’t involve graphs or hierarchical data, it may be more efficient to explore alternatives, such as employing a LOOP statement in conjunction with a non-recursive CTE.

Also, be aware of infinite recursion. We wouldn’t want our SQL to run forever.

Fuzzy matching and approximate joins
It proves to be exceptionally useful in situations where we need to join two datasets with values that, while not identical, share a close resemblance. These scenarios require more sophisticated approaches to ensure accurate data matching. The fuzzy matching technique is a great example of the advanced SQL methods that data analysts often rely on in approximate JOINs.

To illustrate this, consider the following SQL snippet:

with people as (
select 'gmail' as domain, 'john.adams@gmail.com' as email
union all
select 'gmail' as domain, 'dave.robinson@gmail.com' as email
)

, linkedin as (
select
 'gmail'          as domain
,'Dave Robinson'  as name  
)

, similarity as (
select 
  linkedin.name   as name 
, linkedin.domain as domain
, people.email
, fhoffa.x.levenshtein(linkedin.name, people.email) similarity_score
from linkedin 
join people
 on linkedin.domain = people.domain
)

select
*
, row_number() over (partition by name order by similarity_score) as best_match
from 
similarity
We can apply proximity functions such as ngramdistance (available in Clickhouse) and levenshtein (BigQuery) to identify emails that resemble each other.

A lower score indicates a better match:

Image by author
Image by author
This approach proved to be very useful in tasks of matching entities, i.e. individuals, etc. from two separate datasets using their attributes, i.e. email addresses. This is a straightforward scenario when dealing with data from platforms like LinkedIn, Crunchbase, and similar sources where we need to align user information.

Calculating user activity and sessions using LEAD and LAG operators
Window functions proved to be very useful in data science.

Often we need to calculate sessions to aggregate user activity. The example below demonstrates how to do it in SQL.


-- models/sessions.sql
-- mock some data
with raw_event_data as (
    select 'A' as user_id, timestamp_add(current_timestamp(), interval -1 minute) as timestamp union all
    select 'A' as user_id, timestamp_add(current_timestamp(), interval -3 minute) as timestamp union all
    select 'A' as user_id, timestamp_add(current_timestamp(), interval -5 minute) as timestamp union all
    select 'A' as user_id, timestamp_add(current_timestamp(), interval -36 minute) as timestamp union all
    select 'A' as user_id, timestamp_add(current_timestamp(), interval -75 minute) as timestamp

)
-- calculate sessions:
SELECT
    event.user_id || '-' || row_number() over(partition by event.user_id order by event.timestamp) as session_id
    , event.user_id
    , event.timestamp as session_start_at
    , lead(timestamp) over(partition by event.user_id order by event.timestamp) as next_session_start_at
FROM (
    SELECT
        e.user_id
        , e.timestamp
        , DATE_DIFF(
             e.timestamp
            ,LAG(e.timestamp) OVER(
                PARTITION BY e.user_id ORDER BY e.timestamp
                )
            , minute
            ) AS inactivity_time  
        FROM raw_event_data AS e
      ) as event
    WHERE (event.inactivity_time > 30 OR event.inactivity_time is null)
The output would be the following:

Calculating user sessions with SQL. Image by author.
Calculating user sessions with SQL. Image by author.
This is a widely used approach to get an aggregated activity the right way in a scenario when the data science team have to deal with raw user engagement event data.

The benefit of this approach is that we don’t need to rely on data engineers with their streaming techniques or maintain a Kafka server [1].

Mastering Data Streaming in Python

With this data model in place, answering the user analytics questions becomes straightforward. It can be a simple event count but it’s session analytics now. For instance, to compute the average session duration, we can utilize the following SQL:

SELECT
  COUNT(*) AS sessions_count,
  AVG(duration) AS average_session_duration
FROM (
  SELECT session_id
        , DATEDIFF(minutes, MIN(events.timestamp), MAX(events.timestamp)) AS duration
  FROM sessions
  LEFT JOIN events on events.user_id = sessions.user_id
        AND events.timestamp >= events.session_start_at
        AND (events.timestamp < sessions.next_session_start_at OR sessions.next_session_start_at is null)
  GROUP BY 1
)
Using NTILE()
NTILE() is a useful numbering function typically used in analytics to get a distribution of a metric, i.e. sales, revenue, etc. The most common SQL using NTILE() would look like this:

SELECT
    NTILE(4) OVER ( ORDER BY amount ) AS sale_group,
    product_id,
    product_category,
    soccer_team,
    amount as sales_amount
FROM sales
WHERE sale_date >= '2024-12-01' AND sale_date <= '2024-12-31';
It returns the distribution of sales ordered by amount in 4 even buckets.

I find it particularly handy for tracking metrics such as login duration in seconds for a mobile app. For instance, with my app connected to Firebase, I can monitor how long each user’s login process takes.

Image by author
Image by author
This function partitions rows into a specified number of buckets according to their order and assigns each row a bucket number as +1 – a constant integer expression. The rows within each bucket can vary by no more than one. Any remainder from dividing the total number of rows by the number of buckets is distributed evenly across the buckets, starting with bucket 1. If the specified number of buckets is NULL, 0, or negative, an error will be generated. The SQL below explains how I calculate median login duration times:

-- Looker Studio dataset:
select (case when tile = 50 then 'median' when tile = 95 then '95%' else '5%' end) as tile
    , dt
    , max(cast( round(duration/1000) as numeric)/1000 ) max_duration_s
    , min(cast( round(duration/1000) as numeric)/1000 ) min_duration_s

from (
    select 
         trace_info.duration_us duration
        , ntile(100) over (partition by (date(event_timestamp)) order by trace_info.duration_us) tile
        , date(event_timestamp) dt

    from firebase_performance.my_mobile_app 
    where 
        date(_partitiontime) >= parse_date('%y%m%d', @ds_start_date) and date(_partitiontime) <= parse_date('%y%m%d', @ds_end_date)
        and 
        date(event_timestamp) >= parse_date('%y%m%d', @ds_start_date)
        and 
        date(event_timestamp) <= parse_date('%y%m%d', @ds_end_date)
    and lower(event_type) = "duration_trace"
    and lower(event_name) = 'logon'
) x
WHERE tile in (5, 50, 95)
group by dt, tile
order by dt
;
Median and k-th percentile are valuable statistics for analyzing data

Using FOLLOWING AND PRECEDING
FOLLOWING AND PRECEDING are SQL operators we would want to use when we need to check a window before or after that particular record.

Moving average
This is often used to calculate a moving (rolling) average. Consider the SQL below. It explains how to do it and this is a standard task in data analysis.

-- mock data
with temperatures as (
    select 'A' as city, timestamp_add(current_timestamp(), interval -1 day) as timestamp  ,15 as temperature union all
    select 'A' as city, timestamp_add(current_timestamp(), interval -3 day) as timestamp  ,15 union all
    select 'A' as city, timestamp_add(current_timestamp(), interval -5 day) as timestamp  ,15 union all
    select 'A' as city, timestamp_add(current_timestamp(), interval -36 day) as timestamp ,20 union all
    select 'A' as city, timestamp_add(current_timestamp(), interval -75 day) as timestamp ,25

)

SELECT
  city,
  day,
  AVG(temperature) OVER(PARTITION BY city ORDER BY UNIX_DATE(date(timestamp)) 
                RANGE BETWEEN 14 PRECEDING AND CURRENT ROW) AS rolling_avg_14_days,
  AVG(temperature) OVER(PARTITION BY city ORDER BY UNIX_DATE(date(timestamp)) 
                RANGE BETWEEN 30 PRECEDING AND CURRENT ROW) AS rolling_avg_30_days
FROM (
  SELECT date(timestamp) day, city, temperature, timestamp
  FROM temperatures
)
We’ve mocked some data to illustrate the calculation and the output would be the following:

Moving average. Image by author.
Moving average. Image by author.
Indeed, it is very easy to prove that it worked by having just a simple glance at the image above.

Calculating Moving Average Convergence Divergence (MACD)
Widely used by investors, the Moving Average Convergence Divergence (MACD) is a technical indicator to pinpoint optimal market entry points for buying or selling.

MACD can also be calculated using PRECEDING

We will need a 26-period exponential moving average (EMA) and then subtract it from the 12-period EMA. The signal line, which helps to interpret the MACD, is a nine-period EMA of the MACD line itself.

The SQL below explains how to calculate it:

-- mock data
with temperatures as (
    select 'A' as city, timestamp_add(current_timestamp(), interval -1 day) as timestamp  ,15 as temperature union all
    select 'A' as city, timestamp_add(current_timestamp(), interval -3 day) as timestamp  ,15 union all
    select 'A' as city, timestamp_add(current_timestamp(), interval -5 day) as timestamp  ,15 union all
    select 'A' as city, timestamp_add(current_timestamp(), interval -12 day) as timestamp ,20 union all
    select 'A' as city, timestamp_add(current_timestamp(), interval -26 day) as timestamp ,25
)
, data as (
SELECT
  city,
  day,
  temperature,
  AVG(temperature) OVER(PARTITION BY city ORDER BY UNIX_DATE(date(timestamp)) 
                RANGE BETWEEN 12 PRECEDING AND CURRENT ROW) AS rolling_avg_12_days,
  AVG(temperature) OVER(PARTITION BY city ORDER BY UNIX_DATE(date(timestamp)) 
                RANGE BETWEEN 26 PRECEDING AND CURRENT ROW) AS rolling_avg_26_days
FROM (
  SELECT date(timestamp) day, city, temperature, timestamp
  FROM temperatures
)
)

select s.day,
    s.temperature,
    s.rolling_avg_12_days,
    s.rolling_avg_26_days,
    s.rolling_avg_12_days - l.rolling_avg_26_days as macd
from
 data s
join 
 data l
on
 s.day = l.day
Output:

Image by author.
Image by author.
Percentage change
This standard indicator can also be calculated using LEAD and LAG. The SQL below explains how to do it.

-- mock data
with temperatures as (
    select 'A' as city, timestamp_add(current_timestamp(), interval -1 day) as timestamp  ,15 as temperature union all
    select 'A' as city, timestamp_add(current_timestamp(), interval -3 day) as timestamp  ,15 union all
    select 'A' as city, timestamp_add(current_timestamp(), interval -5 day) as timestamp  ,15 union all
    select 'A' as city, timestamp_add(current_timestamp(), interval -12 day) as timestamp ,20 union all
    select 'A' as city, timestamp_add(current_timestamp(), interval -26 day) as timestamp ,25
)

SELECT
  city,
  day,
  temperature,

    (temperature - lag(temperature) over (order by day))*1.0/lag(temperature) over (order by day)*100
FROM (
  SELECT date(timestamp) day, city, temperature, timestamp
  FROM temperatures
)
Output:

Image by author.
Image by author.
Marketing analytics using FOLLOWING AND UNBOUNDED FOLLOWING
Similar to PRECEDING, this is particularly useful when we need to compile a list of items, such as events or purchases, to create a funnel dataset. Using PARTITION BY allows you to group all subsequent events within each partition, regardless of their quantity.

A good example of this concept is marketing funnels.

Our dataset might include a series of recurring events of the same type, but ideally, you want to link each event to the subsequent one of a different type.

Let’s imagine we need to get all events after a user had join_group an event in their funnel. The code below explains how to do it:

-- mock some data
with d as (
select * from unnest([
  struct('0003f' as user_pseudo_id, 12322175 as user_id, timestamp_add(current_timestamp(), interval -1 minute) as event_timestamp, 'join_group' as event_name),
  ('0003',12,timestamp_add(current_timestamp(), interval -1 minute),'set_avatar'),
  ('0003',12,timestamp_add(current_timestamp(), interval -2 minute),'set_avatar'),
  ('0003',12,timestamp_add(current_timestamp(), interval -3 minute),'set_avatar'),
  ('0003',12,timestamp_add(current_timestamp(), interval -4 minute),'join_group'),
  ('0003',12,timestamp_add(current_timestamp(), interval -5 minute),'create_group'),
  ('0003',12,timestamp_add(current_timestamp(), interval -6 minute),'create_group'),
  ('0003',12,timestamp_add(current_timestamp(), interval -7 minute),'in_app_purchase'),
  ('0003',12,timestamp_add(current_timestamp(), interval -8 minute),'spend_virtual_currency'),
  ('0003',12,timestamp_add(current_timestamp(), interval -9 minute),'create_group'),
  ('0003',12,timestamp_add(current_timestamp(), interval -10 minute),'set_avatar')
  ]
  ) as t)

  , event_data as (
SELECT 
    user_pseudo_id
  , user_id
  , event_timestamp
  , event_name
  , ARRAY_AGG(
        STRUCT(
              event_name AS event_name
            , event_timestamp AS event_timestamp
        )
    ) 
    OVER(PARTITION BY user_pseudo_id ORDER BY event_timestamp ROWS BETWEEN 1 FOLLOWING AND  UNBOUNDED FOLLOWING ) as next_events

FROM d
WHERE
DATE(event_timestamp) = current_date()

)

-- Get events after each `join_group` event per user
select
*
from event_data t
where event_name = 'join_group'
;
I previously wrote about it here [2] and there is a more advanced example of marketing funnels:

Advanced SQL techniques for beginners

Exploratory data analysis
It’s often more efficient to conduct analysis directly on the data within your tables using SQL before progressing to ML, AI, data science, or engineering tasks. In fact, you can now even build machine learning models using SQL – BigQuery ML is a prime example of this capability. The trend is clear: everything is increasingly shifting towards data warehouses.

EDA. Image by author.
EDA. Image by author.
Getting unique column values is easily done using Pandas but can we do it in SQL?

The SQL snippet below provides a handy script to achieve this. Run this code in BigQuery (replace ‘your-client’ with your project name):

DECLARE columns ARRAY<STRING>;
DECLARE query STRING;
SET columns = (
  WITH all_columns AS (
    SELECT column_name
    FROM `your-client.staging.INFORMATION_SCHEMA.COLUMNS`
    WHERE table_name = 'churn'
  )
  SELECT ARRAY_AGG((column_name) ) AS columns
  FROM all_columns
);

SET query = (select STRING_AGG('(select count(distinct '||x||')  from `your-client.staging.churn`) '||x ) AS string_agg from unnest(columns) x );
EXECUTE IMMEDIATE 
"SELECT  "|| query
;
Output:

EDA. Image by author.
EDA. Image by author.
Describing the datasets
We can use SQL scripts to describe tables we have in our data warehouse. I will slightly change the SQL mentioned above and add mean, max, min, median, 0.75 tile, 0.25 tile so the final SQL would be like this:

DECLARE columns ARRAY<STRING>;
DECLARE query1, query2, query3, query4, query5, query6, query7 STRING;
SET columns = (
  WITH all_columns AS (
    SELECT column_name
    FROM `your-client.staging.INFORMATION_SCHEMA.COLUMNS`
    WHERE table_name = 'churn' 
        and  data_type IN ('INT64','FLOAT64')
  )
  SELECT ARRAY_AGG((column_name) ) AS columns
  FROM all_columns
);

SET query1 = (select STRING_AGG('(select stddev( '||x||')  from `your-client.staging.churn`) '||x ) AS string_agg from unnest(columns) x );
SET query2 = (select STRING_AGG('(select avg( '||x||')  from `your-client.staging.churn`) '||x ) AS string_agg from unnest(columns) x );
SET query3 = (select STRING_AGG('(select PERCENTILE_CONT( '||x||', 0.5) over()  from `your-client.staging.churn` limit 1) '||x ) AS string_agg from unnest(columns) x );
SET query4 = (select STRING_AGG('(select PERCENTILE_CONT( '||x||', 0.25) over()  from `your-client.staging.churn` limit 1) '||x ) AS string_agg from unnest(columns) x );
SET query5 = (select STRING_AGG('(select PERCENTILE_CONT( '||x||', 0.75) over()  from `your-client.staging.churn` limit 1) '||x ) AS string_agg from unnest(columns) x );
SET query6 = (select STRING_AGG('(select max( '||x||')  from `your-client.staging.churn`) '||x ) AS string_agg from unnest(columns) x );
SET query7 = (select STRING_AGG('(select min( '||x||')  from `your-client.staging.churn`) '||x ) AS string_agg from unnest(columns) x );

EXECUTE IMMEDIATE (
"SELECT 'stddev' ,"|| query1 || " UNION ALL " ||
"SELECT 'mean'   ,"|| query2 || " UNION ALL " ||
"SELECT 'median' ,"|| query3 || " UNION ALL " ||
"SELECT '0.25' ,"|| query4 || " UNION ALL " ||
"SELECT '0.75' ,"|| query5 || " UNION ALL " ||
"SELECT 'max' ,"|| query6 || " UNION ALL " ||
"SELECT 'min' ,"|| query7
)
;
It generates all standard EDA metrics:

Describe() in SQL. Image by author.
Describe() in SQL. Image by author.
EDA can be easily performed using SQL

For instance, we can apply SQL to analyse the correlation between two variables, i.e. CreditScore and Balance. The beauty of SQL-based solutions is that we can easily visualize the results and create scatterplots between all variables using modern BI tools.

Variable distribution. Image by author.
Variable distribution. Image by author.
For instance, in one of my previous stories I compared EDA in SQL and Pandas to calculate such metrics as Standard Deviation and Correlation Matrix [3].

Exploratory Data Analysis with BigQuery SQL? Easy!

Conclusion
Time series analytics is an essential part of data science. In this story, I’ve covered the most popular SQL for data science use cases. I used these queries quite often I hope this will be useful for you in your data science projects.

With SQL scripting, we can automate queries, perform Exploratory Data Analysis, and visualize results directly in any Business Intelligence tool. Modern data warehouses have built-in machine learning tools, i.e. BigQuery ML, etc. and it simplifies ML modelling too.

User Churn Prediction

While Python remains a powerful tool for data scientists, offering robust scripting features, SQL can efficiently handle EDA tasks as well. For visualizing results, an SQL-like setup offers a superior dashboarding experience. Once the dashboard is configured, there’s no need to rerun queries or notebooks, making it a one-time setup that streamlines the process. Adding a modern data modelling tool to this environment setup will put everything to an even higher level of automation with robust data quality checks and unit testing.

Advanced Data Modelling
Tip #4: Formulate a Plan before Writing Code
Press enter or click to view image in full size
Make a plan
Photo by Brett Jordan on Unsplash
Once you have gained your basic SQL skills, understand the question clearly, and know your data in and out, now is the time to start your attack to the challenge by formulating a plan first. I can’t stress enough how important this is. If you only remember one tip from this article, remember this one, make a plan. It’s because the nature of a SQL challenge is complicated data manipulation and usually takes multiple aggregation/layers of abstraction to solve it. Thinking at a certain level of abstracting by planning things out makes it cognitively possible and prevents us from delving too deep to recall the big picture.

The usual way of thinking I use is to think out loud like this: The question is asking for data point A and B, I can get to A by aggregating these 3 columns, get to B by using window functions to print out an ordered list of column C. OK, so for A, we need to create the columns from scratch, which I’ll use inner join to put together. For B, the window function need to be partitioned by column D and ordered by column E, ….

You see, I start from the top level and gradually work my way down, all the way to the specific clause I pick. Talking it out allows it to be heard by myself and reinforces my memory of the problem and plan, thus help me think through it.
Another benefit is: In an actual SQL interview, the recruiters usually expect you to think out loud, so they will know better your thinking process. Most of the time, showcasing the thought process when working on the problem is as important as solving it, if not more so. If you practice this through all the challenges, you’ll get very comfortable(and even a bit excited) doing it. Being comfortable with talking out loud will make you feel and look more confident to the interviewers.
/

An easy way to understand subquery and CTE(Common Table Expression) is to think of it as a function in other programming languages. If you use it in more than one place, why not put it into a function? Whereas subquery is like a lambda function, you write it on the go and throw it away after using it. They help make your code more readable and conceptually easier to digest, which has great value in solving complicated SQL challenges.
Aggregation is the primary way to distill raw data into something more meaningful, closer to what the question asks. GROUP BY helps to condense the data with SUM(), COUNT(), MIN/MAX(), or AVG(), whereas window functions do the aggregation function in the item level (Best for tackling ‘Top N’ kind of problems). Sometimes you’ll need many levels/steps of aggregations to get to where you want. Again, look at your data and challenge question, and carefully design your ‘aggregation map.’
JOIN vs. LEFT JOIN: Honestly, these two are the main JOINs you’ll ever need. Use JOIN when you want to return only rows having pairs on both tables, and use LEFT JOIN when you need all rows from the “left” table, no matter if they have pairs in the “right” table or not.
Order to write the query: The most natural way is to write the query as to how they execute: FROM > WHERE > GROUP BY > HAVING > SELECT > ORDER BY > LIMIT. Resist the urge to start from SELECT. Think like what the machine is thinking.
Work from both ends, your tables, and what the question asks to solve the problem. From the data/tables side: Ask yourself what it can build with the tables? What aggregation could further distill the data? From the end-result side: What data is required? What kind of table will help solve the problem? Imagine some middle-level tables that can help bridge the gap. Try to work on both sides and meet at the middle point. Again, think at different abstraction levels helps a lot.
Top 40 Data Science Statistics Interview Questions
Zaynab
Zaynab
Last Updated : 02 Jun, 2025
 14 min read
302

As Josh Wills once said, “A Data Scientist is a person who is better at statistics than any programmer and better at programming than any statistician“.


Statistics is a fundamental tool when dealing with data and its analysis in Data Science. It provides tools and methods that assist a Data Scientist derive insights and interpret vast amounts of data. It is not enough to master Data Science tools and languages. You should also have a strong understanding of certain core statistical concepts and basics. Keeping this in mind, here is a list of 40 most frequently asked Statistics Data Science Interview Questions and Answers. It will help you refresh your memory of key aspects of statistics and help you prepare for job interviews encompassing Data Science and Machine Learning.

With that said, let’s get into it!

This article was published as a part of the Data Science Blogathon.

Data science interview questions


Statistics Interview Questions and Answers

Q1. What is the difference between a population and a sample?


Answer: A population represents the entirety of all items that are being studied. A sample is a finite subset of the population that is selected to represent the entire group. A sample is usually selected because the population is too large or costly to study in its entirety.


Population and sample

 


An example of population data is a census, and a good example of a sample is a survey.


Q2. What is the difference between inferential and descriptive statistics?


Answer: Descriptive statistics describes some sample or population. Inferential statistics attempts to infer from some sample to the larger population.


inferential and descriptive statistics

 


Q3. What are quantitative and qualitative data?


Answer: Quantitative data are measures of values or counts and are expressed as numbers. Quantitative data refers to numerical data (e.g. how many, how much, or how often). Qualitative data are measures of ‘types’ and may be represented by a name, symbol, or a number code. Qualitative data is also known as categorical data.


Q4. What is the meaning of standard deviation?


Answer: Standard deviation is a statistic that measures the dispersion of a dataset relative to its mean. It is the average amount of variability in your dataset. It tells you, on average, how far each value lies from the mean.

A high standard deviation means that values are generally far from the mean, while a low standard deviation indicates that values are clustered close to the mean.

The standard deviation is calculated as the square root of variance by determining each data point’s deviation relative to the mean.

standard deviation formula
 


Q5. What is the difference between long format and wide format data?


Answer: A dataset can be written in two different formats: wide and long. Wide format is where we have a single row for every data point with multiple columns to hold the values of various attributes. The long format is where for each data point we have as many rows as the number of attributes and each row contains the value of a particular attribute for a given data point.

long format and wide format data
 


Q6. Give an example where the median is a better measure than the mean


Answer: The median is a better measure of central tendency than the mean when the distribution of data values is skewed or when there are clear outliers.

Q7. How do you calculate the needed sample size?

Answer: To calculate the sample size needed for a survey or experiment:

Define the population size: The first thing is to determine the total number of your target demographic. If you are dealing with a larger population, you can approximate the total population between several educated guesses.
Decide on a margin of error: Also known as a “confidence interval”. The margin of error indicates how much of a difference you are willing to allow between your sample mean and the mean of the population.
Choose a confidence level: Your confidence level indicates how assured you are that the actual mean will fall within your chosen margin of error. The most common confidence levels are 90%, 95%, and 99%. Your specified confidence level corresponds with a z-score.
Z-scores for the three most common confidence levels are:
90% = 1.645
95% = 1.96
99% = 2.576
4. Pick a standard of deviation: Next, you will need to determine your standard of deviation, or the level of variance you expect to see in the information gathered. If you don’t know how much variance to expect, a standard deviation of 0.5 is typically a safe choice that will ensure your sample size is large enough.

5. Calculate your sample size: Finally, you can use these values to calculate the sample size. You can do this by using the formula or by using a sample size using a calculator online.

Calculation of sample size
 


Q8. What are the types of sampling in Statistics?


Answer: The four main types of data sampling in Statistics are:

Simple random sampling: This method involves pure random division. Each individual has the same probability of being chosen to be a part of the sample.
simple random sampling
 

2. Cluster sampling: This method involves dividing the entire population into clusters. Clusters are identified and included in the sample based on demographic parameters like sex, age and location.

3. Stratified sampling: This method involves dividing the population into unique groups that represent the entire population. While sampling, these groups can be organized and then drawn a sample from each group separately.

cluster sampling
 

4. Systematic sampling: This sampling method involves choosing the sample members from a larger according to a random starting point but with a fixed, periodic interval called sampling interval. The sampling interval is calculated by diving the population by the desired sample size. This type of sampling method has a predefined range, hence the least time-consuming.

systematic sampling
 


Q9. What is Bessel’s correction?


Answer: In statistics, Bessel’s correction is the use of n-1 instead of n in several formulas, including the sample variance and standard deviation, where n is the number of observations in a sample. This method corrects the bias in the estimation of the population variance. It also partially corrects the bias in the estimation of the population standard deviation, thereby, providing more accurate results.


Q10. What do you understand by the term Normal Distribution?


Answer: Normal distribution, also known as the Gaussian distribution, is a bell-shaped frequency distribution curve. Most of the data values in a normal distribution tend to cluster around the mean.

Normal distribution
 


Q11. What is the assumption of normality?


Answer: This assumption of normality dictates that if many independent random samples are collected from a population and some value of interest (like the sample mean) is calculated, and then a histogram is created to visualize the distribution of sample means, a normal distribution should be observed.


Q12. How do you convert a normal distribution to standard normal distribution?


Answer: The standard normal distribution, also called the z-distribution, is a special normal distribution where the mean is equal to 0 and the standard deviation is equal to 1.

Any nonstandard normal distribution can be standardized by transforming each data value x into a z-score.

To convert the point x from a normal distribution into a z-score with this formula:

z = (x-µ) / σ

Q13. What are left-skewed distribution and right-skewed distribution?


Answer: Skewness is a way to describe the symmetry of a distribution. A left-skewed (Negative Skew) distribution is one in which the left tail is longer than that of the right tail. For this distribution, mean < median < mode. Similarly, right-skewed (Positively Skew) distribution is one in which the right tail is longer than the left one. For this distribution, mean > median > mode.

left-skewed distribution and right-skewed distribution
 


Q14. What are some of the properties of a normal distribution?


Answer: Some of the properties of a Normal Distribution are as follows:

Unimodal: normal distribution has only one peak. (i.e., one mode)
Symmetric: a normal distribution is perfectly symmetrical around its centre. (i.e., the right side of the centre is a mirror image of the left side)
The Mean, Mode, and Median are all located in the centre (i.e., are all equal)
Asymptotic: normal distributions are continuous and have tails that are asymptotic. The curve approaches the x-axis, but it never touches.
normal distribution
 


Q15. What is the Binomial Distribution formula?


Answer: The binomial distribution formula is for any random variable X, given by;

P(x; n, p) = nCx * px (1 – p)n – x

Where:
n = the number of trials
x = 0, 1, 2, ...
p = probability of success on an individual trial
q = 1 - p = probability of failure on an individual trial

Q16. What are the criteria that Binomial distributions must meet?


Answer: The 4 criteria that Binomial distributions must meet are:

There is a fixed number of trials.
The outcome of each trial is independent of one another.
Each trail represents one of two outcomes (“success” or “failure”).
The probability of “success” p is the same across all trials.

Q17. What is an Outlier?


Answer: An outlier is a data point that differs significantly from other data points in a dataset. An outlier may be due to variability in measurement, or it may indicate an experimental error.

Outliers can greatly impact the statistical analyses and skew the results of any hypothesis tests.

outlier
 

It is important to carefully identify potential outliers in the dataset and appropriately deal with them for accurate results.


Q18. Mention methods to screen for outliers in a dataset.


Answer: There are many different ways to screen for outliers ina dataset. Some of them are:

1. A simple way to check whether there is a need to investigate certain data points before using more sophisticated methods is the sorting method. Values in the data can be sorted from low to high and then scanned for extremely low or extremely high values.

2. Visualization (e.g. box plot) is another useful way to see the data distribution at a glance and to detect outliers. This chart highlights statistical information like minimum and maximum values (the range), the median, and the interquartile range for the data. When reviewing a box plot, an outlier is a data point outside the box plot’s whiskers.

Interquartile range method
3. A common method is the Interquartile range method. This method is helpful if there are few values on the extreme ends of the dataset, but you aren’t sure whether any of them might count as outliers. The interquartile range (IQR) also called midspread tells the range of the middle half of a dataset. The IQR can be used to create “fences” around the data then, the outliers can be defined as any values greater than the upper fence or less than the lower fence.

To use the IQR method:

Sort the data from low to high
Identify the first quartile (Q1), the median, and the third quartile (Q3).
Calculate the IQR; IQR = Q3 – Q1
Calculate the upper fence; Q3 + (1.5 * IQR) and the lower fence; Q1 – (1.5 * IQR)
Use the fences to highlight any outliers (all values that fall outside your fences).
4. Another way to identify outliers is to use Z-score. The Z-score is just how many standard deviations away from the mean value that a certain data point is. To calculate z-score use the formula, z = (x-µ) / σ

If the z-score is positive, the data point is above average.
If the z-score is negative, the data point is below average.
If the z-score is close to zero, the data point is close to average.
If the z-score is above or below 3 (assuming z-score = 3 is considered as a cut-off value to set the limit), it is an outlier and the data point is considered unusual.
Other methods to screen outliers include Isolation Forest and DBScan clustering.


Q19. What types of biases can you encounter while sampling?


Answer: Sampling bias occurs when a sample is not representative of a target population during an investigation or a survey. The three main that one can encounter while sampling is:

Selection bias: It involves the selection of individual or grouped data in a way that is not random.
Undercoverage bias: This type of bias occurs when some population members are inadequately represented in the sample.
Survivorship bias occurs when a sample concentrates on the ‘surviving’ or existing observations and ignores those that have already ceased to exist. This can lead to wrong conclusions in numerous different means.

Q20. What is the meaning of an inliner?


Answer: An inlier is a data value that lies within the general distribution of other observed values but is an error. Inliers are difficult to distinguish from good data values, therefore, they are sometimes difficult to find and correct. An example of an inlier might be a value recorded in the wrong units.


Q21. What is hypothesis testing?


Answer: Hypothesis testing is a type of statistical inference that uses data from a sample to conclude about the population data. Before performing the testing, an assumption is made about the population parameter. This assumption is called the null hypothesis and is denoted by H0. An alternative hypothesis (denoted Ha), which is the logical opposite of the null hypothesis, is then defined. The hypothesis testing procedure involves using sample data to determine whether or not H0 should be rejected. The acceptance of the alternative hypothesis (Ha) follows the rejection of the null hypothesis (H0).


Q22. What is the p-value in hypothesis testing?


Answer: A p-value is a number that describes the probability of finding the observed or more extreme results when the null hypothesis (H0) is True. P-values are used in hypothesis testing to help decide whether to reject the null hypothesis or not. The smaller the p-value, the stronger the evidence that you should reject the null hypothesis.


Q23. When should you use a t-test vs. a z-test?


Answer: A T-test asks whether a difference between the means of two groups is unlikely to have occurred because of random chance. It is usually used when dealing with problems with a limited sample size (n < 30). If the population standard deviation is known, the sample size is less than or equal to 30, or if the population standard deviation is unknown, use the T-test.

A Z-test, on the other hand, compares a sample to a defined population and is typically used for dealing with problems relating to large samples (i.e., n > 30). Generally, you should use a Z-test when the population’s standard deviation is known, and the sample size exceeds 30.


Q24. What is the difference between one-tail and two-tail hypothesis testing?


Answer: One-tailed tests allow for the possibility of an effect in one direction. Here, the critical region lies only on one tail.

one-tail hypothesis testing
 

Two-tailed tests test for the possibility of an effect in two directions—positive and negative. Here, the critical region is one of both tails.

two-tail hypothesis testing
 


Q25. What is the difference between type I vs. type II errors?


Answer: A type I error occurs when the null hypothesis true in the population is rejected. It is also known as false-positive. A type II error occurs when the null hypothesis that is false in the population fails to get rejected. It is also known as a false-negative.

null hypothesis
 


Q26. What is the Central Limit Theorem?


Answer: The Central Limit Theorem (CLT) states that, given a sufficiently large sample size from a population with a finite level of variance, the sampling distribution of the mean will be normally distributed regardless of if the population is normally distributed.

Central Limit Theorem
 


Q27. What general conditions must be satisfied for the central limit theorem to hold?


Answer: The central limit theorem states that the sampling distribution of the mean will always follow a normal distribution under the following conditions:

 The sample size is sufficiently large (i.e., the sample size is n ≥ 30).
The samples are independent and identically distributed random variables.
The population’s distribution has finite variance.

Q28. What are correlation and covariance in statistics?


Answer: Correlation indicates how strongly two variables are related. The value of correlation between two variables ranges from -1 to +1.

The -1 value represents a high negative correlation, i.e., if the value in one variable increases, then the value in the other variable will decrease. Similarly, +1 means a positive correlation, i.e., an increase in one variable leads to an increase in the other. Whereas 0 means there is no correlation.

correlation in statistics
 

Covariance, on the other hand, is a measure that indicates the extent to which a pair of random variables vary with each other. A higher number denotes a higher dependency.

covariance in statistics
 


Q29. What is the difference between Point Estimate and Confidence Interval Estimate? 


Answer: A point estimate gives a single value as an estimate of a population parameter. For example, a sample standard deviation is a point estimate of a population’s standard deviation. A confidence interval estimate gives a range of values likely to contain the population parameter. It is the most common type of interval estimate because it tells us how likely this interval is to contain the population parameter.

Point Estimate and Confidence Interval Estimate
 


Q30. Mention the relationship between standard error and margin of error?


Answer: As the standard error increases, the margin of error also increases. The margin of error can be calculated using the standard error with this formula:

Margin of error = Critical value * Standard error of the sample


Q31. How would you define Kurtosis?


Answer: Kurtosis is the extent to which the values of a distribution’s tails differ from the centre of the distribution. Outliers are detected in a data distribution using kurtosis. The higher the kurtosis, the higher the number of outliers in the data.


Q32. What is the proportion of confidence interval that will not contain the population parameter?


Answer: Alpha (α) is the portion of the confidence interval that will not contain the population parameter.

α = 1 – CL = the probability a confidence interval will not include the population parameter.

1 – α = CL = the probability the population parameter will be in the interval

For example, if the confidence level (CL) is 95%, then, α = 1 – 0.95, or α = 0.05.


Q33. What is the Law of Large Numbers in statistics?


Answer: According to the law of large numbers in statistics, an increase in the number of trials performed will cause a positive proportional increase in the average of the results, becoming the expected value. For example, the probability of flipping a fair coin and landing heads is closer to 0.5 when flipped 100, 000 times compared to when flipped 50 times.


Q34. What is the goal of A/B testing?


Answer: A/B testing is statistical hypothesis testing. It is an analytical method for making decisions that estimate population parameters based on sample statistics. The goal is usually to identify any changes to a web page to maximize or increase the outcome of interest. A/B testing is a fantastic method to figure out the best online promotional and marketing strategies for your business.


Q35. What do you understand by sensitivity and specificity?


Answer: Sensitivity is a measure of the proportion of actual positive cases that got predicted as positive (or true positive). Specificity is a measure of the proportion of actual negative cases that got predicted as negative (or true negative). The calculation of Sensitivity and Specificity is pretty straightforward.

sensitivity and specificity
 


Q36. What is Resampling and what are the common methods of resampling?


Answer: Resampling involves the selection of randomized cases with replacement from the original data sample in such a way that each number of the sample drawn has several cases that are similar to the original data sample.

Two common methods of resampling are:

Bootstrapping and Normal resampling
Cross Validation

Q37. What is Linear Regression?


Answer: In statistics, linear regression is an approach for modeling the relationship between one or more predictor variables (X) and one outcome variable (y). If there is one predictor variable, it is called simple linear regression. If there is more than one predictor variable, it is called multiple linear regression.

Linear Regression

Q38. What are the assumptions required for linear regression?


Answer: The linear regression has four key assumptions:

Linear relationship: There’s a linear relationship between X and the mean of Y.
Independence: Observations are independent of each other.
Normality: The distribution of Y along X should be the Normal Distribution.
Homoscedasticity: The variation in the outcome or response variable is the same for any value of X.

Q39. What is a ROC curve?


Answer: The Receiver Operator Characteristic (ROC) curve is a graphical representation of the performance of a classification model at various thresholds. The curve plots True Positive Rate (TPR) vs. False Positive Rate(FPR) at different classification thresholds.

ROC curve
 


Q40. What is Cost Function?


Answer: The cost function is an important parameter that measures the performance of a machine learning model for a given dataset.

It measures how wrong the model is in estimating the relationship between input and output parameters.


Conclusion

This article discussed why a Data Scientist should master Statistics and some important and frequently asked Statistics Data Science Interview Questions and  Answers.


To sum up, the following are the major takeaways from the article:


We learned about Sampling, the different types of Sampling, and how to calculate the needed sample size.
We covered Central Tendency and Probability Distributions.
We discussed Relationship between Variables and the difference between Covariance and Correlation.
We covered Hypotheses Testing and P-value and discussed when to use the T-test and Z-test.
We discussed Regression and the assumptions of Linear Regression.

 
Top 70 SQL Coding Interview Questions
Q1. What is SQL and its significance in databases?
Answer. SQL stands for Structured Query Language. It is a programming language used for managing and manipulating relational databases. SQL is essential for tasks like querying data, updating records, and defining the structure of databases.

Q2. Differentiate between SQL and MySQL.
Answer. SQL, a language for managing and manipulating relational databases, serves as the foundation for MySQL, a relational database management system (RDBMS).

Q3. Explain the basic structure of an SQL query.
Answer. The basic structure of an SQL query consists of a SELECT clause (specifying columns), a FROM clause (specifying tables), and optional clauses like WHERE (specifying conditions) and ORDER BY (sorting results).

Q4. What is normalization, and what are its types?
Answer. Normalization is organizing data in a database to reduce redundancy and dependency. The primary goals of normalization are to eliminate data anomalies, ensure data integrity, and minimize the chances of data redundancy. Break down larger tables into smaller, related tables and define their relationships.

There are several normal forms (NF) that represent different levels of normalization, and each normal form has specific rules to achieve the desired database structure. The most common normal forms are:

First Normal Form (1NF):
Each column in a table must contain atomic (indivisible) values, and each row must be unique.
Second Normal Form (2NF):
The table must be in 1NF, and all non-key attributes must be fully functionally dependent on the primary key. This means that if a table has a composite primary key, each non-key attribute must be dependent on the entire composite key, not just part of it.
Third Normal Form (3NF):
The table must be in 2NF, and no transitive dependencies should exist. In other words, non-key attributes must not be dependent on other non-key attributes.
Boyce-Codd Normal Form (BCNF):
Similar to 3NF but stricter. It states that a table is in BCNF if, for every non-trivial functional dependency, the determinant is a superkey.
Fourth Normal Form (4NF):
It addresses multi-valued dependencies. A table is in 4NF if it has no non-trivial multi-valued dependencies.
Fifth Normal Form (5NF):
Deals with cases where a table has overlapping multi-valued dependencies.
Normalization is an iterative process, and not all databases need to be normalized to the same degree. The level of normalization depends on the specific requirements of the application and the trade-offs between data redundancy and the complexity of database operations. In practice, many databases are normalized up to the third normal form (3NF) to strike a balance between simplicity and data integrity.

Q5. What is denormalization?
Answer. Denormalization is the process of intentionally introducing redundancy into a database by adding redundant data to one or more tables. This is done to improve query performance and simplify data retrieval at the cost of increased storage space and potential data integrity challenges. Denormalization is often applied in situations where the performance of read operations is more critical than the performance of write operations.

Q6. What is a primary key? How is it different from a foreign key?
Answer. A primary key uniquely identifies each record in a table and must have unique values. On the other hand, a foreign key establishes a link between two tables, referencing the primary key of another table.

Q7. What is an index, and why is it used in databases?
Answer. An index is a data structure that improves the speed of data retrieval operations on a database table. It is used to quickly locate and access the rows that match a particular column value.

Q8. Describe the difference between INNER JOIN and LEFT JOIN.
Answer. INNER JOIN returns only the rows with a match in both tables, while LEFT JOIN returns all rows from the left table and the matched rows from the right table.

Q9. How do you add a new record to a table?
Answer. INSERT INTO table_name (column1, column2, ...) VALUES (value1, value2, ...);

Q10. What are the subsets of SQL?
Answer. SQL can be broadly categorized into several subsets based on the types of operations or tasks they address. Here are some common subsets of SQL:

DDL (Data Definition Language):
DDL includes SQL commands that define and manage the structure of the database. Common DDL commands include:
CREATE: Used to create database objects such as tables, indexes, and views.
ALTER: Used to modify the structure of existing database objects.
DROP: Used to delete database objects.
DML (Data Manipulation Language):
DML comprises SQL commands that interact with the data stored in the database. Common DML commands include:
SELECT: Retrieves data from one or more tables.
INSERT: Adds new rows of data into a table.
UPDATE: Modifies existing data in a table.
DELETE: Removes rows of data from a table.
DCL (Data Control Language):
DCL deals with the permissions and access control of the database. Common DCL commands include:
GRANT: Provides specific privileges to database users.
REVOKE: Removes specific privileges from database users.
TCL (Transaction Control Language):
TCL includes commands related to transactions within a database. Common TCL commands include:
COMMIT: Commits a transaction, making changes permanent.
ROLLBACK: Rolls back a transaction, undoing changes made during the transaction.
SAVEPOINT: Sets a point within a transaction to which you can later roll back.
Data Query Language (DQL):
DQL is a subset of SQL specifically focused on querying and retrieving data. The primary command for DQL is SELECT, which is used to retrieve data from one or more tables.
Procedural Extensions (PL/SQL, T-SQL):
Some database systems extend SQL to include procedural programming constructs. For example:
PL/SQL (Procedural Language/Structured Query Language) is used in Oracle Database.
T-SQL (Transact-SQL) is used in Microsoft SQL Server.
OLAP (Online Analytical Processing):
SQL extensions for working with multidimensional data and performing complex analytical queries. These might include extensions like CUBE, ROLLUP, and window functions.
OLTP (Online Transaction Processing):
SQL commands optimized for transactional processing, typically involving simple CRUD (Create, Read, Update, Delete) operations.
Q11. Explain database white box testing and black box testing.
White box testing and black box testing are two different testing methodologies used in the context of databases, including SQL databases. These methodologies focus on different aspects of testing, and they are applicable to various levels of the software development life cycle.

White Box Testing	Black Box Testing
White box testing, also known as structural or glass box testing, is a testing method that examines the internal logic and structure of the database system. Testers have knowledge of the internal workings, code, and implementation details of the database. The objective is to ensure that all paths and branches of the code are executed and that the database functions correctly at the code level.	Black box testing, also known as functional or behavioral testing, is a testing method where the tester has no knowledge of the internal workings of the database. Testers focus on verifying the external behavior and functionality of the database without looking at the code or internal implementation details. The goal is to assess whether the database meets the specified requirements and behaves as expected.
Q12. How can you create empty tables with the same structure as another table?
In SQL, you can create an empty table with the same structure as another table using the CREATE TABLE statement along with the AS clause. This approach is commonly known as creating a table based on another table. Here’s the basic syntax:

CREATE TABLE new_table AS
SELECT * FROM existing_table WHERE 1 = 0;
This syntax involves creating a new table (new_table) because of a SELECT statement that doesn’t retrieve any rows from the existing table (existing_table). The WHERE 1 = 0 condition ensures that no rows are selected from the existing table, creating an empty table with the same structure.

Q13. What is the difference between the DROP and TRUNCATE commands?
Answer. In SQL, DROP is used to permanently delete a table and its structure, releasing occupied space. TRUNCATE removes all data from a table, retaining the structure but not releasing space. DROP is irreversible, while TRUNCATE is faster and safer for data removal, especially in production.

Q14. What are the benefits of SQL database over NoSQL database?
Answer. SQL databases offer structured data management with predefined schemas, enforcing data consistency and integrity through ACID properties. They excel in complex querying, supporting joins, and providing strong consistency. SQL databases scale vertically by adding resources to a single server, making them suitable for applications with intricate relationships between entities. The mature SQL ecosystem offers a wide range of tools and support. In contrast, NoSQL databases prioritize schema flexibility, horizontal scalability, and eventual consistency for distributed systems. They are well-suited for unstructured or semi-structured data and high write loads. The choice between SQL and NoSQL depends on specific application requirements, data structures, and scalability needs. SQL databases excel in scenarios that demand data integrity, complex queries, and transaction support.

Q15. What is a primary key?
In SQL, a primary key is a field or a combination of fields in a table that uniquely identifies each record in that table. The primary key has two main characteristics:

Uniqueness:
Every value in the primary key column (or columns) must be unique across all records in the table. This ensures that each record can be uniquely identified based on its primary key value.
Non-Nullability:
The primary key column (or columns) cannot contain NULL values. Each record must have a non-null value in the primary key column, emphasizing its role as a unique identifier.
Q16. What is meant by table and field in SQL?
In SQL, a table is a structured representation of data organized in rows and columns. It is the fundamental storage unit within a relational database to store and organize related information. Each table consists of columns (also known as fields) and rows.

TABLE	FIELD
– A table is a collection of related data entries organized in rows and columns.
– Each row in a table represents a record, and each column represents a specific attribute or field of that record.
– Tables are named and must have a unique name within the database.	– A field, also referred to as a column, is a specific attribute or property of the data stored in a table.
– Each column has a data type that defines the kind of data it can store (e.g., INT for integers, VARCHAR for variable-length character strings).
– Fields hold the actual data values for each record in the table.
Q17. What is a constraint, and why use constraints?
In SQL, a constraint is a rule applied to a table column to ensure data integrity and enforce specific conditions. Types of constraints include primary key (ensures uniqueness), foreign key (establishes relationships), unique (enforces uniqueness), check (validity checks), and NOT NULL (avoids NULL values). Constraints play a vital role in maintaining data accuracy, relationships, and adherence to business rules, providing a robust foundation for reliable database management.

Q18. What is a subquery?
In SQL, a subquery is a nested query enclosed within parentheses that is used to retrieve data based on the results of another query. It can be employed in various SQL statements, such as SELECT, INSERT, UPDATE, and DELETE. Subqueries enhance the flexibility of queries by allowing operations and decisions to be based on the outcome of another query.

Q19. What is a SQL operator?
In SQL, operators are symbols or keywords used for various operations. Types include arithmetic (e.g., +, -, *), comparison (e.g., =, <, >), logical (e.g., AND, OR, NOT), concatenation (e.g., ||), IN, LIKE, IS NULL, and BETWEEN. Operators are vital for constructing queries to perform calculations, comparisons, and string manipulations in a relational database.

Q20. What is an alias?
In SQL, an alias is a temporary name assigned to a table or column in a query. It improves readability and provides a shorter reference. For example, table aliases (e.g., “e” for “employees”) and column aliases (e.g., “increased_salary” for a calculated column) are commonly used. Aliases are specified using the AS keyword but are often optional for column aliases.

Q21. What is the purpose of the GROUP BY clause in SQL?
Answer. The GROUP BY clause groups rows with the same values in specified columns into summary rows, like finding the total sales per category.

Q22. Explain the concept of a subquery and provide an example.
Answer. A subquery is a query embedded within another query. Example: SELECT * FROM employees WHERE department_id IN (SELECT department_id FROM departments WHERE location = 'New York');

Q23. How can you prevent SQL injection in your queries?
Answer. Use parameterized queries or prepared statements, which allow the database to distinguish between code and data, preventing malicious SQL injection.

Q24. What are stored procedures, and how are they different from functions?
Answer. Stored procedures are precompiled SQL queries stored in the database. Functions return a value and can be used in SQL statements like expressions, while procedures do not return values directly.

Q25. Describe the ACID properties in the context of database transactions.
Answer. ACID stands for Atomicity, Consistency, Isolation, and Durability. These properties ensure the reliability of database transactions by maintaining data integrity and consistency.

Q26. Explain the use of the HAVING clause in SQL.
Answer. The HAVING clause is used with the GROUP BY clause to filter the results of aggregate functions based on specified conditions.

Q27. Differentiate between UNION and UNION ALL.
Answer. UNION combines and returns unique rows from two or more SELECT statements, while UNION ALL returns all rows, including duplicates.

Q28. What is the purpose of the COALESCE function?
Answer. The COALESCE function returns the first non-null expression in a list. It often replaces null values with a default or alternative value.

Q29. How do you perform a self-join in SQL?
Answer. A self-join is performed by joining a table with itself. Example: SELECT e1.name, e2.name FROM employees e1 INNER JOIN employees e2 ON e1.manager_id = e2.employee_id;

Q30. Explain the concept of database transactions.
Answer. A transaction is a sequence of one or more SQL statements that are executed as a single unit. It ensures the consistency and integrity of the database by either committing changes or rolling back to the previous state.

Q31. What is the purpose of the window functions in SQL?
Answer. Window functions perform calculations across a set of rows related to the current row. Examples include ROW_NUMBER(), RANK(), and LEAD().

Q32. Explain the differences between clustered and non-clustered indexes.
Answer. A clustered index determines the physical order of data in a table, while a non-clustered index does not affect the physical order but provides a separate structure to improve query performance.

Q33. How does the SQL Server optimizer work?
Answer. The SQL Server optimizer generates and evaluates various execution plans to choose the most efficient one based on factors like indexes, statistics, and query complexity.

Q34. What are common table expressions (CTEs), and when would you use them?
Answer. CTEs are named temporary result sets defined within the scope of a SELECT, INSERT, UPDATE, or DELETE statement. They are used to simplify complex queries and improve readability.

Q35. Discuss the concept of materialized views in databases.
Answer. Materialized views are precomputed and stored result sets, updated periodically based on the underlying data. They improve query performance by reducing the need to recalculate results.

Q36. How can you optimize a query’s performance in SQL?
Answer. Query performance can be optimized by using indexes, avoiding SELECT *, optimizing joins, and ensuring up-to-date statistics.

Q37. Explain the purpose of the OLAP and OLTP database systems.
Answer. OLAP (Online Analytical Processing) databases are designed for complex queries and reporting, while OLTP (Online Transaction Processing) databases focus on quick, transactional data processing.

Q38. What is the difference between a view and a table?
Answer. A table is a physical storage structure, while a view is a virtual table created by a SELECT query. Views provide a way to simplify complex queries and control access to data.

Q39. Describe the process of database sharding.
Answer. Database sharding involves breaking an extensive database into smaller, more manageable pieces called shards. Each shard is stored on a separate server, improving scalability and performance.

Q40. Explain the use of the APPLY operator in SQL.
Answer. The APPLY operator invokes a table-valued function for each row returned by an outer table expression. It is handy for joining with functions that take parameters.

Q41. How to update a table?
Use the below syntax to update a table:

UPDATE table_name
SET col_1 = value_1, column_2 = value_2
WHERE condition;
Q42. How do you sort records in a table?
To sort records in a table in SQL, you can use the ORDER BY clause in your SELECT statement. The ORDER BY clause allows you to specify one or more columns by which the result set should be sorted. The sorting can be done in ascending (ASC) or descending (DESC) order.

Here is the basic syntax for sorting records in a table:

SELECT column1, column2, ...
FROM table_name
ORDER BY column1 [ASC | DESC], column2 [ASC | DESC], ...;
SELECT: Specifies the columns you want to retrieve.
FROM: Specifies the table from which to retrieve data.
ORDER BY: Specifies the columns by which to sort the result set.
ASC: Optional keyword for ascending order (default if not specified).
DESC: Keyword for descending order.
Q43. What is the DISTINCT statement, and how do you use it?
The DISTINCT keyword in SQL is used to eliminate duplicate rows from the result set of a SELECT query. It returns only unique values for the specified columns.

Here’s the basic syntax for using the DISTINCT statement:

SELECT DISTINCT column1, column2, ...
FROM table_name;
SELECT DISTINCT: Specifies that only distinct (unique) values should be returned.
column1, column2, ...: Specifies the columns for which you want to retrieve distinct values.
FROM table_name: Specifies the table from which to retrieve data.
Example: Consider a “students” table with columns “student_id” and “course_id.” If you want to retrieve a list of unique course IDs from this table, you can use the DISTINCT keyword:

SELECT DISTINCT course_id
FROM students;
This query returns only distinct values from the “course_id” column in the “students” table.

Q44. What are entities? Give some examples.
Answer. In the context of SQL and databases, an “entity” typically refers to a table or an object that can be uniquely identified and is used to store and represent data. Entities are fundamental building blocks in database design and correspond to real-world objects or concepts.

Q45. What scalar functions do you know?
Scalar functions in SQL are functions that operate on a single value and return a single value. These functions can be used within SQL queries to perform operations on individual columns or literals. Here are some commonly used scalar functions in SQL:

String Functions:
UPPER(str): Converts a string to uppercase. SELECT UPPER('hello') AS result; -- Result: 'HELLO'
LOWER(str): Converts a string to lowercase.SELECT LOWER('Hello') AS result; -- Result: 'hello'
CONCAT(str1, str2, …): Concatenates two or more strings.SELECT CONCAT('John', ' ', 'Doe') AS full_name; -- Result: 'John Doe'
LENGTH(str): Returns the length of a string.SELECT LENGTH('apple') AS str_length; -- Result: 5
Numeric Functions:
ABS(num): Returns the absolute value of a number.SELECT ABS(-10) AS absolute_value; -- Result: 10
ROUND(num, decimals): Rounds a number to the specified number of decimal places.SELECT ROUND(3.14159, 2) AS rounded_value; -- Result: 3.14
SQRT(num): Returns the square root of a number.SELECT SQRT(16) AS square_root; -- Result: 4
RAND(): Returns a random number between 0 and 1.SELECT RAND() AS random_number; -- Result: (random value between 0 and 1)
Date and Time Functions:
NOW(): Returns the current date and time.SELECT NOW() AS current_datetime; -- Result: 'YYYY-MM-DD HH:MI:SS'
DATE_FORMAT(date, format): Formats a date according to the specified format.SELECT DATE_FORMAT(NOW(), '%Y-%m-%d') AS formatted_date; -- Result: 'YYYY-MM-DD'
DATEDIFF(date1, date2): Returns the difference in days between two dates.SELECT DATEDIFF('2022-01-01', '2022-01-10') AS date_difference; -- Result: -9
Mathematical Functions:
POWER(base, exponent): Returns the result of raising a number to a specified power.SELECT POWER(2, 3) AS result; -- Result: 8
CEIL(num): Returns the smallest integer greater than or equal to a number.SELECT CEIL(4.3) AS result; -- Result: 5
FLOOR(num): Returns the largest integer less than or equal to a number.SELECT FLOOR(4.8) AS result; -- Result: 4
These are just a few examples of scalar functions in SQL. Different database systems may have variations in syntax or additional functions, so it’s important to refer to the documentation of the specific database you are working with.

Q46. How do we prevent duplicate records when making a query?
To prevent duplicate records when making a query in SQL, you can use the DISTINCT keyword in the SELECT statement. The DISTINCT keyword filters out duplicate rows from the result set, ensuring that only unique values are returned. Here’s the basic syntax:

SELECT DISTINCT column1, column2, ... FROM table_name WHERE conditions;

Q47. How do you add a record to a table?
In SQL, you use the INSERT statement to add a new record or row to a table. The basic syntax for the INSERT statement is as follows:

INSERT INTO table_name (column1, column2, ..., columnN) VALUES (value1, value2, ..., valueN);

Q48. What are SQL operators?
SQL operators are symbols or keywords that perform operations on one or more expressions in SQL queries. They are used to perform mathematical operations, comparison operations, logical operations, and other manipulations on data within the database. SQL operators can be broadly categorized into several types:

Arithmetic Operators:

Addition (+):
Adds two numbers.
SELECT salary + bonus AS total_income FROM employees;
Subtraction (-):
Subtracts the right operand from the left operand.
SELECT total_sales - expenses AS profit FROM financial_data;
Multiplication (*):
Multiplies two numbers.
SELECT quantity * unit_price AS total_cost FROM orders;
Division (/):
Divides the left operand by the right operand.SELECT revenue / num_customers AS average_revenue FROM sales_summary;
Modulus (%):
Returns the remainder of the division of the left operand by the right operand.
SELECT order_id % 1000 AS short_order_id FROM orders;
Comparison Operators:

Equal to (=):
Tests if two expressions are equal.
SELECT * FROM products WHERE category_id = 1;
Not equal to (<> or !=):
Tests if two expressions are not equal.
SELECT * FROM customers WHERE country_code <> 'US';
Greater than (>):
Tests if the left operand is greater than the right operand.
SELECT * FROM employees WHERE salary > 50000;
Less than (<):
Tests if the left operand is less than the right operand.
SELECT * FROM products WHERE unit_price < 10;
Greater than or equal to (>=):
Tests if the left operand is greater than or equal to the right operand.
SELECT * FROM orders WHERE order_date >= '2022-01-01';
Less than or equal to (<=):
Tests if the left operand is less than or equal to the right operand.
SELECT * FROM customers WHERE registration_year <= 2020;
Logical Operators:

AND (AND):
Combines two conditions. Returns true if both conditions are true.
SELECT * FROM products WHERE category_id = 1 AND stock_quantity > 0;
OR (OR):
Combines two conditions. Returns true if at least one condition is true.
SELECT * FROM employees WHERE department_id = 1 OR department_id = 2;
NOT (NOT):
Negates a condition. Returns true if the condition is false, and vice versa.
SELECT * FROM customers WHERE NOT country_code = 'US';
Other Operators:

LIKE (LIKE):
Compares a value to a pattern using wildcard characters.
SELECT * FROM products WHERE product_name LIKE 'Apple%';
IN (IN):
Tests if a value matches any value in a list.
SELECT * FROM orders WHERE customer_id IN (101, 102, 103);
BETWEEN (BETWEEN):
Tests if a value is within a specified range.SELECT * FROM employees WHERE salary BETWEEN 40000 AND 60000;
These operators are fundamental to constructing SQL queries and expressing various conditions and calculations within the database. They play a crucial role in filtering, sorting, and manipulating data.

Q49. What do you mean by data integrity?
Answer. Data integrity in SQL refers to the accuracy, consistency, and reliability of data stored in a relational database. It ensures that the data remains valid and reliable throughout its lifecycle, from the point of entry into the database to its retrieval and modification. Maintaining data integrity is crucial for making informed decisions based on accurate and consistent information.

Q50. Why is the FLOOR function used in SQL Server?
The FLOOR function in SQL Server is used to round a numeric value down to the nearest integer that is less than or equal to the original value. It essentially removes the decimal part of a numeric value and returns the largest integer less than or equal to the input.

The basic syntax of the FLOOR function is as follows:

FLOOR(numeric_expression)

numeric_expression: The numeric value that you want to round down to the nearest integer.
Consider the student table as shown below for question no. 1, 2, and 3.

SQL Coding | SQL Interview questions
51. Write a query to extract the username(characters before @ symbol) from the Email_ID column.
Answer: 

SELECT SUBSTR(Email_ID, 1, INSTR(Email_ID, '@') - 1) FROM STUDENT;
Extract the position of @ from the email id first using INSTR() function then pass this position(after subtracting 1) as an argument for length in SUBSTR() function. 

Output – 


Q52. Write a query to extract domain name like .com, .in, .au etc. from the Email_ID column.
Answer: 

SELECT SUBSTR(Email_ID, INSTR(Email_ID, '.')) FROM STUDENT; 
Extract the position of . (dot character) from the email id first using INSTR() function then pass this position as an argument for starting position in SUBSTR() function.

Output – 

SQL Coding
Q53. Write a query to extract email service provider names like google, yahoo, outlook, etc. from the Email_ID column.
Answer: 

SELECT SUBSTR(Email_ID, INSTR(Email_ID, '@') + 1, INSTR(Email_ID, '.') - INSTR(Email_ID, '@') - 1) FROM STUDENT; 
Extract the position of @ from the email id first using INSTR() function, and pass it (after adding 1) as an argument for starting position in SUBSTR() function.

Now extract this position of . (dot character) and subtract it from the earlier extracted @ position and pass it (after subtracting 1) as an argument for length in SUBSTR() function.

Output – 

SQL Coding
Q54. What is(are) the output of the following query?
SELECT CEIL(-12.43), FLOOR(-11.92) FROM DUAL;
a. -13, -12

b. -12, -12

c. -12, -11

d. -13, -11

Answer: b

CEIL() function returns the smallest integer number that is greater than or equal to the given number. So if we pass -12.43 in ceil, it returns the smallest integer value that is >= -12.43 i.e. -12.

FLOOR() function returns the largest integer number that is less than or equal to the given number. So if we pass -11.92 in floor, it returns the largest integer value that is <= -11.92 i.e. -12.

Output – 


Q55. Write a query to extract all the consonants present in your name.
SELECT TRANSLATE('Narendra', 'xaeiou', 'x') FROM DUAL;
First, extract the consonants from the input name as extracted above, then concatenate these consonants with the character ‘a’ in from_string argument to remove the consonants by not specifying corresponding characters in to_string argument. So if we pass Narendra as the name in the above query, it returns vowels (a, e, a).

Output –


Q56. Write a query to extract all the vowels present in your name.
Answer:

SELECT TRANSLATE('Narendra', 'a' || TRANSLATE('Narendra', 'xaeiou', 'x'), 'a') FROM DUAL;
First, extract the consonants from the input name as extracted above, then concatenate these consonants with character ‘a’ in from_string argument to remove the consonants by not specifying corresponding characters in to_string argument. So if we pass Narendra as the name in the above query, it returns vowels (a, e, a).

Output:


Refer to the emp table as shown below for question no. 7 to 14.


Q57. Write a query to extract the employees’ details who joined in the year 1981.
SELECT * FROM EMP WHERE TO_CHAR(HIREDATE, 'YY') = 81;
Use TO_CHAR() to extract the year part from the hiredate column and select all the employees who were hired in 1981 by using WHERE clause.

Output –


Q58. Write a query to find the hiked salary for each employee after adding the commission.
SELECT EMPNO, ENAME, NVL2(COMM, SAL+COMM, SAL) AS HIKEDSAL FROM EMP;
Since the commission column contains null values so directly adding it to salary will return null wherever the commission is null.

Use NVL2() function to determine hiked salary based on whether the commission is null or not null. If COMM (expr1) is not null then it returns SAL+COMM (expr2). If COMM is null then it returns SAL (expr3).

Output – 


Q59. Write a query to find out the employees drawing a higher salary than their managers.
SELECT E.EMPNO, E.ENAME, E.SAL, M.EMPNO, M.ENAME, M.SAL FROM EMP E, EMP M WHERE E.MGR = M.EMPNO AND E.SAL > M.SAL;
Self join the emp table with itself to compare employees’ salary with their manager’s salary.

Output –


Q60. Write a query to find out the subordinates (reportees) who joined the organization before their managers.
SELECT E.EMPNO, E.ENAME, E.HIREDATE, M.EMPNO, M.ENAME, M.HIREDATE FROM EMP E, EMP M WHERE E.MGR = M.EMPNO AND E.HIREDATE < M.HIREDATE;
Self join the emp table with itself to compare employees hiredate with their manager’s hiredate.

Output –


Q61. Write a query to find out the employees who don’t have any subordinates (reportees) i.e. the employees who are not the managers.
Answer: 

SELECT * FROM EMP WHERE EMPNO NOT IN (SELECT DISTINCT NVL(MGR, 0) FROM EMP);
Using simple subquery first find out the list of distinct managers EMPNOs, then select the EMPNO which does not belong to managers EMPNOs.

Output – 


Q62. Write a query to find out 2nd senior-most employee i.e. who joined the organization second as per hire date.
Answer: 

SELECT * FROM EMP E WHERE 2 = (SELECT COUNT(DISTINCT M.HIREDATE) FROM EMP M WHERE E.HIREDATE >= M.HIREDATE);
Using correlated subquery we can get 2nd senior-most employee by comparing the inner query output against 2 in WHERE clause.

Output – 


Q63. Write a query to find out the 5th maximum salary.
SELECT * FROM EMP E WHERE 5 = (SELECT COUNT(DISTINCT M.SAL) FROM EMP M WHERE E.SAL <= M.SAL);
Using correlated subquery we can get the 5th maximum salary by comparing the inner query output against 5 in WHERE clause.

Output


Q64. Write a query to find out the deviation from average salary for the employees who are getting more than the average salary.
Note – Round the average salary, salary difference up to two digits.

Answer:

SELECT ENAME, SAL, ROUND((SELECT AVG(SAL) FROM EMP),2) AS AVG, ROUND(SAL - (SELECT AVG(SAL) FROM EMP),2) AS DIFF FROM EMP WHERE SAL > (SELECT AVG(SAL) FROM EMP);
First, select the employees who are getting more than the average salary, then calculate the deviation from average salary for such employees.

Output :


Refer to the dept table along with the above emp table for question no. 15 and 16.


Q65. Write a query to find out the employees who are getting the maximum salary in their departments.
SELECT * FROM EMP WHERE SAL IN (SELECT MAX(SAL) FROM EMP GROUP BY DEPTNO);
Using simple subquery first get a list of maximum salary for each department using group by operation, then select the employees who are getting salary as per that list.

Output–

output
Q66. Write a query to find out department-wise minimum salary, maximum salary, total salary, and average salary.
SELECT D.DEPTNO, MIN(SAL), MAX(SAL), SUM(SAL), AVG(SAL) FROM EMP E, DEPT D WHERE E.DEPTNO = D.DEPTNO GROUP BY D.DEPTNO;
First inner join employee and department table then group by DEPTNO to find out minimum, maximum, total and average salary for each department.

Output – 

output 
Q67. Consider the present table structure and desired table structure of the customer table as shown below.

Q68. Choose the correct statement(s) that will result in the desired table.
a. ALTER TABLE CUSTOMER RENAME CustName to Name;

b. ALTER TABLE CUSTOMER RENAME COLUMN CustName to Name;

c. ALTER TABLE CUSTOMER ADD Email VARCHAR2(35);

d. ALTER TABLE CUSTOMER MODIFY Email VARCHAR2(35);

e. ALTER TABLE CUSTOMER DROP FamilySize;

f. ALTER TABLE CUSTOMER DROP COLUMN FamilySize;

Answer: b, d, e

Rename CustName column as Name using –

ALTER TABLE CUSTOMER RENAME COLUMN CustName to Name;
Modify Email column datatype from VARCHAR2(25) to VARCHAR2(35) using –

ALTER TABLE CUSTOMER MODIFY Email VARCHAR2(35);
Drop FamilySize column using –

ALTER TABLE CUSTOMER DROP FamilySize;
Q69. Consider the following table schema and data for the transaction table. 

Choose the valid UPDATE statement(s)

a. UPDATE TRANSACTION SET PrimeStatus = ‘Yes’ WHERE TransactionID = 1102348

b. UPDATE TRANSACTION SET PrimeStatus = ‘VALID’ WHERE CustName = ‘John’

c. UPDATE TRANSACTION SET TransactionID = NULL WHERE CustName = ‘John’

d. UPDATE TRANSACTION SET ShoppingDate = NULL WHERE TransactionID = 1102348

Answer: d

CHECK constraint allows only a set of predefined values so here only Y or N is allowed for PrimeStatus column.

NOT NULL constraint does not allow NULL values so TransactionID can’t be set to NULL.

We can insert NULL values in ShoppingDate Column.

Output –

UPDATE TRANSACTION SET ShoppingDate = NULL WHERE TransactionID = 1102348;

Q70. What is(are) the output of the following SQL statement?
SELECT TRANSLATE('AWESOME', 'WOE', 'VU') FROM DUAL;
a. AVESUME

b. AVSOM

c. AVSUM

d. AWESUME

Answer: c

For an input string, TRANSLATE() function replaces characters specified in from_string argument by their corresponding characters in to_string argument. If there are no corresponding characters in to_string argument then the extra characters present in from_string argument are removed from the input string.

SELECT TRANSLATE('AWESOME', 'WOE', 'VU') FROM DUAL;
Output – 


Q 71. Consider the emp and insurance table as shown below.

How do you get the following output?


a. SELECT * FROM EMP1 LEFT JOIN INSURANCE ON (EMP1.INSURANCETYPE = INSURANCE.INSURANCETYPE);

b. SELECT * FROM EMP1 JOIN INSURANCE ON (EMP1.INSURANCETYPE = INSURANCE.INSURANCETYPE);

c. SELECT * FROM EMP1 RIGHT JOIN INSURANCE ON (EMP1.INSURANCETYPE = INSURANCE.INSURANCETYPE);

d. SELECT * FROM EMP1 FULL JOIN INSURANCE ON (EMP1.INSURANCETYPE = INSURANCE.INSURANCETYPE);

Answer: c, d

Right join returns all the records from the right table along with the matched records from the left table.


Here, since all the emp (left) table’s insurance types are present in the insurance (right) table so full outer join will also return the same output.

NOTE – While analyzing the outputs, focus on the record values rather than their sequence in the output.

output
Conclusion
Top 100 Data Science Interview Questions and Answers 2025
S
Shivansh Kaushal
Last Updated : 04 Apr, 2025
 52 min read
820
Imagine stepping into your first data science interview—your palms are sweaty, your mind racing, and then… you get a question you actually know the answer to. That’s the power of preparation. With data science reshaping how businesses make decisions, the race to hire skilled data scientists is more intense than ever. For freshers, standing out in a sea of talent means more than just knowing the basics—it means being interview-ready. In this article, we’ve handpicked the top 100 data science interview questions that frequently appear in real interviews, giving you the edge you need.

From Python programming and EDA to statistics and machine learning, each question is paired with insights and tips to help you master the concepts and ace your answers. Whether you’re aiming for a startup or a Fortune 500 company, this guide is your secret weapon to land that dream job and kickstart your journey as a successful data scientist.

Table of contents
Data Science Interview Questions Regarding Python
Beginner Interview Python Questions for Data Science
Intermediate Interview Python Data Science Questions
Advanced Python Interview Questions
Interview Questions Regarding EDA and Statistics
Beginner Interview Questions on Statistics
Intermediate Interview Questions on Statistics for Data Science
Advanced Statistics Interview Questions
Interview Questions Related to Machine Learning
Beginner ML Interview Questions for Data Science
Intermediate ML Interview Questions
Advanced ML Interview Questions
Conclusion
Free Certification Courses
Ace a Data Science Interview in 2025
Master Technical & Behavioral interviews with AI-driven prep • Mock Interviews • Case Studies • Expert Tips
Data Science Interview Questions Regarding Python
Let us look at data science interview questions and answers regarding Python.

Beginner Interview Python Questions for Data Science
Q1. Which is faster, python list or Numpy arrays, and why?
A. NumPy arrays are quicker than Python lists when it comes to numerical computations. NumPy is a Python library for array processing, and it offers several functions for performing operations on arrays in an efficient manner.

One of the reasons NumPy arrays are faster than Python lists is that NumPy arrays are written in C, whereas Python lists are written in Python. This implies that operations on NumPy arrays are written in a compiled language and hence are faster than operations on Python lists, which are written in an interpreted language.

Q2. What is the difference between a python list and a tuple?
A. A list in Python is a sequence of objects of varying kinds. Lists are mutable, i.e., you can alter the value of a list item or insert or delete items in a list. Lists are defined using square brackets and a comma-delimited list of values.

A tuple is also an ordered list of objects, but it is immutable, meaning that you cannot alter the value of a tuple object or add or delete elements from a tuple.

Lists are initiated using square brackets ([ ” ]), whereas tuples are initiated using parentheses ((”, )).

Lists have a number of built-in methods for adding, deleting, and manipulating elements, but tuples don’t have these methods.

Generally, tuples are quicker than lists in Python

Q3. What are python sets? Explain some of the properties of sets.
A. In Python, a set is an unordered collection of unique objects. Sets are often used to store a collection of distinct objects and to perform membership tests (i.e., to check if an object is in the set). Sets are defined using curly braces ({ and }) and a comma-separated list of values.

Here are some key properties of sets in Python:

Sets are unordered: Sets do not have a specific order, so you cannot index or slice them like you can with lists or tuples.
Sets are unique: Sets only allow unique objects, so if you try to add a duplicate object to a set, it will not be added.
Sets are mutable: You can add or remove elements from a set using the add and remove methods.
Sets are not indexed: Sets do not support indexing or slicing, so you cannot access individual elements of a set using an index.
Sets are not hashable: Sets are mutable, so they cannot be used as keys in dictionaries or as elements in other sets. If you need to use a mutable object as a key or an element in a set, you can use a tuple or a frozen set (an immutable version of a set).
Q4. What is the difference between split and join?
A. Split and join are both functions of python strings, but they are completely different when it comes to functioning.

The split function is used to create a list from strings based on some delimiter, for eg. space.

a = ‘This is a string’
Li = a.split(‘ ‘)
print(li)
Output:

 [‘This’, ‘is’, ‘a’, ‘string’]
The join() method is a built-in function of Python’s str class that concatenates a list of strings into a single string. It is called on a delimiter string and invoked with a list of strings to be joined. The delimiter string is inserted between each string in the list when the strings are concatenated.

Here is an example of how to use the join() method:

 “ “.join(li)
Output:

This is a string
Here the list is joined with a space in between.

Q5. Explain the logical operations in python.
A. In Python, the logical operations and, or, and not can be used to perform boolean operations on truth values (True and False).

The and operator returns True if both the operands are True, and False otherwise.

The or operator returns True if either of the operands is True, and False if both operands are False.

The not operator inverts the boolean value of its operand. If the operand is True, not return False, and if the operand is False, not return True.

Q6. Explain the top 5 functions used for python strings.
A. Here are the top 5 Python string functions:

Function	Description
len()	Returns the length of a string.
strip()	Removes leading and trailing whitespace from a string.
split()	Splits a string into a list of substrings based on a delimiter.
replace()	Replaces all occurrences of a specified string with another string.
upper()	Converts a string to uppercase.
lower()	Converts a string to lowercase.
s = 'Hello, World!'

len(s)                  # 13
s.strip()               # 'Hello, World!'
s.split(',')            # ['Hello', ' World!']
s.replace('World', 'Universe')  # 'Hello, Universe!'
s.upper()               # 'HELLO, WORLD!'
s.lower()               # 'hello, world!'
Q7. What is the use of the pass keyword in python?
A. pass is a null statement that does nothing. It is often used as a placeholder where a statement is required syntactically, but no action needs to be taken. For example, if you want to define a function or a class but haven’t yet decided what it should do, you can use pass as a placeholder.

Q8. What is the use of the continue keyword in python?
A. continue is used in a loop to skip over the current iteration and move on to the next one. When continue is encountered, the current iteration of the loop is terminated, and the next one begins.

Intermediate Interview Python Data Science Questions
Q9. What are immutable and mutable data types?
A. In Python, an immutable object is an object whose state cannot be modified after it is created. This means that you can’t change the value of an immutable object once it is created. Examples of immutable objects in Python include numbers (such as integers, floats, and complex numbers), strings, and tuples.

On the other hand, a mutable object is an object whose state can be modified after it is created. This means that you can change the value of a mutable object after it is created. Examples of mutable objects in Python include lists and dictionaries.

Understanding the difference between immutable and mutable objects in Python is important because it can affect how you use and manipulate data in your code. For example, if you have a list of numbers and you want to sort the list in ascending order, you can use the built-in sort() method to do this. However, if you have a tuple of numbers, you can’t use the sort() method because tuples are immutable. Instead, you would have to create a new sorted tuple from the original tuple.

Q10. What is the use of try and accept block in python
A. The try and except block in Python are used to handle exceptions. An exception is an error that occurs during the execution of a program.

The try block contains code that might cause an exception to be raised. The except block contains code that is executed if an exception is raised during the execution of the try block.

Using a try-except block will save the code from an error to occur and can be executed with a message or output we want in the except block.

Q11. What are 2 mutable and 2 immutable data types in python?
A. 2 mutable data types are:

Dictionary
List
You can change/edit the values in a python dictionary and a list. It is not necessary to make a new list which means that it satisfies the property of mutability.

2 immutable data types are:

Tuples
String
You cannot edit a string or a value in a tuple once it is created. You need to either assign the values to the tuple or make a new tuple.

Q12. What are python functions, and how do they help in code optimization?
A.  In Python, a function is a block of code that can be called by other parts of your program. Functions are useful because they allow you to reuse code and divide your code into logical blocks that can be tested and maintained separately.

To call a function in Python, you simply use the function name followed by a pair of parentheses and any necessary arguments. The function may or may not return a value that depends on the usage of the turn statement.

Functions can also help in code optimization:

Code reuse: Functions allow you to reuse code by encapsulating it in a single place and calling it multiple times from different parts of your program. This can help to reduce redundancy and make your code more concise and easier to maintain.
Improved readability: By dividing your code into logical blocks, functions can make your code more readable and easier to understand. This can make it easier to identify bugs and make changes to your code.
Easier testing: Functions allow you to test individual blocks of code separately, which can make it easier to find and fix bugs.
Improved performance: Functions can also help to improve the performance of your code by allowing you to use optimized code libraries or by allowing the Python interpreter to optimize the code more effectively.
Q13. Why does NumPy have huge popularity in the field of data science?
A. NumPy (short for Numerical Python) is a popular library for scientific computing in Python. It has gained a lot of popularity in the data science community because it provides fast and efficient tools for working with large arrays and matrices of numerical data.

NumPy provides fast and efficient operations on arrays and matrices of numerical data. It uses optimized C and Fortran code behind the scenes to perform these operations, which makes them much faster than equivalent operations using Python’s built-in data structures. It provides fast and efficient tools for working with large arrays and matrices of numerical data.

NumPy provides a large number of functions for performing mathematical and statistical operations on arrays and matrices.

It allows you to work with large amounts of data efficiently. It provides tools for handling large datasets that would not fit in memory, such as functions for reading and writing data to disk and for loading only a portion of a dataset into memory at a time.

NumPy integrates well with other scientific computing libraries in Python, such as SciPy (Scientific Python) and pandas. This makes it easy to use NumPy with other libraries to perform more complex data science tasks.

Q14. Explain list comprehension and dict comprehension.
A. List comprehension and dict comprehension are both concise ways to create new lists or dictionaries from existing iterables.

List comprehension is a concise way to create a list. It consists of square brackets containing an expression followed by a for clause, then zero or more for or if clauses. The result is a new list that evaluates the expression in the context of the for and if clauses.

Dict comprehension is a concise way to create a dictionary. It consists of curly braces containing a key-value pair, followed by a for clause, then zero or more for or if clauses. A result is a new dictionary that evaluates the key-value pair in the context of the for and if clauses.

Q15. What are global and local variables in python?
A. In Python, a variable that is defined outside of any function or class is a global variable, while a variable that is defined inside a function or class is a local variable.

A global variable can be accessed from anywhere in the program, including inside functions and classes. However, a local variable can only be accessed within the function or class in which it is defined.

It is important to note that you can use the same name for a global variable and a local variable, but the local variable will take precedence over the global variable within the function or class in which it is defined. 

# This is a global variable
x = 10
def func():
  # This is a local variable
  x = 5
  print(x)my_function
func()
print(x)
Output:

This will print 5 and then 10
In the example above, the x variable inside the func() function is a local variable, so it takes precedence over the global variable x. Therefore, when x is printed inside the function, it prints 5; when it is printed outside the function, it prints 10.

Q16. What is an ordered dictionary?
A. An ordered dictionary, also known as an OrderedDict, is a subclass of the built-in Python dictionary class that maintains the order of elements in which they were added. In a regular dictionary, the order of elements is determined by the hash values of their keys, which can change over time as the dictionary grows and evolves. An ordered dictionary, on the other hand, uses a doubly linked list to remember the order of elements, so that the order of elements is preserved regardless of how the dictionary changes.

Q17. What is the difference between return and yield keywords?
A. Return is used to exit a function and return a value to the caller. When a return statement is encountered, the function terminates immediately, and the value of the expression following the return statement is returned to the caller.

yield, on the other hand, is used to define a generator function. A generator function is a special kind of function that produces a sequence of values one at a time, instead of returning a single value. When a yield statement is encountered, the generator function produces a value and suspends its execution, saving its state for later

Advanced Python Interview Questions
Q18. What are lambda functions in python, and why are they important?
A. In Python, a lambda function is a small anonymous function. You can use lambda functions when you don’t want to define a function using the def keyword.

Lambda functions are useful when you need a small function for a short period of time. They are often used in combination with higher-order functions, such as map(), filter(), and reduce().

Here’s an example of a lambda function in Python:

x = lambda a : a + 10
x(5)
15
In this example, the lambda function takes one argument (a) and adds 10 to it. The lambda function returns the result of this operation when it is called.

Lambda functions are important because they allow you to create small anonymous functions in a concise way. They are often used in functional programming, a programming paradigm that emphasizes using functions to solve problems.

Q19. What is the use of the ‘assert’ keyword in python?
A. In Python, the assert statement is used to test a condition. If the condition is True, then the program continues to execute. If the condition is False, then the program raises an AssertionError exception.

The assert statement is often used to check the internal consistency of a program. For example, you might use an assert statement to check that a list is sorted before performing a binary search on the list.

It’s important to note that the assert statement is used for debugging purposes and is not intended to be used as a way to handle runtime errors. In production code, you should use try and except blocks to handle exceptions that might be raised at runtime.

Q20. What are decorators in python?
A. In Python, decorators are a way to modify or extend the functionality of a function, method, or class without changing their source code. Decorators are typically implemented as functions that take another function as an argument and return a new function that has the desired behavior.

A decorator is a special function that starts with the @ symbol and is placed immediately before the function, method, or class it decorates. The @ symbol is used to indicate that the following function is a decorator.

Interview Questions Regarding EDA and Statistics
Let us look at data science interview questions and answers regarding EDA and Statistics.

Beginner Interview Questions on Statistics
Q21. How to perform univariate analysis for numerical and categorical variables?
A. Univariate analysis is a statistical technique used to analyze and describe the characteristics of a single variable. It is a useful tool for understanding the distribution, central tendency, and dispersion of a variable, as well as identifying patterns and relationships within the data. Here are the steps for performing univariate analysis for numerical and categorical variables:

For numerical variables:

Calculate descriptive statistics such as the mean, median, mode, and standard deviation to summarize the distribution of the data.
Visualize the distribution of the data using plots such as histograms, boxplots, or density plots.
Check for outliers and anomalies in the data.
Check for normality in the data using statistical tests or visualizations such as a Q-Q plot.
For categorical variables.

Calculate the frequency or count of each category in the data.
Calculate the percentage or proportion of each category in the data.
Visualize the distribution of the data using plots such as bar plots or pie charts.
Check for imbalances or abnormalities in the distribution of the data.
Note that the specific steps for performing univariate analysis may vary depending on the specific needs and goals of the analysis. It is important to carefully plan and execute the analysis in order to accurately and effectively describe and understand the data.

Q22. What are the different ways in which we can find outliers in the data?
A. Outliers are data points that are significantly different from the majority of the data. They can be caused by errors, anomalies, or unusual circumstances, and they can have a significant impact on statistical analyses and machine learning models. Therefore, it is important to identify and handle outliers appropriately in order to obtain accurate and reliable results.

Here are some common ways to find outliers in the data:

Visual inspection: Outliers can often be identified by visually inspecting the data using plots such as histograms, scatterplots, or boxplots.
Summary statistics: Outliers can sometimes be identified by calculating summary statistics such as the mean, median, or interquartile range, and comparing them to the data. For example, if the mean is significantly different from the median, it could indicate the presence of outliers.
Z-score: The z-score of a data point is a measure of how many standard deviations it is from the mean. Data points with a z-score greater than a certain threshold (e.g., 3 or 4) can be considered outliers.
There are many other methods for detecting outliers in the data, and the appropriate method will depend on the specific characteristics and needs of the data. It is important to carefully evaluate and choose the most appropriate method for identifying outliers in order to obtain accurate and reliable results.

Q23. What are the different ways by which you can impute the missing values in the dataset?
A. There are several ways that you can impute null values (i.e., missing values) in a dataset:

Drop rows: One option is to simply drop rows with null values from the dataset. This is a simple and fast method, but it can be problematic if a large number of rows are dropped, as it can significantly reduce the sample size and impact the statistical power of the analysis.
Drop columns: Another option is to drop columns with null values from the dataset. This can be a good option if the number of null values is large compared to the number of non-null values, or if the column is not relevant to the analysis.
Imputation with mean or median: One common method of imputation is to replace null values with the mean or median of the non-null values in the column. This can be a good option if the data are missing at random and the mean or median is a reasonable representation of the data.
Imputation with mode: Another option is to replace null values with the mode (i.e., the most common value) of the non-null values in the column. This can be a good option for categorical data where the mode is a meaningful representation of the data.
Imputation with a predictive model: Another method of imputation is to use a predictive model to estimate the missing values based on the other available data. This can be a more complex and time-consuming method, but it can be more accurate if the data are not missing at random and there is a strong relationship between the missing values and the other data.
Q24. What are Skewness in statistics and its types?
A. Skewness is a measure of the symmetry of a distribution. A distribution is symmetrical if it is shaped like a bell curve, with most of the data points concentrated around the mean. A distribution is skewed if it is not symmetrical, with more data points concentrated on one side of the mean than the other.

There are two types of skewness: positive skewness and negative skewness.

Positive skewness: Positive skewness occurs when the distribution has a long tail on the right side, with the majority of the data points concentrated on the left side of the mean. Positive skewness indicates that there are a few extreme values on the right side of the distribution that is pulling the mean to the right.
Negative skewness: Negative skewness occurs when the distribution has a long tail on the left side, with the majority of the data points concentrated on the right side of the mean. Negative skewness indicates that there are a few extreme values on the left side of the distribution that is pulling the mean to the left.
data science interview questions
Q25. What are the measures of central tendency?
A. In statistics, measures of central tendency are values that represent the center of a dataset. There are three main measures of central tendency: mean, median, and mode.

The mean is the arithmetic average of a dataset and is calculated by adding all the values in the dataset and dividing by the number of values. The mean is sensitive to outliers, or values that are significantly higher or lower than the majority of the other values in the dataset.

The median is the middle value of a dataset when the values are arranged in order from smallest to largest. To find the median, you must first arrange the values in order and then locate the middle value. If there is an odd number of values, the median is the middle value. If there is an even number of values, the median is the mean of the two middle values. The median is not sensitive to outliers.

The mode is the value that occurs most frequently in a dataset. A dataset may have multiple modes or no modes at all. The mode is not sensitive to outliers.

Q26. Can you explain the difference between descriptive and inferential statistics?
A. Descriptive statistics is used to summarize and describe a dataset by using measures of central tendency (mean, median, mode) and measures of spread (standard deviation, variance, range). Inferential statistics is used to make inferences about a population based on a sample of data and using statistical models, hypothesis testing and estimation.

Q27. What are the key elements of an EDA report and how do they contribute to understanding a dataset?
A. The key elements of an EDA report include univariate analysis, bivariate analysis, missing data analysis, and basic data visualization. Univariate analysis helps in understanding the distribution of individual variables, bivariate analysis helps in understanding the relationship between variables, missing data analysis helps in understanding the quality of data, and data visualization provides a visual interpretation of the data.

Intermediate Interview Questions on Statistics for Data Science
Q28 What is the central limit theorem?
A. The Central Limit Theorem is a fundamental concept in statistics that states that as the sample size increases, the distribution of the sample mean will approach a normal distribution. This is true regardless of the underlying distribution of the population from which the sample is drawn. This means that even if the individual data points in a sample are not normally distributed, by taking the average of a large enough number of them, we can use normal distribution-based methods to make inferences about the population.

Q29. Mention the two kinds of target variables for predictive modeling.
A. The two kinds of target variables are:

Numerical/Continuous variables – Variables whose values lie within a range, could be any value in that range and the time of prediction; values are not bound to be from the same range too.

For example: Height of students – 5; 5.1; 6; 6.7; 7; 4.5; 5.11

Here the range of the values is (4,7)

And, the height of some new students can/cannot be any value from this range.

Categorical variable – Variables that can take on one of a limited, and usually fixed, number of possible values, assigning each individual or other unit of observation to a particular group on the basis of some qualitative property.

A categorical variable that can take on exactly two values is termed a binary variable or a dichotomous variable. Categorical variables with more than two possible values are called polytomous variables

For example Exam Result: Pass, Fail (Binary categorical variable)

The blood type of a person: A, B, O, AB (polytomous categorical variable)

Q30. What will be the case in which the Mean, Median, and Mode will be the same for the dataset?
A. The mean, median, and mode of a dataset will all be the same if and only if the dataset consists of a single value that occurs with 100% frequency.

For example, consider the following dataset: 3, 3, 3, 3, 3, 3. The mean of this dataset is 3, the median is 3, and the mode is 3. This is because the dataset consists of a single value (3) that occurs with 100% frequency.

On the other hand, if the dataset contains multiple values, the mean, median, and mode will generally be different. For example, consider the following dataset: 1, 2, 3, 4, 5. The mean of this dataset is 3, the median is 3, and the mode is 1. The dataset contains multiple values, and no value occurs with 100% frequency.

It is important to note that outliers or extreme values in the dataset can affect the mean, median, and mode. If the dataset contains extreme values, the mean and median may be significantly different from the mode, even if the dataset consists of a single value that occurs with a high frequency.

Q31. What is the difference between Variance and Bias in Statistics?
A. In statistics, variance, and bias are two measures of the quality or accuracy of a model or estimator.

Variance: Variance measures the amount of spread or dispersion in a dataset. It is calculated as the average squared deviation from the mean. A high variance indicates that the data are spread out and may be more prone to error, while a low variance indicates that the data are concentrated around the mean and may be more accurate.
Bias: Bias refers to the difference between the expected value of an estimator and the true value of the parameter being estimated. A high bias indicates that the estimator is consistently under or overestimating the true value, while a low bias indicates that the estimator is more accurate.
It is important to consider both variance and bias when evaluating the quality of a model or estimator. A model with low bias and high variance may be prone to overfitting, while a model with high bias and low variance may be prone to underfitting. Finding the right balance between bias and variance is an important aspect of model selection and optimization.

Data Science interview question
Q32. What is the difference between Type I and Type II errors?
A. Two types of errors can occur in hypothesis testing: Type I errors and Type II errors.

A Type I error, also known as a “false positive,” occurs when the null hypothesis is true but is rejected. This type of error is denoted by the Greek letter alpha (α) and is usually set at a level of 0.05. This means that there is a 5% chance of making a Type I error or a false positive.

A Type II error, also known as a “false negative,” occurs when the null hypothesis is false but is not rejected. This type of error is denoted by the Greek letter beta (β) and is often represented as 1 – β, where β is the power of the test. The power of the test is the probability of correctly rejecting the null hypothesis when it is false.

It’s important to try to minimize the chances of both types of errors in hypothesis testing.

Data Science interview questions
Q33. What is the Confidence Interval in statistics?
A. The confidence interval is the range within which we expect the results to lie if we repeat the experiment. It is the mean of the result plus and minus the expected variation.

The standard error of the estimate determines the latter, while the center of the interval coincides with the mean of the estimate. The most common confidence interval is 95%.

Q34. Can you explain the concept of correlation and covariance?
A. Correlation is a statistical measure that describes the strength and direction of a linear relationship between two variables. A positive correlation indicates that the two variables increase or decrease together, while a negative correlation indicates that the two variables move in opposite directions. Covariance is a measure of the joint variability of two random variables. It is used to measure how two variables are related.

Advanced Statistics Interview Questions
Q35. Why is hypothesis testing useful for a data scientist?
A. Hypothesis testing is a statistical technique used in data science to evaluate the validity of a claim or hypothesis about a population. It is used to determine whether there is sufficient evidence to support a claim or hypothesis and to assess the statistical significance of the results.

There are many situations in data science where hypothesis testing is useful. For example, it can be used to test the effectiveness of a new marketing campaign, to determine if there is a significant difference between the means of two groups, to evaluate the relationship between two variables, or to assess the accuracy of a predictive model.

Hypothesis testing is an important tool in data science because it allows data scientists to make informed decisions based on data, rather than relying on assumptions or subjective opinions. It helps data scientists to draw conclusions about the data that are supported by statistical evidence, and to communicate their findings in a clear and reliable manner. Hypothesis testing is therefore a key component of the scientific method and a fundamental aspect of data science practice.

Q36. What is a chi-square test of independence used for in statistics?
A. A chi-square test of independence is a statistical test used to determine whether there is a significant association between two categorical variables. It is used to test the null hypothesis that the two variables are independent, meaning that the value of one variable does not depend on the value of the other variable.

The chi-square test of independence involves calculating a chi-square statistic and comparing it to a critical value to determine the probability of the observed relationship occurring by chance. If the probability is below a certain threshold (e.g., 0.05), the null hypothesis is rejected and it is concluded that there is a significant association between the two variables.

The chi-square test of independence is commonly used in data science to evaluate the relationship between two categorical variables, such as the relationship between gender and purchasing behavior, or the relationship between education level and voting preference. It is an important tool for understanding the relationship between different variables and for making informed decisions based on the data.

Q37. What is the significance of the p-value?
A. The p-value is used to determine the statistical significance of a result. In hypothesis testing, the p-value is used to assess the probability of obtaining a result that is at least as extreme as the one observed, given that the null hypothesis is true. If the p-value is less than the predetermined level of significance (usually denoted as alpha, α), then the result is considered statistically significant and the null hypothesis is rejected.

The significance of the p-value is that it allows researchers to make decisions about the data based on a predetermined level of confidence. By setting a level of significance before conducting the statistical test, researchers can determine whether the results are likely to have occurred by chance or if there is a real effect present in the data.

Q38.What are the different types of sampling techniques used by data analysts?
A. There are many different types of sampling techniques that data analysts can use, but some of the most common ones include:

Simple random sampling: This is a basic form of sampling in which each member of the population has an equal chance of being selected for the sample.
Stratified random sampling: This technique involves dividing the population into subgroups (or strata) based on certain characteristics, and then selecting a random sample from each stratum.
Cluster sampling: This technique involves dividing the population into smaller groups (or clusters), and then selecting a random sample of clusters. 
Systematic sampling: This technique involves selecting every kth member of the population to be included in the sample. 
Q39.What is Bayes’ theorem and how is it used in data science?
A. Bayes’ theorem is a mathematical formula that describes the probability of an event occurring, based on prior knowledge of conditions that might be related to the event. In data science, Bayes’ theorem is often used in Bayesian statistics and machine learning, for tasks such as classification, prediction, and estimation.

Bayes' Threorem
Q40.What is the difference between a parametric and a non-parametric test?
A. A parametric test is a statistical test that assumes that the data follows a specific probability distribution, such as a normal distribution. A non-parametric test does not make any assumptions about the underlying probability distribution of the data.

Interview Questions Related to Machine Learning
Let us look at data science interview questions and answers regarding Machine Learning.

Beginner ML Interview Questions for Data Science
Q41. What is the difference between feature selection and extraction?
A. Feature selection is the technique in which we filter the features that should be fed to the model. This is the task in which we select the most relevant features. The features that clearly do not hold any importance in determining the prediction of the model are rejected.

Feature selection on the other hand is the process by which the features are extracted from the raw data. It involves transforming raw data into a set of features that can be used to train an ML model. 

Both of these are very important as they help in filtering the features for our ML model which helps in determining the accuracy of the model.

Q42. What are the 5 assumptions for linear regression?
A. Here are the 5 assumptions of linear regression:

Linearity: There is a linear relationship between the independent variables and the dependent variable.
Independence of errors: The errors (residuals) are independent of each other.
Homoscedasticity: The variance of the errors is constant across all predicted values.
Normality: The errors follow a normal distribution.
Independence of predictors: The independent variables are not correlated with each other.
Q43. What is the difference between linear and nonlinear regression?
A.  Linear regression is the method in which is used to find the relationship between a dependent and one or more independent variables. The model finds the best-fit line, which is a linear function (y = mx +c) that helps in fitting the model in such a way that the error is minimum considering all the data points. So the decision boundary of a linear regression function is linear.

A non-Linear regression is used to model the relationship between a dependent and one or more independent variables by a non-linear equation. The non-linear regression models are more flexible and are able to find the more complex relationship between variables.

Q44. How will you identify underfitting in a model?
A. Underfitting occurs when a statistical model or machine learning algorithm is not able to capture the underlying trend of the data. This can happen for a variety of reasons, but one common cause is that the model is too simple and is not able to capture the complexity of the data

Here is how to identify underfitting in a model:

The training error of an underfitting error will be high, i.e., the model will not be able to learn from the training data and will perform poorly on the training data.

The validation error of an underfitting model will also be high as it will perform poorly on the new data as well.

Q45. How will you identify overfitting in a model?
A. Overfitting in a model occurs when the model learns the whole training data instead of taking signals/hints from the data and the model performs extremely well on training data and performs poorly on the testing data.

The testing error of the model is high compared to the training error. The bias of an overfitting model is low whereas the variance is high.

Data Science interview questions
Q46. What are some of the techniques to avoid overfitting?
A. Some techniques that can be used to avoid overfitting;

Train-validation-test split: One way to avoid overfitting is to split your data into training, validation, and test sets. The model is trained on the training set and then evaluated on the validation set. The hyperparameters are then tuned based on the performance on the validation set. Once the model is finalized, it is evaluated on the test set.
Early stopping: Another way to avoid overfitting is to use early stopping. This involves training the model until the validation error reaches a minimum, and then stopping the training process. 
Regularization: Regularization is a technique that can be used to prevent overfitting by adding a penalty term to the objective function. This term encourages the model to have small weights, which can help reduce the complexity of the model and prevent overfitting. 
Ensemble methods: Ensemble methods involve training multiple models and then combining their predictions to make a final prediction. This can help reduce overfitting by averaging out the predictions of the individual models, which can help reduce the variance of the final prediction.
Q47. What are some of the techniques to avoid underfitting?
A. Some techniques to prevent underfitting in a model:

Feature selection: It is important to choose the right feature required for training a model as the selection of the wrong feature can result in underfitting.

Increasing the number of features helps to avoid underfitting

Using a more complex machine-learning model

Using Hyperparameter tuning to fine tune the parameters in the model

Noise: If there is more noise in the data, the model will not be able to detect the complexity of the dataset.

Q48. What is Multicollinearity?
A. Multicollinearity occurs when two or more predictor variables in a multiple regression model are highly correlated. This can lead to unstable and inconsistent coefficients, and make it difficult to interpret the results of the model.

In other words, multicollinearity occurs when there is a high degree of correlation between two or more predictor variables. This can make it difficult to determine the unique contribution of each predictor variable to the response variable, as the estimates of their coefficients may be influenced by the other correlated variables.

Q49. Explain regression and classification problems.
A. Regression is a method of modeling the relationship between one or more independent variables and a dependent variable. The goal of regression is to understand how the independent variables are related to the dependent variable and to be able to make predictions about the value of the dependent variable based on new values of the independent variables.

A classification problem is a type of machine learning problem where the goal is to predict a discrete label for a given input. In other words, it is a problem of identifying to which set of categories a new observation belongs, on the basis of a training set of data containing observations.

Q50. What is the difference between K-means and KNN?
A. K-means and KNN (K-Nearest Neighbors) are two different machine learning algorithms.

K-means is a clustering algorithm that is used to divide a group of data points into K clusters, where each data point belongs to the cluster with the nearest mean. It is an iterative algorithm that assigns data points to a cluster and then updates the cluster centroid (mean) based on the data points assigned to it.

On the other hand, KNN is a classification algorithm that is used to classify data points based on their similarity to other data points. It works by finding the K data points in the training set that are most similar to the data point being classified, and then it assigns the data point to the class that is most common among those K data points.

So, in summary, K-means is used for clustering, and KNN is used for classification.

Q51. What is the difference between Sigmoid and Softmax ?
A. In Sigmoid function if your output is binary (0,1) then use the sigmoid function for the output layer. The sigmoid function appears in the output layer of the deep learning models and is used for predicting probability-based outputs.

The softmax function is another type of Activation Function used in neural networks to compute probability distribution from a vector of real numbers.

This function is mainly used in multi-class models where it returns probabilities of each class, with the target class having the highest probability.

The primary difference between the sigmoid and softmax Activation function is that while the former is used in binary classification, the latter is used for multivariate classification

machine learning interview questions
Q52. Can we use logistic regression for multiclass classification?
A. Yes, logistic regression can be used for multiclass classification.

Logistic regression is a classification algorithm that is used to predict the probability of a data point belonging to a certain class. It is a binary classification algorithm, which means that it can only handle two classes. However, there are ways to extend logistic regression to multiclass classification.

One way to do this is to use one-vs-all (OvA) or one-vs-rest (OvR) strategy, where you train K logistic regression classifiers, one for each class, and assign a data point to the class that has the highest predicted probability. This is called OvA if you train one classifier for each class, and the other class is the “rest” of the classes. This is called OvR if you train one classifier for each class, and the other class is the “all” of the classes.

Another way to do this is to use multinomial logistic regression, which is a generalization of logistic regression to the case where you have more than two classes. In multinomial logistic regression, you train a logistic regression classifier for each pair of classes, and you use the predicted probabilities to assign a data point to the class that has the highest probability.

So, in summary, logistic regression can be used for multiclass classification using OvA/OvR or multinomial logistic regression.

Q53. Can you explain the bias-variance tradeoff in the context of supervised machine learning?
A. In supervised machine learning, the goal is to build a model that can make accurate predictions on unseen data. However, there is a tradeoff between the model’s ability to fit the training data well (low bias) and its ability to generalize to new data (low variance).

A model with high bias tends to underfit the data, which means that it is not flexible enough to capture the patterns in the data. On the other hand, a model with high variance tends to overfit the data, which means that it is too sensitive to noise and random fluctuations in the training data.

The bias-variance tradeoff refers to the tradeoff between these two types of errors. A model with low bias and high variance is likely to overfit the data, while a model with high bias and low variance is likely to underfit the data.

To balance the tradeoff between bias and variance, we need to find a model with the right complexity level for the problem at hand. If the model is too simple, it will have high bias and low variance, but it will not be able to capture the underlying patterns in the data. If the model is too complex, it will have low bias and high variance, but it will be sensitive to the noise in the data and it will not generalize well to new data.

Q54. How do you decide whether a model is suffering from high bias or high variance?
A. There are several ways to determine whether a model is suffering from high bias or high variance. Some common methods are:

Split the data into a training set and a test set, and check the performance of the model on both sets. If the model performs well on the training set but poorly on the test set, it is likely to suffer from high variance (overfitting). If the model performs poorly on both sets, it is likely suffering from high bias (underfitting).

Use cross-validation to estimate the performance of the model. If the model has high variance, the performance will vary significantly depending on the data used for training and testing. If the model has high bias, the performance will be consistently low across different splits of the data.

Plot the learning curve, which shows the performance of the model on the training set and the test set as a function of the number of training examples. A model with high bias will have a high training error and a high test error, while a model with high variance will have a low training error and a high test error.

Q55. What are some techniques for balancing bias and variance in a model?
A. There are several techniques that can be used to balance the bias and variance in a model, including:

Increasing the model complexity by adding more parameters or features: This can help the model capture more complex patterns in the data and reduce bias, but it can also increase variance if the model becomes too complex.

Reducing the model complexity by removing parameters or features: This can help the model avoid overfitting and reduce variance, but it can also increase bias if the model becomes too simple.

Using regularization techniques: These techniques constrain the model complexity by penalizing large weights, which can help the model avoid overfitting and reduce variance. Some examples of regularization techniques are L1 regularization, L2 regularization, and elastic net regularization.

Splitting the data into a training set and a test set: This allows us to evaluate the model’s generalization ability and tune the model complexity to achieve a good balance between bias and variance.

Using cross-validation: This is a technique for evaluating the model’s performance on different splits of the data and averaging the results to get a more accurate estimate 

of the model’s generalization ability.

Q56. How do you choose the appropriate evaluation metric for a classification problem, and how do you interpret the results of the evaluation?
A. There are many evaluation metrics that you can use for a classification problem, and the appropriate metric depends on the specific characteristics of the problem and the goals of the evaluation. Some common evaluation metrics for classification include:

Accuracy: This is the most common evaluation metric for classification. It measures the percentage of correct predictions made by the model.
Precision: This metric measures the proportion of true positive predictions among all positive predictions made by the model.
Recall: This metric measures the proportion of true positive predictions among all actual positive cases in the test set.
F1 Score: This is the harmonic mean of precision and recall. It is a good metric to use when you want to balance precision and recall.
AUC-ROC: This metric measures the ability of the model to distinguish between positive and negative classes. It is commonly used for imbalanced classification problems.
To interpret the results of the evaluation, you should consider the specific characteristics of the problem and the goals of the evaluation. For example, if you are trying to identify fraudulent transactions, you may be more interested in maximizing precision, because you want to minimize the number of false alarms. On the other hand, if you are trying to diagnose a disease, you may be more interested in maximizing recall, because you want to minimize the number of missed diagnoses.

Q57. What is the difference between K-means and hierarchical clustering and when to use what?
A. K-means and hierarchical clustering are two different methods for clustering data. Both methods can be useful in different situations.

K-means is a centroid-based algorithm, or a distance-based algorithm, where we calculate the distances to assign a point to a cluster. K-means is very fast and efficient in terms of computational time, but it can fail to find the global optimum because it uses random initializations for the centroid seeds.

Hierarchical clustering, on the other hand, is a density-based algorithm that does not require us to specify the number of clusters beforehand. It builds a hierarchy of clusters by creating a tree-like diagram, called a dendrogram. There are two main types of hierarchical clustering: agglomerative and divisive. Agglomerative clustering starts with individual points as separate clusters and merges them into larger clusters, while divisive clustering starts with all points in one cluster and divides them into smaller clusters. Hierarchical clustering is a slow algorithm and requires a lot of computational resources, but it is more accurate than K-means.

So, when to use K-means and when to use hierarchical clustering? It really depends on the size and structure of your data, as well as the resources you have available. If you have a large dataset and you want to cluster it quickly, then K-means might be a good choice. If you have a small dataset or if you want more accurate clusters, then hierarchical clustering might be a better choice.

interview questions
Q58. How can you handle imbalanced classes in a logistic regression model?
A. There are several ways to handle imbalanced classes in a logistic regression model. Some approaches include:

Undersampling the majority class: This involves randomly selecting a subset of the majority class samples to use in training the model. This can help to balance the class distribution, but it may also throw away valuable information.
Oversampling the minority class: This involves generating synthetic samples of the minority class to add to the training set. One popular method for generating synthetic samples is called SMOTE (Synthetic Minority Oversampling Technique).
Adjusting the class weights: Many machine learning algorithms allow you to adjust the weighting of each class. In logistic regression, you can do this by setting the class_weight parameter to “balanced”. This will automatically weight the classes inversely proportional to their frequency, so that the model pays more attention to the minority class.
Using a different evaluation metric: In imbalanced classification tasks, it is often more informative to use evaluation metrics that are sensitive to class imbalance, such as precision, recall, and the F1 score.
Using a different algorithm: Some algorithms, such as decision trees and Random Forests, are more robust to imbalanced classes and may perform better on imbalanced datasets.
Q59. When not to use PCA for dimensionality reduction?
A. There are several situations when you may not want to use Principal Component Analysis (PCA) for dimensionality reduction:

When the data is not linearly separable: PCA is a linear technique, so it may not be effective at reducing the dimensionality of data that is not linearly separable.

The data has categorical features: PCA is designed to work with continuous numerical data and may not be effective at reducing the dimensionality of data with categorical features.

When the data has a large number of missing values: PCA is sensitive to missing values and may not work well with data sets that have a large number of missing values.

The goal is to preserve the relationships between the original features: PCA is a technique that looks for patterns in the data and creates new features that are combinations of the original features. As a result, it may not be the best choice if the goal is to preserve the relationships between the original features.

When the data is highly imbalanced: PCA is sensitive to class imbalances and may not produce good results on highly imbalanced data sets.

Q60. What is Gradient descent?
A. Gradient descent is an optimization algorithm used in machine learning to find the values of parameters (coefficients and bias) of a model that minimize the cost function. It is a first-order iterative optimization algorithm that follows the negative gradient of the cost function to converge to the global minimum.

In gradient descent, the model’s parameters are initialized with random values, and the algorithm iteratively updates the parameters in the opposite direction of the gradient of the cost function with respect to the parameters. The size of the update is determined by the learning rate, which is a hyperparameter that controls how fast the algorithm converges to the global minimum.

As the algorithm updates the parameters, the cost function decreases and the model’s performance improves

Q61. What is the difference between MinMaxScaler and StandardScaler?
A. Both the MinMaxScaler and StandardScaler are tools used to transform the features of a dataset so that they can be better modeled by machine learning algorithms. However, they work in different ways.

MinMaxScaler scales the features of a dataset by transforming them to a specific range, usually between 0 and 1. It does this by subtracting the minimum value of each feature from all the values in that feature, and then dividing the result by the range (i.e., the difference between the minimum and maximum values). This transformation is given by the following equation:

x_scaled = (x - x_min) / (x_max - x_min)
StandardScaler standardizes the features of a dataset by transforming them to have zero mean and unit variance. It does this by subtracting the mean of each feature from all the values in that feature, and then dividing the result by the standard deviation. This transformation is given by the following equation:

x_scaled = (x - mean(x)) / std(x)
In general, StandardScaler is more suitable for datasets where the distribution of the features is approximately normal, or Gaussian. MinMaxScaler is more suitable for datasets where the distribution is skewed or where there are outliers. However, it is always a good idea to visualize the data and understand the distribution of the features before choosing a scaling method.

Q62. What is the difference between Supervised and Unsupervised learning?
A. In supervised learning, the training set you feed to the algorithm includes the desired solutions, called labels.

Ex = Spam Filter (Classification problem)

k-Nearest Neighbors

Linear Regression
Logistic Regression
Support Vector Machines (SVMs)
Decision Trees and Random Forests
Neural networks
In unsupervised learning, the training data is unlabeled.

Let’s say, The system tries to learn without a teacher.

Clustering
K-Means
DBSCAN
Hierarchical Cluster Analysis (HCA)
Anomaly detection and novelty detection
One-class SVM
Isolation Forest
Visualization and dimensionality reduction
Principal Component Analysis (PCA)
Kernel PCA
Locally Linear Embedding (LLE)
t-Distributed Stochastic Neighbor Embedding (t-SNE)
Q63. What are some common methods for hyperparameter tuning?
A. There are several common methods for hyperparameter tuning:

Grid Search: This involves specifying a set of values for each hyperparameter, and the model is trained and evaluated using a combination of all possible hyperparameter values. This can be computationally expensive, as the number of combinations grows exponentially with the number of hyperparameters.
Random Search: This involves sampling random combinations of hyperparameters and training and evaluating the model for each combination. This is less computationally intensive than grid search, but may be less effective at finding the optimal set of hyperparameters.
Q64. How do you decide the size of your validation and test sets?
A. You can validate the size of your test sets in the following ways:

Size of the dataset: In general, the larger the dataset, the larger the validation and test sets can be. This is because there is more data to work with, so the validation and test sets can be more representative of the overall dataset.
Complexity of the model: If the model is very simple, it may not require as much data to validate and test. On the other hand, if the model is very complex, it may require more data to ensure that it is robust and generalizes well to unseen data.
Level of uncertainty: If the model is expected to perform very well on the task, the validation and test sets can be smaller. However, if the performance of the model is uncertain or the task is very challenging, it may be helpful to have larger validation and test sets to get a more accurate assessment of the model’s performance.
Resources available: The size of the validation and test sets may also be limited by the computational resources available. It may not be practical to use very large validation and test sets if it takes a long time to train and evaluate the model.
Q65. How do you evaluate a model’s performance for a multi-class classification problem?
A. One approach for evaluating a multi-class classification model is to calculate a separate evaluation metric for each class, and then calculate a macro or micro average. The macro average gives equal weight to all the classes, while the micro average gives more weight to the classes with more observations. Additionally, some commonly used metrics for multi-class classification problems such as confusion matrix, precision, recall, F1 score, Accuracy and ROC-AUC can also be used.

Q66. What is the difference between Statistical learning and Machine Learning with their examples?
A. Statistical learning and machine learning are both methods used to make predictions or decisions based on data. However, there are some key differences between the two approaches:

Statistical learning focuses on making predictions or decisions based on a statistical model of the data. The goal is to understand the relationships between the variables in the data and make predictions based on those relationships. Machine learning, on the other hand, focuses on making predictions or decisions based on patterns in the data, without necessarily trying to understand the relationships between the variables.

Statistical learning methods often rely on strong assumptions about the data distribution, such as normality or independence of errors. Machine learning methods, on the other hand, are often more robust to violations of these assumptions.

Statistical learning methods are generally more interpretable because the statistical model can be used to understand the relationships between the variables in the data. Machine learning methods, on the other hand, are often less interpretable, because they are based on patterns in the data rather than explicit relationships between variables.

For example, linear regression is a statistical learning method that assumes a linear relationship between the predictor and target variables and estimates the coefficients of the linear model using an optimization algorithm. Random forests is a machine learning method that builds an ensemble of decision trees and makes predictions based on the average of the predictions of the individual trees. 

Q67. How is normalized data beneficial for making models in data science?
A. Improved model performance: Normalizing the data can improve the performance of some machine learning models, particularly those that are sensitive to the scale of the input data. For example, normalizing the data can improve the performance of algorithms such as K-nearest neighbors and neural networks.

Easier feature comparison: Normalizing the data can make it easier to compare the importance of different features. Without normalization, features with large scales can dominate the model, making it difficult to determine the relative importance of other features.
Reduced impact of outliers: Normalizing the data can reduce the impact of outliers on the model, as they are scaled down along with the rest of the data. This can improve the robustness of the model and prevent it from being influenced by extreme values.
Improved interpretability: Normalizing the data can make it easier to interpret the results of the model, as the coefficients and feature importances are all on the same scale.
It is important to note that normalization is not always necessary or beneficial for all models. It is necessary to carefully evaluate the specific characteristics and needs of the data and the model in order to determine whether normalization is appropriate.

Intermediate ML Interview Questions
Q68. Why is the harmonic mean calculated in the f1 score and not the mean?
A. The F1 score is a metric that combines precision and recall. Precision is the number of true positive results divided by the total number of positive results predicted by the classifier, and recall is the number of true positive results divided by the total number of positive results in the ground truth. The harmonic mean of precision and recall is used to calculate the F1 score because it is more forgiving of imbalanced class proportions than the arithmetic mean.

If the harmonic means were not used, the F1 score would be higher because it would be based on the arithmetic mean of precision and recall, which would give more weight to the high precision and less weight to the low recall. The use of the harmonic mean in the F1 score helps to balance the precision and recall and gives a more accurate overall assessment of the classifier’s performance.

intrview questions
Q69. What are some ways to select features?
A. Here are some ways to select the features:

Filter methods: These methods use statistical scores to select the most relevant features. 
Example:

Correlation coefficient: Selects features that are highly correlated with the target variable.
Chi-squared test: Selects features that are independent of the target variable.
Wrapper methods: These methods use a learning algorithm to select the best features. 
For example

Forward selection: Begins with an empty set of features and adds one feature at a time until the performance of the model is optimal.
Backward selection: Begins with the full set of features and removes one feature at a time until the performance of the model is optimal.
Embedded methods: These methods learn which features are most important while the model is being trained.
Example:

Lasso regression: Regularizes the model by adding a penalty term to the loss function that shrinks the coefficients of the less important features to zero.
Ridge regression: Regularizes the model by adding a penalty term to the loss function that shrinks the coefficients of all features towards zero, but does not set them to zero.
Feature Importance: We can also use the feature importance parameter which gives us the most important features considered by the model
Q70. What is the difference between bagging boosting difference?
A. Both bagging and boosting are ensemble learning techniques that help in improving the performance of the model.

Bagging is the technique in which different models are trained on the dataset that we have and then the average of the predictions of these models is taken into consideration. The intuition behind taking the predictions of all the models and then averaging the results is making more diverse and generalized predictions that can be more accurate.

Boosting is the technique in which different models are trained but they are trained in a sequential manner. Each successive model corrects the error made by the previous model. This makes the model strong resulting in the least error.

Q71. What is the difference between stochastic gradient boosting and XGboost?
A. XGBoost is an implementation of gradient boosting that is specifically designed to be efficient, flexible, and portable. Stochastic XGBoost is a variant of XGBoost that uses a more randomized approach to building decision trees, which can make the resulting model more robust to overfitting. 

Both XGBoost and stochastic XGBoost are popular choices for building machine-learning models and can be used for a wide range of tasks, including classification, regression, and ranking. The main difference between the two is that XGBoost uses a deterministic tree construction algorithm, while stochastic XGBoost uses a randomized tree construction algorithm.

Q72. What is the difference between catboost and XGboost?
A. Difference between Catboost and XGboost:

Catboost handles categorical features better than XGboost. In catboost, the categorical features are not required to be one-hot encoded which saves a lot of time and memory. XGboost on the other hand can also handle categorical features but they needed to be one-hot encoded first.
XGboost requires manual processing of the data while Catboost does not. They have some differences in the way that they build decision trees and make predictions.
Catboost is faster than XGboost and builds symmetric(balanced) trees, unlike XGboost.

Q73. What is the difference between linear and nonlinear classifiers
A. The difference between the linear and nonlinear classifiers is the nature of the decision boundary.

In a linear classifier, the decision boundary is a linear function of the input. In other words, the boundary is a straight line, a plane, or a hyperplane. 

ex: Linear Regression, Logistic Regression, LDA

A non-linear classifier is one in which the decision boundary is not a linear function of the input.  This means that the classifier cannot be represented by a linear function of the input features. Non-linear classifiers can capture more complex relationships between the input features and the label, but they can also be more prone to overfitting, especially if they have a lot of parameters.

ex: KNN, Decision Tree, Random Forest

ML interview questions
Q74. What are parametric and nonparametric models?
A. A parametric model is a model that is described by a fixed number of parameters. These parameters are estimated from the data using a maximum likelihood estimation procedure or some other method, and they are used to make predictions about the response variable.

Nonparametric models do not assume any specific form for the relationship between variables. They are more flexible than parametric models. They can fit a wider variety of data shapes. However, they have fewer interpretable parameters. This can make them harder to understand.

Q75. How can we use cross-validation to overcome overfitting?
A. The cross-validation technique can be used to identify if the model is underfitting or overfitting but it cannot be used to overcome either of the problems. We can only compare the performance of the model on two different sets of data and find if the data is overfitting or underfitting, or generalized.

Q76. How can you convert a numerical variable to a categorical variable and when can it be useful?
A. There are several ways to convert a numerical variable to a categorical variable. One common method is to use binning, which involves dividing the numerical variable into a set of bins or intervals and treating each bin as a separate category.

Another way to convert a numerical variable to a categorical one is through “discretization.” This means dividing the range into intervals. Each interval is then treated as a separate category. It helps create a more detailed view of the data.

This conversion is useful when the numerical variable has limited values. Grouping these values can make patterns clearer. It also highlights trends instead of focusing on raw numbers.

Q77. What are generalized linear models?
A. Generalized Linear Models are a flexible family of models. They describe the relationship between a response variable and one or more predictors. GLMs offer more flexibility than traditional linear models.

In linear models, the response is normally distributed. The relationship with predictors is assumed to be linear. GLMs relax these rules. The response can follow different distributions. The relationship can also be non-linear. Common GLMs include logistic regression for binary data, Poisson regression for counts, and exponential regression for time-to-event data.

Q78. What is the difference between ridge and lasso regression? How do they differ in terms of their approach to model selection and regularization?
A. Ridge regression and lasso regression are both techniques used to prevent overfitting in linear models by adding a regularization term to the objective function. They differ in how they define the regularization term.

In ridge regression, the regularization term is defined as the sum of the squared coefficients (also called the L2 penalty). This results in a smooth optimization surface, which can help the model generalize better to unseen data. Ridge regression has the effect of driving the coefficients towards zero, but it does not set any coefficients exactly to zero. This means that all features are retained in the model, but their impact on the output is reduced.

On the other hand, lasso regression defines the regularization term as the sum of the absolute values of the coefficients (also called the L1 penalty). This has the effect of driving some coefficients exactly to zero, effectively selecting a subset of the features to use in the model. This can be useful for feature selection, as it allows the model to automatically select the most important features. However, the optimization surface for lasso regression is not smooth, which can make it more difficult to train the model.

In summary, ridge regression shrinks the coefficients of all features towards zero, while lasso regression sets some coefficients exactly to zero. Both techniques can be useful for preventing overfitting, but they differ in how they handle model selection and regularization.

Q79.How does the step size (or learning rate) of an optimization algorithm impact the convergence of the optimization process in logistic regression?
A. The step size, or learning rate, controls how big the steps are during optimization. In logistic regression, we minimize the negative log-likelihood to find the best coefficients. If the step size is too large, the algorithm may overshoot the minimum. It can oscillate or even diverge. If the step size is too small, progress will be slow. The algorithm may take a long time to converge.

Therefore, it is important to choose an appropriate step size in order to ensure the convergence of the optimization process. In general, a larger step size can lead to faster convergence, but it also increases the risk of overshooting the minimum. A smaller step size will be safer, but it will also be slower.

There are several approaches for choosing an appropriate step size. One common approach is to use a fixed step size for all iterations. Another approach is to use a decreasing step size, which starts out large and decreases over time. This can help the optimization algorithm to make faster progress at the beginning and then fine-tune the coefficients as it gets closer to the minimum.

Q80. What is overfitting in decision trees, and how can it be mitigated?
A. Overfitting in decision trees occurs when the model is too complex and has too many branches, leading to poor generalization to new, unseen data. This is because the model has “learned” the patterns in the training data too well, and is not able to generalize these patterns to new, unseen data.

There are several ways to mitigate overfitting in decision trees:

Pruning: This involves removing branches from the tree that do not add significant value to the model’s predictions. Pruning can help reduce the complexity of the model and improve its generalization ability.
Limiting tree depth: By restricting the depth of the tree, you can prevent the tree from becoming too complex and overfitting the training data.
Using ensembles: Ensemble methods such as random forests and gradient boosting can help reduce overfitting by aggregating the predictions of multiple decision trees.
Using cross-validation: By evaluating the model’s performance on multiple train-test splits, you can get a better estimate of the model’s generalization performance and reduce the risk of overfitting.
Q81. Why is SVM called a large margin classifier?
A. Support Vector Machine, is called a large margin classifier because it seeks to find a hyperplane with the largest possible margin, or distance, between the positive and negative classes in the feature space. The margin is the distance between the hyperplane and the nearest data points, and is used to define the decision boundary of the model.

By maximizing the margin, the SVM classifier is able to better generalize to new, unseen data and is less prone to overfitting. The larger the margin, the lower the uncertainty around the decision boundary, and the more confident the model is in its predictions.

Therefore, the goal of the SVM algorithm is to find a hyperplane with the largest possible margin, which is why it is called a large margin classifier.

machin learning, data science interview questions
Q82. What is hinge loss?
A. Hinge loss is a loss function used in support vector machines (SVMs) and other linear classification models. It is defined as the loss that is incurred when a prediction is incorrect.

The hinge loss for a single example is defined as:

loss = max(0, 1 – y * f(x))

where y is the true label (either -1 or 1) and f(x) is the predicted output of the model. The predicted output is the inner product between the input features and the model weights, plus a bias term.

Hinge loss is used in SVMs because it is convex. It penalizes predictions that are not confident and correct. The loss is zero when the prediction is correct. It increases as confidence in a wrong prediction grows. This pushes the model to be confident but careful. It discourages predictions far from the true label.

Advanced ML Interview Questions
Q83. What will happen if we increase the number of neighbors in KNN?
A. Increasing the number of neighbors in KNN makes the classifier more conservative. The decision boundary becomes smoother. This helps reduce overfitting. However, it may miss subtle patterns in the data. A larger k creates a simpler model. This lowers overfitting but increases the risk of underfitting.

To avoid both issues, choosing the right k is important. It should balance complexity and simplicity. It’s best to test different k values. Then, pick the one that works best for your dataset.

Q84. What will happen in the decision tree if the max depth is increased?
A. Increasing the max depth of a decision tree will increase the complexity of the model and make it more prone to overfitting. If you increase the max depth of a decision tree, the tree will be able to make more complex and nuanced decisions, which can improve the model’s ability to fit the training data well. However, if the tree is too deep, it may become overly sensitive to the specific patterns in the training data and not generalize well to unseen data.

interview question, data science interview questions
Q85. What is the difference between extra trees and random forests?
A. The main difference between the two algorithms is how the decision trees are constructed.

In a Random Forest, the decision trees are constructed using bootstrapped samples of the training data and a random subset of the features. This results in each tree being trained on a slightly different set of data and features, leading to a greater diversity of trees and a lower variance.

In an Extra Trees classifier, the decision trees are constructed in a similar way, but instead of selecting a random subset of the features at each split, the algorithm selects the best split among a random subset of the features. This results in a greater number of random splits and a higher degree of randomness, leading to a lower bias and a higher variance.

Q86. When to use one-hot encoding and label encoding?
A. One-hot encoding and label encoding are two different techniques that can be used to encode categorical variables as numerical values. They are often used in machine learning models as a preprocessing step before fitting the model to the data.

One-hot encoding is used for categorical variables without any natural order. It creates binary columns for each category, using 1 for presence and 0 for absence, helping preserve uniqueness and avoid false ordinal assumptions. Label encoding is used when categories have a natural order, assigning each a unique integer to reflect that order. One-hot suits nominal data, while label encoding fits ordinal data, though the final choice depends on the model and dataset.

Q87. What is the problem with using label encoding for nominal data?
A. Label encoding is a method of encoding categorical variables as numerical values, which can be beneficial in certain situations. However, there are some potential problems that you should be aware of when using label encoding for nominal data.

One problem with label encoding is that it can create an ordinal relationship between categories where none exists

If you have a categorical variable with three categories: “red”, “green”, and “blue”, and you apply label encoding to map these categories to numerical values 0, 1, and 2, the model may assume that the category “green” is somehow “between” the categories “red” and “blue”. This can be a problem if your model depends on the assumption that the categories are independent of one another.

Another problem with label encoding is that it can lead to unexpected results if you have an imbalanced dataset. For example, if one category is much more common than the others, it will be assigned a much lower numerical value, which could lead the model to give it less importance than it deserves.

Q88. When can one-hot encoding be a problem?
A. One-hot encoding can be a problem in certain situations because it can create a large number of new columns in the dataset, which can make the data more difficult to work with and potentially lead to overfitting.

One-hot encoding creates a new binary column for each category in a categorical variable. If you have a categorical variable with many categories, this can result in a very large number of new columns.

Another problem with one-hot encoding is that it can lead to overfitting. Especially if you have a small dataset and a large number of categories. When you create many new columns for each category, you are effectively increasing the number of features in the dataset. This can lead to overfitting, because the model may be able to memorize the training data, but it will not generalize well to new data.

Finally, one-hot encoding can also be a problem if you need to add new categories to the dataset in the future. If you have already one-hot encoded the existing categories. Ensure new categories are added clearly to avoid confusion or unexpected results.

Q89. What can be an appropriate encoding technique when you have hundreds of categorical values in a column?
A. A few techniques can be used when we have hundreds of columns in a categorical variable.

Frequency encoding: This involves replacing each category with the frequency of that category in the dataset. This can work well if the categories have a natural ordinal relationship based on their frequency.

Target encoding: This involves replacing each category with the mean of the target variable for that category. This can be effective if the categories have a clear relationship with the target variable.

Q90. What are the sources of randomness in random forest ?
A. Random forests are an ensemble learning method that involves training multiple decision trees on different subsets of the data and averaging the predictions of the individual trees to make a final prediction. There are several sources of randomness in the process of training a random forest:

Bootstrapped samples: When training each decision tree, the algorithm creates a bootstrapped sample of the data by sampling with replacement from the original training set. This means that some data points will be included in the sample multiple times. While others will not be included at all. This creates variation between the training sets of different trees.
Random feature selection: When training each decision tree, the algorithm selects a random subset of the features to consider at each split. This means that different trees will consider different sets of features, leading to variation in the learned trees.
Random threshold selection: When training each decision tree, the algorithm selects a random threshold for each feature to determine the optimal split. This means that different trees will split on different thresholds, leading to variation in the learned trees.
Q91. How do you decide which feature to split on at each node of the tree?
A. When training a decision tree, the algorithm must choose the feature to split on at each node of the tree. There are several strategies that can be used to decide which feature to split on, including:

Greedy search: The algorithm selects the feature that maximizes a splitting criterion (such as information gain or Gini impurity) at each step.
Random Search: The algorithm selects the feature to split on at random at each step.
Exhaustive search: The algorithm considers all possible splits and selects the one that maximizes the splitting criterion.
Forward search: The algorithm starts with an empty tree and adds splits one by one, selecting the split that maximizes the splitting criterion at each step.
Backward search: The algorithm starts with a fully grown tree and prunes split one by one, selecting the split to remove that results in the smallest decrease in the splitting criterion.
Q92. What is the significance of C in SVM?
A. In the support vector machine (SVM) algorithm, the parameter C is a hyperparameter that controls the trade-off between maximizing the margin and minimizing the misclassification error.

C controls the penalty for misclassifying training examples. A smaller C means a higher penalty. The model tries to classify all examples correctly, even with a smaller margin. A larger C means a lower penalty. The model allows some misclassifications to get a larger margin.

In practice, you can think of C as controlling the flexibility of the model. A smaller value of C will result in a more rigid model that may be more prone to underfitting, while a larger value of C will result in a more flexible model that may be more prone to overfitting.

Choose C carefully using cross-validation to balance bias-variance and ensure good performance on unseen data.

Q93. How do c and gamma affect overfitting in SVM?
A. In support vector machines (SVMs), the regularization parameter C and the kernel parameter gamma are used to control overfitting.

C is the penalty for misclassification. A smaller value of C means a larger penalty for misclassification. The model becomes more conservative. It tries harder to avoid mistakes. This can reduce overfitting. However, it may also make the model too cautious. As a result, generalization performance might suffer.

Gamma is a parameter that controls the complexity of the model. A smaller value of gamma means a more complex model, which can lead to overfitting. A larger value of gamma means a simpler model, which can help prevent overfitting but may also result in a model that is too simple to accurately capture the underlying relationships in the data.

Finding the best values for C and gamma is a balance between bias and variance. It usually requires testing different values. The model’s performance should be checked on a validation set. This helps identify the best parameter settings.

Q94. How do you choose the number of models to use in a Boosting or Bagging ensemble?
A. The number of models to use in an ensemble is usually determined by the trade-off between performance and computational cost. As a general rule of thumb, increasing the number of models will improve the performance of the ensemble, but at the cost of increasing the computational cost.

In practice, the number of models is determined by Cross validation which is used to determine the optimal number of models based on the evaluation metric chosen.

Q95. In which scenarios Boosting and Bagging are preferred over single models?
A. Both boosting and bagging are used to improve model performance. They help when individual models have high variance or high bias. Bagging reduces the variance of a model. Boosting reduces bias and improves generalization error. Both methods are useful for models that are sensitive to training data. They also help when there is a high risk of overfitting.

Q96. Can you explain the ROC curve and AUC score and how they are used to evaluate a model’s performance?
A. A ROC (Receiver Operating Characteristic) curve is a graphical representation of the performance of a binary classification model. It plots the true positive rate (TPR) against the false positive rate (FPR) at different thresholds. AUC (Area Under the Curve) is the area under the ROC curve. It gives a single number that represents the model’s overall performance. AUC is useful because it considers all possible thresholds, not just a single point on the ROC curve.

Q97. How do you approach setting the threshold in a binary classification problem when you want to adjust precision and recall by yourself?
A. When setting the threshold in a binary classification problem, it’s important to consider the trade-off between precision and recall. Precision is the ratio of true positives to all predicted positives. Recall is the ratio of true positives to all actual positives. To adjust these metrics, first train the model and evaluate it on a validation set. This set should have a similar distribution to the test data. Then, use a confusion matrix to visualize performance. It shows true positives, false positives, true negatives, and false negatives. This helps identify the current prediction threshold.

Once you know the threshold, you can adjust it to balance precision and recall. Increasing the threshold boosts precision but lowers recall. Decreasing it raises recall but reduces precision. Always consider the specific use case. In medical diagnosis, high recall is vital to catch all positives. In fraud detection, high precision is key to avoid false alarms. The right balance depends on the cost of false positives and false negatives in your scenario.

Q98. What is the difference between LDA (Linear Discriminant Analysis) and PCA (Principal Component Analysis)?
A. The difference between LDA (Linear Discriminant Analysis) and PCA (Principal Component Analysis) are:

Feature	PCA (Principal Component Analysis)	LDA (Linear Discriminant Analysis)
Type	Unsupervised	Supervised
Purpose	Find directions of maximum variance in the data	Maximize class separability
Use Case	Pattern discovery, data compression	Classification tasks (e.g., face, iris, fingerprint recognition)
Based On	Variance in data	Labels and class distribution
Components	Principal components (orthogonal directions of maximum variance)	Linear discriminants (directions that best separate classes)
Data Projection	Projects data onto directions of highest variance	Projects data onto directions that best separate the classes
Orthogonality	Components are mutually orthogonal	Components are not necessarily orthogonal
Output	Lower-dimensional subspace preserving most variance	Lower-dimensional subspace maximizing class discrimination
Q99. How does the Naive Bayes algorithm compare to other supervised learning algorithms?
A. Naive Bayes is a simple and fast algorithm that works well with high-dimensional data and small training sets. It also performs well on datasets with categorical variables and missing data, which are common in many real-world problems. It is good for text classification, spam filtering, and sentiment analysis. However, due to the assumption of independence among features, it does not perform good for problems having high correlation among features. It also often fails to capture the interactions among features, which can result in poor performance on some datasets. Therefore, it is often used as a baseline or starting point, and then other algorithms like SVM, and Random Forest can be used to improve the performance.

Q100. Can you explain the concept of the “kernel trick” and its application in Support Vector Machines (SVMs)?
A. The kernel trick is a technique used in SVMs. It transforms input data into a higher-dimensional feature space. This makes the data linearly separable. The trick replaces the standard inner product with a kernel function. The kernel computes the inner product in a higher-dimensional space. It does this without calculating the actual coordinates. This helps SVMs handle non-linearly separable data. Common kernel functions include the polynomial kernel, RBF kernel, and sigmoid kernel.
Top 100 SQL Interview Questions
Dan Lee's profile image
Dan Lee
Updated Feb 7, 2025 — 8 min read
Feature image - SQL interview prep for data analyst, data scientist and data engineer roles
Table of Contents
📚 SQL Interview Areas
⭐ SQL Interviews Across Data Roles
🎯 Common SQL Topics to Master
✍️ SQL Interview Technical Screen
📝 More SQL Interview Questions
💡 How to Prepare for SQL Interviews
Facebook
X (formerly Twitter)
LinkedIn
Reddit
Looking for REAL SQL interview questions asked in FAANGs and startups? Here's a comprehensive guide with REAL questions!

These questions are fair game across data analyst, data engineer, backend engineer, and database administrator interviews. Companies such as Google, Amazon, Meta, Stripe, Microsoft, and many more all ask SQL questions.

We will explore the question areas, uncover tips, and provide you with a detailed list of interview questions you can use to ace the interviews!

Let's get started👇

📚 SQL Interview Areas
Here are common areas assessed in SQL interviews across data roles.

Area 1 - Basic SQL Queries
Understanding basic SQL queries is fundamental. Expect questions that test your ability to retrieve, filter, and sort data using SELECT, WHERE, ORDER BY, and GROUP BY clauses. Also, SQL joins are essential for combining data from multiple tables. You should be comfortable with INNER JOIN, LEFT JOIN, RIGHT JOIN, and FULL OUTER JOIN.

Sample Questions
Write a SQL query to find the second highest salary from the Employees table.
How do you count the number of records in a table?
Retrieve all records where the status is 'Active' from the Users table.
Explain the difference between an INNER JOIN and a LEFT JOIN.
Write a query to find all customers who have made more than one purchase.
How do you retrieve data from three tables using joins?
💡
You don't need to know all the statements in SQL for interviews. In some cases, the interviewer will allow you to look up the statement. Just know the common ones, as seen in this guide.

Area 2 - Advanced SQL Concepts
Requiring multiple steps to solve, advanced SQL questions assess your understanding of subqueries, window functions, CTEs (Common Table Expressions), and set operations like UNION and INTERSECT.

Sample Questions
Write a query to find the cumulative sum of sales per day.
How do you delete duplicate records from a table?
Calculate the 7-day moving average of daily sales.
💡 You can get practice questions with hands-on coding on DataInterview SQL Pad.

Area 3 - SQL Optimization and Performance
Performance is crucial in SQL. If you are shooting for senior DS or data engineering roles, you should know how to optimize queries, use indexes effectively, and understand execution plans.

Sample Questions
What are the pros and cons of using indexes in a database?
How does a full table scan impact query performance, and how can you avoid it?
What techniques would you use to optimize a query that joins multiple large tables?
Explain how the query execution plan helps in query optimization.
What are common causes of deadlocks in SQL, and how can you prevent them?
How does database partitioning improve query performance, and when should you use it?
What is the difference between a covering index and a composite index, and when would you use each?
Area 4 - SQL in Data Warehousing and ETL
This is the part often assessed in data engineering or analytics engineering roles. Understanding how SQL is used in data warehousing and ETL processes is important for roles that deal with large datasets.

Sample Questions
What is the purpose of a staging table in ETL, and how is it used?
Explain the difference between OLTP and OLAP databases.
How would you design a data pipeline to handle incremental data loads?
What are surrogate keys, and why are they used in data warehousing?
Describe the advantages of using a star schema over a snowflake schema.
What are some common challenges in maintaining data quality in ETL processes, and how do you address them?
How do you implement error handling in an ETL workflow to ensure data integrity?
⭐ SQL Interviews Across Data Roles
SQL questions are highly common across interviews in data roles from Data Analyst, Data Scientist and Data Engineer. Do expect SQL questions to show up in technical screens, and again in final rounds.

In some cases, a recruiter may ask basic SQL questions such as "explain JOINs" and "UNION vs UNION ALL" just to assess your basic knowledge before letting you advance to the technical screen.

Data Analyst (or BI Engineer) - Do expect SQL questions that involve table manipulation, but also in tandem with data visualization questions.
Data Scientist - Expect SQL questions with hands-on coding. Most often the questions pertain to a particular product of the company you are interviewing.
Data Engineer - Do expect intermediate to advanced questions and follow-ups regarding data modeling, optimization and databases.
👉 If you are looking for structured interview prep, join the Data Scientist Interview MasterClass - a live cohort led by FAANG instructors!

🎯 Common SQL Topics to Master
In general, you should have an in-depth understanding of the following topics:

Data Retrieval and Filtering
Aggregate Functions (COUNT, SUM, AVG, etc.)
Joins and Subqueries
Window Functions
Indexes and Query Optimization
Database Transactions and Isolation Levels
Stored Procedures and Functions
💡
Learn the ins-and-outs of these topics by practicing writing queries from scratch. This will be helpful in SQL coding interviews!

✍️ SQL Interview Technical Screen
Here's an example technical screen you may encounter if you interview for data analyst, data engineer or data scientist position at a company like Meta (Facebook) or Amazon. The technical screen round is usually 45 minutes. The first 20-30 minutes is primarily on SQL followed by a business or product case.

Tables Provided:
Listings Table

listing_id	title	seller_id	category	price	created_date
101	Used Bicycle	1	Sports	150	2024-01-10
102	Vintage Chair	2	Furniture	200	2024-03-15
103	Laptop	3	Electronics	800	2024-02-20
104	Smartphone	4	Electronics	500	2024-04-05
105	Dining Table Set	2	Furniture	450	2024-01-25
Sellers Table

seller_id	name	join_date	location	rating
1	Alice	2023-08-12	New York	4.8
2	Bob	2022-05-20	San Francisco	4.5
3	Charlie	2023-01-05	Chicago	4.9
4	Diana	2024-02-18	Austin	4.3
5	Eve	2022-11-11	Seattle	4.7
#1 - List the titles of the top 3 most expensive listings, along with the seller name and location, for sellers who joined in 2023 or later.

SELECT l.title, s.name, s.location
FROM Listings l
JOIN Sellers s ON l.seller_id = s.seller_id
WHERE s.join_date >= '2023-01-01'
ORDER BY l.price DESC
LIMIT 3;
#2 - Find sellers who have not listed any items in the 'Electronics' category. Display their name and total number of listings.

SELECT s.name, COUNT(l.listing_id) AS total_listings
FROM Sellers s
LEFT JOIN Listings l ON s.seller_id = l.seller_id AND l.category = 'Electronics'
WHERE l.listing_id IS NULL
GROUP BY s.name;
#3 - Find the seller(s) who joined the earliest but have listings priced higher than the average price in their respective categories. Display the seller name, title of the listing, and price.

SELECT s.name, l.title, l.price
FROM Sellers s
JOIN Listings l ON s.seller_id = l.seller_id
WHERE s.join_date = (SELECT MIN(join_date) FROM Sellers)
AND l.price > (SELECT AVG(price) 
               FROM Listings l2 
               WHERE l2.category = l.category);
📝 More SQL Interview Questions
These questions are fair game across data analyst, data engineer, backend engineer, and database administrator interviews at companies such as Google, Amazon, Meta, Stripe, and many more👇

Basic SQL Interview Questions
*For all data roles

What is the difference between WHERE and HAVING clauses?
Explain the use of GROUP BY and ORDER BY in SQL.
How do you remove duplicate records from a table?
What is a primary key and a foreign key?
How do you use the LIKE operator?
What are the different types of SQL JOINs?
Explain the difference between DELETE, TRUNCATE, and DROP.
What is the purpose of the DISTINCT keyword?
How do you create and modify tables in SQL?
What are constraints in SQL?
Explain the difference between VARCHAR and CHAR data types.
How do you use aggregate functions (COUNT, SUM, AVG, etc.)?
What is the difference between UNION and UNION ALL?
How do you use aliases in SQL?
What is the purpose of the NULL value?
How do you use wildcards in SQL?
Explain the concept of SQL views.
What is the difference between INNER JOIN and OUTER JOIN?
How do you insert multiple rows into a table?
What is the purpose of the BETWEEN operator?
How do you use the IN operator?
Explain the use of TOP and LIMIT clauses.
What is the difference between DROP TABLE and DELETE FROM TABLE?
How do you use string functions in SQL?
What is the purpose of the COALESCE function?
Advanced SQL Interview Questions (20)
*For all data roles

What are window functions and how do they work?
Explain CTE (Common Table Expressions) and provide an example.
How do you handle transactions in SQL?
What are stored procedures, and why are they used?
Explain the ACID properties in database systems.
How do you use PIVOT and UNPIVOT operations?
What are triggers and how do they work?
Explain the concept of materialized views.
How do you implement error handling in SQL?
What are user-defined functions?
How do you use MERGE statements?
Explain recursive queries and their applications.
What are table partitioning strategies?
How do you implement row-level security?
Explain the use of CROSS APPLY and OUTER APPLY.
What are temporary tables and table variables?
How do you handle dynamic SQL?
Explain database isolation levels.
What are indexed views and when should they be used?
How do you implement hierarchical queries?
Performance and Optimization Questions
*For Data Engineers, sometimes in Senior DS roles.

How do indexes improve query performance?
What is a query execution plan and how do you analyze it?
Explain the difference between clustered and non-clustered indexes.
How do you avoid deadlocks in SQL?
What strategies can you use to optimize a slow-running query?
How do you identify and resolve blocking issues?
What are statistics in SQL Server and why are they important?
How do you optimize joins in large tables?
Explain index fragmentation and how to address it.
What are parameter sniffing issues and how do you resolve them?
How do you optimize stored procedure performance?
What is the impact of implicit conversions on performance?
How do you handle missing indexes?
Explain query plan caching and recompilation.
What are wait statistics and how do you analyze them?
Database Design and Modeling Questions
*For Data Engineers, sometimes in Senior DS roles.

Explain the concept of database normalization.
What are the different types of relationships in databases?
How do you design a database schema for a social media application?
What is denormalization, and when would you use it?
Explain the differences between OLTP and OLAP systems.
How do you implement many-to-many relationships?
What are the different normal forms (1NF through 5NF)?
How do you handle soft deletes in database design?
Explain the concept of data warehousing.
What are dimensional modeling concepts?
How do you implement versioning in database design?
What are the best practices for database naming conventions?
How do you handle temporal data in databases?
Explain the star schema and snowflake schema.
What are the considerations for designing a distributed database?
Two Advanced SQL Techniques That Can Drastically Improve Your Queries
Learn about Common Table Expression (CTE) and Window Functions

Jason Chong
Jun 30, 2023
10 min read
Share
Photo by Karina Szczurek on Unsplash
Photo by Karina Szczurek on Unsplash
SQL is the bread and butter of every data professional. It doesn’t matter if you are a data analyst, data scientist, or data engineer, you need to have a solid understanding of how to write clean and efficient SQL queries.

This is because behind any rigorous data analysis or any sophisticated machine learning model is the underlying data itself, and this data has to come from somewhere.

Hopefully after reading my introductory blog post to SQL, you would have already learned that SQL stands for Structured Query Language and it is a language that is used to retrieve data from a relational database.

In that blog post, we went over some fundamental SQL commands such as SELECT, FROM, and WHERE which should cover most of the basic queries you will come across when using SQL.

But what happens if those simple commands are simply not enough? What happens if the data you want demands a more robust approach to query?

Well, look no further because today, we will go over two new SQL techniques that you can add to your toolkit that will take your queries to the next level. These techniques are called Common Table Expression (CTE) and Window Functions.

To help us learn these techniques, we will be using an online SQL editor called DB Fiddle (set to SQLite v3.39) and the taxi trip duration dataset sourced from Google Cloud (NYC Open Data license).

Data Preparation
If you are not interested to learn how I prepared the dataset, feel free to skip past this section and paste the following code onto DB fiddle to generate the schema.

CREATE TABLE taxi (
  id varchar,
  vendor_id integer,
  pickup_datetime datetime,
  dropoff_datetime datetime,
  trip_seconds integer,
  distance float
);

INSERT INTO taxi 
VALUES
('id2875421', 2, '2016-03-14 17:24:55', '2016-03-14 17:32:30', 455, 0.93), 
('id2377394', 1, '2016-06-12 00:43:35', '2016-06-12 00:54:38', 663, 1.12), 
('id3858529', 2, '2016-01-19 11:35:24', '2016-01-19 12:10:48', 2124, 3.97), 
('id3504673', 2, '2016-04-06 19:32:31', '2016-04-06 19:39:40', 429, 0.92), 
('id2181028', 2, '2016-03-26 13:30:55', '2016-03-26 13:38:10', 435, 0.74), 
('id0801584', 2, '2016-01-30 22:01:40', '2016-01-30 22:09:03', 443, 0.68), 
('id1813257', 1, '2016-06-17 22:34:59', '2016-06-17 22:40:40', 341, 0.82), 
('id1324603', 2, '2016-05-21 07:54:58', '2016-05-21 08:20:49', 1551, 3.55), 
('id1301050', 1, '2016-05-27 23:12:23', '2016-05-27 23:16:38', 255, 0.82), 
('id0012891', 2, '2016-03-10 21:45:01', '2016-03-10 22:05:26', 1225, 3.19), 
('id1436371', 2, '2016-05-10 22:08:41', '2016-05-10 22:29:55', 1274, 2.37), 
('id1299289', 2, '2016-05-15 11:16:11', '2016-05-15 11:34:59', 1128, 2.35), 
('id1187965', 2, '2016-02-19 09:52:46', '2016-02-19 10:11:20', 1114, 1.16), 
('id0799785', 2, '2016-06-01 20:58:29', '2016-06-01 21:02:49', 260, 0.62), 
('id2900608', 2, '2016-05-27 00:43:36', '2016-05-27 01:07:10', 1414, 3.97), 
('id3319787', 1, '2016-05-16 15:29:02', '2016-05-16 15:32:33', 211, 0.41), 
('id3379579', 2, '2016-04-11 17:29:50', '2016-04-11 18:08:26', 2316, 2.13), 
('id1154431', 1, '2016-04-14 08:48:26', '2016-04-14 09:00:37', 731, 1.58), 
('id3552682', 1, '2016-06-27 09:55:13', '2016-06-27 10:17:10', 1317, 2.86), 
('id3390316', 2, '2016-06-05 13:47:23', '2016-06-05 13:51:34', 251, 0.81), 
('id2070428', 1, '2016-02-28 02:23:02', '2016-02-28 02:31:08', 486, 1.56), 
('id0809232', 2, '2016-04-01 12:12:25', '2016-04-01 12:23:17', 652, 1.07), 
('id2352683', 1, '2016-04-09 03:34:27', '2016-04-09 03:41:30', 423, 1.29), 
('id1603037', 1, '2016-06-25 10:36:26', '2016-06-25 10:55:49', 1163, 3.03), 
('id3321406', 2, '2016-06-03 08:15:05', '2016-06-03 08:56:30', 2485, 12.82), 
('id0129640', 2, '2016-02-14 13:27:56', '2016-02-14 13:49:19', 1283, 2.84), 
('id3587298', 1, '2016-02-27 21:56:01', '2016-02-27 22:14:51', 1130, 3.77), 
('id2104175', 1, '2016-06-20 23:07:16', '2016-06-20 23:18:50', 694, 2.33), 
('id3973319', 2, '2016-06-13 21:57:27', '2016-06-13 22:12:19', 892, 1.57), 
('id1410897', 1, '2016-03-23 14:10:39', '2016-03-23 14:49:30', 2331, 6.18);
After running SELECT * from taxi, you should get a resulting table that looks like this.

Image by Author.
Image by Author.
For the keen beans who are wondering how this table actually came about, I filtered the data to the first 30 rows and only kept the columns that you see above. As for the distance field, I computed the orthodromic distance between the pick-up and drop-off coordinates (latitude and longitude).

The orthodromic distance is the shortest distance between two points on a sphere, so this actually turns out to be an underestimate of the real distance travelled by the taxi. However, for the purpose of what we are doing today, we can ignore this for now.

The formula to calculate the orthodromic distance can be found here. Now, back to SQL.

Common Table Expression (CTE)
A common table expression (CTE) is a temporary table that you return within a query. You can think of it as a query within a query. They help to not only split your queries into more readable chunks but you can write new queries based on a CTE that has been defined.

To demonstrate this, suppose we want to analyze taxi trips split by the hour of the day and filter to trips that took place between the months of January and March 2016.

SELECT CAST(STRFTIME('%H', pickup_datetime) AS INT) AS hour_of_day, 
       trip_seconds,
       distance
FROM taxi
WHERE pickup_datetime > '2016-01-01'
  AND pickup_datetime < '2016-04-01'
ORDER BY hour_of_day;
Image by Author.
Image by Author.
Straightforward enough; let’s take this one step further.

Suppose now we want to compute the number of trips and the average speed for each of these hours. This is where we can utilize a CTE to first obtain a temporary table like the one we observe above, followed by a subsequent query to count the number of trips and compute the average speed group by hour of the day.

The way you would define a CTE is by using the WITH and AS statements.

WITH relevantrides AS
(
SELECT CAST(STRFTIME('%H', pickup_datetime) AS INT) AS hour_of_day, 
       trip_seconds,
       distance
FROM taxi
WHERE pickup_datetime > '2016-01-01'
  AND pickup_datetime < '2016-04-01'
ORDER BY hour_of_day
)
SELECT hour_of_day,
       COUNT(1) AS num_trips,
       ROUND(3600 * SUM(distance) / SUM(trip_seconds), 2) AS avg_speed
FROM relevantrides
GROUP BY hour_of_day
ORDER BY hour_of_day;
Image by Author.
Image by Author.
An alternative to using a CTE is simply wrapping the temporary table within a FROM statement (see code below), which would give you the same result. However, this is not advisable from a code readability standpoint. Moreover, imagine if we wanted to create more than just one temporary table.

SELECT hour_of_day,
       COUNT(1) AS num_trips,
       ROUND(3600 * SUM(distance) / SUM(trip_seconds), 2) AS avg_speed
FROM (
 SELECT CAST(STRFTIME('%H', pickup_datetime) AS INT) AS hour_of_day, 
           trip_seconds,
           distance
 FROM taxi
 WHERE pickup_datetime > '2016-01-01'
    AND pickup_datetime < '2016-04-01'
 ORDER BY hour_of_day
)
GROUP BY hour_of_day
ORDER BY hour_of_day;
Bonus: an interesting insight we can pull from this exercise is that taxis tend to move slower (lower average speed) during peak hours most likely due to heavier traffic as people travel to and back from work.

Window Functions
Window functions perform aggregate operations on groups of rows but they produce a result for each row in the original table.

To fully understand how window functions work, it is helpful to first do a quick recap of aggregation via GROUP BY.

Let’s say we wish to compute a list of summary statistics by month using the taxi dataset.

SELECT CAST(STRFTIME('%m', pickup_datetime) AS INT) AS month,
       COUNT(1) AS trip_count,
       ROUND(SUM(distance), 3) AS total_distance,
       ROUND(AVG(distance), 3) AS avg_distance,
       MIN(distance) AS min_distance,
       MAX(distance) AS max_distance
FROM taxi
GROUP BY month;
Image by Author.
Image by Author.
In the example above, we have computed the count, sum, average, minimum, and maximum distance travelled for each individual month in the dataset. Notice how our original taxi table with 30 rows has now been collapsed into six rows, one for each individual month.

So, what is actually happening behind the scenes? Firstly, SQL grouped all 30 rows in the original table based on their months. It then applied the relevant calculations based on the values in these individual groups.

Let’s take January as an example. There are two trips in the dataset that took place in the month of January, with distance travelled of 3.97 and 0.68 respectively. SQL then calculated the count, sum, average, minimum, and maximum based on these two values. The process then repeats for the other months until eventually we get an output that looks like the one above.

Now, hold this thought as we begin to explore how window functions work. There are three broad categories of window functions: aggregate functions, ranking functions, and navigation functions. We will look at examples of each one of them.

Aggregate functions
We have already seen aggregate functions at play in our previous example. Aggregate functions include functions like count, sum, average, minimum, and, maximum.

But where window functions differ from GROUP BY is the number of rows in the final output. Specifically, we saw that after aggregating by months, our output table is left with only six rows (one row for each distinct month).

Window functions, on the other hand, will not summarise the table by the aggregate field, but simply output the result in a new column for each row. The number of rows in the output table will not change. In other words, the output table will always have the same number of rows as the original table.

The syntax to perform a window function is OVER(PARTITION BY ...). You can think of this as the GROUP BY statement in our previous example.

Let’s see how this works in practice.

WITH aggregate AS
(
SELECT id,
       pickup_datetime,
       CAST(STRFTIME('%m', pickup_datetime) AS INT) AS month, 
       distance
FROM taxi
)
SELECT *,
       COUNT(1) OVER(PARTITION BY month) AS trip_count,
       ROUND(SUM(distance) OVER(PARTITION BY month), 3) AS total_month_distance,
       ROUND(AVG(distance) OVER(PARTITION BY month), 3) AS avg_month_distance,
       MIN(distance) OVER(PARTITION BY month) AS min_month_distance,
       MAX(distance) OVER(PARTITION BY month) AS max_month_distance
FROM aggregate;
Image by Author.
Image by Author.
Here, we want the same output as last time, but rather than collapsing the table, we want the output displayed as individual rows in a new column.

You would notice the values after the aggregation did not change but rather, they are simply displayed as repeated rows in the table. For example, the first two rows (January) have the same values for trip count, total month distance, average month distance, minimum month distance, and maximum month distance as before. The same applies to the other months.

In case you are wondering how window functions are useful, it helps us compare each row value with the aggregated value. In this instance, we can easily compare the distance travelled in each row with the monthly average, minimum and maximum, and so on.

Ranking functions
Another type of window function is the ranking function. As the name suggests, this ranks a group of rows based on an aggregate field.

WITH ranking AS
(
SELECT id,
       pickup_datetime,
       CAST(STRFTIME('%m', pickup_datetime) AS INT) AS month, 
       distance
FROM taxi
)
SELECT *,
       RANK() OVER(ORDER BY distance DESC) AS overall_rank,
       RANK() OVER(PARTITION BY month ORDER BY distance DESC) AS month_rank
FROM ranking
ORDER BY pickup_datetime;
Image by Author.
Image by Author.
In the example above, we have two ranking columns: one for the overall rank (from 1–30) and one for the monthly rank, both in descending order.

To specify the order when ranking, you will need to use ORDER BY within the OVER statement.

The way you would interpret the results for the first row is that it has the third-longest distance travelled in the whole dataset and the longest distance travelled for the month of January.

Navigation functions
Last but not least, we have navigation functions.

A navigation function assigns a value based on the value in a different row than the current row. Some common navigation functions include FIRST_VALUE, LAST_VALUE, LEAD, and LAG.

SELECT id,
       pickup_datetime,
       distance,
       LAG(distance) OVER(ORDER BY pickup_datetime) AS prev_distance,
       LEAD(distance) OVER(ORDER BY pickup_datetime) AS next_distance
FROM taxi
ORDER BY pickup_datetime;
Lag returns the value of the preceding row. Image by Author.
Lag returns the value of the preceding row. Image by Author.
Lead returns the value of the subsequent row. Image by Author.
Lead returns the value of the subsequent row. Image by Author.
In the example above, we used the LAG function to return the value of the preceding row and the LEAD function to return the value of the subsequent row. Notice how the first row of the lag column is null whereas the last row of the lead column is null.

SELECT id,
       pickup_datetime,
       distance,
       LAG(distance, 2) OVER(ORDER BY pickup_datetime) AS prev_distance,
       LEAD(distance, 2) OVER(ORDER BY pickup_datetime) AS next_distance
FROM taxi
ORDER BY pickup_datetime;
The first two rows are null when lag offset is set to 2. Image by Author.
The first two rows are null when lag offset is set to 2. Image by Author.
The last two rows are null when lead offset is set to 2. Image by Author.
The last two rows are null when lead offset is set to 2. Image by Author.
On a similar note, we can also offset the LEAD and LAG functions, i.e. to start from a particular index or position. When the offset is set to two, you can see that the first two rows of the lag column are null and the last two rows of the lead column are null.

I hope this blog post has helped introduce you to the concepts of Common Table Expression (CTE) and Window Functions.

To summarise, a CTE is a temporary table or a query within a query. They are used to split queries into more readable chunks and you can write new queries against a CTE that has been defined. Window functions, on the other hand, perform aggregation on groups of rows and return the results for each row in the original table.

If you wish to improve on these techniques, I highly encourage you to start implementing them in your SQL queries either at work, solving interview problems, or just playing around with random datasets. Practice makes perfect, am I right?

SQL SELECT

Basic statements and clauses
SQL LIMIT

Basic statements and clauses
SQL WHERE

Basic statements and clauses
SQL ORDER BY

Basic statements and clauses
SQL CASE

Basic statements and clauses
SQL GROUP BY

Basic statements and clauses
SQL HAVING

Basic statements and clauses
SQL IF

Basic statements and clauses
SQL WITH

Basic statements and clauses
SQL Aliases

Basic statements and clauses
SQL Comments

Basic statements and clauses
SQL Equal To

Comparison operators
SQL Not Equal To

Comparison operators
SQL Greater Than

Comparison operators
SQL Less Than

Comparison operators
SQL Greater Than or Equal To

Comparison operators
SQL Less Than or Equal To

Comparison operators
SQL COUNT

Aggregate functions
SQL AVG

Aggregate functions
SQL SUM

Aggregate functions
SQL DISTINCT

Aggregate functions
SQL MIN

Aggregate functions
SQL MAX

Aggregate functions
SQL ABS

Aggregate functions
SQL ROUND

Aggregate functions
SQL POWER

Aggregate functions
SQL SQRT

Aggregate functions
SQL BIN

Aggregate functions
SQL LIKE

Logical operators
SQL IN

Logical operators
SQL BETWEEN

Logical operators
SQL IS NULL

Logical operators
SQL IS NOT NULL

Logical operators
SQL AND

Logical operators
SQL OR

Logical operators
SQL NOT

Logical operators
SQL NVL

Logical operators
SQL COALESCE

Logical operators
SQL EXISTS

Logical operators
SQL DECODE

Logical operators
SQL NULLIF

Logical operators
SQL LNNVL

Logical operators
SQL NANVL

Logical operators
SQL INTERVAL

Logical operators
SQL % Wildcard

Wildcard operators
SQL _ Wildcard

Wildcard operators
SQL [] Wildcard

Wildcard operators
SQL - Wildcard

Wildcard operators
SQL JOIN

Joins
SQL LEFT JOIN

Joins
SQL RIGHT JOIN

Joins
SQL OUTER JOIN

Joins
SQL FULL OUTER JOIN

Joins
SQL FULL JOIN

Joins
SQL SELF JOIN

Joins
SQL UNION

Joins
SQL JOIN ON

Joins
SQL JOIN WHERE

Joins
SQL Common Table Expression (CTE)

Database functions
SQL Stored Procedures

Database functions
SQL Create Database

Database functions
SQL Drop Database

Database functions
SQL Backup Database

Database functions
SQL Create Table

Database functions
SQL Drop Table

Database functions
SQL Alter Table

Database functions
SQL Create Index

Database functions
SQL Auto Increment

Database functions
SQL Create View

Database functions
SQL NOW

Date functions
SQL CURDATE

Date functions
SQL CURTIME

Date functions
SQL DATEPART

Date functions
SQL EXTRACT

Date functions
SQL DATE_ADD

Date functions
SQL DATEDIFF

Date functions
SQL DATE_FORMAT

Date functions
SQL UNIQUE

Constraints
SQL PRIMARY KEY

Constraints
SQL FOREIGN KEY

Constraints
SQL CHECK

Constraints
SQL DEFAULT

Constraints
SQL NOT NULL

Constraints
SQL Subqueries

Advanced
SQL Window Functions

Advanced
SQL Pivoting Data

Advanced
SQL ROW NUMBER

Advanced
SQL Row Number Over Partition By

Advanced
SQL Relative Dates

Advanced
SQL SELECT INTO

Advanced
SQL LAST_INSERT_ID

Advanced
SQL SESSION_USER

Advanced
SQL SYSTEM_USER

Advanced
SQL USER

Advanced
SQL String Types

Data formatting
SQL Numeric Value Types

Data formatting
SQL Date Formatting

Data formatting
SQL Conversion Functions

Data formatting
User Churn Prediction
Modern data warehousing and Machine Learning

💡Mike Shakhomirov
Dec 23, 2023
13 min read
Share
Photo by Martin Adams on Unsplash
Photo by Martin Adams on Unsplash
No doubt, user retention is a crucial performance metric for many companies and online apps. We will discuss how we can use built-in data warehouse machine learning capabilities to run propensity models on user behaviour data to determine the likelihood of user churn. In this story, I would like to focus on dataset preparation and model training using standard SQL. Modern data warehouses allow this. Indeed, retention is an important business metric that helps understand user behaviour’s mechanics. It provides a high-level overview of how successful our Application is in terms of retaining users by answering one simple question: Is our App good enough at retaining users? It is a well-known fact that it’s cheaper to retain an existing user than to acquire a new one.

In one of my previous articles, I wrote about modern data warehousing [1].

Modern Data Warehousing

Modern DWH has a lot of useful features and components which differentiate them from other data platform types [2].

ML model support seems to be the foundational DWH component when dealing with big data.

In this story, I will use Binary logistic regression, one of the fastest models to train. I will demonstrate how we can use it to predict user propensity to churn. Indeed, We don’t need to know every machine-learning model.

We can’t compete with cloud service providers such as Amazon ang Google in machine learning and data science but we need to know how to use it.

I previously wrote about it in my article here [3]:

How to Become a Data Engineer

In this tutorial, we will learn how to transform raw event data to create a training dataset for our ML model. We will use it to generate predictions for our users. I will use BigQuery ML as an example but there are a lot of other DWH tools that support this feature.

BigQuery ML democratizes machine learning operations and model training, so now data analysts or software engineers can train models with ease. All we need is a good knowledge of SQL and an understanding of user retention dataset logic [4].

The data preparation process is straightforward and should be easy to follow.

We will use standard SQL for this.

Very often, it helps to reveal some useful facts about the data and user base. Analyzing user behaviour and performing exploratory data analysis helps to detect important user behaviour funnels (open funnels)that can later be used for further feature engineering and to improve the model. As an example, we can use one of those free user behaviour datasets with user event data kindly provided by Google. The typical mobile application has two builds – Android and IOS which generate a constant flow of event data. Google Analytics 4 is a good example and this data can be used to measure traffic and engagement levels in our application.

Every model requires a dataset

We would want to create one. We will need to:

Perform exploratory data analysis (EDA) on BigQuery export dataset from Firebase (Google Analytics 4).
Split the dataset into two parts for training and tests with categorical and behavioural attributes for the Machine Learning (ML) model.
Train and Evaluate Machine learning models using BigQuery ML
Make predictions using the BigQuery ML models
Use model insights in practice
In Firebase or Google Analytics 4 all user-behaviour data is being stored as events, which means that each row within a table corresponds to a single event with additional parameters and properties.

Dataset prep and model training diag
Dataset prep and model training diag
We will use a publicly available Google Analytics dataset [5], it has data for a mobile game app called "Flood It!" (Android, iOS) and there are no associated costs to that. However, Google Cloud Platform (GCP) services are billable and might incur costs.

Dataset exaple. Image by author.
Dataset exaple. Image by author.
This dataset contains 5.7M events from over 15k users. Open the link above and click Preview.

It won’t cost anything to run a Preview on any table.

Copy this dataset by running this in the command line below:

gcloud init # Authenticate

# Use bq mk command with --dataset flag to create a new dataset:
bq mk
  --dataset 
  --location=US 
  PROJECT:DATASET

# # Example:
# bq mk
#   --dataset 
#   --location=US 
#   bq-shakhomirov:analytics_153293282

# Use bq mk with --transfer_config flag to copy the dataset with public data:
bq mk
--transfer_config
--project_id=PROJECT  # Replace with your project_id
--data_source=cross_region_copy
--target_dataset=analytics_153293282
--display_name=analytics_153293282
--params='{"source_dataset_id":"analytics_153293282","source_project_id":"firebase-public-project","overwrite_destination_table":"true"}'
Explore the data
Analyzing user behaviour and performing exploratory data analysis [6] helps to understand the user journey better.

Exploratory Data Analysis with BigQuery SQL? Easy!

Run this query to check the dataset structure:

SELECT *
FROM `firebase-public-project.analytics_153293282.events_*`
TABLESAMPLE SYSTEM (1 PERCENT)
;
The limitation of this dataset is that user data doesn’t have an actual user_id that is often assigned after registration. So working with this dataset, we work only with device ids, which is not great in an ideal world as those are constantly updated after re-install or new App version rollout.

Firstly, we would want to pre-process raw event data and create a new dataset that has the right structure for the ML model.

We will label each user as retained (0) if they were active during the last 30 days.
We will use the user_pseudo_id (device_id) that is used by GA4. Firebase by default to identify each user’s device.
Based on our EDA we will exclude some obvious outliers, i.e. spammers (too many events), and bounced users (those who spent less than 10 minutes in the App after the install event).
We will add a few columns with categorical features extracted from raw event data, i.e. platform, device_type, country, etc.
We will finally add user behaviour and activity totals for each user within a certain period of time, i.e. user_engagement, spend_virtual_currency, etc.
These calculations will be used as ML model features, but in a nutshell, they are open event funnels, i.e. user’s steps while using the App.

Our not churned (returning user) definition is that the user was active, and engaging with the App within the last 30 days from the current date. You can try to tweak this parameter. You can also try to predict something, i.e. the likelihood of spending money in the App (in_app_purchase), etc.

Calculating bounced and churned users
If we’re using user_pseudo_id in our model, then we would calculate bounced = 1 if last_touch within 10 min after registration, else 0:

...

 IF (user_last_engagement <= TIMESTAMP_ADD(user_first_engagement, 
      INTERVAL 10 MINUTE),
    1,
    0 ) AS bounced

, IF(last_seen_dt < DATE_SUB(current_date(),INTERVAL 30 DAY) , 1, 0)     AS churned
...
Using current_date() to identify churned users would be ideal if we have a dynamic dataset that is being updated daily. However, we are working with some sample data and therefore, we should use the last known date in that data, i.e. we imagine we collected the data on that particular day (20181003).

Ideally, we would want to track only those users who have passed the registration in the App after they installed it. In that case, we wouldn’t need to calculate bounced, and we will count churned = 1 if last_seen_dt was before _24 hr/ 3 day/ 30 day_s from current_timestamp(), else 0:

...
  , IF( last_seen_ts < timestamp_sub(
        current_timestamp(), interval 30*24 hour) , 1, 0) AS churned
...
Adding behavioural quantitative and categorical features
This is what we will use to build our model. Categorical features can represent some non-quantifiable demographic values, i.e. male/female, etc. Quantitative is what we can measure and count.

A combination of both types of these features helps to create a more predictive model. There is a lot of information in the Firebase/GA4 dataset extract which might be useful for our model, i.e. app_info, device, event_params, geo etc.

Many events are being collected automatically by Firebase, but keep in mind that there is a way to set custom events and properties. This is important for us as mobile developers can integrate a custom event, i.e. user tag. For our example App, it would indicate that a user might have certain in-app privileges (Premium, Influencer, Group Admin, Moderator, etc.).

Having said that, we would want to use the following categorical features:

platform (IOS/Android)
geo.country
device.operating_system
device.language
It is important to say that users might have different values in these fields, i.e. different devices, changed languages, and VPN settings might affect those. So we would need only the first engagement event and the device setting they used on install or registration.

Adding row_number function will help with that:

...
ROW_NUMBER() OVER (PARTITION BY user_pseudo_id ORDER BY event_timestamp DESC) AS row_number
...
Then, to predict user churn, we would want to count the number of events a user encountered within 24 hours/3 days/ 7 days after registration/installation:

In our case, we would want to collect and count these events:

user_engagement
level_start_quickplay
level_end_quickplay
level_complete_quickplay
level_reset_quickplay
post_score
spend_virtual_currency
ad_reward
challenge_a_friend
completed_5_levels
use_extra_steps
For other Apps it might be useful to collect any other events that would describe user behaviour, i.e.:

message_sent
chat_open
spend_virtual_currency
account_topup
set_avatar
group_join
broadcast_listened
achievement_unlocked
reputation_update
Calculate model features
Let’s create a dedicated dataset for our future model and its data: sample_churn_model.

The SQL query in ./sql/churn.sql below demonstrates how these user metrics can be calculated. We can find this file in the code widget. To create this dataset run this in the command line:

bq query --nouse_legacy_sql < sql/churn.sql
./sql/churn.sql:
-- ./sql/churn.sql
create schema if not exists sample_churn_model
  OPTIONS (
    description = 'sample_churn_model',
    location = 'US'
)
;

create or replace table sample_churn_model.churn as
with first_engagement_data as (
select distinct

    user_pseudo_id
  , device.language
  , geo.country
  , platform
  , ROW_NUMBER() OVER (PARTITION BY user_pseudo_id ORDER BY event_timestamp DESC) AS row_num

from `firebase-public-project.analytics_153293282.events_*`

where
-- _TABLE_SUFFIX =  FORMAT_DATE('%Y%m%d', DATE_SUB(current_date(), INTERVAl 1 DAY))
-- and 
lower(event_name) = 'user_engagement'
)

, first_engagement as (
    select * except (row_num)
    from first_engagement_data
    where row_num = 1 
)

, first_last_seen as (
    select
        user_pseudo_id
      , min(event_timestamp) as first_seen_ts
      , max(event_timestamp) as last_seen_ts
    from
      `firebase-public-project.analytics_153293282.events_*`
    where event_name = 'user_engagement'
    group by
      user_pseudo_id
)

, event_aggregates as (
  select
     event_date                                         
    ,user_pseudo_id
    ,lower(event_name)                   as event_name  
    ,count(distinct event_timestamp)     as event_cnt   
  from 
    `firebase-public-project.analytics_153293282.events_*`
--   where
--     _TABLE_SUFFIX =  FORMAT_DATE('%Y%m%d', DATE_SUB(current_date(), INTERVAl 1 DAY))
  group by
     event_date
    ,user_pseudo_id
    ,event_name
)

, daily_data as (
select
     a.user_pseudo_id   
    ,a.event_date       
    ,a.event_name
    ,a.event_cnt
    ,f.first_seen_ts    
    ,f.last_seen_ts     
    ,e.language         
    ,e.country          
    ,e.platform         

    , date_diff(
          parse_date('%Y%m%d', a.event_date)
        , cast(timestamp_micros(first_seen_ts) as date)
        , day
    )  as day_number

    ,if( cast(timestamp_micros(f.last_seen_ts) as date) < 
                date_sub(parse_date('%Y%m%d', '20181003'), interval 30 day) 
        , 1
        , 0)     as churned -- wasn't active last 30 days

    ,if (timestamp_micros(last_seen_ts) <= timestamp_add(timestamp_micros(first_seen_ts), 
                interval 10 minute)
        ,1
        ,0 )     as bounced

from event_aggregates a
left join first_last_seen f on f.user_pseudo_id = a.user_pseudo_id 
left join first_engagement e on e.user_pseudo_id = a.user_pseudo_id
)

-- Final dataset for churn model training:
select
     user_pseudo_id   
    ,first_seen_ts    
    ,last_seen_ts     
    ,language         
    ,country          
    ,platform         
    ,churned          
    ,sum(if(day_number in (0) and event_name = 'user_engagement', event_cnt, 0))            as user_engagement_cnt          
    ,sum(if(day_number in (0) and event_name = 'level_start_quickplay', event_cnt, 0))      as level_start_quickplay_cnt    
    ,sum(if(day_number in (0) and event_name = 'level_end_quickplay', event_cnt, 0))        as level_end_quickplay_cnt      
    ,sum(if(day_number in (0) and event_name = 'level_complete_quickplay', event_cnt, 0))   as level_complete_quickplay_cnt 
    ,sum(if(day_number in (0) and event_name = 'level_reset_quickplay', event_cnt, 0))      as level_reset_quickplay_cnt    
    ,sum(if(day_number in (0) and event_name = 'post_score', event_cnt, 0))                 as post_score_cnt               
    ,sum(if(day_number in (0) and event_name = 'spend_virtual_currency', event_cnt, 0))     as spend_virtual_currency_cnt   
    ,sum(if(day_number in (0) and event_name = 'ad_reward', event_cnt, 0))                  as ad_reward_cnt                
    ,sum(if(day_number in (0) and event_name = 'challenge_a_friend', event_cnt, 0))         as challenge_a_friend_cnt       
    ,sum(if(day_number in (0) and event_name = 'completed_5_levels', event_cnt, 0))         as completed_5_levels_cnt       
    ,sum(if(day_number in (0) and event_name = 'use_extra_steps', event_cnt, 0))            as use_extra_steps_cnt          
from daily_data
-- where bounced = 0
group by
     user_pseudo_id   
    ,first_seen_ts    
    ,last_seen_ts     
    ,language         
    ,country          
    ,platform         
    ,churned          
Let’s see how many churned users we have:

From our dataset, we found that 4030 churned and were inactive during the last 30 days:

SELECT
    churned
    ,COUNT(churned) as count_users

FROM
    sample_churn_model.churn
GROUP BY 1
;
Churned users. Image by author.
Churned users. Image by author.
Model training and classification
There are different model types [7] available in BigQuery ML at the moment:

BOOSTED_TREE_CLASSIFIER
Neural Networks
AutoML Tables
Logistic Regression
Logistic regression might be a good choice to start with as it is the one that can be trained relatively fast. Other types of models might provide better performance but also require more time to train, i.e. Deep Neural Networks.

Each of these models will output a probability score (propensity) between 0 and 1.0 of how likely the model prediction is based on the training data.

Consider the file ./sql/churn_model.sql. It will create and train the model if we run it.

Run this query to evaluate the model:

bq query --nouse_legacy_sql 
'SELECT * FROM ML.EVALUATE(MODEL sample_churn_model.churn_model);'
You will see model performance metrics. Analyzing them might help to choose between different models.

For instance, with regard to model accuracy, anything that is above 70% is considered to be a strong model performance.

Our model has a recall of 0.559 – in other words, it correctly identifies 56% of all churned users.

Model performance metrics. Image by author.
Model performance metrics. Image by author.
We can use a confusion matrix to see how well our model predicted the labels, compared to the actual labels. If we run this SQL below it will generate a confusion matrix.

SELECT
  expected_label,
  _0 AS predicted_0,
  _1 AS predicted_1
FROM
  ML.CONFUSION_MATRIX(MODEL sample_churn_model.churn_model)
This can be interpreted as a comparison of false positive and false negative predictions.

Confusion matrix. Image by author.
Confusion matrix. Image by author.
Using predictions
For our classification model, the most important metric is the user propensity to churn and be inactive. In other words, it is a probability and the closer this probability is to 1 the more likely this user will not return to the App according to the model’s prediction:

SELECT
  user_pseudo_id,
  churned,
  predicted_churned,
  predicted_churned_probs[OFFSET(0)].prob as probability_churned
FROM
  ML.PREDICT(MODEL sample_churn_model.churn_model,
  (SELECT * FROM sample_churn_model.churn)) #can be replaced with a proper test dataset
order by 3 desc
;
Predictions. Image by author.
Predictions. Image by author.
In real-life scenario we would want to create a dataset with predictions that is being updated daily.

Firstly, we will need Firebase/Analytics users who registered yesterday. We will need to schedule our sample_churn_model.churn dataset and incrementally add new users from yesterday, i.e.:

insert into sample_churn_model.churn (...)
select
  ... -- aggregate model features for new users.
from 
  `firebase-public-project.analytics_153293282.events_*`
where
  _TABLE_SUFFIX =  FORMAT_DATE('%Y%m%d', DATE_SUB(current_date(), INTERVAl 1 DAY))
...
Then we would want to generate prediction for them and insert into our new predictions dataset:

SELECT
  user_pseudo_id,
  churned,
  predicted_churned,
  predicted_churned_probs[OFFSET(0)].prob as probability_churned
FROM
  ML.PREDICT(MODEL sample_churn_model.churn_model,
  (SELECT * FROM sample_churn_model.churn 
WHERE date(first_seen_ts) = date_sub(current_date(), interval 1 day)))
With this model we can better understand user behaviour through patterns, and, of course, we would want to do something with this knowledge.

There are a number of ways to use prediction data (activate). For instance, we can read data directly from our DWH solution using SDK and client libraries. For example, we might want to create a data service that collects predictions every day for new users and then sends that data somewhere else, i.e. retargeting service. Yes, we would probably want to retarget users who are likely to churn or users who are likely to stay in the App too.

Conclusion
It is true that acting on machine learning (ML) model data to retain users proved itself extremely useful and might help to gain a competitive advantage in the fast-changing market environment. That is why it is important to be able to forecast user engagement to predict if users are about to leave. We don’t need to be data scientists to create and train ML models. All we need – is a modern DWH, good knowledge of SQL and a good understanding of user retention logic. Modern data warehouses evolved to that state where we have the luxury of all known ML models and they are already set and ready to be created using standard SQL dialect. With predicted retention numbers, we can create and edit audiences. Using ML capabilities in modern DWH we can tailor user experience by targeting our identified users with relevant information, useful offers and promos. Modern DWH solutions democratize machine learning ops and model training. It is an extremely useful feature for data engineers as all these processes can be easily automated, scheduled and triggered depending on the use case scenario.
We can't lie - Data Science Interviews are TOUGH. Especially tricky - probability and statistics questions asked by top tech companies & hedge funds during the Data Science, Data Analyst, and the Quant Trading Interview process.

That's why we put together 40 real probability & statistics data science interview questions asked by companies like Facebook, Amazon, Two Sigma, & Bloomberg. We have solutions to all 40 problems, and to 161 other data interview problems on SQL, Machine Learning, and Product/Business Sense  in our book, Ace The Data Science Interview. You can also practice some of these same exact questions on DataLemur's statistics interview questions section.

DataLemur has hundreds of real Statistics and Probability Interview questions, sourced from real Data Science and Data Analyst interviews at companies like Facebook and Google.  
So, without further ado, here are:
the probability & stat concepts to review before your DS interview
20 probability questions asked by top tech-companies & Wall Street
20 statistics questions asked by FANG & Hedge Funds
solutions to 5 of the probability questions
solutions to 5 of the statistics questions
links to more data science interview resources
Probability & Statistics Concepts To Review Before Your Data Science Interview
Because probability & statistics is foundational to the field of Data Science, before the interview you should review:

Probability Basics & Random Variables
Probability Distributions
Hypothesis Testing
Regression Analysis
In case these statistical concepts sound alien to you, check out some of our favorite Statistics Books for Data Analysts to get a gentle refresher.

Probability Basics and Random Variables
The beginnings of probability start with thinking about sample spaces, basic counting and combinatorial principles. Although it is not necessary to know all of the ins-and-outs of combinatorics, it is helpful to understand the basics for simplifying problems. One classic example here is the “stars and bars” counting method.

The other core topic to study is random variables. Knowing concepts related to expectation, variance, covariance, along with the basic probability distributions is crucial.

Probability Distributions
For modeling random variables, knowing the basics of various probability distributions is essential. Understanding both discrete and continuous examples, combined with expectations and variances, is crucial. The most common distributions discussed in interviews are the Uniform and Normal but there are plenty of other well-known distributions for particular use cases (Poisson, Binomial, Geometric).

Most of the time knowing the basics and their applications should suffice. For example, which distribution would flipping a coin be under? What about waiting for an event? It never hurts being able to do the derivations for expectation, variance, or other higher moments.

Hypothesis Testing
Hypothesis testing is the backbone behind statistical inference and can be broken down into a couple of topics. The first is the Central Limit Theorem, which plays an important role in studying large samples of data. Other core elements of hypothesis testing: sampling distributions, p-values, confidence intervals, type I and II errors. Lastly, it is worth looking at various tests involving proportions, and other hypothesis tests.

Most of these concepts play a crucial role in A/B testing, which is a commonly asked topic during interviews at consumer-tech companies like Facebook, Amazon, and Uber. It's useful to not only understand the technical details but also conceptually how A/B testing operates, what the assumptions are, possible pitfalls, and applications to real-life products.

Modeling
Modeling relies on a strong understanding of probability distributions and hypothesis testing. Since it is a broad term, we will refer to modeling as the areas which have a strong statistical intersection with Machine Learning. This includes topics such as: linear regression, maximum likelihood estimation, & bayesian statistics. For interviews focused on modeling and machine learning, knowing these topics is essential.

‍

20 Probability Interview Problems Asked By Top-Tech Companies & Wall Street
[Facebook - Easy] [Coin Fairness Test on DataLemur] There is a fair coin (one side heads, one side tails) and an unfair coin (both sides tails). You pick one at random, flip it 5 times, and observe that it comes up as tails all five times. What is the chance that you are flipping the unfair coin?
[Lyft - Easy] You and your friend are playing a game. The two of you will continue to toss a coin until the sequence HH or TH shows up. If HH shows up first, you win. If TH shows up first, your friend wins. What is the probability of you winning?
[Google - Easy] What is the probability that a seven-game series goes to 7 games?
[Facebook - Easy] Facebook has a content team that labels pieces of content on the platform as spam or not spam. 90% of them are diligent raters and will label 20% of the content as spam and 80% as non-spam. The remaining 10% are non-diligent raters and will label 0% of the content as spam and 100% as non-spam. Assume the pieces of content are labeled independently from one another, for every rater. Given that a rater has labeled 4 pieces of content as good, what is the probability that they are a diligent rater?
[Bloomberg - Easy] Say you draw a circle and choose two chords at random. What is the probability that those chords will intersect?
[Amazon - Easy] 1/1000 people have a particular disease, and there is a test that is 98% correct if you have the disease. If you don’t have the disease, there is a 1% error rate. If someone tests positive, what are the odds they have the disease?
[Facebook - Easy] There are 50 cards of 5 different colors. Each color has cards numbered between 1 to 10. You pick 2 cards at random. What is the probability that they are not of same color and also not of same number?
[Tesla - Easy] A fair six-sided die is rolled twice. What is the probability of getting 1 on the first roll and not getting 6 on the second roll?
[Facebook - Easy] What is the expected number of rolls needed to see all 6 sides of a fair die?
[Microsoft - Easy] Three friends in Seattle each told you it’s rainy, and each person has a 1/3 probability of lying. What is the probability that Seattle is rainy? Assume the probability of rain on any given day in Seattle is 0.25.
[Uber - Easy] Say you roll three dice, one by one. What is the probability that you obtain 3 numbers in a strictly increasing order?
[Bloomberg - Medium] Three ants are sitting at the corners of an equilateral triangle. Each ant randomly picks a direction and starts moving along the edge of the triangle. What is the probability that none of the ants collide? Now, what if it is k ants on all k corners of an equilateral polygon?
[Two Sigma - Medium] What is the expected number of coin flips needed to get two consecutive heads?
[Amazon - Medium] How many cards would you expect to draw from a standard deck before seeing the first ace?
[Robinhood - Medium] A and B are playing a game where A has n+1 coins, B has n coins, and they each flip all of their coins. What is the probability that A will have more heads than B?
[Airbnb - Medium] Say you are given an unfair coin, with an unknown bias towards heads or tails. How can you generate fair odds using this coin?
[Quora - Medium] Say you have N i.i.d. draws of a normal distribution with parameters μ and σ. What is the probability that k of those draws are larger than some value Y?
[Spotify - Hard] A fair die is rolled n times. What is the probability that the largest number rolled is r, for each r in 1..6?
[Snapchat - Hard] There are two groups of n users, A and B, and each user in A is friends with those in B and vice versa. Each user in A will randomly choose a user in B as their best friend and each user in B will randomly choose a user in A as their best friend. If two people have chosen each other, they are mutual best friends. What is the probability that there will be no mutual best friendships?
[Tesla - Hard] Suppose there is a new vehicle launch upcoming. Initial data suggests that any given day there is either a malfunction with some part of the vehicle or possibility of a crash, with probability p which then requires a replacement. Additionally, each vehicle that has been around for n days must be replaced. What is the long-term frequency of vehicle replacements?
20 Statistics Problems Asked By FAANG & Hedge Funds
[Facebook - Easy] How would you explain a confidence interval to a non-technical audience?
[Two Sigma - Easy] Say you are running a multiple linear regression and believe there are several predictors that are correlated. How will the results of the regression be affected if they are indeed correlated? How would you deal with this problem?
[Uber - Easy] Describe p-values in layman’s terms.
[Facebook - Easy] How would you build and test a metric to compare two user’s ranked lists of movie/tv show preferences?
[Microsoft - Easy] Explain the statistical background behind power.
[Twitter - Easy] Describe A/B testing. What are some common pitfalls?
[Google - Medium] How would you derive a confidence interval from a series of coin tosses?
[Stripe - Medium] Say you model the lifetime for a set of customers using an exponential distribution with parameter λ, and you have the lifetime history (in months) of n customers. What is your best guess for λ?
[Lyft - Medium] Derive the mean and variance of the uniform distribution U(a, b).
[Google - Medium] Say we have X ~ Uniform(0, 1) and Y ~ Uniform(0, 1). What is the expected value of the minimum of X and Y?
[Spotify - Medium] You sample from a uniform distribution [0, d] n times. What is your best estimate of d?
[Quora - Medium] You are drawing from a normally distributed random variable X ~ N(0, 1) once a day. What is the approximate expected number of days until you get a value of more than 2?
[Facebook - Medium] Derive the expectation for a geometric distributed random variable.
[Google - Medium] A coin was flipped 1000 times, and 550 times it showed up heads. Do you think the coin is biased? Why or why not?
[Robinhood - Medium] Say you have n integers 1…n and take a random permutation. For any integers i, j let a swap be defined as when the integer i is in the jth position, and vice versa. What is the expected value of the total number of swaps?
[Uber - Hard] What is the difference between MLE and MAP? Describe it mathematically.
[Google - Hard] Say you have two subsets of a dataset for which you know their means and standard deviations. How do you calculate the blended mean and standard deviation of the total dataset? Can you extend it to K subsets?
[Lyft - Hard] How do you randomly sample a point uniformly from a circle with radius 1?
[Two Sigma - Hard] Say you continually sample from some i.i.d. uniformly distributed (0, 1) random variables until the sum of the variables exceeds 1. How many times do you expect to sample?
[Uber - Hard] Given a random Bernoulli trial generator, how do you return a value sampled from a normal distribution
We've divided this article into the following sections:

SQL Interview Questions
PostgreSQL Interview Questions
In the end, multiple-choice questions are provided to test your understanding.

Play
SQL Interview Questions
1. What is Pattern Matching in SQL?
SQL pattern matching provides for pattern search in data if you have no clue as to what that word should be. This kind of SQL query uses wildcards to match a string pattern, rather than writing the exact word. The LIKE operator is used in conjunction with SQL Wildcards to fetch the required information.

Using the % wildcard to perform a simple search
The % wildcard matches zero or more characters of any type and can be used to define wildcards both before and after the pattern. Search a student in your database with first name beginning with the letter K:

SELECT *
FROM students
WHERE first_name LIKE 'K%'
Omitting the patterns using the NOT keyword
Use the NOT keyword to select records that don't match the pattern. This query returns all students whose first name does not begin with K.

SELECT *
FROM students
WHERE first_name NOT LIKE 'K%'
Matching a pattern anywhere using the % wildcard twice
Search for a student in the database where he/she has a K in his/her first name.

SELECT *
FROM students
WHERE first_name LIKE '%Q%'
Using the _ wildcard to match pattern at a specific position
The _ wildcard matches exactly one character of any type. It can be used in conjunction with % wildcard. This query fetches all students with letter K at the third position in their first name.

SELECT *
FROM students
WHERE first_name LIKE '__K%'
Matching patterns for a specific length
The _ wildcard plays an important role as a limitation when it matches exactly one character. It limits the length and position of the matched results. For example - 

SELECT *   /* Matches first names with three or more letters */
FROM students
WHERE first_name LIKE '___%'

SELECT *   /* Matches first names with exactly four characters */
FROM students
WHERE first_name LIKE '____'

Create a free personalised study plan
Get into your dream companies with expert guidance
Real-Life Problems
Prep for Target Roles
Custom Plan Duration
Create My Plan
2. How to create empty tables with the same structure as another table?
Creating empty tables with the same structure can be done smartly by fetching the records of one table into a new table using the INTO operator while fixing a WHERE clause to be false for all records. Hence, SQL prepares the new table with a duplicate structure to accept the fetched records but since no records get fetched due to the WHERE clause in action, nothing is inserted into the new table.

SELECT * INTO Students_copy
FROM Students WHERE 1 = 2;
3. What is a Recursive Stored Procedure?
A stored procedure that calls itself until a boundary condition is reached, is called a recursive stored procedure. This recursive function helps the programmers to deploy the same set of code several times as and when required. Some SQL programming languages limit the recursion depth to prevent an infinite loop of procedure calls from causing a stack overflow, which slows down the system and may lead to system crashes.

DELIMITER $$     /* Set a new delimiter => $$ */
CREATE PROCEDURE calctotal( /* Create the procedure */
   IN number INT,   /* Set Input and Ouput variables */
   OUT total INT
) BEGIN
DECLARE score INT DEFAULT NULL;   /* Set the default value => "score" */
SELECT awards FROM achievements   /* Update "score" via SELECT query */
WHERE id = number INTO score;
IF score IS NULL THEN SET total = 0;   /* Termination condition */
ELSE
CALL calctotal(number+1);   /* Recursive call */
SET total = total + score;   /* Action after recursion */
END IF;
END $$     /* End of procedure */
DELIMITER ;     /* Reset the delimiter */
You can download a PDF version of Sql Interview Questions.

Download PDF

4. What is a Stored Procedure?
A stored procedure is a subroutine available to applications that access a relational database management system (RDBMS). Such procedures are stored in the database data dictionary. The sole disadvantage of stored procedure is that it can be executed nowhere except in the database and occupies more memory in the database server. It also provides a sense of security and functionality as users who can't access the data directly can be granted access via stored procedures.

DELIMITER $$
CREATE PROCEDURE FetchAllStudents()
BEGIN
SELECT *  FROM myDB.students;
END $$
DELIMITER ;

5. What is Collation? What are the different types of Collation Sensitivity?
Collation refers to a set of rules that determine how data is sorted and compared. Rules defining the correct character sequence are used to sort the character data. It incorporates options for specifying case sensitivity, accent marks, kana character types, and character width. Below are the different types of collation sensitivity:

Case sensitivity: A and a are treated differently.
Accent sensitivity: a and á are treated differently.
Kana sensitivity: Japanese kana characters Hiragana and Katakana are treated differently.
Width sensitivity: Same character represented in single-byte (half-width) and double-byte (full-width) are treated differently.
Learn via our Video Courses
courseSrikanth Varma
Computer Networking Course: Master Computer Networking
star5
enrollemntsEnrolled: 31834
rupeeFree
courseSumeet malik
NSET Course: Mathematics
star4.7
enrollemntsEnrolled: 9508
rupeeFree
courseSumeet malik
NSET Course: Logical reasoning
star4.7
enrollemntsEnrolled: 9605
rupeeFree
courseSrikanth Varma
Operating System Course: Learn Fundamentals of Operating System
star5
enrollemntsEnrolled: 29938
rupeeFree
courseSrikanth Varma
Microsoft Malware Detection using Machine Learning
star5
enrollemntsEnrolled: 1319
rupeeFree
courseSrikanth Varma
Netflix Movie Recommendation System using Machine Learning
star5
enrollemntsEnrolled: 2463
rupeeFree
courseSrikanth Varma
Amazon Fashion Discovery Engine using Machine Learning
star5
enrollemntsEnrolled: 792
rupeeFree
courseSrikanth Varma
Personalized Cancer Diagnosis using Machine Learning
star5
enrollemntsEnrolled: 786
rupeeFree
courseSrikanth Varma
Facebook Friend Recommendation using Graph Mining
star5
enrollemntsEnrolled: 585
rupeeFree
courseSrikanth Varma
Predicting tags for Stackoverflow using Machine Learning
star5
enrollemntsEnrolled: 468
rupeeFree
courseSrikanth Varma
Quora Question Pair Similarity Problem using Machine Learning
star5
enrollemntsEnrolled: 528
rupeeFree
courseSrikanth Varma
Taxi demand prediction in New York City using Machine Learning
star5
enrollemntsEnrolled: 570
rupeeFree
courseMrinal Bhattacharya
React JS Free Course
star4.8
enrollemntsEnrolled: 22137
rupeeFree
courseYash Raj
AWS Free Course
star4.7
enrollemntsEnrolled: 13691
rupeeFree
courseSubhesh Kumar
Object Oriented Programming in Java Course Online
star4.95
enrollemntsEnrolled: 12758
rupeeFree
courseSrikanth Varma
Deep Learning Course: Deep Dive into Deep Learning
star5
enrollemntsEnrolled: 8546
rupeeFree
courseSrikanth Varma
Free Maths for Machine Learning Course
star5
enrollemntsEnrolled: 9622
rupeeFree
courseSrikanth Varma
Supervised Machine Learning Course
star5
enrollemntsEnrolled: 13419
rupeeFree
courseJamshaid Sohail
PyTorch for Deep Learning Course
star4.8
enrollemntsEnrolled: 3692
rupeeFree
courseSrikanth Varma
Unsupervised Machine Learning Course
star5
enrollemntsEnrolled: 4058
rupeeFree
courseYash Sinha
Data Science Course - Mastering the Fundamentals
star4.7
enrollemntsEnrolled: 34906
rupeeFree
coursePrateek Narang
Maths for Programmers
star5
enrollemntsEnrolled: 7428
rupeeFree
courseArnav Gupta
Spring Boot Course: Certified Course for Essential Skills
star5
enrollemntsEnrolled: 20795
rupeeFree
courseGaurav Sisodia
Keras & TensorFlow for Deep Learning
star4.8
enrollemntsEnrolled: 3602
rupeeFree
coursePrateek Narang
Coding Essentials: Learn Logic Building for Beginners Free Course
star5
enrollemntsEnrolled: 15178
rupeeFree
courseArnav Gupta
Machine Coding Tic Tac Toe - LLD Case Study
star5
enrollemntsEnrolled: 3005
rupeeFree
courseMrinal Bhattacharya
Node JS Certification Course - Master the Fundamentals
star4.8
enrollemntsEnrolled: 24393
rupeeFree
coursePrateek Narang
SQL for Beginners: Learn SQL using MySQL and Database Design Course
star5
enrollemntsEnrolled: 50298
rupeeFree
courseAditya Jain
Data Structures in C++ Course
star4.5
enrollemntsEnrolled: 38230
rupeeFree
courseSubhesh Kumar
Java DSA Course - Master the Fundamentals and Beyond
star4.95
enrollemntsEnrolled: 48505
rupeeFree
6. What are the differences between OLTP and OLAP?
OLTP stands for Online Transaction Processing, is a class of software applications capable of supporting transaction-oriented programs. An important attribute of an OLTP system is its ability to maintain concurrency. OLTP systems often follow a decentralized architecture to avoid single points of failure. These systems are generally designed for a large audience of end-users who conduct short transactions. Queries involved in such databases are generally simple, need fast response times, and return relatively few records. A number of transactions per second acts as an effective measure for such systems.

OLAP stands for Online Analytical Processing, a class of software programs that are characterized by the relatively low frequency of online transactions. Queries are often too complex and involve a bunch of aggregations. For OLAP systems, the effectiveness measure relies highly on response time. Such systems are widely used for data mining or maintaining aggregated, historical data, usually in multi-dimensional schemas.


7. What is OLTP?
OLTP stands for Online Transaction Processing, is a class of software applications capable of supporting transaction-oriented programs. An essential attribute of an OLTP system is its ability to maintain concurrency. To avoid single points of failure, OLTP systems are often decentralized. These systems are usually designed for a large number of users who conduct short transactions. Database queries are usually simple, require sub-second response times, and return relatively few records. Here is an insight into the working of an OLTP system [ Note - The figure is not important for interviews ] -



Advance your career with  
Mock Assessments
Real-world coding challenges for top company interviews
Real-Life Problems
Detailed reports
Attempt Now
8. What is User-defined function? What are its various types?
The user-defined functions in SQL are like functions in any other programming language that accept parameters, perform complex calculations, and return a value. They are written to use the logic repetitively whenever required. There are two types of SQL user-defined functions:

Scalar Function: As explained earlier, user-defined scalar functions return a single scalar value.
Table-Valued Functions: User-defined table-valued functions return a table as output.
Inline: returns a table data type based on a single SELECT statement.
Multi-statement: returns a tabular result-set but, unlike inline, multiple SELECT statements can be used inside the function body.
9. What is a UNIQUE constraint?
A UNIQUE constraint ensures that all values in a column are different. This provides uniqueness for the column(s) and helps identify each row uniquely. Unlike primary key, there can be multiple unique constraints defined per table. The code syntax for UNIQUE is quite similar to that of PRIMARY KEY and can be used interchangeably.

CREATE TABLE Students (   /* Create table with a single field as unique */
   ID INT NOT NULL UNIQUE
   Name VARCHAR(255)
);

CREATE TABLE Students (   /* Create table with multiple fields as unique */
   ID INT NOT NULL
   LastName VARCHAR(255)
   FirstName VARCHAR(255) NOT NULL
   CONSTRAINT PK_Student
   UNIQUE (ID, FirstName)
);

ALTER TABLE Students   /* Set a column as unique */
ADD UNIQUE (ID);
ALTER TABLE Students   /* Set multiple columns as unique */
ADD CONSTRAINT PK_Student   /* Naming a unique constraint */
UNIQUE (ID, FirstName);
10. What is a Query?
A query is a request for data or information from a database table or combination of tables. A database query can be either a select query or an action query.

SELECT fname, lname    /* select query */
FROM myDb.students
WHERE student_id = 1;
UPDATE myDB.students    /* action query */
SET fname = 'Captain', lname = 'America'
WHERE student_id = 1;
11. What is Data Integrity?
Data Integrity is the assurance of accuracy and consistency of data over its entire life-cycle and is a critical aspect of the design, implementation, and usage of any system which stores, processes, or retrieves data. It also defines integrity constraints to enforce business rules on the data when it is entered into an application or a database.

12. What is the difference between Clustered and Non-clustered index?
As explained above, the differences can be broken down into three small factors -

Clustered index modifies the way records are stored in a database based on the indexed column. A non-clustered index creates a separate entity within the table which references the original table.
Clustered index is used for easy and speedy retrieval of data from the database, whereas, fetching records from the non-clustered index is relatively slower.
In SQL, a table can have a single clustered index whereas it can have multiple non-clustered indexes.
logo
Get Access to 250+ Guides with Scaler Mobile App!
Experience free learning content on the Scaler Mobile App
logo
4.5
100K+
Play Store
13. What is an Index? Explain its different types.
A database index is a data structure that provides a quick lookup of data in a column or columns of a table. It enhances the speed of operations accessing data from a database table at the cost of additional writes and memory to maintain the index data structure.

CREATE INDEX index_name   /* Create Index */
ON table_name (column_1, column_2);
DROP INDEX index_name;   /* Drop Index */
There are different types of indexes that can be created for different purposes:

Unique and Non-Unique Index:
Unique indexes are indexes that help maintain data integrity by ensuring that no two rows of data in a table have identical key values. Once a unique index has been defined for a table, uniqueness is enforced whenever keys are added or changed within the index.

CREATE UNIQUE INDEX myIndex
ON students (enroll_no);
Non-unique indexes, on the other hand, are not used to enforce constraints on the tables with which they are associated. Instead, non-unique indexes are used solely to improve query performance by maintaining a sorted order of data values that are used frequently.

Clustered and Non-Clustered Index:
Clustered indexes are indexes whose order of the rows in the database corresponds to the order of the rows in the index. This is why only one clustered index can exist in a given table, whereas, multiple non-clustered indexes can exist in the table.

The only difference between clustered and non-clustered indexes is that the database manager attempts to keep the data in the database in the same order as the corresponding keys appear in the clustered index.

Clustering indexes can improve the performance of most query operations because they provide a linear-access path to data stored in the database.

Write a SQL statement to create a UNIQUE INDEX "my_index" on "my_table" for fields "column_1" & "column_2".
14. What is a Cross-Join?
Cross join can be defined as a cartesian product of the two tables included in the join. The table after join contains the same number of rows as in the cross-product of the number of rows in the two tables. If a WHERE clause is used in cross join then the query will work like an INNER JOIN.

SELECT stu.name, sub.subject 
FROM students AS stu
CROSS JOIN subjects AS sub;

Write a SQL statement to CROSS JOIN 'table_1' with 'table_2' and fetch 'col_1' from table_1 & 'col_2' from table_2 respectively. Do not use alias.
Write a SQL statement to perform SELF JOIN for 'Table_X' with alias 'Table_1' and 'Table_2', on columns 'Col_1' and 'Col_2' respectively.
15. What is a Self-Join?
A self JOIN is a case of regular join where a table is joined to itself based on some relation between its own column(s). Self-join uses the INNER JOIN or LEFT JOIN clause and a table alias is used to assign different names to the table within the query.

SELECT A.emp_id AS "Emp_ID",A.emp_name AS "Employee",
B.emp_id AS "Sup_ID",B.emp_name AS "Supervisor"
FROM employee A, employee B
WHERE A.emp_sup = B.emp_id;
16. What is a Join? List its different types.
The SQL Join clause is used to combine records (rows) from two or more tables in a SQL database based on a related column between the two.


There are four different types of JOINs in SQL:

(INNER) JOIN: Retrieves records that have matching values in both tables involved in the join. This is the widely used join for queries.
SELECT *
FROM Table_A
JOIN Table_B;
SELECT *
FROM Table_A
INNER JOIN Table_B;
LEFT (OUTER) JOIN: Retrieves all the records/rows from the left and the matched records/rows from the right table.
SELECT *
FROM Table_A A
LEFT JOIN Table_B B
ON A.col = B.col;
RIGHT (OUTER) JOIN: Retrieves all the records/rows from the right and the matched records/rows from the left table.
SELECT *
FROM Table_A A
RIGHT JOIN Table_B B
ON A.col = B.col;
FULL (OUTER) JOIN: Retrieves all the records where there is a match in either the left or right table.
SELECT *
FROM Table_A A
FULL JOIN Table_B B
ON A.col = B.col;
17. What is a Foreign Key?
A FOREIGN KEY comprises of single or collection of fields in a table that essentially refers to the PRIMARY KEY in another table. Foreign key constraint ensures referential integrity in the relation between two tables.
The table with the foreign key constraint is labeled as the child table, and the table containing the candidate key is labeled as the referenced or parent table.

CREATE TABLE Students (   /* Create table with foreign key - Way 1 */
   ID INT NOT NULL
   Name VARCHAR(255)
   LibraryID INT
   PRIMARY KEY (ID)
   FOREIGN KEY (Library_ID) REFERENCES Library(LibraryID)
);

CREATE TABLE Students (   /* Create table with foreign key - Way 2 */
   ID INT NOT NULL PRIMARY KEY
   Name VARCHAR(255)
   LibraryID INT FOREIGN KEY (Library_ID) REFERENCES Library(LibraryID)
);

ALTER TABLE Students   /* Add a new foreign key */
ADD FOREIGN KEY (LibraryID)
REFERENCES Library (LibraryID);
What type of integrity constraint does the foreign key ensure?
Write a SQL statement to add a FOREIGN KEY 'col_fk' in 'table_y' that references 'col_pk' in 'table_x'.
18. What is a Subquery? What are its types?
A subquery is a query within another query, also known as a nested query or inner query. It is used to restrict or enhance the data to be queried by the main query, thus restricting or enhancing the output of the main query respectively. For example, here we fetch the contact information for students who have enrolled for the maths subject:

SELECT name, email, mob, address
FROM myDb.contacts
WHERE roll_no IN (
 SELECT roll_no
 FROM myDb.students
 WHERE subject = 'Maths');
There are two types of subquery - Correlated and Non-Correlated.

A correlated subquery cannot be considered as an independent query, but it can refer to the column in a table listed in the FROM of the main query.
A non-correlated subquery can be considered as an independent query and the output of the subquery is substituted in the main query.
Write a SQL query to update the field "status" in table "applications" from 0 to 1.
Write a SQL query to select the field "app_id" in table "applications" where "app_id" less than 1000.
Write a SQL query to fetch the field "app_name" from "apps" where "apps.id" is equal to the above collection of "app_id".
19. What is a Primary Key?
The PRIMARY KEY constraint uniquely identifies each row in a table. It must contain UNIQUE values and has an implicit NOT NULL constraint.
A table in SQL is strictly restricted to have one and only one primary key, which is comprised of single or multiple fields (columns).

CREATE TABLE Students (   /* Create table with a single field as primary key */
   ID INT NOT NULL
   Name VARCHAR(255)
   PRIMARY KEY (ID)
);

CREATE TABLE Students (   /* Create table with multiple fields as primary key */
   ID INT NOT NULL
   LastName VARCHAR(255)
   FirstName VARCHAR(255) NOT NULL,
   CONSTRAINT PK_Student
   PRIMARY KEY (ID, FirstName)
);

ALTER TABLE Students   /* Set a column as primary key */
ADD PRIMARY KEY (ID);
ALTER TABLE Students   /* Set multiple columns as primary key */
ADD CONSTRAINT PK_Student   /*Naming a Primary Key*/
PRIMARY KEY (ID, FirstName);
write a sql statement to add primary key 't_id' to the table 'teachers'.
Write a SQL statement to add primary key constraint 'pk_a' for table 'table_a' and fields 'col_b, col_c'.
20. What are Constraints in SQL?
Constraints are used to specify the rules concerning data in the table. It can be applied for single or multiple fields in an SQL table during the creation of the table or after creating using the ALTER TABLE command. The constraints are:

NOT NULL - Restricts NULL value from being inserted into a column.
CHECK - Verifies that all values in a field satisfy a condition.
DEFAULT - Automatically assigns a default value if no value has been specified for the field.
UNIQUE - Ensures unique values to be inserted into the field.
INDEX - Indexes a field providing faster retrieval of records.
PRIMARY KEY - Uniquely identifies each record in a table.
FOREIGN KEY - Ensures referential integrity for a record in another table.
21. What are Tables and Fields?
A table is an organized collection of data stored in the form of rows and columns. Columns can be categorized as vertical and rows as horizontal. The columns in a table are called fields while the rows can be referred to as records.

22. What is the difference between SQL and MySQL?
SQL is a standard language for retrieving and manipulating structured databases. On the contrary, MySQL is a relational database management system, like SQL Server, Oracle or IBM DB2, that is used to manage SQL databases.


23. What is SQL?
SQL stands for Structured Query Language. It is the standard language for relational database management systems. It is especially useful in handling organized data comprised of entities (variables) and relations between different entities of the data.

24. What is RDBMS? How is it different from DBMS?
RDBMS stands for Relational Database Management System. The key difference here, compared to DBMS, is that RDBMS stores data in the form of a collection of tables, and relations can be defined between the common fields of these tables. Most modern database management systems like MySQL, Microsoft SQL Server, Oracle, IBM DB2, and Amazon Redshift are based on RDBMS.


25. What is DBMS?
DBMS stands for Database Management System. DBMS is a system software responsible for the creation, retrieval, updation, and management of the database. It ensures that our data is consistent, organized, and is easily accessible by serving as an interface between the database and its end-users or application software.

26. What is Database?
A database is an organized collection of data, stored and retrieved digitally from a remote or local computer system. Databases can be vast and complex, and such databases are developed using fixed design and modeling approaches.

27. What is the SELECT statement?
SELECT operator in SQL is used to select data from a database. The data returned is stored in a result table, called the result-set.

SELECT * FROM myDB.students;
28. What are some common clauses used with SELECT query in SQL?
Some common SQL clauses used in conjuction with a SELECT query are as follows:

WHERE clause in SQL is used to filter records that are necessary, based on specific conditions.
ORDER BY clause in SQL is used to sort the records based on some field(s) in ascending (ASC) or descending order (DESC).
SELECT *
FROM myDB.students
WHERE graduation_year = 2019
ORDER BY studentID DESC;
GROUP BY clause in SQL is used to group records with identical data and can be used in conjunction with some aggregation functions to produce summarized results from the database.
HAVING clause in SQL is used to filter records in combination with the GROUP BY clause. It is different from WHERE, since the WHERE clause cannot filter aggregated records.
SELECT COUNT(studentId), country
FROM myDB.students
WHERE country != "INDIA"
GROUP BY country
HAVING COUNT(studentID) > 5;
29. What are UNION, MINUS and INTERSECT commands?
The UNION operator combines and returns the result-set retrieved by two or more SELECT statements.
The MINUS operator in SQL is used to remove duplicates from the result-set obtained by the second SELECT query from the result-set obtained by the first SELECT query and then return the filtered results from the first.
The INTERSECT clause in SQL combines the result-set fetched by the two SELECT statements where records from one match the other and then returns this intersection of result-sets.

Certain conditions need to be met before executing either of the above statements in SQL -

Each SELECT statement within the clause must have the same number of columns
The columns must also have similar data types
The columns in each SELECT statement should necessarily have the same order
SELECT name FROM Students   /* Fetch the union of queries */
UNION
SELECT name FROM Contacts;
SELECT name FROM Students   /* Fetch the union of queries with duplicates*/
UNION ALL
SELECT name FROM Contacts;
SELECT name FROM Students   /* Fetch names from students */
MINUS     /* that aren't present in contacts */
SELECT name FROM Contacts;
SELECT name FROM Students   /* Fetch names from students */
INTERSECT    /* that are present in contacts as well */
SELECT name FROM Contacts;
Write a SQL query to fetch "names" that are present in either table "accounts" or in table "registry".
Write a SQL query to fetch "names" that are present in "accounts" but not in table "registry".
Write a SQL query to fetch "names" from table "contacts" that are neither present in "accounts.name" nor in "registry.name".
30. What is Cursor? How to use a Cursor?
A database cursor is a control structure that allows for the traversal of records in a database. Cursors, in addition, facilitates processing after traversal, such as retrieval, addition, and deletion of database records. They can be viewed as a pointer to one row in a set of rows.

Working with SQL Cursor:

DECLARE a cursor after any variable declaration. The cursor declaration must always be associated with a SELECT Statement.
Open cursor to initialize the result set. The OPEN statement must be called before fetching rows from the result set.
FETCH statement to retrieve and move to the next row in the result set.
Call the CLOSE statement to deactivate the cursor.
Finally use the DEALLOCATE statement to delete the cursor definition and release the associated resources.
DECLARE @name VARCHAR(50)   /* Declare All Required Variables */
DECLARE db_cursor CURSOR FOR   /* Declare Cursor Name*/
SELECT name
FROM myDB.students
WHERE parent_name IN ('Sara', 'Ansh')
OPEN db_cursor   /* Open cursor and Fetch data into @name */ 
FETCH next
FROM db_cursor
INTO @name
CLOSE db_cursor   /* Close the cursor and deallocate the resources */
DEALLOCATE db_cursor
31. What are Entities and Relationships?
Entity: An entity can be a real-world object, either tangible or intangible, that can be easily identifiable. For example, in a college database, students, professors, workers, departments, and projects can be referred to as entities. Each entity has some associated properties that provide it an identity.

Relationships: Relations or links between entities that have something to do with each other. For example - The employee's table in a company's database can be associated with the salary table in the same database.


32. List the different types of relationships in SQL.
One-to-One - This can be defined as the relationship between two tables where each record in one table is associated with the maximum of one record in the other table.
One-to-Many & Many-to-One - This is the most commonly used relationship where a record in a table is associated with multiple records in the other table.
Many-to-Many - This is used in cases when multiple instances on both sides are needed for defining a relationship.
Self-Referencing Relationships - This is used when a table needs to define a relationship with itself.
33. What is an Alias in SQL?
An alias is a feature of SQL that is supported by most, if not all, RDBMSs. It is a temporary name assigned to the table or table column for the purpose of a particular SQL query. In addition, aliasing can be employed as an obfuscation technique to secure the real names of database fields. A table alias is also called a correlation name.

An alias is represented explicitly by the AS keyword but in some cases, the same can be performed without it as well. Nevertheless, using the AS keyword is always a good practice.

SELECT A.emp_name AS "Employee"  /* Alias using AS keyword */
B.emp_name AS "Supervisor"
FROM employee A, employee B   /* Alias without AS keyword */
WHERE A.emp_sup = B.emp_id;
Write an SQL statement to select all from table "Limited" with alias "Ltd".
34. What is a View?
A view in SQL is a virtual table based on the result-set of an SQL statement. A view contains rows and columns, just like a real table. The fields in a view are fields from one or more real tables in the database.


35. What is Normalization?
Normalization represents the way of organizing structured data in the database efficiently. It includes the creation of tables, establishing relationships between them, and defining rules for those relationships. Inconsistency and redundancy can be kept in check based on these rules, hence, adding flexibility to the database.

36. What is Denormalization?
Denormalization is the inverse process of normalization, where the normalized schema is converted into a schema that has redundant information. The performance is improved by using redundancy and keeping the redundant data consistent. The reason for performing denormalization is the overheads produced in the query processor by an over-normalized structure.

37. What are the various forms of Normalization?
Normal Forms are used to eliminate or reduce redundancy in database tables. The different forms are as follows:

First Normal Form:
A relation is in first normal form if every attribute in that relation is a single-valued attribute. If a relation contains a composite or multi-valued attribute, it violates the first normal form. Let's consider the following students table. Each student in the table, has a name, his/her address, and the books they issued from the public library -
Students Table

Student 	Address 	Books Issued 	Salutation
Sara 	Amanora Park Town 94 	Until the Day I Die (Emily Carpenter), Inception (Christopher Nolan)	Ms.
Ansh	62nd Sector A-10 	The Alchemist (Paulo Coelho), Inferno (Dan Brown) 	Mr.
Sara 	24th Street Park Avenue 	Beautiful Bad (Annie Ward), Woman 99 (Greer Macallister)	Mrs.
Ansh 	Windsor Street 777 	Dracula (Bram Stoker)	Mr.
As we can observe, the Books Issued field has more than one value per record, and to convert it into 1NF, this has to be resolved into separate individual records for each book issued. Check the following table in 1NF form -

Students Table (1st Normal Form)

Student 	Address 	Books Issued 	Salutation
Sara	Amanora Park Town 94	Until the Day I Die (Emily Carpenter) 	Ms.
Sara	Amanora Park Town 94	Inception (Christopher Nolan) 	Ms.
Ansh	62nd Sector A-10	The Alchemist (Paulo Coelho) 	Mr.
Ansh	62nd Sector A-10	Inferno (Dan Brown) 	Mr.
Sara	24th Street Park Avenue	Beautiful Bad (Annie Ward) 	Mrs.
Sara	24th Street Park Avenue	Woman 99 (Greer Macallister) 	Mrs.
Ansh	Windsor Street 777	Dracula (Bram Stoker) 	Mr.
Second Normal Form:
A relation is in second normal form if it satisfies the conditions for the first normal form and does not contain any partial dependency. A relation in 2NF has no partial dependency, i.e., it has no non-prime attribute that depends on any proper subset of any candidate key of the table. Often, specifying a single column Primary Key is the solution to the problem. Examples -

Example 1 - Consider the above example. As we can observe, the Students Table in the 1NF form has a candidate key in the form of [Student, Address] that can uniquely identify all records in the table. The field Books Issued (non-prime attribute) depends partially on the Student field. Hence, the table is not in 2NF. To convert it into the 2nd Normal Form, we will partition the tables into two while specifying a new Primary Key attribute to identify the individual records in the Students table. The Foreign Key constraint will be set on the other table to ensure referential integrity.

Students Table (2nd Normal Form)

Student_ID 	Student	Address 	Salutation
1	Sara	Amanora Park Town 94 	Ms.
2	Ansh	62nd Sector A-10 	Mr.
3	Sara	24th Street Park Avenue 	Mrs.
4	Ansh	Windsor Street 777 	Mr.
Books Table (2nd Normal Form)

Student_ID 	Book Issued
1	Until the Day I Die (Emily Carpenter)
1	Inception (Christopher Nolan)
2	The Alchemist (Paulo Coelho)
2	Inferno (Dan Brown)
3	Beautiful Bad (Annie Ward)
3	Woman 99 (Greer Macallister)
4	Dracula (Bram Stoker)
Example 2 - Consider the following dependencies in relation to R(W,X,Y,Z)

 WX -> Y    [W and X together determine Y] 
 XY -> Z    [X and Y together determine Z] 
Here, WX is the only candidate key and there is no partial dependency, i.e., any proper subset of WX doesn’t determine any non-prime attribute in the relation.

Third Normal Form
A relation is said to be in the third normal form, if it satisfies the conditions for the second normal form and there is no transitive dependency between the non-prime attributes, i.e., all non-prime attributes are determined only by the candidate keys of the relation and not by any other non-prime attribute.

Example 1 - Consider the Students Table in the above example. As we can observe, the Students Table in the 2NF form has a single candidate key Student_ID (primary key) that can uniquely identify all records in the table. The field Salutation (non-prime attribute), however, depends on the Student Field rather than the candidate key. Hence, the table is not in 3NF. To convert it into the 3rd Normal Form, we will once again partition the tables into two while specifying a new Foreign Key constraint to identify the salutations for individual records in the Students table. The Primary Key constraint for the same will be set on the Salutations table to identify each record uniquely.

Students Table (3rd Normal Form)

Student_ID 	Student 	Address 	Salutation_ID
1	Sara	Amanora Park Town 94 	1
2	Ansh	62nd Sector A-10 	2
3	Sara	24th Street Park Avenue 	3
4	Ansh	Windsor Street 777 	1
Books Table (3rd Normal Form)

Student_ID	Book Issued
1	Until the Day I Die (Emily Carpenter)
1	Inception (Christopher Nolan)
2	The Alchemist (Paulo Coelho)
2	Inferno (Dan Brown)
3	Beautiful Bad (Annie Ward)
3	Woman 99 (Greer Macallister)
4	Dracula (Bram Stoker)
Salutations Table (3rd Normal Form)

Salutation_ID	Salutation
1	Ms.
2	Mr.
3	Mrs.
Example 2 - Consider the following dependencies in relation to R(P,Q,R,S,T)

 P -> QR     [P together determine C] 
 RS -> T     [B and C together determine D] 
 Q -> S 
 T -> P 
For the above relation to exist in 3NF, all possible candidate keys in the above relation should be {P, RS, QR, T}.

Boyce-Codd Normal Form
A relation is in Boyce-Codd Normal Form if satisfies the conditions for third normal form and for every functional dependency, Left-Hand-Side is super key. In other words, a relation in BCNF has non-trivial functional dependencies in form X –> Y, such that X is always a super key. For example - In the above example, Student_ID serves as the sole unique identifier for the Students Table and Salutation_ID for the Salutations Table, thus these tables exist in BCNF. The same cannot be said for the Books Table and there can be several books with common Book Names and the same Student_ID.

38. What are the TRUNCATE, DELETE and DROP statements?
DELETE statement is used to delete rows from a table.

DELETE FROM Candidates
WHERE CandidateId > 1000;
TRUNCATE command is used to delete all the rows from the table and free the space containing the table.

TRUNCATE TABLE Candidates;
DROP command is used to remove an object from the database. If you drop a table, all the rows in the table are deleted and the table structure is removed from the database.

DROP TABLE Candidates;
Write a SQL statement to wipe a table 'Temporary' from memory.
Write a SQL query to remove first 1000 records from table 'Temporary' based on 'id'.
Write a SQL statement to delete the table 'Temporary' while keeping its relations intact.
39. What is the difference between DROP and TRUNCATE statements?
If a table is dropped, all things associated with the tables are dropped as well. This includes - the relationships defined on the table with other tables, the integrity checks and constraints, access privileges and other grants that the table has. To create and use the table again in its original form, all these relations, checks, constraints, privileges and relationships need to be redefined. However, if a table is truncated, none of the above problems exist and the table retains its original structure.

40. What is the difference between DELETE and TRUNCATE statements?
The TRUNCATE command is used to delete all the rows from the table and free the space containing the table.
The DELETE command deletes only the rows from the table based on the condition given in the where clause or deletes all the rows from the table if no condition is specified. But it does not free the space containing the table.

41. What are Aggregate and Scalar functions?
An aggregate function performs operations on a collection of values to return a single scalar value. Aggregate functions are often used with the GROUP BY and HAVING clauses of the SELECT statement. Following are the widely used SQL aggregate functions:

AVG() - Calculates the mean of a collection of values.
COUNT() - Counts the total number of records in a specific table or view.
MIN() - Calculates the minimum of a collection of values.
MAX() - Calculates the maximum of a collection of values.
SUM() - Calculates the sum of a collection of values.
FIRST() - Fetches the first element in a collection of values.
LAST() - Fetches the last element in a collection of values.
Note: All aggregate functions described above ignore NULL values except for the COUNT function.

A scalar function returns a single value based on the input value. Following are the widely used SQL scalar functions:

LEN() - Calculates the total length of the given field (column).
UCASE() - Converts a collection of string values to uppercase characters.
LCASE() - Converts a collection of string values to lowercase characters.
MID() - Extracts substrings from a collection of string values in a table.
CONCAT() - Concatenates two or more strings.
RAND() - Generates a random collection of numbers of a given length.
ROUND() - Calculates the round-off integer value for a numeric field (or decimal point values).
NOW() - Returns the current date & time.
FORMAT() - Sets the format to display a collection of values.
PostgreSQL Interview Questions
1. What is PostgreSQL?
PostgreSQL was first called Postgres and was developed by a team led by Computer Science Professor Michael Stonebraker in 1986. It was developed to help developers build enterprise-level applications by upholding data integrity by making systems fault-tolerant. PostgreSQL is therefore an enterprise-level, flexible, robust, open-source, and object-relational DBMS that supports flexible workloads along with handling concurrent users. It has been consistently supported by the global developer community. Due to its fault-tolerant nature, PostgreSQL has gained widespread popularity among developers.

2. What is the capacity of a table in PostgreSQL?
The maximum size of PostgreSQL is 32TB.

3. What is the importance of the TRUNCATE statement?
TRUNCATE TABLE name_of_table statement removes the data efficiently and quickly from the table.
The truncate statement can also be used to reset values of the identity columns along with data cleanup as shown below:

TRUNCATE TABLE name_of_table 
RESTART IDENTITY;
We can also use the statement for removing data from multiple tables all at once by mentioning the table names separated by comma as shown below:

TRUNCATE TABLE 
   table_1, 
   table_2,
   table_3;
4. Define tokens in PostgreSQL?
A token in PostgreSQL is either a keyword, identifier, literal, constant, quotes identifier, or any symbol that has a distinctive personality. They may or may not be separated using a space, newline or a tab. If the tokens are keywords, they are usually commands with useful meanings. Tokens are known as building blocks of any PostgreSQL code.

5. What are partitioned tables called in PostgreSQL?
Partitioned tables are logical structures that are used for dividing large tables into smaller structures that are called partitions. This approach is used for effectively increasing the query performance while dealing with large database tables. To create a partition, a key called partition key which is usually a table column or an expression, and a partitioning method needs to be defined. There are three types of inbuilt partitioning methods provided by Postgres:

Range Partitioning: This method is done by partitioning based on a range of values. This method is most commonly used upon date fields to get monthly, weekly or yearly data. In the case of corner cases like value belonging to the end of the range, for example: if the range of partition 1 is 10-20 and the range of partition 2 is 20-30, and the given value is 10, then 10 belongs to the second partition and not the first.
List Partitioning: This method is used to partition based on a list of known values. Most commonly used when we have a key with a categorical value. For example, getting sales data based on regions divided as countries, cities, or states.
Hash Partitioning: This method utilizes a hash function upon the partition key. This is done when there are no specific requirements for data division and is used to access data individually. For example, you want to access data based on a specific product, then using hash partition would result in the dataset that we require.
The type of partition key and the type of method used for partitioning determines how positive the performance and the level of manageability of the partitioned table are.

6. How can we start, restart and stop the PostgreSQL server?
To start the PostgreSQL server, we run:
service postgresql start
Once the server is successfully started, we get the below message:
Starting PostgreSQL: ok
To restart the PostgreSQL server, we run:
service postgresql restart
Once the server is successfully restarted, we get the message:

Restarting PostgreSQL: server stopped
ok
To stop the server, we run the command:
service postgresql stop
Once stopped successfully, we get the message:

Stopping PostgreSQL: server stopped
ok
7. What is the command used for creating a database in PostgreSQL?
The first step of using PostgreSQL is to create a database. This is done by using the createdb command as shown below: createdb db_name
After running the above command, if the database creation was successful, then the below message is shown:

CREATE DATABASE
8. How will you change the datatype of a column?
This can be done by using the ALTER TABLE statement as shown below:

Syntax:

ALTER TABLE tname
ALTER COLUMN col_name [SET DATA] TYPE new_data_type;
9. How do you define Indexes in PostgreSQL?
Indexes are the inbuilt functions in PostgreSQL which are used by the queries to perform search more efficiently on a table in the database. Consider that you have a table with thousands of records and you have the below query that only a few records can satisfy the condition, then it will take a lot of time to search and return those rows that abide by this condition as the engine has to perform the search operation on every single to check this condition. This is undoubtedly inefficient for a system dealing with huge data. Now if this system had an index on the column where we are applying search, it can use an efficient method for identifying matching rows by walking through only a few levels. This is called indexing.

Select * from some_table where table_col=120
10. Define sequence.
A sequence is a schema-bound, user-defined object which aids to generate a sequence of integers. This is most commonly used to generate values to identity columns in a table. We can create a sequence by using the CREATE SEQUENCE statement as shown below:

CREATE SEQUENCE serial_num START 100;
To get the next number 101 from the sequence, we use the nextval() method as shown below:

SELECT nextval('serial_num');
We can also use this sequence while inserting new records using the INSERT command:

INSERT INTO ib_table_name VALUES (nextval('serial_num'), 'interviewbit');
11. What are string constants in PostgreSQL?
They are character sequences bound within single quotes. These are using during data insertion or updation to characters in the database.
There are special string constants that are quoted in dollars. Syntax: $tag$<string_constant>$tag$ The tag in the constant is optional and when we are not specifying the tag, the constant is called a double-dollar string literal.

12. How can you get a list of all databases in PostgreSQL?
This can be done by using the command \l -> backslash followed by the lower-case letter L.

13. How can you delete a database in PostgreSQL?
This can be done by using the DROP DATABASE command as shown in the syntax below:

DROP DATABASE database_name;
If the database has been deleted successfully, then the following message would be shown:

DROP DATABASE
14. What are ACID properties? Is PostgreSQL compliant with ACID?
ACID stands for Atomicity, Consistency, Isolation, Durability. They are database transaction properties which are used for guaranteeing data validity in case of errors and failures.

Atomicity: This property ensures that the transaction is completed in all-or-nothing way.
Consistency: This ensures that updates made to the database is valid and follows rules and restrictions.
Isolation: This property ensures integrity of transaction that are visible to all other transactions.
Durability: This property ensures that the committed transactions are stored permanently in the database.
PostgreSQL is compliant with ACID properties.

15. Can you explain the architecture of PostgreSQL?
The architecture of PostgreSQL follows the client-server model.
The server side comprises of background process manager, query processer, utilities and shared memory space which work together to build PostgreSQL’s instance that has access to the data. The client application does the task of connecting to this instance and requests data processing to the services. The client can either be GUI (Graphical User Interface) or a web application. The most commonly used client for PostgreSQL is pgAdmin.

16. What do you understand by multi-version concurrency control?
MVCC or Multi-version concurrency control is used for avoiding unnecessary database locks when 2 or more requests tries to access or modify the data at the same time. This ensures that the time lag for a user to log in to the database is avoided. The transactions are recorded when anyone tries to access the content.

For more information regarding this, you can refer here.

17. What do you understand by command enable-debug?
The command enable-debug is used for enabling the compilation of all libraries and applications. When this is enabled, the system processes get hindered and generally also increases the size of the binary file. Hence, it is not recommended to switch this on in the production environment. This is most commonly used by developers to debug the bugs in their scripts and help them spot the issues. For more information regarding how to debug, you can refer here.

18. How do you check the rows affected as part of previous transactions?
SQL standards state that the following three phenomena should be prevented whilst concurrent transactions. SQL standards define 4 levels of transaction isolations to deal with these phenomena.

Dirty reads: If a transaction reads data that is written due to concurrent uncommitted transaction, these reads are called dirty reads.
Phantom reads: This occurs when two same queries when executed separately return different rows. For example, if transaction A retrieves some set of rows matching search criteria. Assume another transaction B retrieves new rows in addition to the rows obtained earlier for the same search criteria. The results are different.
Non-repeatable reads: This occurs when a transaction tries to read the same row multiple times and gets different values each time due to concurrency. This happens when another transaction updates that data and our current transaction fetches that updated data, resulting in different values.
To tackle these, there are 4 standard isolation levels defined by SQL standards. They are as follows:

Read Uncommitted – The lowest level of the isolations. Here, the transactions are not isolated and can read data that are not committed by other transactions resulting in dirty reads.
Read Committed – This level ensures that the data read is committed at any instant of read time. Hence, dirty reads are avoided here. This level makes use of read/write lock on the current rows which prevents read/write/update/delete of that row when the current transaction is being operated on.
Repeatable Read – The most restrictive level of isolation. This holds read and write locks for all rows it operates on. Due to this, non-repeatable reads are avoided as other transactions cannot read, write, update or delete the rows.
Serializable – The highest of all isolation levels. This guarantees that the execution is serializable where execution of any concurrent operations are guaranteed to be appeared as executing serially.
The following table clearly explains which type of unwanted reads the levels avoid:

Isolation levels 	Dirty Reads 	Phantom Reads 	Non-repeatable reads
Read Uncommitted 	Might occur	Might occur	Might occur
Read Committed 	Won’t occur	Might occur	Might occur
Repeatable Read	Won’t occur	Might occur	Won’t occur
Serializable	Won’t occur	Won’t occur	Won’t occur
19. What can you tell about WAL (Write Ahead Logging)?
Write Ahead Logging is a feature that increases the database reliability by logging changes before any changes are done to the database. This ensures that we have enough information when a database crash occurs by helping to pinpoint to what point the work has been complete and gives a starting point from the point where it was discontinued.

For more information, you can refer here.

20. What is the main disadvantage of deleting data from an existing table using the DROP TABLE command?
DROP TABLE command deletes complete data from the table along with removing the complete table structure too. In case our requirement entails just remove the data, then we would need to recreate the table to store data in it. In such cases, it is advised to use the TRUNCATE command.

21. How do you perform case-insensitive searches using regular expressions in PostgreSQL?
To perform case insensitive matches using a regular expression, we can use POSIX (~*) expression from pattern matching operators. For example:

'interviewbit' ~* '.*INTervIewBit.*'
22. How will you take backup of the database in PostgreSQL?
We can achieve this by using the pg_dump tool for dumping all object contents in the database into a single file. The steps are as follows:

Step 1: Navigate to the bin folder of the PostgreSQL installation path.

C:\>cd C:\Program Files\PostgreSQL\10.0\bin
Step 2: Execute pg_dump program to take the dump of data to a .tar folder as shown below:

pg_dump -U postgres -W -F t sample_data > C:\Users\admin\pgbackup\sample_data.tar
The database dump will be stored in the sample_data.tar file on the location specified.

23. Does PostgreSQL support full text search?
Full-Text Search is the method of searching single or collection of documents stored on a computer in a full-text based database. This is mostly supported in advanced database systems like SOLR or ElasticSearch. However, the feature is present but is pretty basic in PostgreSQL.

24. What are parallel queries in PostgreSQL?
Parallel Queries support is a feature provided in PostgreSQL for devising query plans capable of exploiting multiple CPU processors to execute the queries faster.


25. Differentiate between commit and checkpoint.
The commit action ensures that the data consistency of the transaction is maintained and it ends the current transaction in the section. Commit adds a new record in the log that describes the COMMIT to the memory. Whereas, a checkpoint is used for writing all changes that were committed to disk up to SCN which would be kept in datafile headers and control files.

Conclusion:
SQL is a language for the database. It has a vast scope and robust capability of creating and manipulating a variety of database objects using commands like CREATE, ALTER, DROP, etc, and also in loading the database objects using commands like INSERT. It also provides options for Data Manipulation using commands like DELETE, TRUNCATE and also does effective retrieval of data using cursor commands like FETCH, SELECT, etc. There are many such commands which provide a large amount of control to the programmer to interact with the database in an efficient way without wasting many resources. The popularity of SQL has grown so much that almost every programmer relies on this to implement their application's storage functionalities thereby making it an exciting language to learn. Learning this provides the developer a benefit of understanding the data structures used for storing the organization's data and giving an additional level of control and in-depth understanding of the application.

PostgreSQL being an open-source database system having extremely robust and sophisticated ACID, Indexing, and Transaction supports has found widespread popularity among the developer community. 

References and Resources:

PostgreSQL Download
PostgreSQL Tutorial
SQL Guide
SQL Server Interview Questions
SQL Query Interview Questions and Answers
SQL Interview Questions for Data Science
MySQL Interview Questions
DBMS Interview Questions
PL SQL Interview Questions
MongoDB Interview Questions
Database Testing Interview Questions
SQL Vs MySQL
PostgreSQL vs MySQL
Difference Between SQL and PLSQL
Difference between RDBMS and DBMS
SQL Vs NoSQL
SQL IDE
SQL Projects
MySQL Commands
SQL Books
OLTP vs OLAP
Coding Problems
0/3
Easy Problems
SQL Programming

Engineers Joined
Easy
25.11 Mins
Solve

Student Query
Easy
8.10 Mins
Solve

Country Filtration
Easy
6.8 Mins
Solve

0/1
Intermediate Problems
SQL Programming

0/2
Advanced Problems
SQL Programming

View All Problems
SQL MCQ
1.
An SQL query to delete a table from the database and memory while keeping the structure of the table intact?


DROP TABLE table_name;

DROP FROM TABLE table_name;

DELETE FROM TABLE table_name;

TRUNCATE TABLE table_name;
2.
What is a pre-requisite for creating a database in PostgreSQL?To create a database in PostgreSQL, you must have the special CREATEDB privilege or


Super user privilege or CREATEDB privilege

Admin privilege

CREATEDBL privilege and Super user privilege

Just run the script
3.
Which of the following is known as a virtual table in SQL?


SELF JOIN

INNER JOIN

VIEW

NONE
4.
What is the main advantage of a clustered index over a non-clustered index?


It is easier to create and manipulate.

It requires extra memory but allows for speedy retrieval of records.

It does not require additional memory and allows for speedy retrieval of records.

None of the above.
5.
SQL query used to fetch unique values from a field?


SELECT UNIQUE column_name FROM table_name;

SELECT DISTINCT column_name FROM table_name;

SELECT column_name FROM table_name WHERE COUNT(column_name) = 1;

SELECT UNIQUE column_name FROM table_name WHERE COUNT(column_name) = 1;
6.
Which statement is used to update data in the database?


MODIFY

UPDATE

ALTER TABLE

SAVE AS
7.
Which statement is false for the ORDER BY statement?


Requires a ASC or DESC keyword explicitly to sort the result set.

Sorts the result set in descending order using the DESC keyword.

Can sort based on multiple columns

None of the above.
8.
What statement is used for adding data to PostgreSQL?


UPDATE

ADD

APPEND

INSERT
9.
Normalization which has neither composite values nor partial dependencies?


Second Normal Formal

Third Normal Form

Boyce-Codd Normal Form

All of the above
10.
What does SQL stand for?


Structured Question Language

Strong Query Language

Structured Query Language

Strong Question Language
11.
Which statement is true for a PRIMARY KEY constraint?


Primary key defines a realtionship between two tables.

A table in SQL must have a primary key associated with it to uniquely identify its records.

A table in SQL is indexed by default based on its primary key.

Primary key may or may not be unique but can be comprised of multiple fields.
12.
What is the order of results shown by default if the ASC or DESC parameter is not specified with the ORDER BY command?


Results are shown in descending order

Results are shown in ascending order

Results display is random

Results are shown in ascending and descending order alternately.
13.
What allows us to define how various tables are related to each other formally in a database?


Views

Foreign Key Constraints

Primary Key Constraints

Database manager
14.
What is the name of the component that requests data to the PostgreSQL server?


Client

Thin Client

Workstation

Interface
15.
What languages are supported by PostgreSQL?


PL/pgSQL, PL/Tcl, PL/Perl and PL/Python

PL/pgSQL, PL/Pcl, PL/Ruby and PL/Java

PL/Perl, PL/Dcl, PL/Dtl and PL/Dml

Only SQL
16.
What command is used for restoring the backup of PostgreSQL which was created using pg_dump?


psql -R db_dump.psql db_name

psql -r db_dump.psql db_name

psql -f db_dump.psql db_name

psql -F db_dump.psql db_name
17.
Query to select all records with "bar" in their name?


SELECT * FROM people WHERE name = "%bar%";

SELECT * FROM people WHERE name LIKE "%bar%";

SELECT * FROM people WHERE name IN ("bar");

SELECT * FROM people WHERE name = "_bar_"
18.
Which command is used to tell PostgreSQL to make all changes made to the database permanent?


Submit

Execute

Apply

Commit
19.
Which statement is false for a FOREIGN KEY constraint?


Foreign key defines a relationship between two tables.

Foreign Key is automatically created when two tables are joined.

Foreign Key uniquely identifies all the records in the referenced table.

Foreign key may or may not be unique but can be comprised of multiple fields.
20.
What is a Query?


A SELECT or UPDATE statement in SQL.

A request for data from a table in the database.

A request to input data from the user.


When you conduct multiple statistical tests at the same time—such as testing multiple variations of a landing page or analyzing multiple segments of data (e.g., different demographic groups)—you increase your risk of making a **Type I error (false positive)**. This is because the more tests you run, the higher the chance that at least one will show a significant result purely by random chance.

To mitigate this, there are various **multiple-comparison corrections**. Two common ones are the **Bonferroni Correction** and methods that control the **False Discovery Rate (FDR)**.

---

## 1. Bonferroni Correction

### Core Idea

* If you have **m** different hypotheses (or statistical tests), you adjust the significance threshold (α) by dividing it by **m**.
* Equivalently, you can keep α the same and multiply your *p-value* by **m**.

### How it Works

* If you typically use α = 0.05 for a single test, and now you are running **m = 10** tests:

  * You **lower α** to α’ = 0.05 / 10 = 0.005
    **or**
  * For each test, if the unadjusted p-value is **p**, the *adjusted* p-value = p × 10.
* This ensures that your overall chance of making at least one Type I error across all tests is still approximately 5% (though strictly speaking it’s a bit conservative).

### Pros

* Very straightforward to apply.
* Very conservative (i.e., it greatly reduces the chance of false positives).

### Cons

* It can be *too* conservative when the number of comparisons is large, increasing the chance of Type II errors (failing to detect a real effect).

---

## 2. False Discovery Rate (FDR) Control

### Core Idea

* Instead of controlling the probability of getting *any* false positives (as in Bonferroni), methods like **Benjamini-Hochberg (BH)** control the *proportion* of false positives among all your “significant” findings.
* This is often more flexible for large-scale testing (e.g., A/B testing many variants or analyzing thousands of metrics).

### How it Works (Benjamini-Hochberg)

1. Rank all your p-values from smallest to largest: ( p_{(1)} \leq p_{(2)} \leq \ldots \leq p_{(m)} ).
2. For each p-value ( p_{(i)} ), calculate:
   [
   p_{\text{adjusted}, i} = p_{(i)} \times \frac{m}{i}
   ]
3. Compare each adjusted p-value to your desired FDR threshold (often denoted ( q ), e.g., 0.05).
4. Mark any test with an adjusted p-value below ( q ) as significant.

### Pros

* Less conservative than Bonferroni, leading to more “discoveries” while still keeping the false positives under control.
* Especially useful in scenarios with *many* tests (e.g., multiple product variations, large analytics queries, genomics research, etc.).

### Cons

* You allow for some proportion of findings to be false positives (e.g., up to 5%), so you have to be comfortable with that trade-off.

---

## 3. When to Use Which Method?

**Bonferroni Correction** is often used when:

* The number of tests is relatively small.
* You want to be extremely sure of any significant result you declare (risk-averse environment).
* The cost of a false positive is very high.

**FDR Control (Benjamini-Hochberg, etc.)** is often used when:

* You have a large number of tests.
* You’re okay with some proportion of false positives as long as the overall rate is controlled.
* You want to retain more statistical power and find more true positives.

---

## 4. Example Scenario

### Multiple Variants in an A/B Test

* Suppose you have **one control** (Version A) and **four new variants** (B, C, D, and E).
* Instead of running four separate tests (A vs. B, A vs. C, A vs. D, A vs. E) and checking each at α = 0.05, you could do:

1. **Bonferroni Correction**:

   * Adjusted α for each test = ( 0.05 / 4 = 0.0125 ).
   * Only consider a variant’s difference “significant” if p-value < 0.0125.

2. **Benjamini-Hochberg**:

   * Rank your p-values from each of the four comparisons.
   * Adjust each p-value according to the formula ( p_{\text{adjusted}, i} = p_{(i)} \times \frac{4}{i} ).
   * Decide which comparisons are significant based on your chosen FDR threshold (often 0.05).

Using FDR control might let you declare, for example, two variants as significant if their adjusted p-values are still below 0.05, whereas with Bonferroni, you might declare only one variant significant. The choice depends on how conservative your experiment design needs to be.

---

### Summary

**Bonferroni Correction** and **FDR Control** are methods to handle the increased risk of false positives when you test multiple hypotheses simultaneously.

* **Bonferroni** directly cuts down the chance of *any* false positive at the cost of power, making it a good choice when you need high certainty for each claimed discovery.
* **FDR** allows some proportion of false positives but yields more power to detect real effects, making it well-suited for large-scale testing scenarios.

In a practical A/B testing environment—like at Upstart or any other data-driven company—these adjustments become crucial when running multiple tests at once or examining multiple metrics at the same time. They ensure your results are valid and reliable rather than artifacts of “p-value fishing.”

While it’s true that a successful data scientist is expected to master a wide range of skills, SQL is undeniably the bread and butter of data science. This article aims to distill the key patterns discovered during an intensive week of tackling SQL’s toughest questions.

Press enter or click to view image in full size

PREREQUISITE: SQL’s Logical Order of Operations
Let’s revisit the core of SQL efficiency — the logical order of operations. This critical yet frequently overlooked skill is key in slashing processing times and securing considerable financial savings for companies each year. An absolute must-have for anyone handling extensive datasets and aiming for optimal performance. Below is a screenshot of the logical order of operations, sourced from learnsql.com.

Press enter or click to view image in full size

Logical Order of Operations in SQL
Having covered the foundational aspect of query optimization, let’s now delve into the core patterns unearthed from facing SQL’s toughest puzzles:

1. Recursive CTEs
Common Table Expressions (CTEs) and Recursive CTEs are powerful SQL features that enhance the readability, structure, and maintainability of complex queries.

Recursive CTEs are a special type of CTE that can refer to itself, making them exceptionally useful for dealing with hierarchical or tree-structured data, such as organizational charts, folder structures, or even complex mathematical calculations like Fibonacci sequences. They allow for the execution of a repeated process: each iteration of the process (recursion) builds upon the previous one until a specified condition is met.

Recursive CTEs typically consist of 2 parts: an anchor member (the initial query that forms the base result set) and a recursive member (the query that references the CTE itself). These two parts are combined using a UNION ALL operator.

-- Revisit: CTEs Syntax

WITH cte AS (
    SELECT col1, col2, ...
    FROM table_name
    WHERE condition
)
SELECT * FROM cte;
-- Recursive CTEs Syntax

WITH RECURSIVE cte AS (
    -- Anchor member
    SELECT col1, col2, ...
    FROM table_name
    WHERE condition
    UNION ALL -- Remember to UNION ALL
    -- Recursive member
    SELECT col1, col2, ...
    FROM table_name
    JOIN cte ON condition
    WHERE condition -- Terminating Criteria
)

SELECT * FROM cte;
- LC 1336. Number of Transactions per Visit
- LC 1384. Total Sales Amount by Year
- LC 1635. Hopper Company Queries I
- LC 1645. Hopper Company Queries II
- LC 1651. Hopper Company Queries III
- LC 1767. Find the Subtasks that Did Not Execute
- LC 2153. The Number of Passengers in Each Bus II ***

2. WINDOW FUNCTIONS
Window functions in SQL are powerful tools for performing complex calculations across sets of rows related to the current row. Unlike standard aggregate functions, window functions do not collapse rows; they allow for the computation of values across rows while still retaining individual row identity. Here’s a concise overview:

Aggregate Functions:
Used within a window function, they compute values over a set of rows. These include avg, count, max, min & sum.

Ranking Functions:
Assign ranks to rows within a partition:

ROW_NUMBER(): Assigns a unique number to each row starting from 1.
RANK(): Assigns the same rank to identical values, with gaps in subsequent ranks.
DENSE_RANK(): Similar to RANK(), but without gaps.
Analytic Functions:
Provide insights into row values without aggregation:

LEAD() and LAG(): Access subsequent or preceding row values.
FIRST_VALUE() and LAST_VALUE(): Get the first or last value in a partition.
NTH_VALUE(): Fetches the Nth value in a window.
NTILE(): Divides the partition into a specified number of ranked groups.
Window Frame:
Specifies the set of rows used for each calculation through ROWS, RANGE, or GROUPS between defined bounds:

UNBOUNDED PRECEDING and UNBOUNDED FOLLOWING: Includes all rows from the start or end of the partition.
N PRECEDING and N FOLLOWING: Considers rows within a specified distance before or after the current row.
CURRENT ROW: The current row's boundary for calculation.
Below is the SQL Window Function Cheat Sheet from learnsql.com.

Press enter or click to view image in full size

Press enter or click to view image in full size

[Ranking Functions]
- LC 185. Department Top 3 Salaries
- LC 1159. Market Analysis II
- LC 1369. Get the Second Most Recent Activity
- LC 1412. Find the Quiet Students in All Exams
- LC 1972. First & Last Call on the Same Day
- LC 2010. The Number of Seniors & Juniors to Join the Company II
- LC 2362. Generate the Invoice
- LC 2720. Popularity Percentage
- LC 2793. Status of Flight Tickets
- LC 2995. Viewers Turned Streamers
- LC 1194. Tournament Winners ***

[Analytics Functions — Lead/Lag]
- LC 579. Find Cumulative Salary of an Employee
- LC 601. Human Traffic of Stadium
- LC 1651. Hopper Company Queries III

[Window Frame]
- LC 2494. Merge Overlapping Events in the Same Hall ***

3. Using Min/Max for Pivoting
Pivoting with aggregates, particularly using MIN and MAX functions, is a versatile SQL technique for transforming rows into columns, enabling a more structured and readable presentation of data. This method is especially useful for summarizing and comparing grouped data across different categories or time periods.

The MIN/MAX trick in pivoting involves two main steps:

Grouping and Ranking: First, data is organized into groups (e.g., by category, date, or another criterion) and optionally ranked or ordered within those groups. This can involve window functions like ROW_NUMBER(), RANK(), or others to prepare the data for pivoting.
Pivoting with Aggregates: Next, the pivot itself is performed by selecting the minimum or maximum value within each group for a specific field. This is typically done in a SELECT statement using CASE expressions to differentiate between the groups and apply the MIN or MAX function accordingly.
- LC 618. Students Report by Geography
- LC 2991. Top Three Wineries

4. Special Trick — Gaps & Islands
The “Gaps and Islands” problem in SQL refers to a class of problems that involve finding ranges of continuous data (“islands”) and breaks between these ranges (“gaps”) within a dataset. This problem is commonly encountered in datasets that include sequences of numbers, dates, or times where the objective is to group or separate these sequences based on their continuity or discontinuity.

Understanding Gaps and Islands
Gaps: These are discontinuities in the data. For example, if you have a sequence of dates or numbers, a gap would be a missing date or number that breaks the sequence.
Islands: These are continuous ranges of data without any gaps. In a sequence of dates, an island would be a consecutive series of dates without any breaks.
Gaps and Islands problems often arise in scenarios like:

Analyzing periods of activity or inactivity in time-series data (e.g., login sessions).
Grouping consecutive numerical data (e.g., order numbers, event IDs).
Identifying continuous availability or booking slots.
Solving Gaps and Islands problems typically involves using SQL window functions to compare rows against their preceding or following rows. The key is to identify when a gap starts or an island ends. Some common approaches include:

ROW_NUMBER() and DENSE_RANK(): These functions can be used to generate sequences that help in identifying and separating islands.
LAG() and LEAD(): These functions compare the current row with the previous or next row, respectively, to detect discontinuities.
Differences in Sequences: Calculating the difference between the current row’s value and a sequence number can help identify gaps and islands.
- LC 601. Human Traffic of Stadium
- LC 1225. Report Contiguous Dates
- LC 2173. Longest Winning Streak
- LC 2701. Consecutive Transactions with Increasing Amounts
- LC 2752. Customers with Maximum Number of Transactions on Consecutive Days

5. Date Functions
SQL date functions are essential tools for manipulating and analyzing data that contains dates and times. These functions allow you to extract specific components from a date, perform calculations with dates, and format dates for display or further processing. Here’s an overview of some of the most commonly used SQL date functions:

WEEKDAY: Monday (0) — Sunday (6)
WEEK(date_col)
DATE_ADD(date_col, INTERVAL n DAY)
DAY_SUB(date_col, INTERVAL n DAY)
DATEDIFF(date_1, date_2) = date_1 — date_2
EXTRACT(YEAR FROM date_col)
- LC 1097. Game Play Analysis V
- LC 1479. Sales by Day of the Week
- LC 2993. Friday Purchases I
- LC 2994. Friday Purchases II

6. CONCAT & GROUP_CONCAT
In SQL, the functions CONCAT and GROUP_CONCAT (or their equivalents, depending on the SQL dialect) are used to merge strings from multiple columns or rows, respectively. These functions are essential for data manipulation, especially when working with textual data or generating formatted output directly from queries.

The CONCAT function merges two or more string values into a single string. It is straightforward and works by taking multiple string arguments and combining them in order.

CONCAT(str_1, str_2, ..., str_N)
The GROUP_CONCAT function is used to concatenate values from multiple rows into a single string within a group. This is particularly useful in queries that involve grouping results with GROUP BY.

GROUP_CONCAT(  [DISTINCT] expression 
               [ORDER BY some_column] 
               [SEPARATOR 'separator_string']
             )

GROUP_CONCAT(email SEPARATOR '; ') 
- LC 2118. Build the Equation
- LC 2199. Finding the Topic of Each Post
- LC 2991. Top Three Wineries

7. JOIN — SELF JOIN || JOINING WITH ON VS WHERE
A self join is a technique used in SQL to join a table to itself. It’s particularly useful for comparing rows within the same table or querying hierarchical data, such as finding all pairs of employees who work in the same department. To perform a self join, you typically assign different aliases to the table in the FROM clause to distinguish the two instances of the same table in the query.

- LC 2474. Customers with Strictly Increasing Purchases

In SQL, the distinction between using the ON clause and the WHERE clause for joins is crucial, particularly when applying filters like date conditions. Here’s a concise overview:

Filtering with ON in Outer Joins: Preserves all rows from the primary table, using conditions to manage matches from the secondary table. Essential for analyses where maintaining a complete dataset from one table is critical.
Filtering with WHERE: Applies conditions to the combined dataset, which can exclude unmatched rows from the primary table, affecting the inclusivity of your results.
Example: Filtering for the year after a join, using WHERE YEAR(order_date) = 2019, excludes users without 2019 orders from the results. Conversely, incorporating the filter directly in a LEFT JOIN, as in LEFT JOIN Orders AS o ON u.user_id = o.buyer_id
AND YEAR(order_date) = '2019', ensures users without matching 2019 orders still appear in the results, with a null order_date, thus maintaining a row in the final results, potentially with a count of 0.

SELECT 
  u.user_id AS buyer_id, u.join_date, 
  IFNULL(COUNT(order_id), 0) AS orders_in_2019
FROM users u LEFT JOIN Orders o ON o.buyer_id = u.user_id 
AND YEAR(order_date) = 2019
GROUP BY 1, 2
- LC 1158. Market Analysis I
- LC 1159. Market Analysis II

Worth Mentioning
- LC 571. Find Median Given Frequency of Numbers
- LC 1917. Leetcodify Friends Recommendations
[ STORED PROCEDURE]
- LC 2252. Dynamic Pivoting of a Table
- LC 2253. Dynamic Unpivoting of a Table

P/S: The triple asterisk (***) indicates a higher difficulty level, taking me longer to solve than other hard questions.
🥇 Question 1: The Classic Second Highest Salary
The Challenge: Find the employee with the second highest salary without using LIMIT or OFFSET.

Why It Matters: This tests your ranking skills and alternative thinking. Every data team needs someone who can find “second best” scenarios — second highest sales, second most active users, etc.

The Magic Solution:

SELECT DISTINCT salary
FROM (
    SELECT salary, 
           DENSE_RANK() OVER (ORDER BY salary DESC) as rank
    FROM employees
) ranked
WHERE rank = 2;
Pro Tip: Use DENSE_RANK() instead of RANK() to handle salary ties properly! 🎯

🔥 Question 2: Consecutive Login Streak
The Challenge: Find users who logged in for 3 or more consecutive days.

Why It’s Gold: User engagement analysis is HUGE in data roles. Companies want to know who their most loyal users are and identify patterns in user behavior.

The Brilliant Approach:

WITH login_groups AS (
    SELECT user_id, 
           login_date,
           DATE_SUB(login_date, INTERVAL 
               ROW_NUMBER() OVER (PARTITION BY user_id ORDER BY login_date) DAY
           ) as group_identifier
    FROM user_logins
),
streak_counts AS (
    SELECT user_id, 
           COUNT(*) as consecutive_days
    FROM login_groups
    GROUP BY user_id, group_identifier
)
SELECT user_id
FROM streak_counts
WHERE consecutive_days >= 3;
The Secret: Subtract row numbers from dates to group consecutive sequences! 🧠

💰 Question 3: Running Total with Reset Condition
The Challenge: Calculate running sales total that resets to zero whenever there’s a return (negative amount).

Why Interviewers Love It: Real business logic! E-commerce and finance teams deal with this constantly — tracking cumulative values that reset under certain conditions.

The Smart Solution:

WITH reset_groups AS (
    SELECT *,
           SUM(CASE WHEN amount < 0 THEN 1 ELSE 0 END) 
           OVER (ORDER BY date ROWS UNBOUNDED PRECEDING) as reset_group
    FROM transactions
)
SELECT transaction_id,
       date,
       amount,
       SUM(CASE WHEN amount > 0 THEN amount ELSE 0 END) 
       OVER (PARTITION BY reset_group ORDER BY date) as running_total
FROM reset_groups;
🕳️ Question 4: Finding Missing Sequential Numbers
The Challenge: Identify gaps in a sequence of IDs (like missing order numbers).

Why It’s Critical: Data quality issues are everywhere! Missing records can indicate system problems, and finding gaps is essential for data integrity checks.

The Detective Work:

WITH expected_sequence AS (
    SELECT generate_series(
        (SELECT MIN(order_id) FROM orders),
        (SELECT MAX(order_id) FROM orders)
    ) as expected_id
)
SELECT expected_id as missing_order_id
FROM expected_sequence e
LEFT JOIN orders o ON e.expected_id = o.order_id
WHERE o.order_id IS NULL;
📊 Question 5: Pivot Table Without PIVOT Function
The Challenge: Transform rows to columns manually (convert monthly sales data to show each month as a column).

Why It’s Powerful: Not all databases support PIVOT, and understanding the manual approach shows deep SQL knowledge. Plus, dynamic reporting often requires this skill.

The Transformation:

SELECT 
    product_id,
    SUM(CASE WHEN MONTH(sale_date) = 1 THEN amount ELSE 0 END) as Jan,
    SUM(CASE WHEN MONTH(sale_date) = 2 THEN amount ELSE 0 END) as Feb,
    SUM(CASE WHEN MONTH(sale_date) = 3 THEN amount ELSE 0 END) as Mar,
    SUM(CASE WHEN MONTH(sale_date) = 4 THEN amount ELSE 0 END) as Apr
FROM sales
GROUP BY product_id;
🎯 Question 6: Top N Per Group
The Challenge: Find the top 3 highest-paid employees in each department.

Why It’s Essential: Every business needs to analyze top performers, best-selling products, or highest-value customers within different categories.

The Ranking Magic:

SELECT department, employee_name, salary
FROM (
    SELECT department, 
           employee_name, 
           salary,
           ROW_NUMBER() OVER (PARTITION BY department ORDER BY salary DESC) as rank
    FROM employees
) ranked
WHERE rank <= 3;
🔄 Question 7: Self-Join for Hierarchical Data
The Challenge: Find all employees and their managers’ names from an employee table that references itself.

Why It Matters: Organizational data, category hierarchies, and parent-child relationships are everywhere in business data.

The Relationship Builder:

SELECT 
    e.employee_name as employee,
    m.employee_name as manager
FROM employees e
LEFT JOIN employees m ON e.manager_id = m.employee_id;
📈 Question 8: Moving Average Calculation
The Challenge: Calculate a 7-day moving average of daily sales.

Why It’s Valuable: Smoothing out data trends is crucial for forecasting and identifying patterns in noisy data.

The Smooth Operator:

SELECT 
    sale_date,
    daily_sales,
    AVG(daily_sales) OVER (
        ORDER BY sale_date 
        ROWS BETWEEN 6 PRECEDING AND CURRENT ROW
    ) as moving_avg_7_days
FROM daily_sales
ORDER BY sale_date;
🎪 Question 9: Complex Date Calculations
The Challenge: Find customers who made purchases in consecutive months.

Why It’s Tricky: Date logic trips up many candidates, but it’s fundamental for subscription analysis, customer lifecycle tracking, and retention studies.

The Time Traveler:

WITH monthly_purchases AS (
    SELECT customer_id,
           DATE_TRUNC('month', purchase_date) as purchase_month
    FROM purchases
    GROUP BY customer_id, DATE_TRUNC('month', purchase_date)
),
month_sequences AS (
    SELECT customer_id,
           purchase_month,
           LAG(purchase_month) OVER (PARTITION BY customer_id ORDER BY purchase_month) as prev_month
    FROM monthly_purchases
)
SELECT DISTINCT customer_id
FROM month_sequences
WHERE purchase_month = prev_month + INTERVAL '1 month';
🏆 Question 10: Recursive CTE for Organizational Hierarchy
The Challenge: Show the complete reporting chain from CEO down to all employees.

Why It’s Advanced: Recursive queries demonstrate sophisticated SQL knowledge and are essential for hierarchical data analysis.

The Hierarchy Explorer:

WITH RECURSIVE org_hierarchy AS (
    -- Base case: CEO (no manager)
    SELECT employee_id, employee_name, manager_id, 1 as level
    FROM employees
    WHERE manager_id IS NULL
    
    UNION ALL
    
    -- Recursive case: employees with managers
    SELECT e.employee_id, e.employee_name, e.manager_id, oh.level + 1
    FROM employees e
    JOIN org_hierarchy oh ON e.manager_id = oh.employee_id
)
SELECT * FROM org_hierarchy
ORDER BY level, employee_name;
💡 Question 11: Conditional Aggregation
The Challenge: Calculate different metrics based on conditions in a single query.

Why It’s Smart: Efficient reporting often requires multiple calculations in one pass, reducing query complexity and improving performance.

The Multi-Calculator:

SELECT 
    product_category,
    COUNT(*) as total_orders,
    COUNT(CASE WHEN order_status = 'completed' THEN 1 END) as completed_orders,
    COUNT(CASE WHEN order_status = 'cancelled' THEN 1 END) as cancelled_orders,
    AVG(CASE WHEN order_status = 'completed' THEN order_amount END) as avg_completed_amount
FROM orders
GROUP BY product_category;
🔍 Question 12: Finding Duplicate Records
The Challenge: Identify and handle duplicate customer records based on email and phone.

Why It’s Real: Data quality issues are everywhere, and deduplication is a daily task for data engineers.

The Duplicate Detective:

-- Find duplicates
SELECT email, phone, COUNT(*) as duplicate_count
FROM customers
GROUP BY email, phone
HAVING COUNT(*) > 1;

-- Remove duplicates (keep latest)
DELETE FROM customers
WHERE customer_id NOT IN (
    SELECT MAX(customer_id)
    FROM customers
    GROUP BY email, phone
);
📅 Question 13: Date Range Overlaps
The Challenge: Find overlapping date ranges in a booking system.

Why It’s Practical: Scheduling conflicts, resource allocation, and availability checking are common business problems.

The Overlap Finder:

SELECT 
    a.booking_id as booking1,
    b.booking_id as booking2,
    a.start_date,
    a.end_date,
    b.start_date,
    b.end_date
FROM bookings a
JOIN bookings b ON a.resource_id = b.resource_id
WHERE a.booking_id < b.booking_id
  AND a.start_date <= b.end_date
  AND b.start_date <= a.end_date;
🎲 Question 14: Percentile Calculations
The Challenge: Find customers in the top 10% by purchase amount.

Why It’s Important: Percentile analysis helps identify high-value customers, outliers, and performance benchmarks.

The Percentile Pro:

SELECT customer_id,
       total_purchases,
       PERCENT_RANK() OVER (ORDER BY total_purchases) as percentile_rank
FROM (
    SELECT customer_id, SUM(purchase_amount) as total_purchases
    FROM purchases
    GROUP BY customer_id
) customer_totals
WHERE PERCENT_RANK() OVER (ORDER BY total_purchases) >= 0.9;
🔗 Question 15: Complex JOIN Scenarios
The Challenge: Find customers who bought Product A but never bought Product B.

Why It’s Clever: This tests your understanding of different JOIN types and set operations — crucial for customer segmentation and behavior analysis.

The Exclusion Expert:

SELECT DISTINCT c.customer_id, c.customer_name
FROM customers c
JOIN purchases p1 ON c.customer_id = p1.customer_id
JOIN products pr1 ON p1.product_id = pr1.product_id
LEFT JOIN (
    SELECT DISTINCT p2.customer_id
    FROM purchases p2
    JOIN products pr2 ON p2.product_id = pr2.product_id
    WHERE pr2.product_name = 'Product B'
) product_b_buyers ON c.customer_id = product_b_buyers.customer_id
WHERE pr1.product_name = 'Product A'
  AND product_b_buyers.customer_id IS NULL;
📊 Question 16: Dynamic SQL for Reporting
The Challenge: Create a query that shows sales by different time periods (daily, weekly, monthly) in one result.

Why It’s Advanced: Flexible reporting requirements often need dynamic grouping and multiple aggregation levels.

The Time Bender:

SELECT 
    'Daily' as period_type,
    DATE(sale_date) as period,
    SUM(amount) as total_sales
FROM sales
GROUP BY DATE(sale_date)

UNION ALL
SELECT 
    'Weekly' as period_type,
    DATE_TRUNC('week', sale_date) as period,
    SUM(amount) as total_sales
FROM sales
GROUP BY DATE_TRUNC('week', sale_date)
UNION ALL
SELECT 
    'Monthly' as period_type,
    DATE_TRUNC('month', sale_date) as period,
    SUM(amount) as total_sales
FROM sales
GROUP BY DATE_TRUNC('month', sale_date)
ORDER BY period_type, period;
🎯 Question 17: Cohort Analysis
The Challenge: Analyze customer retention by signup month.

Why It’s Gold: Cohort analysis is fundamental for understanding user lifecycle, churn rates, and business growth patterns.

The Cohort Master:

WITH customer_cohorts AS (
    SELECT 
        customer_id,
        DATE_TRUNC('month', signup_date) as cohort_month,
        DATE_TRUNC('month', purchase_date) as purchase_month
    FROM customers c
    JOIN purchases p ON c.customer_id = p.customer_id
),
cohort_data AS (
    SELECT 
        cohort_month,
        purchase_month,
        COUNT(DISTINCT customer_id) as customers,
        EXTRACT(MONTH FROM AGE(purchase_month, cohort_month)) as period_number
    FROM customer_cohorts
    GROUP BY cohort_month, purchase_month
)
SELECT 
    cohort_month,
    period_number,
    customers,
    FIRST_VALUE(customers) OVER (PARTITION BY cohort_month ORDER BY period_number) as cohort_size,
    ROUND(customers * 100.0 / FIRST_VALUE(customers) OVER (PARTITION BY cohort_month ORDER BY period_number), 2) as retention_rate
FROM cohort_data
ORDER BY cohort_month, period_number;
🚀 Question 18: Performance Optimization
The Challenge: Optimize a slow query that finds customers with no purchases in the last 6 months.

Why It’s Critical: Query performance directly impacts business operations, and optimization skills separate senior engineers from juniors.

The Speed Demon:

-- Slow approach (avoid this)
SELECT c.customer_id, c.customer_name
FROM customers c
WHERE NOT EXISTS (
    SELECT 1 FROM purchases p
    WHERE p.customer_id = c.customer_id
    AND p.purchase_date >= CURRENT_DATE - INTERVAL '6 months'
);

-- Optimized approach
SELECT c.customer_id, c.customer_name
FROM customers c
LEFT JOIN (
    SELECT DISTINCT customer_id
    FROM purchases
    WHERE purchase_date >= CURRENT_DATE - INTERVAL '6 months'
) recent_buyers ON c.customer_id = recent_buyers.customer_id
WHERE recent_buyers.customer_id IS NULL;
🎪 Question 19: Advanced Window Functions
The Challenge: Calculate the difference between each employee’s salary and the department average.

Why It’s Sophisticated: Window functions with multiple aggregation levels show advanced analytical thinking.

The Window Wizard:

SELECT 
    employee_name,
    department,
    salary,
    AVG(salary) OVER (PARTITION BY department) as dept_avg_salary,
    salary - AVG(salary) OVER (PARTITION BY department) as salary_diff_from_avg,
    CASE 
        WHEN salary > AVG(salary) OVER (PARTITION BY department) THEN 'Above Average'
        WHEN salary < AVG(salary) OVER (PARTITION BY department) THEN 'Below Average'
        ELSE 'Average'
    END as performance_category
FROM employees;
🏁 Question 20: The Ultimate Challenge — Data Pipeline Logic
The Challenge: Create a complete data transformation that combines multiple business rules.

Why It’s The Final Boss: This tests everything — JOINs, aggregations, window functions, conditional logic, and business understanding.

The Grand Finale:

WITH customer_metrics AS (
    SELECT 
        c.customer_id,
        c.customer_name,
        c.signup_date,
        COUNT(p.purchase_id) as total_purchases,
        SUM(p.amount) as total_spent,
        MAX(p.purchase_date) as last_purchase_date,
        AVG(p.amount) as avg_purchase_amount
    FROM customers c
    LEFT JOIN purchases p ON c.customer_id = p.customer_id
    GROUP BY c.customer_id, c.customer_name, c.signup_date
),
customer_segments AS (
    SELECT *,
        CASE 
            WHEN total_spent >= 1000 AND total_purchases >= 5 THEN 'VIP'
            WHEN total_spent >= 500 OR total_purchases >= 3 THEN 'Regular'
            WHEN total_purchases > 0 THEN 'Occasional'
            ELSE 'Inactive'
        END as customer_segment,
        CASE 
            WHEN last_purchase_date >= CURRENT_DATE - INTERVAL '30 days' THEN 'Active'
            WHEN last_purchase_date >= CURRENT_DATE - INTERVAL '90 days' THEN 'At Risk'
            WHEN last_purchase_date IS NOT NULL THEN 'Churned'
            ELSE 'Never Purchased'
        END as activity_status
    FROM customer_metrics
)
SELECT 
    customer_segment,
    activity_status,
    COUNT(*) as customer_count,
    AVG(total_spent) as avg_total_spent,
    AVG(total_purchases) as avg_total_purchases
FROM customer_segments
GROUP BY customer_segment, activity_status
ORDER BY customer_segment, activity_status;
🎉 Your Next Steps to Interview Success
Congratulations! You’ve just mastered 20 SQL scenarios that will make you stand out in any data interview. But here’s the secret sauce — practice these with real data.

Pro Tips for Interview Day:

🗣️ Talk through your thought process — Interviewers love seeing how you think
🎯 Ask clarifying questions — Show you understand business context
⚡ Mention performance considerations — Discuss indexes, query optimization
🔄 Suggest alternative approaches — Show you know multiple solutions
WITH RECURSIVE exploded_sales AS (
	SELECT product_id,period_start,period_end,average_daily_sales
	FROM sales_1384
	UNION
	SELECT product_id,period_start+1 AS period_start,period_end,average_daily_sales 
	FROM exploded_sales
	WHERE period_start < period_end
)
SELECT es.product_id,p.product_name,EXTRACT(YEAR FROM period_start) AS report_year,SUM(average_daily_sales) 
FROM exploded_sales es
INNER JOIN product_1384 p ON es.product_id=p.product_id
GROUP BY es.product_id,p.product_name,EXTRACT(YEAR FROM period_start)
ORDER BY es.product_id,report_year;


WITH International_Call AS 
(
  SELECT SUM(
              CASE WHEN caller.country_id !=  receiver.country_id THEN 1.0
              ELSE 0.0 END
            ) AS Number_internationals
  FROM phone_calls AS pc
  INNER JOIN phone_info AS caller
  ON pc.caller_id = caller.caller_id
  INNER JOIN phone_info AS receiver
  ON pc.receiver_id = receiver.caller_id
)


SELECT ROUND(
               (Number_internationals * 100 / (SELECT COUNT(*) FROM phone_calls))
               , 1
            )
FROM International_Call
SELECT COUNT(CASE WHEN device_type = 'laptop' THEN 1 END) laptop_views,
       COUNT(CASE WHEN device_type IN ('tablet', 'phone') THEN 1 END) mobile_views
FROM viewership;
SELECT profile_id
FROM personal_profiles pf 
INNER JOIN company_pages  cp ON  pf.employer_id =  cp.company_id
WHERE pf.followers > cp.followers
ORDER BY profile_id;
WITH CTE AS
(
  SELECT searches
  FROM search_frequency
  GROUP BY searches, GENERATE_SERIES(1, num_users)
)

SELECT ROUND(PERCENTILE_CONT(0.50) WITHIN GROUP(ORDER BY searches)::DECIMAL, 1) AS median
FROM CTE;
SELECT COUNT(*) AS nyc_count
FROM phone_info AS pf
INNER JOIN phone_calls AS pc
ON pf.caller_id = pc.caller_id OR pf.caller_id = pc.receiver_id
WHERE LEFT(phone_number, 6) = '+1-212';
WITH CT AS
(

SELECT measurement_id, CAST(measurement_time AS DATE) AS measurement_day, measurement_value, 
ROW_NUMBER() OVER(PARTITION BY(CAST(measurement_time AS DATE)) ORDER BY measurement_id) AS RN
FROM measurements
)

SELECT measurement_day,
SUM(CASE WHEN RN % 2 != 0 THEN measurement_value END) AS odd_sum,
SUM(CASE WHEN RN % 2 = 0 THEN measurement_value END) AS even_sum
FROM CT
GROUP BY measurement_day;
SELECT P.page_id 
FROM pages P
WHERE P.page_id NOT IN
  (
    SELECT PL.page_id FROM page_likes PL
  );

-- Another Solution
SELECT page_id
FROM pages
EXCEPT
SELECT page_id
FROM page_likes
ORDER BY page_id

SELECT COUNT(cnt) AS member_count
FROM 
(
  SELECT COUNT(policy_holder_id) AS cnt
  FROM callers
  GROUP BY policy_holder_id
  HAVING COUNT(policy_holder_id) >= 3
) AS TEMP
SELECT  drug,
       (total_sales - cogs) AS total_profit
FROM pharmacy_sales
ORDER BY total_profit DESC
LIMIT 3;
SELECT manufacturer,
       COUNT(drug) AS drug_count,
       ABS(SUM(total_sales - cogs)) AS total_loss
FROM pharmacy_sales
WHERE total_sales <= cogs
GROUP BY manufacturer
ORDER BY total_loss DESC
  

SELECT manufacturer,
       CONCAT('$', total_result, ' million') AS sale
FROM
(
  SELECT manufacturer,
         ROUND( SUM(total_sales) / 1000000) AS total_result
  FROM pharmacy_sales
  GROUP BY manufacturer
  ORDER BY SUM(total_sales) DESC, manufacturer
) AS TP

SELECT E.user_id 
FROM texts T INNER JOIN emails E ON E.email_id = T.email_id
WHERE T.signup_action = 'Confirmed' AND DATE(T.action_date) - DATE(E.signup_date) = 1;
WITH GP AS
(
  SELECT age_bucket, 
  SUM(CASE WHEN activity_type = 'open' THEN time_spent END) open,
  SUM(CASE WHEN activity_type = 'send' THEN time_spent END) send
  FROM activities, age_breakdown
  WHERE age_breakdown.user_id = activities.user_id
  GROUP BY age_bucket
)


SELECT GP.age_bucket, 
ROUND( send / (SUM(open) + SUM(send)) * 100.0, 2) AS send_perc,
ROUND( open / (SUM(open) + SUM(send)) * 100.0, 2) AS open_perc
FROM GP
GROUP BY GP.age_bucket, open, send
ORDER BY age_bucket;
SELECT ROUND
       (
          SUM(CASE WHEN signup_action = 'Confirmed' THEN 1.0 ELSE 0.0 END) / 
          COUNT(emails.email_id)
          , 2
       )
FROM texts
LEFT JOIN emails
USING(email_id) 
WITH CT AS
(
 SELECT datacenter_id, SUM(monthly_demand) AS sum_cap
 FROM forecasted_demand
 GROUP BY datacenter_id
)

SELECT CT.datacenter_id, dc.monthly_capacity - CT.sum_cap
FROM CT, datacenters dc
WHERE CT.datacenter_id = dc.datacenter_id
ORDER BY CT.datacenter_id
WITH SuperCloud AS
(

  SELECT customer_id, 
         COUNT(DISTINCT products.product_category) AS NumberOfProducts
  FROM customer_contracts
  RIGHT JOIN products
  ON customer_contracts.product_id = products.product_id
  GROUP BY customer_id
)

SELECT customer_id
FROM SuperCloud
WHERE NumberOfProducts = 
        (
          SELECT COUNT(DISTINCT product_category) 
          FROM products
        )
SELECT sender_id, COUNT(DISTINCT receiver_id)  AS message_count
FROM messages  m1
WHERE DATE_PART('year', sent_date) = 2022
GROUP BY sender_id
ORDER BY message_count DESC
LIMIT 2;
WITH CTE AS
(
  
  SELECT artist_name,
         COUNT(GSR.song_id) AS count_songs
         
  FROM global_song_rank AS GSR
  INNER JOIN songs
  ON songs.song_id = GSR.song_id
  INNER JOIN artists
  ON songs.artist_id = artists.artist_id
  
  WHERE rank <= 10
  GROUP BY artist_name
)

SELECT *
FROM 
(
  SELECT artist_name, 
        DENSE_RANK() OVER(ORDER BY count_songs DESC) AS artist_rank
  FROM CTE
) AS TP

WHERE artist_rank <= 5





SELECT SUM(CASE WHEN  review_stars >= 4 THEN 1 ELSE 0 END) AS business_count, 
SUM(CASE WHEN  review_stars >= 4 THEN 1 ELSE 0 END) * 100 / COUNT(*) AS  top_rated_pct
FROM reviews

-- T-SQL Solution
WITH CTE AS
(
  SELECT department_name,
         name,
         salary,
         DENSE_RANK() OVER(PARTITION BY department_name ORDER BY salary DESC)  AS RN
  FROM employee AS E
  JOIN department AS D
  ON E.department_id = D.department_id
)

SELECT department_name,
       name,
       salary
FROM CTE
WHERE RN IN (1, 2, 3)
ORDER BY department_name, salary DESC, name;
SELECT user_id,
       tweet_date,
       ROUND(
              AVG(tweet_count) 
              OVER(PARTITION BY user_id ORDER BY tweet_date ROWS BETWEEN 2 PRECEDING AND CURRENT ROW)
              , 2
           )
FROM tweets;
SELECT part, 
       assembly_step
FROM parts_assembly
WHERE finish_date IS NULL;

SELECT user_id, spend, transaction_date
FROM (
      SELECT *,ROW_NUMBER () OVER (PARTITION BY user_id ORDER BY transaction_date) AS RD
      FROM transactions
     ) AS T
     
WHERE RD = 3
SELECT emp.employee_id,
       emp.name
FROM employee AS emp
INNER JOIN employee AS mang
ON emp.manager_id = mang.employee_id AND emp.salary > mang.salary;

WITH CTE AS
(
  SELECT DATE_PART('year', transaction_date) AS year,
         product_id,
         spend AS curr_year_spend,
         LAG(spend) OVER(PARTITION BY product_id ORDER BY transaction_date) AS prev_year_spend

  FROM user_transactions
)

SELECT *, ROUND( (curr_year_spend - prev_year_spend) / prev_year_spend * 100, 2) AS yoy_rate
FROM CTE
SELECT firstname,lastname,city,state
FROM person_175 p 
LEFT JOIN address_175 a ON p.personid = a.personid;

SELECT DISTINCT salary 
FROM employee_176
ORDER BY salary DESC
LIMIT 1 
OFFSET 1;

SELECT DISTINCT salary 
FROM employee_176
ORDER BY salary DESC
LIMIT 1 
OFFSET N-1;

SELECT score,
        DENSE_RANK() OVER(w) as rank
FROM scores_178
WINDOW w AS (ORDER BY score DESC);

-----------------------------------------------------------
--Solution 1 :
-----------------------------------------------------------
WITH cte AS(
	SELECT id,Num,
		LEAD(Num,1) OVER() as Next1,
		LEAD(Num,2) OVER() as Next2
	FROM logs_180
)
SELECT DISTINCT Num AS ConsecutiveNums
FROM cte
WHERE Num = Next1 AND Num = Next2;


-----------------------------------------------------------
--Solution 2 :
-----------------------------------------------------------

WITH cte AS(
	SELECT id,Num,
		LAG(Num) OVER() as Prev,
		LEAD(Num) OVER() as Next
	FROM logs_180
)
SELECT DISTINCT Num AS ConsecutiveNums
FROM cte
WHERE Num = Prev AND Num = Next;


-----------------------------------------------------------
--Solution 3 :
-----------------------------------------------------------
SELECT DISTINCT l1.Num AS ConsecutiveNums
FROM logs_180 l1
JOIN logs_180 l2 ON l1.id=l2.id-1 AND l1.Num=l2.Num
JOIN logs_180 l3 ON l1.id=l3.id-2 AND l2.Num=l3.Num


-----------------------------------------------------------
--Exensible Solution (Best) :
-----------------------------------------------------------

WITH ranked AS (
	SELECT *,
		(id-ROW_NUMBER() OVER (PARTITION BY num ORDER BY id)) AS diff
	FROM logs_180
)
SELECT DISTINCT num AS "ConsecutiveNums"
FROM ranked
GROUP BY diff,num
HAVING COUNT(id) >= 3;

SELECT e.name as Employee
FROM employee_181 e 
JOIN employee_181 m ON e.manager_id = m.id AND e.salary>m.salary;

SELECT email
FROM person_182 
GROUP BY email 
HAVING cnt>2;

SELECT name 
FROM customers_183 
WHERE id NOT IN (SELECT DISTINCT customer_id 
		 FROM orders_183);

SELECT d.name AS Department,e.name AS Employee,e.salary AS Salary
FROM employee_184 e
JOIN department_184 d ON e.department_id = d.id
WHERE (e.department_id,e.salary) IN (SELECT department_id,MAX(salary)
				FROM employee_184
				GROUP BY department_id);

(OR)

WITH cte AS(
SELECT d.name AS Department,e.name AS Employee,
	DENSE_RANK() OVER w rnk
FROM employee_184 e
JOIN department_184 d ON e.department_id = d.id
WINDOW w AS (PARTITION BY d.name ORDER BY e.salary DESC)
)

SELECT Department,Employee
FROM cte 
WHERE rnk = 1;

WITH cte AS(
SELECT d.name AS Department,e.name AS Employee,e.salary AS Salary,
	DENSE_RANK() OVER w rnk
FROM employee_185 e
JOIN department_185 d ON e.department_id = d.id
WINDOW w AS (PARTITION BY d.name ORDER BY e.salary DESC)
)

SELECT Department,Employee,Salary,rnk
FROM cte 
WHERE rnk BETWEEN 1 AND 3;

WITH duplicate_id_higher AS (
	SELECT p1.id AS higher_id FROM person_196_ans p1 JOIN person_196_ans p2 ON p1.email = p2.email AND p1.id > p2.id
)

DELETE FROM person_196_ans WHERE id IN (SELECT higher_id FROM duplicate_id_higher);

SELECT w1.id   
FROM weather_197 w1 
JOIN weather_197 w2 ON w1.record_date-1=w2.record_date AND w1.temperature>w2.temperature;

WITH cancelled AS(
SELECT t.request_at,COUNT(*) AS cancelled_count
FROM trips_262 t
JOIN users_262 c ON t.client_id = c.user_id AND c.banned like 'No'
JOIN users_262 d ON t.driver_id = d.user_id AND d.banned like 'No'
WHERE t.status LIKE 'cancelled_by_client' OR t.status LIKE 'cancelled_by_driver'
GROUP BY t.request_at),

total AS(
SELECT t.request_at,COUNT(*) AS total_count
FROM trips_262 t
JOIN users_262 c ON t.client_id = c.user_id AND c.banned like 'No'
JOIN users_262 d ON t.driver_id = d.user_id AND d.banned like 'No'
GROUP BY t.request_at)

SELECT t.request_at,(COALESCE(c.cancelled_count::FLOAT,0.0)/t.total_count::FLOAT)
FROM cancelled c
RIGHT JOIN total t ON c.request_at = t.request_at;

(OR)

SELECT request_at,ROUND(COUNT(CASE WHEN status <> 'completed' THEN 1 ELSE NULL END)::NUMERIC/COUNT(*),2) AS cancellation_rate
FROM trips_262
WHERE request_at BETWEEN '2013-10-01' AND '2013-10-03' AND
	client_id NOT IN (SELECT user_id FROM users_262 WHERE banned LIKE 'Yes' AND role LIKE 'client') AND
	driver_id NOT IN (SELECT user_id FROM users_262 WHERE banned LIKE 'Yes' AND role LIKE 'driver')
GROUP BY request_at;

SELECT player_id,MIN(event_date) AS first_login
FROM activity_511
GROUP BY player_id
ORDER BY player_id;

(OR)

WITH ranked AS(
	SELECT *,DENSE_RANK() OVER w AS rnk
	FROM activity_511
	WINDOW w AS (PARTITION BY player_id ORDER BY event_date)
)

SELECT player_id,event_date
FROM ranked
WHERE rnk = 1
ORDER BY player_id;

WITH ranked AS(
	SELECT *,DENSE_RANK() OVER w AS rnk
	FROM activity_511
	WINDOW w AS (PARTITION BY player_id ORDER BY event_date)
)

SELECT player_id,device_id
FROM ranked
WHERE rnk = 1
ORDER BY player_id;


(OR)


WITH cte AS(
	SELECT player_id,MIN(event_date) AS first_login
	FROM activity_511
	GROUP BY player_id
)

SELECT player_id,device_id
FROM activity_511
WHERE (player_id,event_date) IN (SELECT * FROM cte);

SELECT a.player_id,a.event_date,SUM(b.games_played)
FROM activity_534 a
JOIN activity_534 b ON a.player_id = b.player_id AND a.event_date >= b.event_date
GROUP BY a.player_id,a.event_date
ORDER BY 1,2;

(OR)

SELECT player_id,event_date,
	SUM(games_played) OVER w AS games_played
FROM activity_534
WINDOW w AS (PARTITION BY player_id ORDER BY event_date);

SELECT ROUND(COUNT(DISTINCT b.player_id)::NUMERIC/COUNT(DISTINCT a.player_id),2)
FROM activity_550 a
LEFT JOIN activity_550 b ON a.player_id = b.player_id AND a.event_date + 1 = b.event_date;

WITH cte AS (
	SELECT id,company,salary,
		ABS(ROW_NUMBER() OVER (PARTITION BY company ORDER BY salary,id) - ROW_NUMBER() OVER (PARTITION BY company ORDER BY salary DESC,id DESC)) AS diff
	FROM employee_569
)
SELECT id,company,salary
FROM cte 
WHERE diff = 0 OR diff = 1;

WITH managers AS(
	SELECT manager_id
	FROM employee_570
	GROUP BY manager_id
	HAVING COUNT(manager_id)>=5
)

SELECT name
FROM employee_570
WHERE id IN (SELECT * FROM managers);

WITH cte AS(
	SELECT *,
		SUM(frequency) OVER w AS e,
		SUM(frequency) OVER () AS t
	FROM numbers_571_tc_2
	WINDOW w AS (ORDER BY number)
),
cte2 AS(	
	SELECT number,frequency,
		CASE WHEN (LAG(e::INT,1) OVER w) IS NULL THEN 1 ELSE (LAG(e::INT,1) OVER w)+1 END AS s,
		e,t
	FROM cte
	WINDOW w AS (ORDER BY number)
)

SELECT ROUND(AVG(number),1)
FROM cte2
WHERE (t::NUMERIC/2 BETWEEN s AND e) OR (t::NUMERIC/2+1 BETWEEN s AND e);

--------------------------- OR ---------------------------

WITH RECURSIVE cte AS (
	SELECT number,frequency,1 AS cnt
	FROM numbers_571
	UNION ALL
	SELECT number,frequency,cnt+1 AS cnt
	FROM cte
	WHERE cnt < frequency
),
cte2 AS (
	SELECT number,
		ROW_NUMBER() OVER (ORDER BY number) AS a,
		COUNT(*) OVER () c
	FROM cte
)
SELECT ROUND(AVG(number),1) 
FROM cte2
WHERE a BETWEEN (SELECT CEIL(AVG(c)::NUMERIC/2) FROM cte2) AND (SELECT CEIL((AVG(c)+1::NUMERIC)/2) FROM cte2)

--------------------------- OR ---------------------------

WITH RECURSIVE cte AS (
	SELECT number,frequency,1 AS cnt
	FROM numbers_571
	UNION ALL
	SELECT number,frequency,cnt+1 AS cnt
	FROM cte
	WHERE cnt < frequency
),
cte2 AS (
	SELECT number,
		ROW_NUMBER() OVER (ORDER BY number) AS a
	FROM cte
),
cte3 AS (
	SELECT number,a,
		ROW_NUMBER() OVER (ORDER BY a DESC) AS d
	FROM cte2	
)
SELECT ROUND(AVG(number),1)
FROM cte3
WHERE ABS(a-d) = 0 OR ABS(a-d) = 1;


SELECT name
FROM candidate_574
WHERE id IN (
	SELECT candidate_id 
	FROM vote_574
	GROUP BY candidate_id
	ORDER BY COUNT(candidate_id) DESC
	LIMIT 1
);

SELECT name,bonus
FROM employee_577 e
JOIN bonus_577 b ON e.empId = b.empId AND b.bonus < 1000;

SELECT question_id
FROM surveylog_578
GROUP BY question_id
ORDER BY COUNT(CASE WHEN action='answer' THEN question_id ELSE NULL END)/COUNT(CASE WHEN action='show' THEN question_id ELSE NULL END) DESC
LIMIT 1;

WITH cte AS(
	SELECT id,month,	
		SUM(salary) OVER w AS salary,
		ROW_NUMBER() OVER w AS row_num,
		COUNT(*) OVER w1 AS count
	FROM employee_579
	WINDOW w AS (PARTITION BY id ORDER BY month),
		w1 AS (PARTITION BY id)
)

SELECT id,month,salary
FROM cte
WHERE row_num<count
ORDER BY id,month DESC;

----------------------------- OR -----------------------------

SELECT e1.id,e1.month,SUM(e2.salary)
FROM employee_579 e1
JOIN employee_579 e2 ON e1.id = e2.id AND e1.month - e2.month BETWEEN 0 AND 2
WHERE (e1.id,e1.month) NOT IN (SELECT id,MAX(MONTH) FROM employee_579 GROUP BY id)
GROUP BY e1.id,e1.month
ORDER BY e1.id,e1.month DESC;

WITH cte AS(
	SELECT dept_id,COUNT(*) AS student_count 
	FROM student_580
	GROUP BY dept_id
)

SELECT dept_name,COALESCE(student_count,0) AS student_count
FROM department_580 d
LEFT JOIN cte c ON c.dept_id = d.dept_id
ORDER BY student_count DESC,dept_name ASC;

SELECT * FROM customer_584 WHERE reference_id <> 21 OR reference_id IS NULL;

SELECT * FROM customer_584
EXCEPT
SELECT * FROM customer_584 WHERE reference_id = 2;

--(1st approach is called co-relevant subquery because inner query is dependent on outer query)

SELECT SUM(tiv_2016) AS tiv_2016_sum 
FROM insurance_585 i
WHERE tiv_2015 IN (SELECT tiv_2015 
		FROM insurance_585
		WHERE pid <> i.pid) AND
	(lat,lon) NOT IN (SELECT lat,lon
		FROM insurance_585
		WHERE pid <> i.pid);


(OR)


SELECT SUM(tiv_2016) AS tiv_2016_sum
FROM insurance_585
WHERE tiv_2015 IN (SELECT tiv_2015
		   FROM insurance_585
		   GROUP BY tiv_2015
		   HAVING COUNT(*) > 1) AND
(lat,lon) IN (SELECT lat,lon
	      FROM insurance_585
	      GROUP BY lat,lon
	      HAVING COUNT(*) = 1);

SELECT customer_number
FROM orders_586
GROUP BY customer_number
ORDER BY COUNT(order_number) DESC
LIMIT 1;


--ANSWER OF EXTRA QUESTION:

WITH cte AS (
SELECT COUNT(order_number) AS count
FROM orders_586
GROUP BY customer_number
ORDER BY count DESC
LIMIT 1)

SELECT customer_number
FROM orders_586
GROUP BY customer_number
HAVING COUNT(order_number) IN (SELECT count FROM cte);

SELECT name,population,area
FROM world_595
WHERE area > 3000000 OR population > 25000000;

SELECT class
FROM courses_596
GROUP BY class
HAVING COUNT(DISTINCT student) >=5;

SELECT ROUND(COUNT(DISTINCT CASE WHEN a.requester_id IS NOT NULL AND a.accepter_id IS NOT NULL THEN CONCAT(a.requester_id,' ',a.accepter_id) END)::NUMERIC/COUNT(DISTINCT CASE WHEN r.sender_id IS NOT NULL AND r.send_to_id IS NOT NULL THEN CONCAT(r.sender_id,' ',r.send_to_id) END ),2) AS accept_rate
FROM friend_request_597 r
LEFT JOIN request_accepted_597 a ON r.sender_id = a.requester_id AND r.send_to_id = a.accepter_id;

SELECT DISTINCT s1.*
FROM stadium_601 s1 
JOIN stadium_601 s2
JOIN stadium_601 s3
ON ((s1.id = s2.id-1 AND s1.id = s3.id-2) OR (s1.id = s2.id+1 AND s1.id = s3.id-1) OR (s1.id = s2.id+1 AND s1.id = s3.id+2))
WHERE s1.people >= 100 AND s2.people >= 100 AND s3.people>=100
ORDER BY visit_date;

------------------------------------------------------------------------------------------------------------------------------------

WITH ranked AS(
	SELECT *,
		ROW_NUMBER() OVER w AS rn,
		(id - ROW_NUMBER() OVER w) AS diff
	FROM stadium_601
	WHERE people>=100
	WINDOW w AS (ORDER BY visit_date)
),
consecutive AS(
	SELECT diff,COUNT(diff) count
	FROM ranked
	GROUP BY diff
)

SELECT id,visit_date,people
FROM ranked r
LEFT JOIN consecutive c ON r.diff = c.diff
WHERE c.count >=3
ORDER BY visit_date;

------------------------------------------------------------------------------------------------------------------------------------

WITH ranked AS (
	SELECT *,
		id-ROW_NUMBER() OVER (ORDER BY id) AS diff
	FROM stadium_601
	WHERE people >= 100
),
consecutives AS (
	SELECT *,
		COUNT(id) OVER (PARTITION BY diff) AS cnt
	FROM ranked
)
SELECT id,visit_date,people
FROM consecutives
WHERE cnt >= 3
ORDER BY visit_date;

WITH cte AS(
	SELECT requester_id AS uid
	FROM request_accepted_602
	UNION ALL
	SELECT accepter_id AS uid
	FROM request_accepted_602
)

SELECT uid,COUNT(uid) num_of_friends
FROM cte
GROUP BY uid
ORDER BY num_of_friends DESC
LIMIT 1;


--Answer of follow-up question:

WITH cte AS(
	SELECT requester_id AS uid
	FROM request_accepted_602
	UNION ALL
	SELECT accepter_id AS uid
	FROM request_accepted_602
)

SELECT uid
FROM cte
GROUP BY uid
HAVING COUNT(uid) = (SELECT COUNT(uid) num_of_friends
		     FROM cte
		     GROUP BY uid
		     ORDER BY num_of_friends DESC
		     LIMIT 1);

SELECT DISTINCT c1.seat_id
FROM cinema_603 c1
INNER JOIN cinema_603 c2 ON ABS(c1.seat_id-c2.seat_id)=1
WHERE c1.free AND c2.free;

-------------------------------------------------------------------------------------------------------------------------------

WITH cte AS(
  	SELECT seat_id,free,
		LEAD(free) OVER() as next_seat,
		LAG(free) OVER() as prev_seat
	FROM cinema_603
)

SELECT DISTINCT seat_id
FROM cte
WHERE (free=1 AND next_seat=1) OR (free=1 AND prev_seat=1);

-------------------------------------------------------------------------------------------------------------------------------

WITH ranked AS (
	SELECT *,
		seat_id-ROW_NUMBER () OVER (ORDER BY seat_id) AS diff
	FROM cinema_603
	WHERE free = 1
),
consecutive_free_seats AS (
	SELECT *,
		COUNT(seat_id) OVER (PARTITION BY diff) AS cnt
	FROM ranked
)
SELECT seat_id
FROM consecutive_free_seats
WHERE cnt >= 2;

WITH cte AS(
	SELECT s.sales_id
	FROM orders_607 o
	JOIN company_607 c ON o.com_id = c.com_id
	JOIN salesperson_607 s ON o.sales_id = s.sales_id
	WHERE c.name like 'RED'
)

SELECT DISTINCT name
FROM salesperson_607
WHERE sales_id NOT IN (SELECT * 
			FROM cte);

SELECT DISTINCT t1.id,
	CASE WHEN t1.p_id IS NULL THEN 'Root'
	     WHEN t2.id IS NULL THEN 'Leaf'
	     ELSE 'Inner' END AS Type
FROM tree_608 t1
LEFT JOIN tree_608 t2 ON t1.id = t2.p_id
ORDER BY t1.id;

SELECT *,
	CASE WHEN x+y>z AND x+z>y AND y+z>x THEN 'Yes' 
	     ELSE 'No' 
	END AS triangle
FROM triangle_610;

SELECT ROUND(MIN(SQRT(POWER(a.x-b.x,2)+POWER(a.y-b.y,2)))::NUMERIC,2) AS shortest
FROM point_2d_612 a
JOIN point_2d_612 b ON (a.x,a.y) <> (b.x,b.y);

SELECT MIN(ABS(ABS(a.x)-ABS(b.x))) AS shortest
FROM point_613 a
JOIN point_613 b ON a.x <> b.x;

SELECT b.followee AS follower,COUNT(b.follower) AS num
FROM follow_614 a
JOIN follow_614 b ON a.follower = b.followee
GROUP BY b.followee;

SELECT DISTINCT TO_CHAR(pay_date,'YYYY-MM') AS pay_month,b.department_id,
	CASE WHEN (AVG(amount) OVER w1) = (AVG(amount) OVER w2) THEN 'same'
	     WHEN (AVG(amount) OVER w1) > (AVG(amount) OVER w2) THEN 'lower'
	     ELSE 'higher'
	END AS comparison
FROM salary_615 a
JOIN employee_615 b ON a.employee_id=b.employee_id
WINDOW w1 AS (PARTITION BY TO_CHAR(pay_date,'YYYY-MM')),
	w2 AS (PARTITION BY TO_CHAR(pay_date,'YYYY-MM'),department_id)
ORDER BY 1;

WITH ranked AS(
	SELECT *,
		ROW_NUMBER() OVER w AS rnk
	FROM student_618
	WINDOW w AS (PARTITION BY continent ORDER BY name)
)
SELECT
	MAX(CASE WHEN continent = 'America' THEN name END) AS America,
	MAX(CASE WHEN continent = 'Europe' THEN name END) AS Europe,
	MAX(CASE WHEN continent = 'Asia' THEN name END) AS Asia
FROM ranked
GROUP BY rnk
ORDER BY rnk;

--Why we need to rank the rows? Without it below will be the result.

SELECT
	CASE WHEN continent = 'America' THEN name END AS America,
	CASE WHEN continent = 'Europe' THEN name END AS Europe,
	CASE WHEN continent = 'Asia' THEN name END AS Asia
FROM student_618;

SELECT num
FROM number_619
GROUP BY num
HAVING COUNT(num) = 1
ORDER BY num DESC
LIMIT 1;

SELECT *
FROM cinema_620
WHERE id%2 = 1 AND description <> 'boring'
ORDER BY rating DESC;

WITH ranked AS(
	SELECT id,student,
		LAG(id) OVER (w) AS lag,
		LEAD(id) OVER (w) AS lead
	FROM seat_626
	WINDOW w AS (ORDER BY id)
)

SELECT 
	CASE WHEN MOD(id,2) = 1 AND lead IS NOT NULL THEN lead
	     WHEN MOD(id,2) = 0 THEN lag
	     ELSE id
	END AS id,
	student
FROM ranked
ORDER BY id;

UPDATE salary_627 
SET sex = (
	CASE WHEN sex = 'm' THEN 'f' 
	     ELSE 'm'
	END
);

SELECT customer_id
FROM customer_1045
GROUP BY customer_id
HAVING COUNT(customer_id) = (SELECT COUNT(product_key) FROM product_1045)
ORDER BY customer_id;

SELECT actor_id,director_id
FROM actor_director_1050
GROUP BY actor_id,director_id
HAVING COUNT(1)>=3;

SELECT p.product_name,s.year,s.price
FROM sales_1068 s
JOIN product_1068 p ON s.product_id = p.product_id;

SELECT product_id,SUM(quantity) AS total_quantity
FROM sales_1068
GROUP BY product_id;

SELECT product_id,year,quantity,price
FROM sales_1068
WHERE (product_id,year) IN (SELECT product_id,MIN(year)
			  FROM sales_1068
			  GROUP BY product_id);

SELECT p.project_id,ROUND(AVG(e.experience_years),2)
FROM project_1075 p
JOIN employee_1075 e ON p.employee_id=e.employee_id
GROUP BY p.project_id
ORDER BY 1;

SELECT project_id
FROM project_1075
GROUP BY project_id
ORDER BY COUNT(employee_id) DESC
LIMIT 1;

SELECT p.project_id,p.employee_id
FROM project_1077 p
JOIN employee_1077 e ON p.employee_id=e.employee_id
WHERE (p.project_id,e.experience_years) IN (SELECT p.project_id,MAX(e.experience_years)
					     FROM project_1077 p
					     JOIN employee_1077 e ON p.employee_id=e.employee_id
					     GROUP BY p.project_id)
ORDER BY 1;


--(OR)


WITH ranked AS(
	SELECT p.project_id,p.employee_id,
		DENSE_RANK() OVER (w) rnk
	FROM project_1077 p
	JOIN employee_1077 e ON p.employee_id=e.employee_id
	WINDOW w AS (PARTITION BY p.project_id ORDER BY e.experience_years DESC)
)

SELECT project_id,employee_id
FROM ranked
WHERE rnk =1;

SELECT seller_id
FROM sales_1082
GROUP BY seller_id
HAVING SUM(price) IN (
			SELECT SUM(price) AS m_sum
			FROM sales_1082
			GROUP BY seller_id
			ORDER BY m_sum DESC 
			LIMIT 1
			);

SELECT DISTINCT s.buyer_id
FROM sales_1082 s
LEFT JOIN product_1082 p ON s.product_id = p.product_id
WHERE p.product_name = 'S8' AND
	s.buyer_id NOT IN (SELECT s.buyer_id
			    FROM sales_1082 s LEFT JOIN product_1082 p ON
			    s.product_id = p.product_id
			    WHERE p.product_name = 'iPhone');

SELECT DISTINCT p.product_id,p.product_name
FROM product_1082 p
INNER JOIN sales_1082 s ON p.product_id = s.product_id AND (s.sale_date BETWEEN '2019-01-01' AND '2019-03-31')
EXCEPT
SELECT DISTINCT p.product_id,p.product_name
FROM product_1082 p
INNER JOIN sales_1082 s ON p.product_id = s.product_id AND (s.sale_date < '2019-01-01' OR s.sale_date > '2019-03-31');

WITH install_dates AS(
	SELECT player_id,MIN(event_date) AS install_date
	FROM activity_1097
	GROUP BY player_id
),
new AS(
	SELECT i.player_id,i.install_date,a.event_date
	FROM install_dates i
	LEFT JOIN activity_1097 a ON i.player_id = a.player_id AND i.install_date + 1 = a.event_date
)

SELECT install_date,COUNT(player_id),ROUND(COUNT(event_date)::NUMERIC/COUNT(player_id),2)
FROM new
GROUP BY install_date;

SELECT b.book_id, b.name
FROM books_1098 b 
LEFT JOIN (
	SELECT book_id, SUM(quantity) nsold
	FROM orders_1098
	WHERE dispatch_date BETWEEN '2018-06-23' AND '2019-06-23'
	GROUP BY book_id
	) o
ON b.book_id = o.book_id
WHERE (o.nsold < 10 OR o.nsold IS NULL) AND '2019-06-23'::DATE-b.available_from > 30;

WITH ranked AS(
	SELECT *,
		ROW_NUMBER() OVER (PARTITION BY user_id ORDER BY activity_date) AS rnk
	FROM traffic_1107
	WHERE activity = 'login'
)
SELECT activity_date,COUNT(DISTINCT user_id)
FROM ranked
WHERE ('2019-06-30'::DATE-activity_date)<=90 AND rnk =1
GROUP BY activity_date;

--(OR)

WITH ranked AS(
	SELECT user_id,MIN(activity_date) AS activity_date
	FROM traffic_1107
	WHERE activity = 'login'
	GROUP BY user_id
)
SELECT activity_date,COUNT(user_id)
FROM ranked
WHERE ('2019-06-30'::DATE-activity_date)<=90
GROUP BY activity_date;


WITH ranked AS(
	SELECT *,
		RANK() OVER (PARTITION BY student_id ORDER BY grade DESC,course_id ASC) AS rnk
	FROM enrollments_1112
)

SELECT student_id,course_id,grade
FROM ranked
WHERE rnk = 1;

SELECT extra,COUNT(DISTINCT post_id)
FROM actions_1113
WHERE extra IS NOT NULL AND action_date = DATE '2019-07-05'-1
GROUP BY extra;

WITH cte AS(
	SELECT *,
		AVG(occurences) OVER(PARTITION BY event_type) AS avg
	FROM events_1126
)

SELECT business_id 
FROM cte
WHERE occurences > avg
GROUP BY business_id
HAVING COUNT(business_id) > 1;

WITH each_day_platform AS(
	SELECT spend_date,UNNEST(ARRAY['both','mobile','desktop']) AS platform_type
	FROM spending_1127
	GROUP BY spend_date
),
cte AS(
	SELECT a.spend_date,
		CASE WHEN b.user_id IS NOT NULL THEN 'both'
		     WHEN a.platform = 'mobile' THEN 'mobile'
		     ELSE 'desktop'
		END AS platform_type,
		COUNT(DISTINCT a.user_id) AS total_users,
		SUM(a.amount) AS amount
	FROM spending_1127 a
	LEFT JOIN spending_1127 b ON a.user_id = b.user_id AND a.spend_date = b.spend_date AND a.platform <> b.platform
	GROUP BY a.spend_date,platform_type
)

SELECT a.spend_date,a.platform_type,COALESCE(total_users,0) AS total_users,COALESCE(amount,0) AS amount
FROM each_day_platform a
LEFT JOIN cte b ON a.spend_date=b.spend_date AND a.platform_type = b.platform_type;

WITH spammed AS(
	SELECT *
	FROM actions_1132
	WHERE extra = 'spam'
),

percentage AS(
	SELECT (COUNT(r.post_id)::NUMERIC/COUNT(s.post_id))*100 AS per
	FROM spammed s
	LEFT JOIN removals_1132 r ON s.post_id = r.post_id
	GROUP BY s.action_date
)

SELECT ROUND(AVG(per),2) AS avg_daily_percent FROM percentage;

--------------------------- OR ---------------------------

WITH cte AS (
	SELECT a.action_date,
			ROUND(COUNT(CASE WHEN a.extra = 'spam' AND r.post_id IS NOT NULL THEN 1 ELSE NULL END)*100::NUMERIC/COUNT(DISTINCT a.post_id),2) AS removed_spammed_post_percentage
	FROM actions_1132 a
	LEFT JOIN removals_1132 r ON a.post_id = r.post_id
	GROUP BY a.action_date
)
SELECT ROUND(AVG(removed_spammed_post_percentage),2)
FROM cte
WHERE removed_spammed_post_percentage <> 0;

SELECT activity_date,COUNT(DISTINCT user_id)
FROM activity_1141
WHERE activity_date <= '2019-07-27' AND activity_date >= DATE '2019-07-27'-30
GROUP BY activity_date;

WITH cnt AS(
	SELECT COUNT(DISTINCT session_id) AS c
	FROM activity_1142
	WHERE activity_date <= '2019-07-27' AND activity_date > '2019-07-27'::DATE - 30
	GROUP BY user_id
)

SELECT ROUND(AVG(c),2) AS average_sessions_per_user
FROM cnt;

SELECT DISTINCT author_id
FROM views_1148
WHERE viewer_id = author_id;

SELECT viewer_id
FROM views_1149
GROUP BY viewer_id,view_date
HAVING COUNT(DISTINCT article_id)>1
ORDER BY 1 ASC;

SELECT u.user_id,u.join_date,COALESCE(b.orders_in_2019,0)
FROM users_1158 u
LEFT JOIN
	(SELECT buyer_id,COUNT(order_id) AS orders_in_2019
	FROM orders_1158
	WHERE EXTRACT(YEAR FROM order_date) = 2019
	GROUP BY buyer_id) b
ON u.user_id = b.buyer_id;

WITH cte AS(
	SELECT *,
		ROW_NUMBER() OVER (PARTITION BY seller_id ORDER BY order_date) AS rn
	FROM orders_1159
),
cte2 AS(
	SELECT seller_id,item_id
	FROM cte
	WHERE rn = 2
)

SELECT u.user_id,
	CASE WHEN c.item_id=i.item_id THEN 'yes'
	     ELSE 'no'
	END AS "2nd_item_fav_brand"
FROM users_1159 u
INNER JOIN items_1159 i ON u.favorite_brand = i.item_brand
LEFT JOIN cte2 c ON u.user_id = c.seller_id
ORDER BY 1;

WITH cte AS(
	SELECT product_id,MAX(change_date) AS max_date
	FROM products_1164
	WHERE change_date <= '2019-08-16'
	GROUP BY product_id
)

SELECT p.product_id,
	MAX(CASE WHEN c.product_id IS NULL THEN 10
	     WHEN p.change_date = c.max_date THEN p.new_price
	END) AS price
FROM products_1164 p
LEFT JOIN cte c ON p.product_id = c.product_id
GROUP BY p.product_id
ORDER BY p.product_id;

SELECT ROUND((COUNT(CASE WHEN order_date = customer_pref_delivery_date THEN 1 ELSE NULL END)::NUMERIC/COUNT(*))*100,2) 
	AS immediate_percentage
FROM delivery_1173;

WITH cte AS(
	SELECT customer_id,MIN(order_date) AS first_order
	FROM delivery_1174
	GROUP BY customer_id
)

SELECT ROUND((COUNT(CASE WHEN d.order_date = d.customer_pref_delivery_date THEN 1 ELSE NULL END)::NUMERIC/COUNT(*))*100,2) 
	AS immediate_percentage
FROM delivery_1174 d
INNER JOIN cte c ON d.customer_id = c.customer_id AND d.order_date = c.first_order;

SELECT id,
	SUM(CASE WHEN month = 'Jan' THEN revenue ELSE NULL END) AS Jan_Revenue,
	SUM(CASE WHEN month = 'Feb' THEN revenue ELSE NULL END) AS Feb_Revenue,
	SUM(CASE WHEN month = 'Mar' THEN revenue ELSE NULL END) AS Mar_Revenue,
	SUM(CASE WHEN month = 'Apr' THEN revenue ELSE NULL END) AS Apr_Revenue,
	SUM(CASE WHEN month = 'May' THEN revenue ELSE NULL END) AS May_Revenue,
	SUM(CASE WHEN month = 'Jun' THEN revenue ELSE NULL END) AS Jun_Revenue,
	SUM(CASE WHEN month = 'Jul' THEN revenue ELSE NULL END) AS Jul_Revenue,
	SUM(CASE WHEN month = 'Aug' THEN revenue ELSE NULL END) AS Aug_Revenue,
	SUM(CASE WHEN month = 'Sep' THEN revenue ELSE NULL END) AS Sep_Revenue,
	SUM(CASE WHEN month = 'Oct' THEN revenue ELSE NULL END) AS Oct_Revenue,
	SUM(CASE WHEN month = 'Nov' THEN revenue ELSE NULL END) AS Nov_Revenue,
	SUM(CASE WHEN month = 'Dec' THEN revenue ELSE NULL END) AS Dec_Revenue
FROM department_1179
GROUP BY id
ORDER BY id;


-- Extra 
(SELECT id::TEXT,
	SUM(CASE WHEN month = 'Jan' THEN revenue ELSE 0 END) AS Jan_Revenue,
	SUM(CASE WHEN month = 'Feb' THEN revenue ELSE 0 END) AS Feb_Revenue,
	SUM(CASE WHEN month = 'Mar' THEN revenue ELSE 0 END) AS Mar_Revenue,
	SUM(revenue) AS Total
FROM department_1179
GROUP BY id)
UNION
(SELECT NULL,
	SUM(CASE WHEN month = 'Jan' THEN revenue ELSE 0 END) AS JTR,
	SUM(CASE WHEN month = 'Feb' THEN revenue ELSE 0 END) AS FTR,
	SUM(CASE WHEN month = 'Mar' THEN revenue ELSE 0 END) AS MTR,
	SUM(revenue) AS TR
FROM department_1179
GROUP BY 1)
ORDER BY 1;


SELECT TO_CHAR(trans_date,'YYYY-MM') AS month,country,
	COUNT(id) AS trans_count,
	COUNT(CASE WHEN state = 'approved' THEN 1 ELSE NULL END) AS approved_count,
	SUM(amount) AS trans_total_amount
FROM transactions_1193
GROUP BY month,country
ORDER BY month;

WITH player_scores AS(
	(SELECT first_player AS player,first_score AS score
	FROM matches_1194)
	UNION ALL
	(SELECT second_player AS player,second_score AS score
	FROM matches_1194)
),

all_player_scores AS(
	SELECT player,SUM(score) AS score
	FROM player_scores
	GROUP BY player
	ORDER BY player
),

ranked AS (
	SELECT p.*,ps.score AS score,
		DENSE_RANK() OVER(PARTITION BY group_id ORDER BY score DESC,player_id ASC) AS rnk
	FROM players_1194 p
	INNER JOIN all_player_scores ps ON p.player_id = ps.player
)

SELECT group_id,player_id 
FROM ranked
WHERE rnk=1
ORDER BY group_id;

WITH cte AS(
	SELECT *,
		SUM(weight) OVER (ORDER BY turn) AS wsum
	FROM queue_1204
)

SELECT person_name
FROM cte 
WHERE turn = (SELECT MAX(turn) FROM cte WHERE wsum<=1000);

WITH cte1 AS (
	SELECT TO_CHAR(c.charge_date,'YYYY-MM') AS month,t.country,
		COUNT(c.trans_id) AS chargeback_count,
		SUM(t.amount) AS chargeback_amount
	FROM chargebacks_1205 c
	JOIN transactions_1205 t ON t.id = c.trans_id
	GROUP BY TO_CHAR(charge_date,'YYYY-MM'),t.country
),

cte2 AS (
	SELECT TO_CHAR(trans_date,'YYYY-MM') AS month,country,
		COUNT(CASE WHEN state='approved' THEN 1 ELSE NULL END) AS approved_count,
		SUM(CASE WHEN state='approved' THEN amount ELSE NULL END) AS approved_amount
	FROM transactions_1205
	GROUP BY TO_CHAR(trans_date,'YYYY-MM'),country
)

SELECT c1.month,c1.country,
	COALESCE(c2.approved_count,0) AS approved_count,
	COALESCE(c2.approved_amount,0) AS approved_amount,
	COALESCE(c1.chargeback_count,0) AS chargeback_count,
	COALESCE(c1.chargeback_amount,0) AS chargeback_amount
FROM cte1 c1 
FULL OUTER JOIN cte2 c2 ON c1.month = c2.month
ORDER BY c1.month;

SELECT query_name,
	ROUND(AVG(rating::NUMERIC/position),2) AS quality,
	ROUND((COUNT(CASE WHEN rating < 3 THEN 1 ELSE NULL END)::NUMERIC/COUNT(*))*100,2) AS poor_query_percentage
FROM queries_1211
GROUP BY query_name;

WITH cte AS(
	SELECT host_team,guest_team,host_goals,guest_goals
	FROM matches_1212
	UNION ALL
	SELECT guest_team AS host_team,host_team AS guest_team,host_goals,guest_goals
	FROM matches_1212
	WHERE host_goals=guest_goals
),
cte2 AS (
	SELECT 
		CASE WHEN host_goals > guest_goals THEN host_team
		     WHEN host_goals < guest_goals THEN guest_team
		     ELSE host_team
		END AS winner,
		CASE WHEN host_goals > guest_goals THEN 3
		     WHEN host_goals < guest_goals THEN 3
		     ELSE 1
		END AS points
	FROM cte
)

SELECT t.team_id,t.team_name,COALESCE(SUM(c.points),0) AS points
FROM cte2 c
RIGHT JOIN teams_1212 t ON t.team_id = c.winner
GROUP BY t.team_id,t.team_name
ORDER BY points DESC;

WITH cte1 AS(
	SELECT fail_date AS dt,'failed' AS status
	FROM failed_1225
	WHERE EXTRACT(YEAR from fail_date) = 2019
	UNION
	SELECT success_date AS dt,'succeeded' AS status
	FROM succeeded_1225
	WHERE EXTRACT(YEAR from success_date) = 2019
),
cte2 AS (
	SELECT *,
		LAG(status) OVER (ORDER BY dt) AS lagged_status
	FROM cte1
),
cte3 AS (
	SELECT *,
		(CASE WHEN status = lagged_status THEN 0 ELSE 1 END) AS marker
	FROM cte2
),
cte4 AS (
	SELECT *,
		SUM(marker) OVER (ORDER BY dt) AS rolling_sum
	FROM cte3
)
SELECT MAX(status) AS period_state, MIN(dt) AS start_date, MAX(dt) AS end_date
FROM cte4
GROUP BY rolling_sum;

---------------------------------------------------------------------------------------------------------------------------------------------
--Simplified Query
---------------------------------------------------------------------------------------------------------------------------------------------

WITH tasks AS (
	SELECT fail_date AS dt,'failed' AS status
	FROM failed_1225
	UNION
	SELECT success_date AS dt,'succeeded' AS status
	FROM succeeded_1225
),
ranked AS (
	SELECT *,
		ROW_NUMBER() OVER (ORDER BY dt)-ROW_NUMBER() OVER (PARTITION BY status ORDER BY dt) AS diff
	FROM tasks
	WHERE dt BETWEEN '01-01-2019' AND '31-12-2019'
	ORDER BY dt
)
SELECT status,
	MIN(dt) AS start_date,MAX(dt) AS end_date
FROM ranked
GROUP BY status,diff
ORDER BY start_date;

WITH posts AS (
	SELECT *
	FROM submissions_1241
	WHERE parent_id IS NULL
),
cmnts AS (
	SELECT *
	FROM submissions_1241
	WHERE parent_id IS NOT NULL
),
cte AS (
	SELECT DISTINCT p.sub_id AS post_id,c.sub_id AS cmnt_id
	FROM posts p
	LEFT JOIN cmnts c
	ON p.sub_id = c.parent_id
)
SELECT post_id,COUNT(cmnt_id)
FROM cte
GROUP BY post_id
ORDER BY post_id;

SELECT u.product_id,ROUND(SUM(u.units*p.price)::NUMERIC/SUM(u.units),2)
FROM unit_sold_1251 u
INNER JOIN prices_1251 p 
ON u.purchase_date BETWEEN p.start_date AND p.end_date AND 
	u.product_id = p.product_id
GROUP BY u.product_id;

WITH cte AS(
	SELECT user_id1,user_id2 
	FROM friendship_1264
	UNION 
	SELECT user_id2,user_id1
	FROM friendship_1264
),

friends AS(
	SELECT user_id2 AS friends
	FROM cte WHERE user_id1 = 1
)

SELECT DISTINCT page_id
FROM likes_1264
WHERE user_id IN (SELECT * FROM friends) AND 
      page_id NOT IN (SELECT DISTINCT page_id FROM likes_1264 WHERE user_id = 1)
ORDER BY 1;

--------------------------- OR ----------------------------

WITH likes AS (
	SELECT user_id,ARRAY_AGG(page_id) as liked_pages
	FROM likes_1264
	GROUP BY user_id
),friends AS (
	SELECT user_id1,user_id2,page_id
	FROM friendship_1264 f
	JOIN likes_1264 l ON f.user_id1 = l.user_id 
	UNION ALL
	SELECT user_id2,user_id1,page_id
	FROM friendship_1264 f
	JOIN likes_1264 l ON f.user_id2 = l.user_id 
),reco AS(
	SELECT f.user_id1 AS from_user, f.user_id2 AS friend, f.page_id AS page_to_reco, liked_pages AS friend_already_liked_pages
	FROM friends f
	JOIN likes l ON f.user_id2 = l.user_id AND (NOT f.page_id = ANY (liked_pages))
)
SELECT DISTINCT page_to_reco
FROM reco
WHERE friend = 1
ORDER BY page_to_reco;

SELECT e1.employee_id
FROM employees_1270 e1 
INNER JOIN employees_1270 e2 
ON e1.manager_id = e2.employee_id 
	AND e1.manager_id = 1
	AND e1.employee_id <> e2.employee_id
UNION
SELECT e1.employee_id
FROM employees_1270 e1 
INNER JOIN employees_1270 e2 
ON e1.manager_id = e2.employee_id
	AND e1.employee_id <> e2.employee_id
INNER JOIN employees_1270 e3
ON e2.manager_id = e3.employee_id
	AND e2.manager_id = 1
	AND e2.employee_id <> e3.employee_id
UNION
SELECT e1.employee_id
FROM employees_1270 e1 
INNER JOIN employees_1270 e2 
ON e1.manager_id = e2.employee_id
	AND e1.employee_id <> e2.employee_id
INNER JOIN employees_1270 e3
ON e2.manager_id = e3.employee_id
	AND e2.employee_id <> e3.employee_id
INNER JOIN employees_1270 e4
ON e3.manager_id = e4.employee_id
	AND e3.manager_id = 1
	AND e3.employee_id <> e4.employee_id;
	
--------------(OR)------------

SELECT e1.employee_id
FROM employees_1270 e1 
INNER JOIN employees_1270 e2 
ON e1.manager_id = e2.employee_id 
INNER JOIN employees_1270 e3 
ON e2.manager_id = e3.employee_id 
WHERE e3.manager_id = 1 AND e1.employee_id <> 1


--------------(OR)------------

WITH RECURSIVE cte AS (
	SELECT employee_id,employee_name,manager_id,1 AS level
	FROM employees_1270
	WHERE employee_id = 1
	UNION
	SELECT e.employee_id,e.employee_name,e.manager_id,level+1 AS level
	FROM cte c
	INNER JOIN employees_1270 e ON c.employee_id = e.manager_id
	WHERE level < 4
)
SELECT DISTINCT employee_id,employee_name FROM cte WHERE employee_id <> 1;

WITH exams AS (
	SELECT student_id,subject_name,COUNT(1) AS attended_exams
	FROM examinations_1280
	GROUP BY student_id,subject_name
),
combinations AS (
	SELECT st.*,sb.subject_name
	FROM students_1280 st
	CROSS JOIN subjects_1280 sb
)
SELECT c.student_id,c.student_name,c.subject_name,COALESCE(e.attended_exams,0)
FROM combinations c
LEFT JOIN exams e ON e.student_id = c.student_id AND e.subject_name = c.subject_name
ORDER BY c.student_id,c.subject_name;

WITH ranked AS (
	SELECT log_id,
		log_id-ROW_NUMBER() OVER (ORDER BY log_id) AS diff
	FROM logs_1285
)

SELECT MIN(log_id) AS start_id,MAX(log_id) AS end_id
FROM ranked
GROUP BY diff
ORDER BY start_id;

SELECT country_name,
	CASE WHEN AVG(weather_state) <= 15 THEN 'Cold'
		 WHEN AVG(weather_state) >= 25 THEN 'Hot'
		 ELSE 'Warm'
	END AS weather_type
FROM weather_1294 w
INNER JOIN countries_1294 c ON w.country_id = c.country_id
WHERE EXTRACT(month FROM day) = 11
GROUP BY country_name;


SELECT employee_id,
	COUNT(employee_id) OVER (PARTITION BY team_id) AS team_size
FROM employee_1303
ORDER BY employee_id;

SELECT gender,day,
	SUM(score_points) OVER (PARTITION BY gender ORDER BY day)
FROM scores_1308;

WITH grouped AS(
	SELECT visited_on,SUM(amount) AS amount
	FROM customer_1321
	GROUP BY visited_on
),
cte AS (
	SELECT *,ROW_NUMBER() OVER (ORDER BY visited_on) AS num
	FROM grouped
),
cte2 AS(
	SELECT *,
		SUM(amount) OVER (ORDER BY visited_on ROWS BETWEEN 6 PRECEDING AND CURRENT ROW) AS sum_amount,
		ROUND(AVG(amount) OVER (ORDER BY visited_on ROWS BETWEEN 6 PRECEDING AND CURRENT ROW),2) AS average_amount
	FROM cte
)
SELECT visited_on,sum_amount,average_amount
FROM cte2
WHERE num>=7;

---------------------------------------------------------------------------------------------------------------------------------

WITH daily_spent AS (
	SELECT visited_on,SUM(amount) AS amount,
		ROW_NUMBER() OVER (ORDER BY visited_on) AS rn
	FROM customer_1321
	GROUP BY visited_on
),
moving_averages AS (
	SELECT visited_on,rn,
		ROUND((AVG(amount) OVER (ORDER BY visited_on ROWS BETWEEN 6 PRECEDING AND CURRENT ROW)),2) AS running_avg
	FROM daily_spent
	ORDER BY visited_on
)
SELECT *
FROM moving_averages
WHERE rn >= 7;

-- Window functions are evaluated after group by clause got executed, and window functions are applied to the result of group by clause.

SELECT ad_id,
	COALESCE(ROUND(AVG(
	CASE WHEN action = 'Clicked' THEN 1
	     WHEN action = 'Viewed' THEN 0
	     ELSE NULL 
	END)*100,2),0.00) AS ctr
FROM ads_1322
GROUP BY ad_id
ORDER BY ctr DESC;

SELECT p.product_name,SUM(o.unit)
FROM orders_1327 o
INNER JOIN products_1327 p ON o.product_id = p.product_id
WHERE DATE_TRUNC('MONTH',o.order_date)::DATE = '2020-02-01'
GROUP BY p.product_name
HAVING SUM(o.unit) >= 100;


WITH RECURSIVE cte AS(
	SELECT v.user_id,v.visit_date,t.amount
	FROM visits_1336 v
	LEFT JOIN transactions_1336 t ON v.user_id=t.user_id AND v.visit_date=t.transaction_date
),
cte1 AS (
	SELECT user_id,visit_date,COUNT(1) AS transactions_count
	FROM cte
	WHERE amount IS NOT NULL
	GROUP BY user_id,visit_date
),
cte2 AS (
	SELECT 0 AS transactions_count,COUNT(1) AS visits_count
	FROM cte
	WHERE amount IS NULL
	GROUP BY transactions_count
),
cte3 AS (
	SELECT transactions_count,COUNT(transactions_count) AS visit_count
	FROM cte1
	GROUP BY transactions_count
	UNION
	SELECT *
	FROM cte2
),
nums AS (
	SELECT 0 AS n
	UNION
	SELECT n+1 AS n
	FROM nums
	WHERE n < (SELECT MAX(transactions_count) FROM cte3)
)

SELECT n.n AS transactions_count,COALESCE(visit_count,0) AS visit_count
FROM nums n
LEFT JOIN cte3 c ON n.n=c.transactions_count
ORDER BY 1;

(SELECT u.name
FROM movie_rating_1341 mr 
INNER JOIN users_1341 u ON mr.user_id = u.user_id
GROUP BY u.name
ORDER BY COUNT(mr.rating) DESC,u.name
LIMIT 1)
UNION 
(SELECT m.title
FROM movie_rating_1341 mr 
INNER JOIN movies_1341 m ON mr.movie_id = m.movie_id
WHERE EXTRACT(MONTH FROM created_at)=2
GROUP BY m.title
ORDER BY AVG(mr.rating) DESC,m.title
LIMIT 1);

SELECT s.id,s.name
FROM students_1350 s
LEFT JOIN departments_1350 d ON s.department_id = d.id
WHERE d.id IS NULL;

WITH cte AS (
	SELECT activity,COUNT(activity) AS cnt
	FROM friends_1355
	GROUP BY activity
),
cte1 AS (
	SELECT activity,cnt,
		MAX(cnt) OVER () AS max_cnt,
		MIN(cnt) OVER () AS min_cnt
	FROM cte
)
SELECT activity
FROM cte1 
WHERE cnt <> max_cnt AND cnt <> min_cnt;

WITH all_contacts AS(
	SELECT user_id,COUNT(contact_name) AS a_contacts
	FROM contacts_1364
	GROUP BY user_id
),
trusted_contacts AS(
	SELECT user_id,COUNT(contact_name) AS t_contacts
	FROM contacts_1364 ct
	INNER JOIN customers_1364 cs ON ct.contact_name = cs.customer_name
	GROUP BY user_id
)

SELECT i.*,COALESCE(a_contacts,0) all_contacts,COALESCE(t_contacts,0) trusted_contacts
FROM invoices_1364 i
LEFT JOIN all_contacts ac ON i.user_id = ac.user_id
LEFT JOIN trusted_contacts tc ON i.user_id = tc.user_id
ORDER BY 1;

SELECT eu.unique_id,e.name
FROM employee_1378 e
LEFT JOIN employee_uni_1378 eu ON e.id = eu.id
ORDER BY e.name;

WITH RECURSIVE exploded_sales AS (
	SELECT product_id,period_start,period_end,average_daily_sales
	FROM sales_1384
	UNION
	SELECT product_id,period_start+1 AS period_start,period_end,average_daily_sales 
	FROM exploded_sales
	WHERE period_start < period_end
)
SELECT es.product_id,p.product_name,EXTRACT(YEAR FROM period_start) AS report_year,SUM(average_daily_sales) 
FROM exploded_sales es
INNER JOIN product_1384 p ON es.product_id=p.product_id
GROUP BY es.product_id,p.product_name,EXTRACT(YEAR FROM period_start)
ORDER BY es.product_id,report_year;

SELECT stock_name,
	SUM(CASE WHEN operation = 'Buy' THEN price * -1 
		ELSE price END) AS capital_gain_loss
FROM stocks_1393
GROUP BY stock_name
ORDER BY capital_gain_loss DESC;

SELECT DISTINCT o1.customer_id,c.customer_name
FROM orders_1398 o1
LEFT JOIN orders_1398 o2 ON o1.product_name = 'A' AND o2.product_name = 'B' AND o1.customer_id = o2.customer_id
LEFT JOIN orders_1398 o3 ON o1.product_name = 'A' AND o3.product_name = 'C' AND o1.customer_id = o3.customer_id
INNER JOIN customers_1398 c ON o1.customer_id = c.customer_id
WHERE o1.product_name IS NOT NULL AND o2.product_name IS NOT NULL AND o3.product_name IS NULL;

SELECT u.name,COALESCE(SUM(r.distance),0) AS travelled_distance
FROM users_1407 u
LEFT JOIN rides_1407 r ON u.id = r.user_id
GROUP BY u.name
ORDER BY travelled_distance DESC,u.name;

WITH cte AS(
	SELECT *,
		MIN(score) OVER (PARTITION BY exam_id) AS lowest_score,
		MAX(score) OVER (PARTITION BY exam_id) AS highest_score
	FROM exams_1412
),
cte1 AS(
	SELECT DISTINCT student_id
	FROM exams_1412
	EXCEPT
	SELECT DISTINCT student_id
	FROM cte c
	WHERE score = lowest_score OR score = highest_score
)
SELECT s.*
FROM cte1 c 
INNER JOIN students_1412 s ON c.student_id = s.student_id;

SELECT q.id,q.year,COALESCE(n.npv,0)
FROM queries_1421 q
LEFT JOIN npv_1421 n ON q.id = n.id AND q.year = n.year;

WITH bins AS (
    SELECT '[0-5>' AS bin,  0 AS min_duration, 5*60 AS max_duration
    UNION ALL
    SELECT '[5-10>' AS bin,  5*60 AS min_duration, 10*60 AS max_duration
    UNION ALL
    SELECT '[10-15>' AS bin, 10*60 AS min_duration, 15*60 AS max_duration
    UNION ALL
    SELECT '15 or more' AS bin,  15*60 as min_duration, 2147483647 AS max_duration
)
SELECT b.bin, COUNT(s.session_id) AS total
FROM bins b
LEFT JOIN sessions_1435 s
		ON s.duration >= min_duration 
        AND s.duration < max_duration				 
GROUP BY b.bin;

SELECT e.*,
	CASE WHEN operator = '=' THEN v1.value = v2.value
	     WHEN operator = '>' THEN v1.value > v2.value
	     WHEN operator = '<' THEN v1.value < v2.value
	END AS value
FROM expressions_1440 e
INNER JOIN variables_1440 v1 ON e.left_operand = v1.name
INNER JOIN variables_1440 v2 ON e.right_operand = v2.name;

SELECT s1.sale_date,s1.sold_num-s2.sold_num AS diff
FROM sales_1445 s1
INNER JOIN sales_1445 s2 ON s1.sale_date=s2.sale_date AND s1.fruit <> s2.fruit AND s1.fruit = 'apples';

WITH dedup AS (
	SELECT *
	FROM logins_1454
	GROUP BY id,login_date
),
cte AS (
	SELECT id,login_date,
		LEAD(login_date,4) OVER (PARTITION BY id ORDER BY login_date) AS date_5
	FROM dedup
)
SELECT a.id,a.name
FROM cte c
INNER JOIN accounts_1454 a ON a.id = c.id
WHERE c.date_5-c.login_date=4;

SELECT p1.id AS p1,p2.id AS p2,ABS(p1.x_value-p2.x_value)*ABS(p1.y_value-p2.y_value) AS area
FROM points_1459 p1
INNER JOIN points_1459 p2 ON p1.id < p2.id AND ABS(p1.x_value-p2.x_value)*ABS(p1.y_value-p2.y_value)<>0
ORDER BY area DESC;

WITH cte AS (
	SELECT *,
		MAX(salary) OVER (PARTITION BY company_id) AS max_salary
	FROM salaries_1468
)
SELECT *,
	ROUND(
	CASE WHEN max_salary<1000 THEN salary
	     WHEN max_salary<=10000 THEN salary-(salary*24)/100
	     ELSE salary-(salary*49)/100
	END) AS new_salary
FROM cte;

SELECT i.item_category AS "Category",
	SUM(CASE WHEN EXTRACT(ISODOW FROM o.order_date)=1 THEN o.quantity ELSE 0 END) AS "Monday",
	SUM(CASE WHEN EXTRACT(ISODOW FROM o.order_date)=2 THEN o.quantity ELSE 0 END) AS "Tuesday",
	SUM(CASE WHEN EXTRACT(ISODOW FROM o.order_date)=3 THEN o.quantity ELSE 0 END) AS "Wednesday",
	SUM(CASE WHEN EXTRACT(ISODOW FROM o.order_date)=4 THEN o.quantity ELSE 0 END) AS "Thursday",
	SUM(CASE WHEN EXTRACT(ISODOW FROM o.order_date)=5 THEN o.quantity ELSE 0 END) AS "Friday",
	SUM(CASE WHEN EXTRACT(ISODOW FROM o.order_date)=6 THEN o.quantity ELSE 0 END) AS "Saturday",
	SUM(CASE WHEN EXTRACT(ISODOW FROM o.order_date)=7 THEN o.quantity ELSE 0 END) AS "Sunday"
FROM items_1479 i
LEFT JOIN orders_1479 o ON o.item_id=i.item_id::INT
GROUP BY i.item_category
ORDER BY i.item_category;

SELECT sell_date,COUNT(DISTINCT product) AS num_sold,STRING_AGG(DISTINCT product,',' ORDER BY product) AS products
FROM activities_1484
GROUP BY sell_date
ORDER BY sell_date;

SELECT DISTINCT c.title
FROM tv_program_1495 t
INNER JOIN content_1495 c 
ON t.content_id = c.content_id AND 
	c.kids_content = 'Y' AND 
	DATE_TRUNC('MONTH',t.program_date)::DATE = '2020-06-01';

WITH cte AS (
	SELECT caller_id AS person_id,duration
	FROM calls_1501
	UNION
	SELECT callee_id AS person_id,duration
	FROM calls_1501
),
avg_duration AS(
	SELECT cn.name AS country_name,c.duration AS duration,
		AVG(duration) OVER () avg_global_duration,
		AVG(duration) OVER (PARTITION BY cn.name) avg_country_duration
	FROM cte c
	INNER JOIN person_1501 p ON c.person_id=p.id
	INNER JOIN country_1501 cn ON cn.country_code=SUBSTR(p.phone_number,1,3)
)
SELECT DISTINCT country_name
FROM avg_duration
WHERE avg_country_duration>avg_global_duration;

WITH par_spent AS (
	SELECT c.name,DATE_TRUNC('MONTH',o.order_date)::DATE,SUM(quantity*price) AS spent
	FROM orders_1511 o
	INNER JOIN product_1511 p
	ON o.product_id = p.product_id AND 
	(DATE_TRUNC('MONTH',o.order_date)::DATE = '2020-06-01' OR DATE_TRUNC('MONTH',o.order_date)::DATE = '2020-07-01')
	INNER JOIN customers_1511 c
	ON c.customer_id = o.customer_id
	GROUP BY c.name,DATE_TRUNC('MONTH',o.order_date)::DATE
	HAVING SUM(quantity*price)>=100
)
SELECT name
FROM par_spent
GROUP BY name
HAVING COUNT(name) = 2;

SELECT *
FROM users_1517
WHERE mail SIMILAR TO '[a-zA-Z][a-zA-Z0-9_.-]*@leetcode[.]com';

SELECT *
FROM patients_1527
WHERE conditions LIKE '% DIAB1%' OR conditions LIKE 'DIAB1%';

WITH ranked AS (
	SELECT *,
		DENSE_RANK() OVER (PARTITION BY customer_id ORDER BY order_date DESC) AS rn
	FROM orders_1532
)
SELECT c.name AS customer_name,r.customer_id,r.order_id,r.order_date
FROM ranked r
INNER JOIN customers_1532 c ON r.customer_id=c.customer_id
WHERE rn<=3
ORDER BY c.name,c.customer_id,r.order_date DESC;

---------------Without Window function---------------

SELECT o1.customer_id,o1.order_date,COUNT(o2.order_date)
FROM orders_1532 o1
INNER JOIN orders_1532 o2 ON o1.customer_id=o2.customer_id AND o1.order_date<=o2.order_date
GROUP BY o1.customer_id,o1.order_date
HAVING COUNT(o2.order_date)<=3
ORDER BY 1,2 DESC;

-- Main logic is over now we only need to apply 2 joins to bring other columns.

SELECT LOWER(TRIM(product_name)) AS product_name,TO_CHAR(sale_date,'YYYY-MM') AS sale_date,COUNT(sale_id) AS total
FROM sales_1543
GROUP BY LOWER(TRIM(product_name)),TO_CHAR(sale_date,'YYYY-MM')
ORDER BY product_name,sale_date;

WITH recent_orders AS (
	SELECT o1.*
	FROM orders_1549 o1
	LEFT JOIN orders_1549 o2 ON o1.product_id = o2.product_id AND o1.order_date < o2.order_date
	WHERE o2.order_id IS NULL
)
SELECT p.product_name,p.product_id,ro.order_id,ro.order_date
FROM recent_orders ro
INNER JOIN products_1549 p ON ro.product_id = p.product_id
ORDER BY p.product_name,p.product_id,ro.order_id;

WITH trans AS (
	SELECT paid_by AS usr,amount*-1 AS amount
	FROM transactions_1555
	UNION ALL
	SELECT paid_to AS usr,amount
	FROM transactions_1555	
),
agg_trans AS (
	SELECT usr,SUM(amount) AS cr
	FROM trans
	GROUP BY usr
)

SELECT u.user_id,u.user_name,(COALESCE(t.cr,0)+u.credit) AS credit,
	CASE WHEN (COALESCE(t.cr,0)+u.credit) < 0 THEN 'Yes'
	     ELSE 'No'
	END AS credit_limit_breached
FROM users_1555 u
LEFT JOIN agg_trans t ON t.usr = u.user_id;

SELECT TO_CHAR(order_date,'YYYY-MM') AS month,
	COUNT(order_id) AS order_count,
	COUNT(DISTINCT customer_id) AS customer_count
FROM orders_1565
WHERE invoice > 20
GROUP BY TO_CHAR(order_date,'YYYY-MM');

SELECT w.name AS warehouse_name,SUM(p.width*p.height*p.length*w.units) AS volume
FROM warehouse_1571 w
INNER JOIN products_1571 p ON w.product_id = p.product_id
GROUP BY w.name;

SELECT v.customer_id,COUNT(v.visit_id)
FROM visits_1581 v
LEFT JOIN transactions_1581 t ON v.visit_id = t.visit_id
WHERE t.transaction_id IS NULL
GROUP BY v.customer_id
ORDER BY v.customer_id;

SELECT u.name,SUM(t.amount)
FROM transactions_1587 t
INNER JOIN users_1587 u ON t.account=u.account
GROUP BY u.name
HAVING SUM(t.amount) > 10000


SELECT seller_name
FROM seller_1607 
WHERE seller_id NOT IN 
(SELECT DISTINCT seller_id
FROM orders_1607
WHERE EXTRACT(YEAR FROM sale_date) = 2020);

WITH RECURSIVE max_id AS(
	SELECT MAX(customer_id) AS mx_id
	FROM customers_1613
),
cte AS (
	SELECT 1 AS id
	UNION ALL
	SELECT id+1 AS id
	FROM cte c
	INNER JOIN max_id m ON true
	WHERE c.id<m.mx_id
)
SELECT ct.id AS ids
FROM cte ct
LEFT JOIN customers_1613 c ON ct.id = c.customer_id
WHERE c.customer_id IS NULL
ORDER BY ids;

SELECT a.student_name,b.student_name,c.student_name
FROM school_a_1623 a
CROSS JOIN school_b_1623 b
CROSS JOIN school_c_1623 c
WHERE a.student_id <> b.student_id AND a.student_id <> c.student_id AND b.student_id <> c.student_id AND
	a.student_name <> b.student_name AND a.student_name <> c.student_name AND b.student_name <> c.student_name;

SELECT contest_id,ROUND((COUNT(DISTINCT user_id)*100.0)/user_count.cnt,2) AS percentage
FROM register_1633
CROSS JOIN (SELECT COUNT(*) AS cnt FROM users_1633) user_count
GROUP BY contest_id,user_count.cnt
ORDER BY percentage DESC,contest_id;

--OR--

SELECT contest_id,ROUND((COUNT(DISTINCT user_id)*100.0)/(SELECT COUNT(*) AS cnt FROM users_1633),2) AS percentage
FROM register_1633
GROUP BY contest_id
ORDER BY percentage DESC,contest_id;

WITH RECURSIVE ac_rides AS (
	SELECT ar.ride_id,r.requested_at
	FROM accepted_rides_1635 ar
	INNER JOIN rides_1635 r ON ar.ride_id = r.ride_id AND EXTRACT(YEAR FROM r.requested_at)<=2020
),
months AS (
	SELECT 1 AS num
	UNION 
	SELECT num+1 AS num
	FROM months
	WHERE num<=11
),
ride_details AS (
	SELECT *
	FROM months m
	LEFT JOIN ac_rides ar ON EXTRACT(MONTH FROM ar.requested_at)=m.num
),
aggr_details AS (
	SELECT num,COUNT(DISTINCT ride_id) AS rides
	FROM ride_details
	GROUP BY num
),
avail_drivers AS (
	SELECT *,
		ROW_NUMBER() OVER (ORDER BY join_date) AS drivers
	FROM drivers_1635
),
drivers_2020 AS (
	SELECT *
	FROM months m
	LEFT JOIN avail_drivers a ON EXTRACT(YEAR FROM a.join_date)=2020 AND m.num>=EXTRACT(MONTH FROM a.join_date)
),
driver_count AS (
	SELECT num,MAX(drivers) AS drivers
	FROM drivers_2020
	GROUP BY num
)
SELECT a.num AS month,a.rides,d.drivers
FROM aggr_details a
INNER JOIN driver_count d ON a.num=d.num;

WITH RECURSIVE months AS (
	SELECT 1 AS m
	UNION
	SELECT m+1 AS m
	FROM months
	WHERE m <= 11 
),
accepted_rides_2020 AS (
	SELECT mn.m,COUNT(ar.ride_id) AS accepted_rides
	FROM accepted_rides_1645 ar 
	INNER JOIN rides_1645 r ON ar.ride_id = r.ride_id AND EXTRACT(year FROM r.requested_at)=2020
	RIGHT JOIN months mn ON mn.m = EXTRACT(month FROM r.requested_at)
	GROUP BY m
),
running_drivers AS (
	SELECT *,
		COUNT(driver_id) OVER (ORDER BY join_date) AS drivers_cnt 
	FROM drivers_1645
),
drivers AS (
	SELECT mn.m,ar.accepted_rides,MAX(d.drivers_cnt) AS drivers
	FROM running_drivers d
	RIGHT JOIN months mn ON mn.m >= EXTRACT(month FROM d.join_date) AND EXTRACT(year FROM d.join_date)=2020
	INNER JOIN accepted_rides_2020 ar ON ar.m=mn.m
	GROUP BY mn.m,ar.accepted_rides
)
SELECT m AS month,ROUND(accepted_rides*100.0/drivers,2) AS working_percentage
FROM drivers
ORDER BY month;

WITH RECURSIVE months AS (
	select 1 as m
	union
	select m+1 as m
	from months
	where m<=11
),
cte AS (
	SELECT mn.m,SUM(COALESCE(ar.ride_distance,0)) AS ride_distance,SUM(COALESCE(ar.ride_duration,0)) AS ride_duration,
		CASE WHEN m BETWEEN 1 AND 3 THEN 'q1'
		     WHEN m BETWEEN 4 AND 6 THEN 'q2'
		     WHEN m BETWEEN 7 AND 9 THEN 'q3'
		     ELSE 'q4'
		END AS quater
	FROM accepted_rides_1651 ar
	INNER JOIN rides_1651 r ON ar.ride_id = r.ride_id AND EXTRACT(year FROM requested_at) = 2020
	RIGHT JOIN months mn ON mn.m = EXTRACT(month FROM requested_at)
	GROUP BY mn.m
)
SELECT m,
	ROUND(AVG(ride_distance) OVER (ORDER BY m ROWS BETWEEN CURRENT ROW AND 2 FOLLOWING),2) AS average_ride_distance,
	ROUND(AVG(ride_duration) OVER (ORDER BY m ROWS BETWEEN CURRENT ROW AND 2 FOLLOWING),2) AS average_ride_duration
FROM cte;

SELECT s.machine_id,ROUND(AVG(e.timestamp-s.timestamp)::NUMERIC,3) AS processing_time
FROM activity_1661 s
INNER JOIN activity_1661 e 
ON s.activity_type = 'start' AND e.activity_type = 'end' AND
	s.machine_id = e.machine_id AND s.process_id = e.process_id
GROUP BY s.machine_id;

SELECT user_id,INITCAP(name)
FROM users_1667
ORDER BY user_id;

SELECT p.name,SUM(i.rest) AS rest,SUM(i.paid) AS paid,SUM(i.canceled) AS canceled,SUM(i.refunded) AS refunded
FROM invoice_1677 i
INNER JOIN product_1677 p ON i.product_id = p.product_id
GROUP BY p.name
ORDER BY p.name;

SELECT tweet_id
FROM tweets_1683
WHERE LENGTH(content)>15;

SELECT date_id,make_name,COUNT(DISTINCT lead_id) AS unique_leads,COUNT(DISTINCT partner_id) AS unique_partners
FROM daily_sales_1693
GROUP BY date_id,make_name;

WITH calls AS (
	SELECT from_id,to_id,duration
	FROM calls_1699
	UNION ALL
	SELECT to_id,from_id,duration
	FROM calls_1699
)
SELECT from_id,to_id,COUNT(*) AS call_count,SUM(duration) AS total_duration
FROM calls
WHERE from_id<to_id
GROUP BY from_id,to_id;

WITH user_visits_1709 AS (
	SELECT *
	FROM user_visits_1709
	UNION ALL
	SELECT DISTINCT user_id,'2021-1-1'::DATE
	FROM user_visits_1709
),
ranked_visits AS (
	SELECT *,
		ROW_NUMBER() OVER (PARTITION BY user_id ORDER BY visit_date) AS rn
	FROM user_visits_1709
),
user_windows AS(
	SELECT r1.*,r2.visit_date,r2.visit_date-r1.visit_date AS win
	FROM ranked_visits r1
	INNER JOIN ranked_visits r2 ON r1.user_id = r2.user_id AND r1.rn+1 = r2.rn
)
SELECT user_id,MAX(win) AS biggest_window
FROM user_windows
GROUP BY user_id;

SELECT SUM(b.apple_count+COALESCE(c.apple_count,0)) AS apple_count,SUM(b.orange_count+COALESCE(c.orange_count,0)) AS orange_count
FROM boxes_1715 b
LEFT JOIN chests_1715 c ON b.chest_id = c.chest_id;

SELECT user_id,COUNT(follower_id) AS follower_count
FROM followers_1729
GROUP BY user_id
ORDER BY user_id;

SELECT mgr.employee_id,mgr.name,COUNT(emp.name) AS reports_count,ROUND(AVG(emp.age)) AS average_age
FROM employees_1731 mgr
INNER JOIN employees_1731 emp ON mgr.employee_id = emp.reports_to
GROUP BY mgr.employee_id,mgr.name
ORDER BY mgr.employee_id;

SELECT event_day AS day,emp_id,SUM(out_time-in_time) AS total_time
FROM employees_1741
GROUP BY event_day,emp_id
ORDER BY day,emp_id;

SELECT l1.account_id
FROM log_info_1747 l1
INNER JOIN log_info_1747 l2 ON l1.account_id = l2.account_id AND l1.login BETWEEN l2.login AND l2.logout AND l1.ip_address <> l2.ip_address;

SELECT product_id
FROM products_1757
WHERE low_fats = 'Y' AND recyclable = 'Y';

WITH RECURSIVE all_subtasks AS (
	SELECT task_id,subtasks_count,1 AS subtask_id
	FROM tasks_1767
	UNION ALL
	SELECT task_id,subtasks_count,subtask_id+1 AS subtask_id
	FROM all_subtasks
	WHERE subtask_id<subtasks_count
)
SELECT a.task_id,a.subtask_id
FROM all_subtasks a
LEFT JOIN executed_1767 e ON a.task_id = e.task_id AND a.subtask_id = e.subtask_id
WHERE e.task_id IS NULL
ORDER BY 1,2;

CREATE OR REPLACE FUNCTION pivot_products_1777()
RETURNS TEXT
LANGUAGE PLPGSQL
AS
$$
DECLARE
	stores_array TEXT[];
	store_name TEXT;
	query_text TEXT := '';
BEGIN
	SELECT ARRAY_AGG(DISTINCT store ORDER BY store ASC)
	INTO stores_array
	FROM products_1777;
	
	query_text := query_text || 'SELECT product_id, ';
	FOREACH store_name IN ARRAY stores_array
	LOOP
		query_text := query_text || 'SUM(CASE WHEN store = ''' || store_name || ''' THEN price ELSE NULL END) AS "' || store_name || '",';
	END LOOP;
	query_text := LEFT(query_text,LENGTH(query_text)-1);
	
	query_text := query_text || ' FROM products_1777 GROUP BY product_id ORDER BY product_id;';
	
	RETURN query_text;
END
$$;

SELECT pivot_products_1777();

/*
SELECT product_id, 
	SUM(CASE WHEN store = 'store1' THEN price ELSE NULL END) AS "store1",
	SUM(CASE WHEN store = 'store2' THEN price ELSE NULL END) AS "store2",
	SUM(CASE WHEN store = 'store3' THEN price ELSE NULL END) AS "store3" 
FROM products_1777 
GROUP BY product_id 
ORDER BY product_id;
*/

WITH winners AS (
	SELECT wimbledon AS winner_id FROM championships_1783
	UNION ALL
	SELECT fr_open AS winner_id FROM championships_1783
	UNION ALL
	SELECT us_open AS winner_id FROM championships_1783
	UNION ALL
	SELECT au_open AS winner_id FROM championships_1783
)

SELECT p.player_id,p.player_name,COUNT(p.player_id) AS num_wins
FROM winners w
INNER JOIN players_1783 p ON w.winner_id = p.player_id
GROUP BY p.player_id,p.player_name;

WITH employee_departments AS (
	SELECT *,
		COUNT(employee_id) OVER (PARTITION BY employee_id) AS cnt
	FROM employee_1789
)
SELECT employee_id,department_id
FROM employee_departments
WHERE primary_flag = 'Y' OR (primary_flag = 'N' AND cnt = 1);

SELECT DISTINCT session_id
FROM playback_1809 pb 
LEFT JOIN ads_1809 ad 
ON pb.customer_id = ad.customer_id AND 
	ad.timestamp BETWEEN start_time AND end_time
WHERE ad_id IS NULL;


-- (Works for 3 consecutive contests only)


WITH gold_medal_users AS (
	SELECT DISTINCT gold_medal AS usr
	FROM contests_1811
	GROUP BY gold_medal
	HAVING COUNT(contest_id) = 3
),
all_users AS (
	SELECT gold_medal AS usr,contest_id FROM contests_1811
	UNION ALL
	SELECT silver_medal AS usr,contest_id FROM contests_1811
	UNION ALL
	SELECT bronz_medal AS usr,contest_id FROM contests_1811
),
consecutive_medal_users AS (
	SELECT DISTINCT a1.usr
	FROM all_users a1
	INNER JOIN all_users a2 ON a1.usr = a2.usr AND a1.contest_id - 1 = a2.contest_id
	INNER JOIN all_users a3 ON a1.usr = a3.usr AND a1.contest_id + 1 = a3.contest_id
),
inerview_candidates AS (
	SELECT usr
	FROM gold_medal_users
	UNION
	SELECT usr
	FROM consecutive_medal_users
)
SELECT name,mail
FROM inerview_candidates ic
INNER JOIN users_1811 u ON ic.usr = u.user_id;



--OR-- (Generic Query : Works for any number of consecutive contests)



WITH gold_medal_users AS (
	SELECT DISTINCT gold_medal AS usr
	FROM contests_1811
	GROUP BY gold_medal
	HAVING COUNT(contest_id) = 3
),
all_users AS (
	SELECT gold_medal AS usr,contest_id FROM contests_1811
	UNION ALL
	SELECT silver_medal AS usr,contest_id FROM contests_1811
	UNION ALL
	SELECT bronz_medal AS usr,contest_id FROM contests_1811
),
ranked_users AS (
	SELECT *,
		contest_id-ROW_NUMBER() OVER (PARTITION BY usr ORDER BY contest_id) AS diff
	FROM all_users
),
consecutive_medal_users AS (
	SELECT usr,contest_id,
		COUNT(diff) OVER (PARTITION BY usr,diff) AS num_consecutive_contests
	FROM ranked_users
),
inerview_candidates AS (
	SELECT usr
	FROM gold_medal_users
	UNION
	SELECT DISTINCT usr
	FROM consecutive_medal_users
	WHERE num_consecutive_contests >= 3
)
SELECT name,mail
FROM inerview_candidates ic
INNER JOIN users_1811 u ON ic.usr = u.user_id;

SELECT customer_id
FROM customers_1821
WHERE year = 2021 AND revenue > 0;

-- (Using MAX() function)

WITH updated_transactions AS (
	SELECT *,
		MAX(amount) OVER (PARTITION BY DATE_TRUNC('DAY',day)) as max_amount
	FROM transactions_1831
)
SELECT transactions_id
FROM updated_transactions
WHERE amount = max_amount
ORDER BY transactions_id;

--OR (Without using MAX() function)

WITH updated_transactions AS (
	SELECT *,
		RANK() OVER (PARTITION BY DATE_TRUNC('DAY',day) ORDER BY amount DESC) as rn
	FROM transactions_1831
)
SELECT transactions_id
FROM updated_transactions
WHERE rn= 1
ORDER BY transactions_id;

--OR (Without using window functions)

WITH max_amounts AS (
	SELECT DISTINCT t1.day,t1.amount AS max_amt
	FROM transactions_1831 t1
	LEFT JOIN transactions_1831 t2 ON DATE_TRUNC('DAY',t1.day)=DATE_TRUNC('DAY',t2.day) AND t2.amount>t1.amount
	WHERE t2.transactions_id IS NULL
)
SELECT transactions_id
FROM transactions_1831
WHERE (day,amount) IN (SELECT * FROM max_amounts)
ORDER BY transactions_id;

WITH all_matches AS(
	SELECT home_team_id,away_team_id,home_team_goals,away_team_goals
	FROM matches_1841
	UNION 
	SELECT away_team_id,home_team_id,away_team_goals,home_team_goals
	FROM matches_1841
),
report_data AS (
	SELECT *,
		CASE WHEN home_team_goals < away_team_goals THEN 0
			 WHEN home_team_goals > away_team_goals THEN 3
			 ELSE 1
		END AS home_team_points,
		CASE WHEN home_team_goals < away_team_goals THEN 3
			 WHEN home_team_goals > away_team_goals THEN 0
			 ELSE 1
		END AS away_team_points
	FROM all_matches
)
SELECT t.team_name AS team_name,
		COUNT(t.team_name) AS matches_played,
		SUM(home_team_points) AS points,
		SUM(home_team_goals) AS goals_for,
		SUM(away_team_goals) AS goals_against,
		SUM(home_team_goals)-SUM(away_team_goals) AS goals_diff
FROM report_data r
INNER JOIN teams_1841 t ON r.home_team_id = t.team_id
GROUP BY t.team_name
ORDER BY points DESC,goals_diff DESC,t.team_name;

WITH monthly_income_data AS (
	SELECT t.account_id,EXTRACT(MONTH FROM day) AS mnth,SUM(amount) AS total_income
	FROM transactions_1843 t
	INNER JOIN accounts_1843 a ON t.account_id = a.account_id AND t.type = 'Creditor'
	GROUP BY t.account_id,EXTRACT(MONTH FROM day),a.max_income
	HAVING SUM(amount)>a.max_income
)
SELECT DISTINCT m1.account_id
FROM monthly_income_data m1 
INNER JOIN monthly_income_data m2 
ON m1.account_id=m2.account_id AND m1.mnth + 1 = m2.mnth;

SELECT TO_CHAR(day,'Day, Month DD, YYYY') AS day
FROM days_1853;

WITH orders_stat AS (
	SELECT 	order_id,
			AVG(quantity) AS avg_quantity,
			MAX(quantity) AS max_quantity
	FROM orders_details_1867
	GROUP BY order_id
)
SELECT order_id
FROM orders_stat
WHERE max_quantity > ALL( SELECT avg_quantity
			 FROM orders_stat);

SELECT employee_id,
	CASE WHEN employee_id%2 <> 0 AND LEFT(name,1)<>'M' THEN salary 
		 ELSE 0
	END AS bonus
FROM employees_1873;

WITH cnts AS (
	SELECT *,
	COUNT(salary) OVER (PARTITION BY salary) AS c
	FROM employees_1875
)
SELECT employee_id,name,salary,
	DENSE_RANK() OVER (ORDER BY salary) AS team_id
FROM cnts
WHERE c <> 1;

SELECT user_id,MAX(time_stamp) AS last_stamp
FROM logins_1890
WHERE EXTRACT(YEAR FROM time_stamp) = 2020
GROUP BY user_id;

WITH friends AS (
	SELECT user1_id,user2_id
	FROM friendship_1892
	UNION ALL
	SELECT user2_id,user1_id
	FROM friendship_1892
),
possible_recommendation AS (
	SELECT f.user1_id,f.user2_id,l2.page_id
	FROM friends f
	INNER JOIN likes_1892 l1 ON f.user1_id = l1.user_id
	INNER JOIN likes_1892 l2 ON f.user2_id = l2.user_id AND l1.page_id <> l2.page_id
)
SELECT user1_id,page_id,COUNT(DISTINCT user2_id) AS friends_likes
FROM possible_recommendation
WHERE (user1_id,page_id) NOT IN (SELECT * FROM likes_1892)
GROUP BY user1_id,page_id
ORDER BY user1_id,page_id;

WITH tagged_accounts AS (
	SELECT *,
			CASE WHEN income < 20000 THEN 'Low Salary'
				 WHEN income >= 20000 AND income <= 50000 THEN 'Average Salary'
				 ELSE 'High Salary'
			END AS salary_tag
	FROM accounts_1907
),
salary_tags AS (
	SELECT UNNEST(ARRAY['Low Salary','Average Salary','High Salary']) AS salary_tag
)
SELECT st.salary_tag,COALESCE(COUNT(account_id),0) AS accounts_count
FROM salary_tags st
LEFT JOIN tagged_accounts ta ON st.salary_tag = ta.salary_tag
GROUP BY st.salary_tag;

WITH all_recommendations AS (
	SELECT l1.user_id AS user_id1,l2.user_id AS user_id2,COUNT(l1.song_id) AS listened_songs
	FROM listens_1917 l1
	INNER JOIN listens_1917 l2
	ON l1.user_id <> l2.user_id AND 
		l1.song_id = l2.song_id AND
		l1.day = l2.day
	GROUP BY l1.user_id,l2.user_id
	HAVING COUNT(l1.song_id)>=3
),
friends AS (
	SELECT user1_id,user2_id 
	FROM friendship_1917
	UNION 
	SELECT user2_id,user1_id 
	FROM friendship_1917
)
SELECT r.user_id1,r.user_id2
FROM all_recommendations r
LEFT JOIN friends f ON f.user1_id = r.user_id1 AND f.user2_id = r.user_id2
WHERE f.user1_id IS NULL;


WITH similar_friends AS(
	SELECT l1.user_id AS user_id1,l2.user_id AS user_id2,COUNT(l1.song_id)
	FROM listens_1919 l1
	INNER JOIN listens_1919 l2 
	ON l1.user_id < l2.user_id AND
		l1.song_id = l2.song_id AND
		l1.day = l2.day
	GROUP BY l1.user_id,l2.user_id
	HAVING COUNT(l1.song_id) >= 3
)
SELECT f.*
FROM similar_friends sf
INNER JOIN friendship_1919 f ON sf.user_id1 = f.user1_id AND sf.user_id2 = f.user2_id;

WITH users AS (
	SELECT user_id,
		ROUND(COUNT(CASE WHEN action = 'confirmed' THEN 1 ELSE NULL END)::NUMERIC/
		COUNT(1),2) AS confirmation_rate
	FROM confirmations_1934
	GROUP BY user_id
)
SELECT s.user_id,COALESCE(u.confirmation_rate,0.00)
FROM signups_1934 s
LEFT JOIN users u ON s.user_id = u.user_id;

SELECT c1.user_id
FROM confirmations_1939 c1
INNER JOIN confirmations_1939 c2 ON c1.user_id = c2.user_id AND c1.time_stamp < c2.time_stamp
WHERE EXTRACT(EPOCH FROM (c2.time_stamp-c1.time_stamp)) <= 24*60*60;

WITH friends AS (
	SELECT user1_id,user2_id
	FROM friendship_1949
	UNION
	SELECT user2_id,user1_id
	FROM friendship_1949
)
SELECT f1.user1_id,f2.user1_id,COUNT(f2.user1_id) AS num_mututal_friends
FROM friends f1
INNER JOIN friends f2 
ON f1.user1_id <> f2.user1_id AND f1.user2_id = f2.user2_id
WHERE f1.user1_id < f2.user1_id
GROUP BY f1.user1_id,f2.user1_id
HAVING COUNT(f2.user1_id) >=3

WITH common_followers AS (
	SELECT r1.user_id AS user1,r2.user_id AS user2,
		COUNT(r1.follower_id) AS cmn_fwlr
	FROM relations_1951 r1
	INNER JOIN relations_1951 r2
	ON r1.user_id < r2.user_id AND r1.follower_id = r2.follower_id
	GROUP BY r1.user_id,r2.user_id
),
max_common_followers AS (
	SELECT user1,user2,cmn_fwlr,
		MAX(cmn_fwlr) OVER () mx_cmn_fwlr
	FROM common_followers
)
SELECT user1,user2
FROM max_common_followers
WHERE mx_cmn_fwlr = cmn_fwlr;

SELECT COALESCE(e.employee_id,s.employee_id)
FROM employees_1965 e
FULL OUTER JOIN salaries_1965 s ON e.employee_id = s.employee_id
WHERE e.name IS NULL OR s.salary IS NULL;

WITH calls AS (
	SELECT caller_id,recipient_id,call_time
	FROM calls_1972
	UNION 
	SELECT recipient_id,caller_id,call_time
	FROM calls_1972
),
first_last_calls AS (
	SELECT *,
		MIN(call_time) OVER (PARTITION BY caller_id,EXTRACT(DAY FROM call_time)) AS first_call,
		MAX(call_time) OVER (PARTITION BY caller_id,EXTRACT(DAY FROM call_time)) AS last_call
	FROM calls
)
SELECT DISTINCT f.caller_id AS user_id
FROM first_last_calls f
INNER JOIN first_last_calls l
ON f.caller_id = l.caller_id AND 
	f.recipient_id = l.recipient_id AND 
	f.call_time = f.first_call AND 
	l.call_time = l.last_call;

SELECT DISTINCT e1.employee_id
FROM employees_1978 e1
LEFT JOIN employees_1978 e2 
ON e1.manager_id = e2.employee_id
WHERE e1.salary < 30000 AND e2.employee_id IS NULL;

SELECT s.school_id,COALESCE(MIN(e.score),-1) AS max_filled_students
FROM school_1988 s
LEFT JOIN exam_1988 e
ON s.capacity >= student_count
GROUP BY s.school_id
ORDER BY s.school_id;

WITH platforms AS (
	SELECT UNNEST(ARRAY['Android','IOS','Web']) AS pf
),
activities AS (
	SELECT UNNEST(ARRAY['Programming','Sports','Reading']) AS act
),
combinations AS (
	SELECT *
	FROM platforms p
	CROSS JOIN
	activities a
)
SELECT c.pf AS platform,c.act AS experiment_name,COALESCE(COUNT(e.experiment_id),0) AS num_experiments
FROM combinations c
LEFT JOIN experiments_1990 e ON e.platform = c.pf AND e.experiment_name = c.act
GROUP BY c.pf,c.act;

-- Table Name for Test-Case 1: candidates_2004_tc_2
-- Table Name for Test-Case 2: candidates_2004

WITH seniors AS (
	SELECT *,
		SUM(salary) OVER (ORDER BY salary,employee_id) AS occupied_budget,
		70000-SUM(salary) OVER (ORDER BY salary,employee_id) AS remaining_budget
	FROM candidates_2004_tc_2
	WHERE experience = 'Senior'
),
left_budget AS (
	SELECT COALESCE(MIN(remaining_budget),70000) AS budget
	FROM seniors
	WHERE remaining_budget >= 0
),
juniors AS (
	SELECT c.*,
		SUM(c.salary) OVER (ORDER BY c.salary,c.employee_id) AS occupied_budget,
		lb.budget-SUM(c.salary) OVER (ORDER BY c.salary,c.employee_id) AS remaining_budget
	FROM candidates_2004_tc_2 c
	CROSS JOIN left_budget lb
	WHERE experience = 'Junior'
),
hired_candidates AS (
	SELECT employee_id,experience
	FROM juniors
	WHERE remaining_budget >= 0
	UNION 
	SELECT employee_id,experience
	FROM seniors
	WHERE remaining_budget >= 0
	UNION
	SELECT NULL,'Senior'
	UNION
	SELECT NULL,'Junior'
)
SELECT experience,COALESCE(COUNT(employee_id),0)
FROM hired_candidates
GROUP BY experience;

-- Table Name for Test-Case 1: candidates_2010
-- Table Name for Test-Case 2: candidates_2010_tc_2

WITH seniors AS (
	SELECT *,
		SUM(salary) OVER (ORDER BY salary,employee_id) AS occupied_budget,
		70000-SUM(salary) OVER (ORDER BY salary,employee_id) AS remaining_budget
	FROM candidates_2010
	WHERE experience = 'Senior'
),
left_budget AS (
	SELECT COALESCE(MIN(remaining_budget),70000) AS budget
	FROM seniors
	WHERE remaining_budget >= 0
),
juniors AS (
	SELECT c.*,
		SUM(c.salary) OVER (ORDER BY c.salary,c.employee_id) AS occupied_budget,
		lb.budget-SUM(c.salary) OVER (ORDER BY c.salary,c.employee_id) AS remaining_budget
	FROM candidates_2010 c
	CROSS JOIN left_budget lb
	WHERE experience = 'Junior'
),
hired_candidates AS (
	SELECT *
	FROM juniors
	WHERE remaining_budget >= 0
	UNION 
	SELECT *
	FROM seniors
	WHERE remaining_budget >= 0
)
SELECT employee_id
FROM hired_candidates;

WITH accounts AS (
	SELECT account_id
	FROM subscriptions_2020
	WHERE EXTRACT(YEAR FROM start_date)<=2021 AND EXTRACT(YEAR FROM end_date)>=2021
)
SELECT COUNT(DISTINCT account_id) AS accounts_count
FROM streams_2020
WHERE EXTRACT(YEAR FROM stream_date) <> 2021 AND account_id IN (SELECT * FROM accounts);

SELECT problem_id
FROM problems_2026
WHERE (likes*100.0/(likes+dislikes)) < 60
ORDER BY problem_id;

SELECT candidate_id
FROM candidates_2041
WHERE years_of_exp >= 2 AND 
      interview_id IN (SELECT interview_id 
		       FROM rounds_2041
		       GROUP BY interview_id
		       HAVING SUM(score) > 15);

WITH categorized_members AS (
	SELECT v.member_id,
		CASE WHEN COUNT(p.visit_id)*100/COUNT(v.visit_id)>=80 THEN 'Diamond'
			 WHEN COUNT(p.visit_id)*100/COUNT(v.visit_id)>=50 AND 
			 	  COUNT(p.visit_id)*100/COUNT(v.visit_id)< 80 THEN 'Gold'
			 WHEN COUNT(p.visit_id)*100/COUNT(v.visit_id)< 50 THEN 'Silver'
		END AS category
	FROM visits_2051 v
	LEFT JOIN purchases_2051 p ON v.visit_id = p.visit_id
	GROUP BY v.member_id
)
SELECT m.*,COALESCE(category,'Bronze') AS category
FROM members_2051 m
LEFT JOIN categorized_members c ON m.member_id=c.member_id
ORDER BY m.member_id;

WITH fixed_amount AS (
	SELECT account_id,day,
		CASE WHEN type = 'Deposit' THEN amount
			 ELSE amount*-1
		END AS amount
	FROM transactions_2066
)
SELECT account_id,day,
	SUM(amount) OVER (PARTITION BY account_id ORDER BY day) AS balance
FROM fixed_amount
ORDER BY account_id,day;

WITH ny AS (
	SELECT COUNT(student_id) AS ny_cnt
	FROM newyork_2072
	WHERE score >= 90
),
cf AS (
	SELECT COUNT(student_id) AS cf_cnt
	FROM california_2072
	WHERE score >= 90
)
SELECT
	CASE WHEN ny_cnt > cf_cnt THEN 'New York University'
		 WHEN ny_cnt < cf_cnt THEN 'California University'
		 ELSE 'No Winner'
	END AS winner
FROM ny
INNER JOIN cf ON true;

SELECT COUNT(DISTINCT customer_id) AS rich_count
FROM store_2082
WHERE amount > 500;

SELECT * FROM orders_2084
WHERE (customer_id, order_type) 
IN (SELECT customer_id, MIN(order_type) 
    FROM orders_2084
    GROUP BY customer_id)

WITH airports AS (
	SELECT departure_airport AS airport,flights_count
	FROM flights_2112
	UNION ALL
	SELECT arrival_airport AS airport,flights_count
	FROM flights_2112
),
grouped_airport AS (
	SELECT airport,SUM(flights_count) AS flights_count
	FROM airports
	GROUP BY airport
)
SELECT airport
FROM grouped_airport
WHERE flights_count = (SELECT MAX(flights_count) FROM grouped_airport);

-- Table Name for Test-Case 1: terms_2118
-- Table Name for Test-Case 2: terms_2118_tc_2

WITH terms AS (
	SELECT power,ABS(factor) AS factor,
		(CASE WHEN factor < 0 THEN '-' ELSE '+' END) AS sign,
		(CASE WHEN power = 1 THEN CONCAT(ABS(factor),'X')
			  WHEN power = 0 THEN ABS(factor)::TEXT
			  ELSE CONCAT(ABS(factor),'X^',power)
		 END) AS term
	FROM terms_2118
)
SELECT CONCAT(STRING_AGG(CONCAT(sign,term),'' ORDER BY power DESC),'=0') AS equation
FROM terms;

-- Solution of the follow-up question (Table : terms_2118_tc_3)

WITH grouped_terms AS (
	SELECT power,SUM(factor) AS factor
	FROM terms_2118_tc_3
	GROUP BY power
),
terms AS (
	SELECT power,ABS(factor) AS factor,
		(CASE WHEN factor < 0 THEN '-' ELSE '+' END) AS sign,
		(CASE WHEN power = 1 THEN CONCAT(ABS(factor),'X')
			  WHEN power = 0 THEN ABS(factor)::TEXT
			  ELSE CONCAT(ABS(factor),'X^',power)
		 END) AS term
	FROM grouped_terms
)
SELECT CONCAT(STRING_AGG(CONCAT(sign,term),'' ORDER BY power DESC),'=0') AS equation
FROM terms;

WITH running_total_passengers AS (
	SELECT *,
			COUNT(passenger_id) OVER (PARTITION BY bus_id ORDER BY b.arrival_time) AS passengers
	FROM buses_2142 b
	LEFT JOIN passengers_2142 p ON p.arrival_time <= b.arrival_time
)
SELECT r1.bus_id,r1.passengers-COALESCE(r2.passengers,0) AS passengers_cnt
FROM running_total_passengers r1
LEFT JOIN running_total_passengers r2 ON r1.bus_id=r2.bus_id+1
ORDER BY r1.passengers,r1.bus_id;

--(Here we have made an assumption that smaller bus_id arrived first which will not be the case always)
-- Better Query

WITH running_total_passengers AS (
	SELECT b.bus_id,b.arrival_time AS bus_arrival_time,
			p.passenger_id,p.arrival_time AS passenger_arrival_time,
			COUNT(passenger_id) OVER (PARTITION BY bus_id ORDER BY b.arrival_time) AS passengers
	FROM buses_2142 b
	LEFT JOIN passengers_2142 p ON p.arrival_time <= b.arrival_time
)
SELECT DISTINCT r1.bus_id,r1.passengers-COALESCE(r2.passengers,0) AS passengers_cnt
FROM running_total_passengers r1
LEFT JOIN running_total_passengers r2 ON r1.bus_arrival_time > r2.bus_arrival_time AND r1.bus_id=r2.bus_id+1

WITH ranked_first_column AS (
	SELECT first_col,
		ROW_NUMBER() OVER (ORDER BY first_col) AS rn
	FROM data_2159
),
ranked_second_column AS (
	SELECT second_col,
		ROW_NUMBER() OVER (ORDER BY second_col DESC) AS rn
	FROM data_2159
)
SELECT f.first_col,s.second_col
FROM ranked_first_column f
JOIN ranked_second_column s ON f.rn = s.rn;

-- Table name for Test-Case1 : matches_2173
-- Table name for Test-Case2 : matches_2173_tc_2

WITH ranked_all_matches AS (
	SELECT *,
		ROW_NUMBER() OVER (PARTITION BY player_id ORDER BY match_day) AS rn
	FROM matches_2173
),
ranked_won_matches AS (
	SELECT player_id,result,rn,
		ROW_NUMBER() OVER (PARTITION BY player_id ORDER BY match_day) AS wrn
	FROM ranked_all_matches
	WHERE result = 'Win'
),
winning_streaks AS (
	SELECT player_id,result,rn-wrn AS diff,COUNT(1) AS winning_streak
	FROM ranked_won_matches
	GROUP BY player_id,result,rn-wrn
),
players AS (
	SELECT DISTINCT player_id
	FROM matches_2173
)
SELECT p.player_id,COALESCE(MAX(winning_streak),0) AS longest_streak
FROM players p
LEFT JOIN winning_streaks w ON w.player_id = p.player_id
GROUP BY p.player_id;

/*
pid	rn	wrn	diff
-----------------------------
1 	1	1   	0
1 	2	2   	0
1 	3	3   	0
1 	4	-   	-
1 	5	4   	1
1 	6	5   	1
1 	7	-   	-
1 	8	6   	2
1 	9	7   	2
1 	10	8   	2
1 	11	9   	2
1 	12	-   	-

2 	1 	-   	-
2 	2 	1   	1
2 	3 	-   	-
2 	4   	2   	2
2 	5   	3   	2

3 	1   	1   	0
*/

-- More Readable, But Requires two Joins
WITH before_update_ranked_teams AS (
	SELECT *,
		DENSE_RANK() OVER (ORDER BY points DESC,name) before_rn
	FROM team_points_2175
),
after_update_ranked_teams AS (
	SELECT tp.team_id,tp.name,tp.points+pc.points_change AS points,
		DENSE_RANK() OVER (ORDER BY tp.points+pc.points_change DESC,tp.name) after_rn
	FROM team_points_2175 tp
	INNER JOIN points_change_2175 pc ON tp.team_id = pc.team_id
)
SELECT au.team_id,au.name,au.points,au.after_rn-bu.before_rn AS rank_diff
FROM before_update_ranked_teams bu
INNER JOIN after_update_ranked_teams au ON bu.team_id = au.team_id
ORDER BY au.points DESC,au.name;

-- Using a single Join

SELECT tp.team_id,tp.name,
	DENSE_RANK() OVER (ORDER BY tp.points+pc.points_change DESC,tp.name)-
	DENSE_RANK() OVER (ORDER BY tp.points DESC,name) AS rank_diff
FROM team_points_2175 tp
INNER JOIN points_change_2175 pc ON tp.team_id = pc.team_id
ORDER BY tp.points+pc.points_change DESC,tp.name;

SELECT p.post_id,COALESCE(STRING_AGG(DISTINCT k.topic_id::TEXT,',' ORDER BY k.topic_id::TEXT),'Ambiguous!') AS topic
FROM posts_2199 p
LEFT JOIN keywords_2199 k ON POSITION(LOWER(' '||k.word||' ') IN LOWER(' '||p.content||' '))!=0
GROUP BY p.post_id;

SELECT COUNT(DISTINCT user_id) AS user_cnt
FROM purchases_2205
WHERE (DATE_TRUNC('DAY',time_stamp) BETWEEN '2022-03-08' AND '2022-03-20') AND amount >= 1000;

SELECT DISTINCT p1.user_id
FROM purchases_2228 p1
INNER JOIN purchases_2228 p2 
ON p1.purchase_id <> p2.purchase_id AND 
   p1.user_id = p2.user_id AND
   ABS(EXTRACT(DAY FROM p1.purchase_date)-EXTRACT(DAY FROM p2.purchase_date))<=7

SELECT DISTINCT user_id
FROM purchases_2230
WHERE (DATE_TRUNC('DAY',time_stamp) BETWEEN '2022-03-08' AND '2022-03-20') AND amount >= 1000;

SELECT r1.driver_id,COUNT(DISTINCT r2.ride_id) AS cnt
FROM rides_2238 r1
LEFT JOIN rides_2238 r2 ON r1.driver_id = r2.passenger_id
GROUP BY r1.driver_id;

CREATE OR REPLACE FUNCTION pivot_products_2252()
RETURNS TEXT
LANGUAGE PLPGSQL
AS 
$$
DECLARE
	store_name_array TEXT[];
	store_name TEXT;
	query_text TEXT;
BEGIN
	--query to find all the stores given in the table
	SELECT ARRAY_AGG(DISTINCT store ORDER BY store)
	INTO store_name_array
	FROM products_2252;
	--RAISE NOTICE 'store_name_array = %',store_name_array;
	
	--prepare query
	query_text := 'SELECT product_id, ';
	
	--prepare case statements for all the store_name in store_name_array
	FOREACH store_name IN ARRAY store_name_array
	LOOP
		query_text := query_text || 'SUM(CASE WHEN store = ''' || store_name || ''' THEN price ELSE NULL END) AS "' || store_name || '",';
	END LOOP;
	
	--prepare query
	query_text := LEFT(query_text,LENGTH(query_text)-1);
	query_text := query_text || ' FROM products_2252 GROUP BY product_id ORDER BY product_id';
	--RAISE NOTICE '%',query_text;
	
	--return the query as text
	RETURN query_text;
END $$;

SELECT pivot_products_2252();

-- output of the function:
SELECT product_id, 
		SUM(CASE WHEN store = 'LC_Store' THEN price ELSE NULL END) AS "LC_Store",
		SUM(CASE WHEN store = 'Nozama' THEN price ELSE NULL END) AS "Nozama",
		SUM(CASE WHEN store = 'Shop' THEN price ELSE NULL END) AS "Shop",
		SUM(CASE WHEN store = 'Souq' THEN price ELSE NULL END) AS "Souq" 
FROM products_2252 
GROUP BY product_id 
ORDER BY product_id;

--running this query manually will give us expected results

CREATE OR REPLACE FUNCTION unpivot_products_2253()
RETURNS TEXT
LANGUAGE PLPGSQL
AS
$$
DECLARE
	stores_array TEXT[];
	query_text TEXT := '';
	store_name TEXT;
BEGIN
	--query to find all the stores columns of the products table except product_id column
	SELECT ARRAY_AGG(column_name)
	INTO stores_array
	FROM information_schema.columns
	WHERE table_name = 'products_2253' AND column_name <> 'product_id';

	-- prepare query
	FOREACH store_name IN ARRAY stores_array
	LOOP
		query_text := query_text || 'SELECT product_id, ''' || store_name || ''' AS store, "' || store_name ||'" FROM products_2253 WHERE "' || store_name || '" IS NOT NULL';
		query_text := query_text || ' UNION ';
	END LOOP;
	
	query_text := LEFT(query_text,LENGTH(query_text)-6);
	query_text := query_text || ' ORDER BY product_id,store;';

	--return the query as text	
	RETURN query_text;
END
$$;

SELECT unpivot_products_2253();

-- output of the function:
SELECT product_id, 'LC_Store' AS store, "LC_Store" 
FROM products_2253 
WHERE "LC_Store" IS NOT NULL 
UNION 
SELECT product_id, 'Nozama' AS store, "Nozama" 
FROM products_2253 
WHERE "Nozama" IS NOT NULL 
UNION 
SELECT product_id, 'Shop' AS store, "Shop" 
FROM products_2253 
WHERE "Shop" IS NOT NULL 
UNION 
SELECT product_id, 'Souq' AS store, "Souq" 
FROM products_2253 
WHERE "Souq" IS NOT NULL  
ORDER BY product_id,store;

--running this query manually will give us expected results

WITH order_counts AS (
	SELECT product_id,EXTRACT(year FROM purchase_date) AS yr,COUNT(order_id) AS order_count 
	FROM orders_2292
	GROUP BY product_id,EXTRACT(year FROM purchase_date)
	HAVING COUNT(order_id) >= 3
	ORDER BY 1,2
)
SELECT DISTINCT oc1.product_id
FROM order_counts oc1
INNER JOIN order_counts oc2 
ON oc1.product_id = oc2.product_id AND oc1.yr+1 = oc2.yr;

SELECT
	COUNT(CASE WHEN EXTRACT(ISODOW FROM submit_date) > 5 THEN 1 ELSE NULL END) AS weekend_cnt,
	COUNT(CASE WHEN EXTRACT(ISODOW FROM submit_date) <= 5 THEN 1 ELSE NULL END) AS working_cnt
FROM tasks_2298;

WITH ranked_genders AS (
	SELECT *,
		RANK() OVER (PARTITION BY gender ORDER BY user_id) AS rnk,
		CASE WHEN gender = 'female' THEN 0
			 WHEN gender = 'other' THEN 1
			 ELSE 2
		END AS rnk2
	FROM genders_2308
)
SELECT user_id,gender
FROM ranked_genders
ORDER BY rnk,rnk2;

WITH ranked AS (
	SELECT *,
		RANK() OVER (PARTITION BY city_id ORDER BY degree DESC,day) AS rnk
	FROM weather_2314
)
SELECT city_id,day,degree
FROM ranked
WHERE rnk = 1
ORDER BY city_id;

WITH grouped_sales AS (
	SELECT product_id,user_id,SUM(quantity) AS quantity
	FROM sales_2324
	GROUP BY product_id,user_id
),
ranked_sales AS (
	SELECT s.product_id,s.user_id,s.quantity*p.price AS spent,
		RANK() OVER (PARTITION BY s.user_id ORDER BY s.quantity*p.price DESC) AS rnk
	FROM grouped_sales s
	INNER JOIN product_2324 p ON s.product_id = p.product_id
)
SELECT user_id,product_id
FROM ranked_sales
WHERE rnk = 1;

SELECT user_id,SUM(s.quantity*p.price) AS spending
FROM sales_2329 s
INNER JOIN product_2329 p ON s.product_id = p.product_id
GROUP BY user_id
ORDER BY spending DESC,user_id;

SELECT t1.team_name AS home_team,t2.team_name AS away_team
FROM teams_2339 t1
INNER JOIN teams_2339 t2 ON t1.team_name <> t2.team_name;

SELECT student_id,department_id,
	ROUND((RANK() OVER (PARTITION BY department_id ORDER BY mark DESC)-1)*100/
	(COUNT(student_id) OVER (PARTITION BY department_id)-1),2) AS percentage
FROM students_2346;

SELECT teacher_id,COUNT(DISTINCT subject_id)
FROM teacher_2356
GROUP BY teacher_id;


SELECT pc.product_id,pc.quantity,pc.quantity*pd.price AS price
FROM purchases_2362 pc
INNER JOIN products_2362 pd ON pc.product_id = pd.product_id
WHERE invoice_id = (SELECT pc.invoice_id
		    FROM purchases_2362 pc
		    INNER JOIN products_2362 pd ON pc.product_id = pd.product_id
		    GROUP BY pc.invoice_id
		    ORDER BY SUM(quantity*price) DESC,pc.invoice_id ASC
		    LIMIT 1);

WITH sales AS (
	SELECT c.salesperson_id,SUM(s.price) AS total
	FROM customer_2372 c
	INNER JOIN sales_2372 s ON c.customer_id = s.customer_id
	GROUP BY c.salesperson_id
)
SELECT sp.salesperson_id,sp.name,COALESCE(s.total,0) AS total
FROM salesperson_2372 sp
LEFT JOIN sales s ON sp.salesperson_id = s.salesperson_id;

SELECT *
FROM olympic_2377
ORDER BY gold_medals DESC,silver_medals DESC,bronze_medals DESC,country ASC;

WITH flagged_coffee AS (
	SELECT *,
		ROW_NUMBER() OVER () AS rn,
		CASE WHEN drink IS NOT NULL THEN 1 ELSE 0 END AS null_flag
	FROM coffee_shop_2388
),
running_sum AS (
	SELECT *,
		SUM(null_flag) OVER (ORDER BY rn) AS rsum
	FROM flagged_coffee
)
SELECT id,
	FIRST_VALUE(drink) OVER (PARTITION BY rsum) AS drink
FROM running_sum;

SELECT e.employee_id
FROM employees_2394 e
LEFT JOIN logs_2394 l ON e.employee_id = l.employee_id
GROUP BY e.employee_id,e.needed_hours
HAVING COALESCE(SUM(EXTRACT(hour FROM (out_time-in_time))+
		FLOOR((EXTRACT(minute FROM (out_time-in_time)) + CEIL(EXTRACT(second FROM (out_time-in_time))/60))/60)),0) < e.needed_hours

WITH RECURSIVE customer_purchase_years AS (
	SELECT customer_id,MIN(EXTRACT(YEAR FROM order_date)) AS min_year,MAX(EXTRACT(YEAR FROM order_date)) AS max_year
	FROM orders_2474
	GROUP BY customer_id
),
all_years AS (
	SELECT customer_id,min_year AS year,max_year
	FROM customer_purchase_years
	UNION
	SELECT customer_id,year+1 AS year,max_year
	FROM all_years
	WHERE year<max_year
),
all_year_purchases AS (
	SELECT ay.customer_id,ay.year,COALESCE(SUM(o.price),0) AS total_purchase
	FROM all_years ay
	LEFT JOIN orders_2474 o ON ay.customer_id = o.customer_id AND ay.year = EXTRACT(YEAR FROM o.order_date)
	GROUP BY ay.customer_id,ay.year
),
ranked AS (
	SELECT *,
		DENSE_RANK() OVER (PARTITION BY customer_id ORDER BY year) -
		DENSE_RANK() OVER (PARTITION BY customer_id ORDER BY total_purchase) AS diff
	FROM all_year_purchases
)
SELECT DISTINCT customer_id
FROM ranked
WHERE customer_id NOT IN (SELECT DISTINCT customer_id FROM ranked WHERE diff <> 0);

--OR--

WITH cte AS(
	SELECT customer_id, EXTRACT(YEAR FROM order_date) AS year, price
	FROM orders_2474
),
cte1 AS(
	SELECT customer_id,year,SUM(price) AS prices
	FROM cte
	GROUP BY customer_id,year
),
cte2 AS(
	SELECT *,
		DENSE_RANK() OVER(PARTITION BY customer_id ORDER BY prices) AS rn
	FROM cte1
),
cte3 AS(
	SELECT DISTINCT customer_id,year-rn AS new_line
	FROM cte2
)
SELECT customer_id 
FROM cte3
GROUP BY customer_id
HAVING (COUNT(new_line)=1);

-- Question 65
-- Table: Events

-- +---------------+---------+
-- | Column Name   | Type    |
-- +---------------+---------+
-- | business_id   | int     |
-- | event_type    | varchar |
-- | occurences    | int     | 
-- +---------------+---------+
-- (business_id, event_type) is the primary key of this table.
-- Each row in the table logs the info that an event of some type occured at some business for a number of times.
 

-- Write an SQL query to find all active businesses.

-- An active business is a business that has more than one event type with occurences greater than the average occurences of that event type among all businesses.

-- The query result format is in the following example:

-- Events table:
-- +-------------+------------+------------+
-- | business_id | event_type | occurences |
-- +-------------+------------+------------+
-- | 1           | reviews    | 7          |
-- | 3           | reviews    | 3          |
-- | 1           | ads        | 11         |
-- | 2           | ads        | 7          |
-- | 3           | ads        | 6          |
-- | 1           | page views | 3          |
-- | 2           | page views | 12         |
-- +-------------+------------+------------+

-- Result table:
-- +-------------+
-- | business_id |
-- +-------------+
-- | 1           |
-- +-------------+ 
-- Average for 'reviews', 'ads' and 'page views' are (7+3)/2=5, (11+7+6)/3=8, (3+12)/2=7.5 respectively.
-- Business with id 1 has 7 'reviews' events (more than 5) and 11 'ads' events (more than 8) so it is an active business.

-- Solution
select c.business_id
from(
select *
from events e
join
(select event_type as event, round(avg(occurences),2) as average from events group by event_type) b
on e.event_type = b.event) c
where c.occurences>c.average
group by c.business_id
having count(*) > 1

--Question 94
-- Table Accounts:

-- +---------------+---------+
-- | Column Name   | Type    |
-- +---------------+---------+
-- | id            | int     |
-- | name          | varchar |
-- +---------------+---------+
-- the id is the primary key for this table.
-- This table contains the account id and the user name of each account.
 

-- Table Logins:

-- +---------------+---------+
-- | Column Name   | Type    |
-- +---------------+---------+
-- | id            | int     |
-- | login_date    | date    |
-- +---------------+---------+
-- There is no primary key for this table, it may contain duplicates.
-- This table contains the account id of the user who logged in and the login date. A user may log in multiple times in the day.
 

-- Write an SQL query to find the id and the name of active users.

-- Active users are those who logged in to their accounts for 5 or more consecutive days.

-- Return the result table ordered by the id.

-- The query result format is in the following example:

-- Accounts table:
-- +----+----------+
-- | id | name     |
-- +----+----------+
-- | 1  | Winston  |
-- | 7  | Jonathan |
-- +----+----------+

-- Logins table:
-- +----+------------+
-- | id | login_date |
-- +----+------------+
-- | 7  | 2020-05-30 |
-- | 1  | 2020-05-30 |
-- | 7  | 2020-05-31 |
-- | 7  | 2020-06-01 |
-- | 7  | 2020-06-02 |
-- | 7  | 2020-06-02 |
-- | 7  | 2020-06-03 |
-- | 1  | 2020-06-07 |
-- | 7  | 2020-06-10 |
-- +----+------------+

-- Result table:
-- +----+----------+
-- | id | name     |
-- +----+----------+
-- | 7  | Jonathan |
-- +----+----------+
-- User Winston with id = 1 logged in 2 times only in 2 different days, so, Winston is not an active user.
-- User Jonathan with id = 7 logged in 7 times in 6 different days, five of them were consecutive days, so, Jonathan is an active user.

-- Solution
with t1 as (
select id,login_date,
lead(login_date,4) over(partition by id order by login_date) date_5
from (select distinct * from Logins) b
)

select distinct a.id, a.name from t1
inner join accounts a 
on t1.id = a.id
where datediff(t1.date_5,login_date) = 4
order by id
-- Question 77
-- Table: Friends

-- +---------------+---------+
-- | Column Name   | Type    |
-- +---------------+---------+
-- | id            | int     |
-- | name          | varchar |
-- | activity      | varchar |
-- +---------------+---------+
-- id is the id of the friend and primary key for this table.
-- name is the name of the friend.
-- activity is the name of the activity which the friend takes part in.
-- Table: Activities

-- +---------------+---------+
-- | Column Name   | Type    |
-- +---------------+---------+
-- | id            | int     |
-- | name          | varchar |
-- +---------------+---------+
-- id is the primary key for this table.
-- name is the name of the activity.
 

-- Write an SQL query to find the names of all the activities with neither maximum, nor minimum number of participants.

-- Return the result table in any order. Each activity in table Activities is performed by any person in the table Friends.

-- The query result format is in the following example:

-- Friends table:
-- +------+--------------+---------------+
-- | id   | name         | activity      |
-- +------+--------------+---------------+
-- | 1    | Jonathan D.  | Eating        |
-- | 2    | Jade W.      | Singing       |
-- | 3    | Victor J.    | Singing       |
-- | 4    | Elvis Q.     | Eating        |
-- | 5    | Daniel A.    | Eating        |
-- | 6    | Bob B.       | Horse Riding  |
-- +------+--------------+---------------+

-- Activities table:
-- +------+--------------+
-- | id   | name         |
-- +------+--------------+
-- | 1    | Eating       |
-- | 2    | Singing      |
-- | 3    | Horse Riding |
-- +------+--------------+

-- Result table:
-- +--------------+
-- | activity     |
-- +--------------+
-- | Singing      |
-- +--------------+

-- Eating activity is performed by 3 friends, maximum number of participants, (Jonathan D. , Elvis Q. and Daniel A.)
-- Horse Riding activity is performed by 1 friend, minimum number of participants, (Bob B.)
-- Singing is performed by 2 friends (Victor J. and Jade W.)

-- Solution
with t1 as(
select max(a.total) as total
from(
    select activity, count(*) as total
    from friends
    group by activity) a
	union all
	select min(b.total) as low
    from(
    select activity, count(*) as total
    from friends
    group by activity) b), 
t2 as
(
    select activity, count(*) as total
    from friends
    group by activity
)

select activity
from t1 right join t2
on t1.total = t2.total
where t1.total is null
	
-- Question 21
-- Table: ActorDirector

-- +-------------+---------+
-- | Column Name | Type    |
-- +-------------+---------+
-- | actor_id    | int     |
-- | director_id | int     |
-- | timestamp   | int     |
-- +-------------+---------+
-- timestamp is the primary key column for this table.
 

-- Write a SQL query for a report that provides the pairs (actor_id, director_id) where the actor have cooperated with the director at least 3 times.

-- Example:

-- ActorDirector table:
-- +-------------+-------------+-------------+
-- | actor_id    | director_id | timestamp   |
-- +-------------+-------------+-------------+
-- | 1           | 1           | 0           |
-- | 1           | 1           | 1           |
-- | 1           | 1           | 2           |
-- | 1           | 2           | 3           |
-- | 1           | 2           | 4           |
-- | 2           | 1           | 5           |
-- | 2           | 1           | 6           |
-- +-------------+-------------+-------------+

-- Result table:
-- +-------------+-------------+
-- | actor_id    | director_id |
-- +-------------+-------------+
-- | 1           | 1           |
-- +-------------+-------------+
-- The only pair is (1, 1) where they cooperated exactly 3 times.

-- Solution 
Select actor_id, director_id
from actordirector
group by actor_id, director_id
having count(*)>=3
-- Question 13
-- Table: Ads

-- +---------------+---------+
-- | Column Name   | Type    |
-- +---------------+---------+
-- | ad_id         | int     |
-- | user_id       | int     |
-- | action        | enum    |
-- +---------------+---------+
-- (ad_id, user_id) is the primary key for this table.
-- Each row of this table contains the ID of an Ad, the ID of a user and the action taken by this user regarding this Ad.
-- The action column is an ENUM type of ('Clicked', 'Viewed', 'Ignored').
 

-- A company is running Ads and wants to calculate the performance of each Ad.

-- Performance of the Ad is measured using Click-Through Rate (CTR) where:



-- Write an SQL query to find the ctr of each Ad.

-- Round ctr to 2 decimal points. Order the result table by ctr in descending order and by ad_id in ascending order in case of a tie.

-- The query result format is in the following example:

-- Ads table:
-- +-------+---------+---------+
-- | ad_id | user_id | action  |
-- +-------+---------+---------+
-- | 1     | 1       | Clicked |
-- | 2     | 2       | Clicked |
-- | 3     | 3       | Viewed  |
-- | 5     | 5       | Ignored |
-- | 1     | 7       | Ignored |
-- | 2     | 7       | Viewed  |
-- | 3     | 5       | Clicked |
-- | 1     | 4       | Viewed  |
-- | 2     | 11      | Viewed  |
-- | 1     | 2       | Clicked |
-- +-------+---------+---------+
-- Result table:
-- +-------+-------+
-- | ad_id | ctr   |
-- +-------+-------+
-- | 1     | 66.67 |
-- | 3     | 50.00 |
-- | 2     | 33.33 |
-- | 5     | 0.00  |
-- +-------+-------+
-- for ad_id = 1, ctr = (2/(2+1)) * 100 = 66.67
-- for ad_id = 2, ctr = (1/(1+2)) * 100 = 33.33
-- for ad_id = 3, ctr = (1/(1+1)) * 100 = 50.00
-- for ad_id = 5, ctr = 0.00, Note that ad_id = 5 has no clicks or views.
-- Note that we don't care about Ignored Ads.
-- Result table is ordered by the ctr. in case of a tie we order them by ad_id

-- Solution
with t1 as(
select ad_id, sum(case when action in ('Clicked') then 1 else 0 end) as clicked
from ads
group by ad_id
)

, t2 as
(
Select ad_id as ad, sum(case when action in ('Clicked','Viewed') then 1 else 0 end) as total
from ads
group by ad_id
)

Select a.ad_id, coalesce(round((clicked +0.0)/nullif((total +0.0),0)*100,2),0) as ctr
from
(
select *
from t1 join t2
on t1.ad_id = t2.ad) a
order by ctr desc, ad_id
-- Question 55
-- Table: Employees

-- +---------------+---------+
-- | Column Name   | Type    |
-- +---------------+---------+
-- | employee_id   | int     |
-- | employee_name | varchar |
-- | manager_id    | int     |
-- +---------------+---------+
-- employee_id is the primary key for this table.
-- Each row of this table indicates that the employee with ID employee_id and name employee_name reports his
-- work to his/her direct manager with manager_id
-- The head of the company is the employee with employee_id = 1.
 

-- Write an SQL query to find employee_id of all employees that directly or indirectly report their work to the head of the company.

-- The indirect relation between managers will not exceed 3 managers as the company is small.

-- Return result table in any order without duplicates.

-- The query result format is in the following example:

-- Employees table:
-- +-------------+---------------+------------+
-- | employee_id | employee_name | manager_id |
-- +-------------+---------------+------------+
-- | 1           | Boss          | 1          |
-- | 3           | Alice         | 3          |
-- | 2           | Bob           | 1          |
-- | 4           | Daniel        | 2          |
-- | 7           | Luis          | 4          |
-- | 8           | Jhon          | 3          |
-- | 9           | Angela        | 8          |
-- | 77          | Robert        | 1          |
-- +-------------+---------------+------------+

-- Result table:
-- +-------------+
-- | employee_id |
-- +-------------+
-- | 2           |
-- | 77          |
-- | 4           |
-- | 7           |
-- +-------------+

-- The head of the company is the employee with employee_id 1.
-- The employees with employee_id 2 and 77 report their work directly to the head of the company.
-- The employee with employee_id 4 report his work indirectly to the head of the company 4 --> 2 --> 1. 
-- The employee with employee_id 7 report his work indirectly to the head of the company 7 --> 4 --> 2 --> 1.
-- The employees with employee_id 3, 8 and 9 don't report their work to head of company directly or indirectly.

-- Solution
select employee_id
from employees
where manager_id = 1 and employee_id != 1
union
select employee_id
from employees
where manager_id = any (select employee_id
from employees
where manager_id = 1 and employee_id != 1)
union
select employee_id
from employees
where manager_id = any (select employee_id
from employees
where manager_id = any (select employee_id
from employees
where manager_id = 1 and employee_id != 1))
-- Question 66
-- Table: Sales

-- +---------------+---------+
-- | Column Name   | Type    |
-- +---------------+---------+
-- | sale_date     | date    |
-- | fruit         | enum    | 
-- | sold_num      | int     | 
-- +---------------+---------+
-- (sale_date,fruit) is the primary key for this table.
-- This table contains the sales of "apples" and "oranges" sold each day.
 

-- Write an SQL query to report the difference between number of apples and oranges sold each day.

-- Return the result table ordered by sale_date in format ('YYYY-MM-DD').

-- The query result format is in the following example:

 

-- Sales table:
-- +------------+------------+-------------+
-- | sale_date  | fruit      | sold_num    |
-- +------------+------------+-------------+
-- | 2020-05-01 | apples     | 10          |
-- | 2020-05-01 | oranges    | 8           |
-- | 2020-05-02 | apples     | 15          |
-- | 2020-05-02 | oranges    | 15          |
-- | 2020-05-03 | apples     | 20          |
-- | 2020-05-03 | oranges    | 0           |
-- | 2020-05-04 | apples     | 15          |
-- | 2020-05-04 | oranges    | 16          |
-- +------------+------------+-------------+

-- Result table:
-- +------------+--------------+
-- | sale_date  | diff         |
-- +------------+--------------+
-- | 2020-05-01 | 2            |
-- | 2020-05-02 | 0            |
-- | 2020-05-03 | 20           |
-- | 2020-05-04 | -1           |
-- +------------+--------------+

-- Day 2020-05-01, 10 apples and 8 oranges were sold (Difference  10 - 8 = 2).
-- Day 2020-05-02, 15 apples and 15 oranges were sold (Difference 15 - 15 = 0).
-- Day 2020-05-03, 20 apples and 0 oranges were sold (Difference 20 - 0 = 20).
-- Day 2020-05-04, 15 apples and 16 oranges were sold (Difference 15 - 16 = -1).

-- Solution
Select sale_date, sold_num-sold as diff
from 
((select *
from sales
where fruit = 'apples') a
join 
(select sale_date as sale, fruit, sold_num as sold
from sales
where fruit = 'oranges') b
on a.sale_date = b.sale) 
-- Question 81
-- Table: Views

-- +---------------+---------+
-- | Column Name   | Type    |
-- +---------------+---------+
-- | article_id    | int     |
-- | author_id     | int     |
-- | viewer_id     | int     |
-- | view_date     | date    |
-- +---------------+---------+
-- There is no primary key for this table, it may have duplicate rows.
-- Each row of this table indicates that some viewer viewed an article (written by some author) on some date. 
-- Note that equal author_id and viewer_id indicate the same person.
 

-- Write an SQL query to find all the people who viewed more than one article on the same date, sorted in ascending order by their id.

-- The query result format is in the following example:

-- Views table:
-- +------------+-----------+-----------+------------+
-- | article_id | author_id | viewer_id | view_date  |
-- +------------+-----------+-----------+------------+
-- | 1          | 3         | 5         | 2019-08-01 |
-- | 3          | 4         | 5         | 2019-08-01 |
-- | 1          | 3         | 6         | 2019-08-02 |
-- | 2          | 7         | 7         | 2019-08-01 |
-- | 2          | 7         | 6         | 2019-08-02 |
-- | 4          | 7         | 1         | 2019-07-22 |
-- | 3          | 4         | 4         | 2019-07-21 |
-- | 3          | 4         | 4         | 2019-07-21 |
-- +------------+-----------+-----------+------------+

-- Result table:
-- +------+
-- | id   |
-- +------+
-- | 5    |
-- | 6    |
-- +------+

-- Solution
select distinct viewer_id as id#, count(distinct article_id) as total
from views
group by viewer_id, view_date
having count(distinct article_id)>1
order by 1
-- Question 42
-- Table: Views

-- +---------------+---------+
-- | Column Name   | Type    |
-- +---------------+---------+
-- | article_id    | int     |
-- | author_id     | int     |
-- | viewer_id     | int     |
-- | view_date     | date    |
-- +---------------+---------+
-- There is no primary key for this table, it may have duplicate rows.
-- Each row of this table indicates that some viewer viewed an article (written by some author) on some date. 
-- Note that equal author_id and viewer_id indicate the same person.
 

-- Write an SQL query to find all the authors that viewed at least one of their own articles, sorted in ascending order by their id.

-- The query result format is in the following example:

-- Views table:
-- +------------+-----------+-----------+------------+
-- | article_id | author_id | viewer_id | view_date  |
-- +------------+-----------+-----------+------------+
-- | 1          | 3         | 5         | 2019-08-01 |
-- | 1          | 3         | 6         | 2019-08-02 |
-- | 2          | 7         | 7         | 2019-08-01 |
-- | 2          | 7         | 6         | 2019-08-02 |
-- | 4          | 7         | 1         | 2019-07-22 |
-- | 3          | 4         | 4         | 2019-07-21 |
-- | 3          | 4         | 4         | 2019-07-21 |
-- +------------+-----------+-----------+------------+

-- Result table:
-- +------+
-- | id   |
-- +------+
-- | 4    |
-- | 7    |
-- +------+

-- Solution
select distinct author_id as id
from views
where author_id = viewer_id
order by author_id
-- Question 108
-- Given two tables as below, write a query to display the comparison result (higher/lower/same) of the 
-- average salary of employees in a department to the company's average salary.
 

-- Table: salary
-- | id | employee_id | amount | pay_date   |
-- |----|-------------|--------|------------|
-- | 1  | 1           | 9000   | 2017-03-31 |
-- | 2  | 2           | 6000   | 2017-03-31 |
-- | 3  | 3           | 10000  | 2017-03-31 |
-- | 4  | 1           | 7000   | 2017-02-28 |
-- | 5  | 2           | 6000   | 2017-02-28 |
-- | 6  | 3           | 8000   | 2017-02-28 |
 

-- The employee_id column refers to the employee_id in the following table employee.
 

-- | employee_id | department_id |
-- |-------------|---------------|
-- | 1           | 1             |
-- | 2           | 2             |
-- | 3           | 2             |
 

-- So for the sample data above, the result is:
 

-- | pay_month | department_id | comparison  |
-- |-----------|---------------|-------------|
-- | 2017-03   | 1             | higher      |
-- | 2017-03   | 2             | lower       |
-- | 2017-02   | 1             | same        |
-- | 2017-02   | 2             | same        |
 

-- Explanation
 

-- In March, the company's average salary is (9000+6000+10000)/3 = 8333.33...
 

-- The average salary for department '1' is 9000, which is the salary of employee_id '1' since there is only one employee in this department. So the comparison result is 'higher' since 9000 > 8333.33 obviously.
 

-- The average salary of department '2' is (6000 + 10000)/2 = 8000, which is the average of employee_id '2' and '3'. So the comparison result is 'lower' since 8000 < 8333.33.
 

-- With he same formula for the average salary comparison in February, the result is 'same' since both the department '1' and '2' have the same average salary with the company, which is 7000.

-- Solution
with t1 as(
select date_format(pay_date,'%Y-%m') as pay_month, department_id, avg(amount) over(partition by month(pay_date),department_id) as dept_avg, 
avg(amount) over(partition by month(pay_date)) as comp_avg
from salary s join employee e
using (employee_id))

select distinct pay_month, department_id, 
case when dept_avg>comp_avg then "higher"
when dept_avg = comp_avg then "same"
else "lower"
end as comparison
from t1
order by 1 desc

-- Question 39
-- Table: Prices

-- +---------------+---------+
-- | Column Name   | Type    |
-- +---------------+---------+
-- | product_id    | int     |
-- | start_date    | date    |
-- | end_date      | date    |
-- | price         | int     |
-- +---------------+---------+
-- (product_id, start_date, end_date) is the primary key for this table.
-- Each row of this table indicates the price of the product_id in the period from start_date to end_date.
-- For each product_id there will be no two overlapping periods. That means there will be no two intersecting periods for the same product_id.
 

-- Table: UnitsSold

-- +---------------+---------+
-- | Column Name   | Type    |
-- +---------------+---------+
-- | product_id    | int     |
-- | purchase_date | date    |
-- | units         | int     |
-- +---------------+---------+
-- There is no primary key for this table, it may contain duplicates.
-- Each row of this table indicates the date, units and product_id of each product sold. 
 

-- Write an SQL query to find the average selling price for each product.

-- average_price should be rounded to 2 decimal places.

-- The query result format is in the following example:

-- Prices table:
-- +------------+------------+------------+--------+
-- | product_id | start_date | end_date   | price  |
-- +------------+------------+------------+--------+
-- | 1          | 2019-02-17 | 2019-02-28 | 5      |
-- | 1          | 2019-03-01 | 2019-03-22 | 20     |
-- | 2          | 2019-02-01 | 2019-02-20 | 15     |
-- | 2          | 2019-02-21 | 2019-03-31 | 30     |
-- +------------+------------+------------+--------+
 
-- UnitsSold table:
-- +------------+---------------+-------+
-- | product_id | purchase_date | units |
-- +------------+---------------+-------+
-- | 1          | 2019-02-25    | 100   |
-- | 1          | 2019-03-01    | 15    |
-- | 2          | 2019-02-10    | 200   |
-- | 2          | 2019-03-22    | 30    |
-- +------------+---------------+-------+

-- Result table:
-- +------------+---------------+
-- | product_id | average_price |
-- +------------+---------------+
-- | 1          | 6.96          |
-- | 2          | 16.96         |
-- +------------+---------------+
-- Average selling price = Total Price of Product / Number of products sold.
-- Average selling price for product 1 = ((100 * 5) + (15 * 20)) / 115 = 6.96
-- Average selling price for product 2 = ((200 * 15) + (30 * 30)) / 230 = 16.96

-- Solution
Select d.product_id, round((sum(price*units)+0.00)/(sum(units)+0.00),2) as average_price
from(
Select *
from prices p
natural join 
unitssold u
where u.purchase_date between p.start_date and p.end_date) d
group by d.product_id 
-- Question 5
-- There is a table World

-- +-----------------+------------+------------+--------------+---------------+
-- | name            | continent  | area       | population   | gdp           |
-- +-----------------+------------+------------+--------------+---------------+
-- | Afghanistan     | Asia       | 652230     | 25500100     | 20343000      |
-- | Albania         | Europe     | 28748      | 2831741      | 12960000      |
-- | Algeria         | Africa     | 2381741    | 37100000     | 188681000     |
-- | Andorra         | Europe     | 468        | 78115        | 3712000       |
-- | Angola          | Africa     | 1246700    | 20609294     | 100990000     |
-- +-----------------+------------+------------+--------------+---------------+
-- A country is big if it has an area of bigger than 3 million square km or a population of more than 25 million.

-- Write a SQL solution to output big countries' name, population and area.

-- For example, according to the above table, we should output:

-- +--------------+-------------+--------------+
-- | name         | population  | area         |
-- +--------------+-------------+--------------+
-- | Afghanistan  | 25500100    | 652230       |
-- | Algeria      | 37100000    | 2381741      |
-- +--------------+-------------+--------------+


-- Solution
Select name, population, area
from world
where population > 25000000 OR area>3000000
-- Question 24
-- Table my_numbers contains many numbers in column num including duplicated ones.
-- Can you write a SQL query to find the biggest number, which only appears once.

-- +---+
-- |num|
-- +---+
-- | 8 |
-- | 8 |
-- | 3 |
-- | 3 |
-- | 1 |
-- | 4 |
-- | 5 |
-- | 6 | 
-- For the sample data above, your query should return the following result:
-- +---+
-- |num|
-- +---+
-- | 6 |
-- Note:
-- If there is no such number, just output null.

-- Solution
Select max(a.num) as num
from 
( 
    select num, count(*)
    from my_numbers
    group by num
    having count(*)=1
) a
-- Question 74
-- Table Salaries:

-- +---------------+---------+
-- | Column Name   | Type    |
-- +---------------+---------+
-- | company_id    | int     |
-- | employee_id   | int     |
-- | employee_name | varchar |
-- | salary        | int     |
-- +---------------+---------+
-- (company_id, employee_id) is the primary key for this table.
-- This table contains the company id, the id, the name and the salary for an employee.
 

-- Write an SQL query to find the salaries of the employees after applying taxes.

-- The tax rate is calculated for each company based on the following criteria:

-- 0% If the max salary of any employee in the company is less than 1000$.
-- 24% If the max salary of any employee in the company is in the range [1000, 10000] inclusive.
-- 49% If the max salary of any employee in the company is greater than 10000$.
-- Return the result table in any order. Round the salary to the nearest integer.

-- The query result format is in the following example:

-- Salaries table:
-- +------------+-------------+---------------+--------+
-- | company_id | employee_id | employee_name | salary |
-- +------------+-------------+---------------+--------+
-- | 1          | 1           | Tony          | 2000   |
-- | 1          | 2           | Pronub        | 21300  |
-- | 1          | 3           | Tyrrox        | 10800  |
-- | 2          | 1           | Pam           | 300    |
-- | 2          | 7           | Bassem        | 450    |
-- | 2          | 9           | Hermione      | 700    |
-- | 3          | 7           | Bocaben       | 100    |
-- | 3          | 2           | Ognjen        | 2200   |
-- | 3          | 13          | Nyancat       | 3300   |
-- | 3          | 15          | Morninngcat   | 1866   |
-- +------------+-------------+---------------+--------+

-- Result table:
-- +------------+-------------+---------------+--------+
-- | company_id | employee_id | employee_name | salary |
-- +------------+-------------+---------------+--------+
-- | 1          | 1           | Tony          | 1020   |
-- | 1          | 2           | Pronub        | 10863  |
-- | 1          | 3           | Tyrrox        | 5508   |
-- | 2          | 1           | Pam           | 300    |
-- | 2          | 7           | Bassem        | 450    |
-- | 2          | 9           | Hermione      | 700    |
-- | 3          | 7           | Bocaben       | 76     |
-- | 3          | 2           | Ognjen        | 1672   |
-- | 3          | 13          | Nyancat       | 2508   |
-- | 3          | 15          | Morninngcat   | 5911   |
-- +------------+-------------+---------------+--------+
-- For company 1, Max salary is 21300. Employees in company 1 have taxes = 49%
-- For company 2, Max salary is 700. Employees in company 2 have taxes = 0%
-- For company 3, Max salary is 7777. Employees in company 3 have taxes = 24%
-- The salary after taxes = salary - (taxes percentage / 100) * salary
-- For example, Salary for Morninngcat (3, 15) after taxes = 7777 - 7777 * (24 / 100) = 7777 - 1866.48 = 5910.52, which is rounded to 5911.

-- Solution
with t1 as (
select company_id, employee_id, employee_name, salary as sa, max(salary) over(partition by company_id) as maximum
from salaries)

select company_id, employee_id, employee_name,
case when t1.maximum<1000 then t1.sa
when t1.maximum between 1000 and 10000 then round(t1.sa*.76,0)
else round(t1.sa*.51,0)
end as salary
from t1
-- Question 61
-- Table: Stocks

-- +---------------+---------+
-- | Column Name   | Type    |
-- +---------------+---------+
-- | stock_name    | varchar |
-- | operation     | enum    |
-- | operation_day | int     |
-- | price         | int     |
-- +---------------+---------+
-- (stock_name, day) is the primary key for this table.
-- The operation column is an ENUM of type ('Sell', 'Buy')
-- Each row of this table indicates that the stock which has stock_name had an operation on the day operation_day with the price.
-- It is guaranteed that each 'Sell' operation for a stock has a corresponding 'Buy' operation in a previous day.
 

-- Write an SQL query to report the Capital gain/loss for each stock.

-- The capital gain/loss of a stock is total gain or loss after buying and selling the stock one or many times.

-- Return the result table in any order.

-- The query result format is in the following example:

-- Stocks table:
-- +---------------+-----------+---------------+--------+
-- | stock_name    | operation | operation_day | price  |
-- +---------------+-----------+---------------+--------+
-- | Leetcode      | Buy       | 1             | 1000   |
-- | Corona Masks  | Buy       | 2             | 10     |
-- | Leetcode      | Sell      | 5             | 9000   |
-- | Handbags      | Buy       | 17            | 30000  |
-- | Corona Masks  | Sell      | 3             | 1010   |
-- | Corona Masks  | Buy       | 4             | 1000   |
-- | Corona Masks  | Sell      | 5             | 500    |
-- | Corona Masks  | Buy       | 6             | 1000   |
-- | Handbags      | Sell      | 29            | 7000   |
-- | Corona Masks  | Sell      | 10            | 10000  |
-- +---------------+-----------+---------------+--------+

-- Result table:
-- +---------------+-------------------+
-- | stock_name    | capital_gain_loss |
-- +---------------+-------------------+
-- | Corona Masks  | 9500              |
-- | Leetcode      | 8000              |
-- | Handbags      | -23000            |
-- +---------------+-------------------+
-- Leetcode stock was bought at day 1 for 1000$ and was sold at day 5 for 9000$. Capital gain = 9000 - 1000 = 8000$.
-- Handbags stock was bought at day 17 for 30000$ and was sold at day 29 for 7000$. Capital loss = 7000 - 30000 = -23000$.
-- Corona Masks stock was bought at day 1 for 10$ and was sold at day 3 for 1010$. It was bought again at day 4 for 1000$ and was sold at day 5 for 500$. At last, it was bought at day 6 for 1000$ and was sold at day 10 for 10000$. Capital gain/loss is the sum of capital gains/losses for each ('Buy' --> 'Sell') 
-- operation = (1010 - 10) + (500 - 1000) + (10000 - 1000) = 1000 - 500 + 9000 = 9500$.

-- Solution
select stock_name, (one-two) as capital_gain_loss
from(
(select stock_name, sum(price) as one
from stocks
where operation = 'Sell'
group by stock_name) b
left join
(select stock_name as name, sum(price) as two
from stocks
where operation = 'Buy'
group by stock_name) c
on b.stock_name = c.name)
order by capital_gain_loss desc
-- Question7
-- There is a table courses with columns: student and class

-- Please list out all classes which have more than or equal to 5 students.

-- For example, the table:

-- +---------+------------+
-- | student | class      |
-- +---------+------------+
-- | A       | Math       |
-- | B       | English    |
-- | C       | Math       |
-- | D       | Biology    |
-- | E       | Math       |
-- | F       | Computer   |
-- | G       | Math       |
-- | H       | Math       |
-- | I       | Math       |
-- +---------+------------+

-- Solution
select class
from courses
group by class
having count(distinct student)>=5
-- Question 14
-- Table: Person

-- +-------------+---------+
-- | Column Name | Type    |
-- +-------------+---------+
-- | PersonId    | int     |
-- | FirstName   | varchar |
-- | LastName    | varchar |
-- +-------------+---------+
-- PersonId is the primary key column for this table.
-- Table: Address

-- +-------------+---------+
-- | Column Name | Type    |
-- +-------------+---------+
-- | AddressId   | int     |
-- | PersonId    | int     |
-- | City        | varchar |
-- | State       | varchar |
-- +-------------+---------+
-- AddressId is the primary key column for this table.
 

-- Write a SQL query for a report that provides the following information for each person in the Person table,
-- regardless if there is an address for each of those people:

-- FirstName, LastName, City, State


-- Solution
select FirstName, LastName, City, State
from Person P left join Address A
on P.PersonId = A.PersonId
-- Question 37
-- Several friends at a cinema ticket office would like to reserve consecutive available seats.
-- Can you help to query all the consecutive available seats order by the seat_id using the following cinema table?
-- | seat_id | free |
-- |---------|------|
-- | 1       | 1    |
-- | 2       | 0    |
-- | 3       | 1    |
-- | 4       | 1    |
-- | 5       | 1    |
 

-- Your query should return the following result for the sample case above.
 

-- | seat_id |
-- |---------|
-- | 3       |
-- | 4       |
-- | 5       |
-- Note:
-- The seat_id is an auto increment int, and free is bool ('1' means free, and '0' means occupied.).
-- Consecutive available seats are more than 2(inclusive) seats consecutively available.

-- Solution
Select seat_id
from(
select seat_id, free,
lead(free,1) over() as next,
lag(free,1) over() as prev
from cinema) a
where a.free=True and (next = True or prev=True)
order by seat_id
-- Question 52
-- Write a SQL query to find all numbers that appear at least three times consecutively.

-- +----+-----+
-- | Id | Num |
-- +----+-----+
-- | 1  |  1  |
-- | 2  |  1  |
-- | 3  |  1  |
-- | 4  |  2  |
-- | 5  |  1  |
-- | 6  |  2  |
-- | 7  |  2  |
-- +----+-----+
-- For example, given the above Logs table, 1 is the only number that appears consecutively for at least three times.

-- +-----------------+
-- | ConsecutiveNums |
-- +-----------------+
-- | 1               |
-- +-----------------+

-- Solution
select distinct a.num as ConsecutiveNums
from(
select *,
lag(num) over() as prev,
lead(num) over() as next
from logs) a
where a.num = a.prev and a.num=a.next
-- Question 87
-- A university uses 2 data tables, student and department, to store data about its students
-- and the departments associated with each major.

-- Write a query to print the respective department name and number of students majoring in each
-- department for all departments in the department table (even ones with no current students).

-- Sort your results by descending number of students; if two or more departments have the same number of students, 
-- then sort those departments alphabetically by department name.

-- The student is described as follow:

-- | Column Name  | Type      |
-- |--------------|-----------|
-- | student_id   | Integer   |
-- | student_name | String    |
-- | gender       | Character |
-- | dept_id      | Integer   |
-- where student_id is the student's ID number, student_name is the student's name, gender is their gender, and dept_id is the department ID associated with their declared major.

-- And the department table is described as below:

-- | Column Name | Type    |
-- |-------------|---------|
-- | dept_id     | Integer |
-- | dept_name   | String  |
-- where dept_id is the department's ID number and dept_name is the department name.

-- Here is an example input:
-- student table:

-- | student_id | student_name | gender | dept_id |
-- |------------|--------------|--------|---------|
-- | 1          | Jack         | M      | 1       |
-- | 2          | Jane         | F      | 1       |
-- | 3          | Mark         | M      | 2       |
-- department table:

-- | dept_id | dept_name   |
-- |---------|-------------|
-- | 1       | Engineering |
-- | 2       | Science     |
-- | 3       | Law         |
-- The Output should be:

-- | dept_name   | student_number |
-- |-------------|----------------|
-- | Engineering | 2              |
-- | Science     | 1              |
-- | Law         | 0              |

-- Solution
select dept_name, count(s.dept_id) as student_number
from department d
left join student s
on d.dept_id = s.dept_id
group by d.dept_id
order by count(s.dept_id) desc, dept_name
-- Question 110
-- Table Person:

-- +----------------+---------+
-- | Column Name    | Type    |
-- +----------------+---------+
-- | id             | int     |
-- | name           | varchar |
-- | phone_number   | varchar |
-- +----------------+---------+
-- id is the primary key for this table.
-- Each row of this table contains the name of a person and their phone number.
-- Phone number will be in the form 'xxx-yyyyyyy' where xxx is the country code (3 characters) and yyyyyyy is the 
-- phone number (7 characters) where x and y are digits. Both can contain leading zeros.
-- Table Country:

-- +----------------+---------+
-- | Column Name    | Type    |
-- +----------------+---------+
-- | name           | varchar |
-- | country_code   | varchar |
-- +----------------+---------+
-- country_code is the primary key for this table.
-- Each row of this table contains the country name and its code. country_code will be in the form 'xxx' where x is digits.
 

-- Table Calls:

-- +-------------+------+
-- | Column Name | Type |
-- +-------------+------+
-- | caller_id   | int  |
-- | callee_id   | int  |
-- | duration    | int  |
-- +-------------+------+
-- There is no primary key for this table, it may contain duplicates.
-- Each row of this table contains the caller id, callee id and the duration of the call in minutes. caller_id != callee_id
-- A telecommunications company wants to invest in new countries. The country intends to invest in the countries where the average call duration of the calls in this country is strictly greater than the global average call duration.

-- Write an SQL query to find the countries where this company can invest.

-- Return the result table in any order.

-- The query result format is in the following example.

-- Person table:
-- +----+----------+--------------+
-- | id | name     | phone_number |
-- +----+----------+--------------+
-- | 3  | Jonathan | 051-1234567  |
-- | 12 | Elvis    | 051-7654321  |
-- | 1  | Moncef   | 212-1234567  |
-- | 2  | Maroua   | 212-6523651  |
-- | 7  | Meir     | 972-1234567  |
-- | 9  | Rachel   | 972-0011100  |
-- +----+----------+--------------+

-- Country table:
-- +----------+--------------+
-- | name     | country_code |
-- +----------+--------------+
-- | Peru     | 051          |
-- | Israel   | 972          |
-- | Morocco  | 212          |
-- | Germany  | 049          |
-- | Ethiopia | 251          |
-- +----------+--------------+

-- Calls table:
-- +-----------+-----------+----------+
-- | caller_id | callee_id | duration |
-- +-----------+-----------+----------+
-- | 1         | 9         | 33       |
-- | 2         | 9         | 4        |
-- | 1         | 2         | 59       |
-- | 3         | 12        | 102      |
-- | 3         | 12        | 330      |
-- | 12        | 3         | 5        |
-- | 7         | 9         | 13       |
-- | 7         | 1         | 3        |
-- | 9         | 7         | 1        |
-- | 1         | 7         | 7        |
-- +-----------+-----------+----------+

-- Result table:
-- +----------+
-- | country  |
-- +----------+
-- | Peru     |
-- +----------+
-- The average call duration for Peru is (102 + 102 + 330 + 330 + 5 + 5) / 6 = 145.666667
-- The average call duration for Israel is (33 + 4 + 13 + 13 + 3 + 1 + 1 + 7) / 8 = 9.37500
-- The average call duration for Morocco is (33 + 4 + 59 + 59 + 3 + 7) / 6 = 27.5000 
-- Global call duration average = (2 * (33 + 3 + 59 + 102 + 330 + 5 + 13 + 3 + 1 + 7)) / 20 = 55.70000
-- Since Peru is the only country where average call duration is greater than the global average, it's the only recommended country.

-- Solution
with t1 as(
select caller_id as id, duration as total
from
(select caller_id, duration
from calls
union all
select callee_id, duration
from calls) a
)
select name as country
from
(select distinct avg(total) over(partition by code) as avg_call, avg(total) over() as global_avg, c.name
from
((select *, coalesce(total,0) as duration, substring(phone_number from 1 for 3) as code
from person right join t1
using (id)) b
join country c
on c.country_code = b.code)) d
where avg_call > global_avg
-- Question 2
-- Table: Sessions

-- +---------------------+---------+
-- | Column Name         | Type    |
-- +---------------------+---------+
-- | session_id          | int     |
-- | duration            | int     |
-- +---------------------+---------+
-- session_id is the primary key for this table.
-- duration is the time in seconds that a user has visited the application.
 

-- You want to know how long a user visits your application. You decided to create bins of "[0-5>", "[5-10>", "[10-15>" and "15 minutes or more" and count the number of sessions on it.

-- Write an SQL query to report the (bin, total) in any order.

-- The query result format is in the following example.

-- Sessions table:
-- +-------------+---------------+
-- | session_id  | duration      |
-- +-------------+---------------+
-- | 1           | 30            |
-- | 2           | 199           |
-- | 3           | 299           |
-- | 4           | 580           |
-- | 5           | 1000          |
-- +-------------+---------------+

-- Result table:
-- +--------------+--------------+
-- | bin          | total        |
-- +--------------+--------------+
-- | [0-5>        | 3            |
-- | [5-10>       | 1            |
-- | [10-15>      | 0            |
-- | 15 or more   | 1            |
-- +--------------+--------------+

-- For session_id 1, 2 and 3 have a duration greater or equal than 0 minutes and less than 5 minutes.
-- For session_id 4 has a duration greater or equal than 5 minutes and less than 10 minutes.
-- There are no session with a duration greater or equial than 10 minutes and less than 15 minutes.
-- For session_id 5 has a duration greater or equal than 15 minutes.

-- Solution 2
(Select '[0-5>' as bin, 
 sum(case when duration/60 < 5 then 1 else 0 end) as total from Sessions)
 union
(Select '[5-10>' as bin, 
 sum(case when ((duration/60 >= 5) and (duration/60 < 10)) then 1 else 0 end) as total from Sessions)
 union
(Select '[10-15>' as bin, 
 sum(case when ((duration/60 >= 10) and (duration/60 < 15)) then 1 else 0 end) as total from Sessions)
 union
(Select '15 or more' as bin, 
 sum(case when duration/60 >= 15 then 1 else 0 end) as total from Sessions)
-- Question 102
-- The Employee table holds the salary information in a year.

-- Write a SQL to get the cumulative sum of an employee's salary over a period of 3 months but exclude the most recent month.

-- The result should be displayed by 'Id' ascending, and then by 'Month' descending.

-- Example
-- Input

-- | Id | Month | Salary |
-- |----|-------|--------|
-- | 1  | 1     | 20     |
-- | 2  | 1     | 20     |
-- | 1  | 2     | 30     |
-- | 2  | 2     | 30     |
-- | 3  | 2     | 40     |
-- | 1  | 3     | 40     |
-- | 3  | 3     | 60     |
-- | 1  | 4     | 60     |
-- | 3  | 4     | 70     |
-- Output

-- | Id | Month | Salary |
-- |----|-------|--------|
-- | 1  | 3     | 90     |
-- | 1  | 2     | 50     |
-- | 1  | 1     | 20     |
-- | 2  | 1     | 20     |
-- | 3  | 3     | 100    |
-- | 3  | 2     | 40     |
 

-- Explanation
-- Employee '1' has 3 salary records for the following 3 months except the most recent month '4': salary 40 for month '3', 30 for month '2' and 20 for month '1'
-- So the cumulative sum of salary of this employee over 3 months is 90(40+30+20), 50(30+20) and 20 respectively.

-- | Id | Month | Salary |
-- |----|-------|--------|
-- | 1  | 3     | 90     |
-- | 1  | 2     | 50     |
-- | 1  | 1     | 20     |
-- Employee '2' only has one salary record (month '1') except its most recent month '2'.
-- | Id | Month | Salary |
-- |----|-------|--------|
-- | 2  | 1     | 20     |
 

-- Employ '3' has two salary records except its most recent pay month '4': month '3' with 60 and month '2' with 40. So the cumulative salary is as following.
-- | Id | Month | Salary |
-- |----|-------|--------|
-- | 3  | 3     | 100    |
-- | 3  | 2     | 40     |

-- Solution
with t1 as(
select *, max(month) over(partition by id) as recent_month
from employee)

select id, month, sum(salary) over(partition by id order by month rows between 2 preceding and current row) as salary
from t1
where month<recent_month
order by 1, 2 desc


-- Question 8
-- Query the customer_number from the orders table for the customer who has placed the largest number of orders.

-- It is guaranteed that exactly one customer will have placed more orders than any other customer.

-- The orders table is defined as follows:

-- | Column            | Type      |
-- |-------------------|-----------|
-- | order_number (PK) | int       |
-- | customer_number   | int       |
-- | order_date        | date      |
-- | required_date     | date      |
-- | shipped_date      | date      |
-- | status            | char(15)  |
-- | comment           | char(200) |
-- Sample Input

-- | order_number | customer_number | order_date | required_date | shipped_date | status | comment |
-- |--------------|-----------------|------------|---------------|--------------|--------|---------|
-- | 1            | 1               | 2017-04-09 | 2017-04-13    | 2017-04-12   | Closed |         |
-- | 2            | 2               | 2017-04-15 | 2017-04-20    | 2017-04-18   | Closed |         |
-- | 3            | 3               | 2017-04-16 | 2017-04-25    | 2017-04-20   | Closed |         |
-- | 4            | 3               | 2017-04-18 | 2017-04-28    | 2017-04-25   | Closed |         |
-- Sample Output

-- | customer_number |
-- |-----------------|
-- | 3               |
-- Explanation

-- The customer with number '3' has two orders, 
-- which is greater than either customer '1' or '2' because each of them  only has one order. 
-- So the result is customer_number '3'.


-- Solution
-- Ranking them according to the number of orders to have same rank for 
-- customers with same number of orders
With t1 as 
(
  Select customer_number, 
  Rank() over(order by count(customer_number) desc) as rk
  from orders
  group by customer_number
) 

Select t1.customer_number
from t1
where t1.rk=1

-- Question 72
-- Table: Customers

-- +---------------------+---------+
-- | Column Name         | Type    |
-- +---------------------+---------+
-- | customer_id         | int     |
-- | customer_name       | varchar |
-- +---------------------+---------+
-- customer_id is the primary key for this table.
-- customer_name is the name of the customer.
 

-- Table: Orders

-- +---------------+---------+
-- | Column Name   | Type    |
-- +---------------+---------+
-- | order_id      | int     |
-- | customer_id   | int     |
-- | product_name  | varchar |
-- +---------------+---------+
-- order_id is the primary key for this table.
-- customer_id is the id of the customer who bought the product "product_name".
 

-- Write an SQL query to report the customer_id and customer_name of customers who bought products "A", "B" but did not buy the product "C" since we want to recommend them buy this product.

-- Return the result table ordered by customer_id.

-- The query result format is in the following example.

 

-- Customers table:
-- +-------------+---------------+
-- | customer_id | customer_name |
-- +-------------+---------------+
-- | 1           | Daniel        |
-- | 2           | Diana         |
-- | 3           | Elizabeth     |
-- | 4           | Jhon          |
-- +-------------+---------------+

-- Orders table:
-- +------------+--------------+---------------+
-- | order_id   | customer_id  | product_name  |
-- +------------+--------------+---------------+
-- | 10         |     1        |     A         |
-- | 20         |     1        |     B         |
-- | 30         |     1        |     D         |
-- | 40         |     1        |     C         |
-- | 50         |     2        |     A         |
-- | 60         |     3        |     A         |
-- | 70         |     3        |     B         |
-- | 80         |     3        |     D         |
-- | 90         |     4        |     C         |
-- +------------+--------------+---------------+

-- Result table:
-- +-------------+---------------+
-- | customer_id | customer_name |
-- +-------------+---------------+
-- | 3           | Elizabeth     |
-- +-------------+---------------+
-- Only the customer_id with id 3 bought the product A and B but not the product C.

-- Solution
with t1 as
(
select customer_id
from orders
where product_name = 'B' and
customer_id in (select customer_id
from orders
where product_name = 'A'))

Select t1.customer_id, c.customer_name
from t1 join customers c
on t1.customer_id = c.customer_id
where t1.customer_id != all(select customer_id
from orders
where product_name = 'C')
-- Question 93
-- Table: Customer

-- +-------------+---------+
-- | Column Name | Type    |
-- +-------------+---------+
-- | customer_id | int     |
-- | product_key | int     |
-- +-------------+---------+
-- product_key is a foreign key to Product table.
-- Table: Product

-- +-------------+---------+
-- | Column Name | Type    |
-- +-------------+---------+
-- | product_key | int     |
-- +-------------+---------+
-- product_key is the primary key column for this table.
 

-- Write an SQL query for a report that provides the customer ids from the Customer table that bought all the products in the Product table.

-- For example:

-- Customer table:
-- +-------------+-------------+
-- | customer_id | product_key |
-- +-------------+-------------+
-- | 1           | 5           |
-- | 2           | 6           |
-- | 3           | 5           |
-- | 3           | 6           |
-- | 1           | 6           |
-- +-------------+-------------+

-- Product table:
-- +-------------+
-- | product_key |
-- +-------------+
-- | 5           |
-- | 6           |
-- +-------------+

-- Result table:
-- +-------------+
-- | customer_id |
-- +-------------+
-- | 1           |
-- | 3           |
-- +-------------+
-- The customers who bought all the products (5 and 6) are customers with id 1 and 3.

-- Solution
select customer_id
from customer
group by customer_id
having count(distinct product_key) = (select COUNT(distinct product_key) from product)
-- Question 13
-- Suppose that a website contains two tables, 
-- the Customers table and the Orders table. Write a SQL query to find all customers who never order anything.

-- Table: Customers.

-- +----+-------+
-- | Id | Name  |
-- +----+-------+
-- | 1  | Joe   |
-- | 2  | Henry |
-- | 3  | Sam   |
-- | 4  | Max   |
-- +----+-------+
-- Table: Orders.

-- +----+------------+
-- | Id | CustomerId |
-- +----+------------+
-- | 1  | 3          |
-- | 2  | 1          |
-- +----+------------+
-- Using the above tables as example, return the following:

-- +-----------+
-- | Customers |
-- +-----------+
-- | Henry     |
-- | Max       |
-- +-----------+


-- Solution
Select Name as Customers
from Customers
where id != All(select c.id
                from Customers c, Orders o
                where c.id = o.Customerid) 
-- Question 32
-- Write a SQL query to delete all duplicate email entries in a table named Person, keeping only unique emails based on its smallest Id.

-- +----+------------------+
-- | Id | Email            |
-- +----+------------------+
-- | 1  | john@example.com |
-- | 2  | bob@example.com  |
-- | 3  | john@example.com |
-- +----+------------------+
-- Id is the primary key column for this table.
-- For example, after running your query, the above Person table should have the following rows:

-- +----+------------------+
-- | Id | Email            |
-- +----+------------------+
-- | 1  | john@example.com |
-- | 2  | bob@example.com  |
-- +----+------------------+

-- Solution
With t1 as
(
 Select *,
    row_number() over(partition by email order by id) as rk
    from person
)
Delete from person
where id in (Select t1.id from t1 where t1.rk>1)
-- Question 57
-- The Employee table holds all employees. Every employee has an Id, a salary, and there is also a column for the department Id.

-- +----+-------+--------+--------------+
-- | Id | Name  | Salary | DepartmentId |
-- +----+-------+--------+--------------+
-- | 1  | Joe   | 70000  | 1            |
-- | 2  | Jim   | 90000  | 1            |
-- | 3  | Henry | 80000  | 2            |
-- | 4  | Sam   | 60000  | 2            |
-- | 5  | Max   | 90000  | 1            |
-- +----+-------+--------+--------------+
-- The Department table holds all departments of the company.

-- +----+----------+
-- | Id | Name     |
-- +----+----------+
-- | 1  | IT       |
-- | 2  | Sales    |
-- +----+----------+
-- Write a SQL query to find employees who have the highest salary in each of the departments. 
-- For the above tables, your SQL query should return the following rows (order of rows does not matter).

-- +------------+----------+--------+
-- | Department | Employee | Salary |
-- +------------+----------+--------+
-- | IT         | Max      | 90000  |
-- | IT         | Jim      | 90000  |
-- | Sales      | Henry    | 80000  |
-- +------------+----------+--------+
-- Explanation:

-- Max and Jim both have the highest salary in the IT department and Henry has the highest salary in the Sales department.

-- Solution
select a.Department, a.Employee, a.Salary
from(
select d.name as Department, e.name as Employee, Salary,
rank() over(partition by d.name order by salary desc) as rk
from employee e
join department d
on e.departmentid = d.id) a
where a.rk=1
-- Question 14
-- The Employee table holds all employees. Every employee has an Id, and there is also a column for the department Id.

-- +----+-------+--------+--------------+
-- | Id | Name  | Salary | DepartmentId |
-- +----+-------+--------+--------------+
-- | 1  | Joe   | 85000  | 1            |
-- | 2  | Henry | 80000  | 2            |
-- | 3  | Sam   | 60000  | 2            |
-- | 4  | Max   | 90000  | 1            |
-- | 5  | Janet | 69000  | 1            |
-- | 6  | Randy | 85000  | 1            |
-- | 7  | Will  | 70000  | 1            |
-- +----+-------+--------+--------------+
-- The Department table holds all departments of the company.

-- +----+----------+
-- | Id | Name     |
-- +----+----------+
-- | 1  | IT       |
-- | 2  | Sales    |
-- +----+----------+
-- Write a SQL query to find employees who earn the top three salaries in each of the department. For the above tables, your SQL query should return the following rows (order of rows does not matter).

-- +------------+----------+--------+
-- | Department | Employee | Salary |
-- +------------+----------+--------+
-- | IT         | Max      | 90000  |
-- | IT         | Randy    | 85000  |
-- | IT         | Joe      | 85000  |
-- | IT         | Will     | 70000  |
-- | Sales      | Henry    | 80000  |
-- | Sales      | Sam      | 60000  |
-- +------------+----------+--------+
-- Explanation:

-- In IT department, Max earns the highest salary, both Randy and Joe earn the second highest salary, 
-- and Will earns the third highest salary. 
-- There are only two employees in the Sales department, 
-- Henry earns the highest salary while Sam earns the second highest salary.

-- Solution
select a.department, a.employee, a.salary
from (
select d.name as department, e.name as employee, salary, 
    dense_rank() over(Partition by d.name order by salary desc) as rk
from Employee e join Department d
on e.departmentid = d.id) a
where a.rk<4
-- Question 11
-- Write a SQL query to find all duplicate emails in a table named Person.

-- +----+---------+
-- | Id | Email   |
-- +----+---------+
-- | 1  | a@b.com |
-- | 2  | c@d.com |
-- | 3  | a@b.com |
-- +----+---------+
-- For example, your query should return the following for the above table:

-- +---------+
-- | Email   |
-- +---------+
-- | a@b.com |
-- +---------+


-- Solution
Select Email
from
(Select Email, count(Email)
from person
group by Email
having count(Email)>1) a
-- Question 4
-- Select all employee's name and bonus whose bonus is < 1000.

-- Table:Employee

-- +-------+--------+-----------+--------+
-- | empId |  name  | supervisor| salary |
-- +-------+--------+-----------+--------+
-- |   1   | John   |  3        | 1000   |
-- |   2   | Dan    |  3        | 2000   |
-- |   3   | Brad   |  null     | 4000   |
-- |   4   | Thomas |  3        | 4000   |
-- +-------+--------+-----------+--------+
-- empId is the primary key column for this table.
-- Table: Bonus

-- +-------+-------+
-- | empId | bonus |
-- +-------+-------+
-- | 2     | 500   |
-- | 4     | 2000  |
-- +-------+-------+
-- empId is the primary key column for this table.
-- Example ouput:

-- +-------+-------+
-- | name  | bonus |
-- +-------+-------+
-- | John  | null  |
-- | Dan   | 500   |
-- | Brad  | null  |
-- +-------+-------+


-- Solution
Select E.name, B.bonus
From Employee E left join Bonus B
on E.empId = B.empId
where B.bonus< 1000 or B.Bonus IS NULL
-- Question 15
-- The Employee table holds all employees including their managers. 
-- Every employee has an Id, and there is also a column for the manager Id.

-- +----+-------+--------+-----------+
-- | Id | Name  | Salary | ManagerId |
-- +----+-------+--------+-----------+
-- | 1  | Joe   | 70000  | 3         |
-- | 2  | Henry | 80000  | 4         |
-- | 3  | Sam   | 60000  | NULL      |
-- | 4  | Max   | 90000  | NULL      |
-- +----+-------+--------+-----------+
-- Given the Employee table, write a SQL query that finds out employees who earn more than their managers. 
-- For the above table, Joe is the only employee who earns more than his manager.

-- +----------+
-- | Employee |
-- +----------+
-- | Joe      |
-- +----------+

-- Solution
select a.Name as Employee
from employee a, employee b
where a.salary>b.salary and a.managerid=b.id
-- Question 78
-- Table Variables:

-- +---------------+---------+
-- | Column Name   | Type    |
-- +---------------+---------+
-- | name          | varchar |
-- | value         | int     |
-- +---------------+---------+
-- name is the primary key for this table.
-- This table contains the stored variables and their values.
 

-- Table Expressions:

-- +---------------+---------+
-- | Column Name   | Type    |
-- +---------------+---------+
-- | left_operand  | varchar |
-- | operator      | enum    |
-- | right_operand | varchar |
-- +---------------+---------+
-- (left_operand, operator, right_operand) is the primary key for this table.
-- This table contains a boolean expression that should be evaluated.
-- operator is an enum that takes one of the values ('<', '>', '=')
-- The values of left_operand and right_operand are guaranteed to be in the Variables table.
 

-- Write an SQL query to evaluate the boolean expressions in Expressions table.

-- Return the result table in any order.

-- The query result format is in the following example.

-- Variables table:
-- +------+-------+
-- | name | value |
-- +------+-------+
-- | x    | 66    |
-- | y    | 77    |
-- +------+-------+

-- Expressions table:
-- +--------------+----------+---------------+
-- | left_operand | operator | right_operand |
-- +--------------+----------+---------------+
-- | x            | >        | y             |
-- | x            | <        | y             |
-- | x            | =        | y             |
-- | y            | >        | x             |
-- | y            | <        | x             |
-- | x            | =        | x             |
-- +--------------+----------+---------------+

-- Result table:
-- +--------------+----------+---------------+-------+
-- | left_operand | operator | right_operand | value |
-- +--------------+----------+---------------+-------+
-- | x            | >        | y             | false |
-- | x            | <        | y             | true  |
-- | x            | =        | y             | false |
-- | y            | >        | x             | true  |
-- | y            | <        | x             | false |
-- | x            | =        | x             | true  |
-- +--------------+----------+---------------+-------+
-- As shown, you need find the value of each boolean exprssion in the table using the variables table.

-- Solution
with t1 as(
select e.left_operand, e.operator, e.right_operand, v.value as left_val, v_1.value as right_val
from expressions e
join variables v
on v.name = e.left_operand 
join variables v_1
on v_1.name = e.right_operand)

select t1.left_operand, t1.operator, t1.right_operand,
case when t1.operator = '<' then (select t1.left_val< t1.right_val)
when t1.operator = '>' then (select t1.left_val > t1.right_val)
when t1.operator = '=' then (select t1.left_val = t1.right_val)
else FALSE
END AS VALUE
from t1

-- Question 56
-- Mary is a teacher in a middle school and she has a table seat storing students' names and their corresponding seat ids.

-- The column id is continuous increment.
 

-- Mary wants to change seats for the adjacent students.
 

-- Can you write a SQL query to output the result for Mary?
 

-- +---------+---------+
-- |    id   | student |
-- +---------+---------+
-- |    1    | Abbot   |
-- |    2    | Doris   |
-- |    3    | Emerson |
-- |    4    | Green   |
-- |    5    | Jeames  |
-- +---------+---------+
-- For the sample input, the output is:
 

-- +---------+---------+
-- |    id   | student |
-- +---------+---------+
-- |    1    | Doris   |
-- |    2    | Abbot   |
-- |    3    | Green   |
-- |    4    | Emerson |
-- |    5    | Jeames  |
-- +---------+---------+

-- Solution
select row_number() over (order by (if(id%2=1,id+1,id-1))) as id, student
from seat
-- Question 10
-- Given a table customer holding customers information and the referee.

-- +------+------+-----------+
-- | id   | name | referee_id|
-- +------+------+-----------+
-- |    1 | Will |      NULL |
-- |    2 | Jane |      NULL |
-- |    3 | Alex |         2 |
-- |    4 | Bill |      NULL |
-- |    5 | Zack |         1 |
-- |    6 | Mark |         2 |
-- +------+------+-----------+
-- Write a query to return the list of customers NOT referred by the person with id '2'.

-- For the sample data above, the result is:

-- +------+
-- | name |
-- +------+
-- | Will |
-- | Jane |
-- | Bill |
-- | Zack |
-- +------+

-- Solution
Select name
from customer 
where referee_id != 2
or referee_id is NULL
-- Question 107
-- The Numbers table keeps the value of number and its frequency.

-- +----------+-------------+
-- |  Number  |  Frequency  |
-- +----------+-------------|
-- |  0       |  7          |
-- |  1       |  1          |
-- |  2       |  3          |
-- |  3       |  1          |
-- +----------+-------------+
-- In this table, the numbers are 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 3, so the median is (0 + 0) / 2 = 0.

-- +--------+
-- | median |
-- +--------|
-- | 0.0000 |
-- +--------+
-- Write a query to find the median of all numbers and name the result as median.

-- Solution
with t1 as(
select *,
sum(frequency) over(order by number) as cum_sum, (sum(frequency) over())/2 as middle
from numbers)

select avg(number) as median
from t1
where middle between (cum_sum - frequency) and cum_sum

-- Question 106
-- Table: Student

-- +---------------------+---------+
-- | Column Name         | Type    |
-- +---------------------+---------+
-- | student_id          | int     |
-- | student_name        | varchar |
-- +---------------------+---------+
-- student_id is the primary key for this table.
-- student_name is the name of the student.
 

-- Table: Exam

-- +---------------+---------+
-- | Column Name   | Type    |
-- +---------------+---------+
-- | exam_id       | int     |
-- | student_id    | int     |
-- | score         | int     |
-- +---------------+---------+
-- (exam_id, student_id) is the primary key for this table.
-- Student with student_id got score points in exam with id exam_id.
 

-- A "quite" student is the one who took at least one exam and didn't score neither the high score nor the low score.

-- Write an SQL query to report the students (student_id, student_name) being "quiet" in ALL exams.

-- Don't return the student who has never taken any exam. Return the result table ordered by student_id.

-- The query result format is in the following example.

 

-- Student table:
-- +-------------+---------------+
-- | student_id  | student_name  |
-- +-------------+---------------+
-- | 1           | Daniel        |
-- | 2           | Jade          |
-- | 3           | Stella        |
-- | 4           | Jonathan      |
-- | 5           | Will          |
-- +-------------+---------------+

-- Exam table:
-- +------------+--------------+-----------+
-- | exam_id    | student_id   | score     |
-- +------------+--------------+-----------+
-- | 10         |     1        |    70     |
-- | 10         |     2        |    80     |
-- | 10         |     3        |    90     |
-- | 20         |     1        |    80     |
-- | 30         |     1        |    70     |
-- | 30         |     3        |    80     |
-- | 30         |     4        |    90     |
-- | 40         |     1        |    60     |
-- | 40         |     2        |    70     |
-- | 40         |     4        |    80     |
-- +------------+--------------+-----------+

-- Result table:
-- +-------------+---------------+
-- | student_id  | student_name  |
-- +-------------+---------------+
-- | 2           | Jade          |
-- +-------------+---------------+

-- For exam 1: Student 1 and 3 hold the lowest and high score respectively.
-- For exam 2: Student 1 hold both highest and lowest score.
-- For exam 3 and 4: Studnet 1 and 4 hold the lowest and high score respectively.
-- Student 2 and 5 have never got the highest or lowest in any of the exam.
-- Since student 5 is not taking any exam, he is excluded from the result.
-- So, we only return the information of Student 2.

-- Solution
with t1 as(
select student_id
from
(select *,
min(score) over(partition by exam_id) as least,
max(score) over(partition by exam_id) as most
from exam) a
where least = score or most = score)


select distinct student_id, student_name
from exam join student
using (student_id)
where student_id != all(select student_id from t1)
order by 1
-- Question 80
-- Table: Logs

-- +---------------+---------+
-- | Column Name   | Type    |
-- +---------------+---------+
-- | log_id        | int     |
-- +---------------+---------+
-- id is the primary key for this table.
-- Each row of this table contains the ID in a log Table.

-- Since some IDs have been removed from Logs. Write an SQL query to find the start and end number of continuous ranges in table Logs.

-- Order the result table by start_id.

-- The query result format is in the following example:

-- Logs table:
-- +------------+
-- | log_id     |
-- +------------+
-- | 1          |
-- | 2          |
-- | 3          |
-- | 7          |
-- | 8          |
-- | 10         |
-- +------------+

-- Result table:
-- +------------+--------------+
-- | start_id   | end_id       |
-- +------------+--------------+
-- | 1          | 3            |
-- | 7          | 8            |
-- | 10         | 10           |
-- +------------+--------------+
-- The result table should contain all ranges in table Logs.
-- From 1 to 3 is contained in the table.
-- From 4 to 6 is missing in the table
-- From 7 to 8 is contained in the table.
-- Number 9 is missing in the table.
-- Number 10 is contained in the table.

-- Solution
select min(log_id) as start_id, max(log_id) as end_id
from(
select log_id, log_id-row_number() over (order by log_id) as rk
from logs) a
group by rk
-- Question 47
-- Table: Employee

-- +---------------+---------+
-- | Column Name   | Type    |
-- +---------------+---------+
-- | employee_id   | int     |
-- | team_id       | int     |
-- +---------------+---------+
-- employee_id is the primary key for this table.
-- Each row of this table contains the ID of each employee and their respective team.
-- Write an SQL query to find the team size of each of the employees.

-- Return result table in any order.

-- The query result format is in the following example:

-- Employee Table:
-- +-------------+------------+
-- | employee_id | team_id    |
-- +-------------+------------+
-- |     1       |     8      |
-- |     2       |     8      |
-- |     3       |     8      |
-- |     4       |     7      |
-- |     5       |     9      |
-- |     6       |     9      |
-- +-------------+------------+
-- Result table:
-- +-------------+------------+
-- | employee_id | team_size  |
-- +-------------+------------+
-- |     1       |     3      |
-- |     2       |     3      |
-- |     3       |     3      |
-- |     4       |     1      |
-- |     5       |     2      |
-- |     6       |     2      |
-- +-------------+------------+
-- Employees with Id 1,2,3 are part of a team with team_id = 8.
-- Employees with Id 4 is part of a team with team_id = 7.
-- Employees with Id 5,6 are part of a team with team_id = 9.

-- Solution
Select employee_id, b.team_size
from employee e
join 
(
Select team_id, count(team_id) as team_size
from employee
group by team_id) b
on e.team_id = b.team_id
-- Question 49
-- In social network like Facebook or Twitter, people send friend requests and accept others’ requests as well. Now given two tables as below:
 

-- Table: friend_request
-- | sender_id | send_to_id |request_date|
-- |-----------|------------|------------|
-- | 1         | 2          | 2016_06-01 |
-- | 1         | 3          | 2016_06-01 |
-- | 1         | 4          | 2016_06-01 |
-- | 2         | 3          | 2016_06-02 |
-- | 3         | 4          | 2016-06-09 |
 

-- Table: request_accepted
-- | requester_id | accepter_id |accept_date |
-- |--------------|-------------|------------|
-- | 1            | 2           | 2016_06-03 |
-- | 1            | 3           | 2016-06-08 |
-- | 2            | 3           | 2016-06-08 |
-- | 3            | 4           | 2016-06-09 |
-- | 3            | 4           | 2016-06-10 |
 

-- Write a query to find the overall acceptance rate of requests rounded to 2 decimals, which is the number of acceptance divide the number of requests.
 

-- For the sample data above, your query should return the following result.
 

-- |accept_rate|
-- |-----------|
-- |       0.80|
 

-- Note:
-- The accepted requests are not necessarily from the table friend_request. In this case, you just need to simply count the total accepted requests (no matter whether they are in the original requests), and divide it by the number of requests to get the acceptance rate.
-- It is possible that a sender sends multiple requests to the same receiver, and a request could be accepted more than once. In this case, the ‘duplicated’ requests or acceptances are only counted once.
-- If there is no requests at all, you should return 0.00 as the accept_rate.
 

-- Explanation: There are 4 unique accepted requests, and there are 5 requests in total. 
-- So the rate is 0.80.

-- Solution
with t1 as
(
	select distinct sender_id, send_to_id
	from friend_request
), t2 as
(
	select distinct requester_id, accepter_id
	from request_accepted
)

Select 
ifnull((
        select distinct
        round((select count(*) from t2) / ( select count(*) from t1),2) from t1,t2
    ),0) 'accept_rate'

-- Question 60
-- In social network like Facebook or Twitter, people send friend requests and accept others' requests as well.

-- Table request_accepted

-- +--------------+-------------+------------+
-- | requester_id | accepter_id | accept_date|
-- |--------------|-------------|------------|
-- | 1            | 2           | 2016_06-03 |
-- | 1            | 3           | 2016-06-08 |
-- | 2            | 3           | 2016-06-08 |
-- | 3            | 4           | 2016-06-09 |
-- +--------------+-------------+------------+
-- This table holds the data of friend acceptance, while requester_id and accepter_id both are the id of a person.
 

-- Write a query to find the the people who has most friends and the most friends number under the following rules:

-- It is guaranteed there is only 1 people having the most friends.
-- The friend request could only been accepted once, which mean there is no multiple records with the same requester_id and accepter_id value.
-- For the sample data above, the result is:

-- Result table:
-- +------+------+
-- | id   | num  |
-- |------|------|
-- | 3    | 3    |
-- +------+------+
-- The person with id '3' is a friend of people '1', '2' and '4', so he has 3 friends in total, which is the most number than any others.

-- Solution
select requester_id as id, b.total as num
from(
select requester_id, sum(one) as total
from((
select requester_id, count(distinct accepter_id) as one
from request_accepted
group by requester_id)
union all
(select accepter_id, count(distinct requester_id) as two
from request_accepted
group by accepter_id)) a
group by requester_id
order by total desc) b
limit 1
-- Question 115
-- Write an SQL query to report the distinct titles of the kid-friendly movies streamed in June 2020.

-- Return the result table in any order.

-- The query result format is in the following example.

 

-- TVProgram table:
-- +--------------------+--------------+-------------+
-- | program_date       | content_id   | channel     |
-- +--------------------+--------------+-------------+
-- | 2020-06-10 08:00   | 1            | LC-Channel  |
-- | 2020-05-11 12:00   | 2            | LC-Channel  |
-- | 2020-05-12 12:00   | 3            | LC-Channel  |
-- | 2020-05-13 14:00   | 4            | Disney Ch   |
-- | 2020-06-18 14:00   | 4            | Disney Ch   |
-- | 2020-07-15 16:00   | 5            | Disney Ch   |
-- +--------------------+--------------+-------------+

-- Content table:
-- +------------+----------------+---------------+---------------+
-- | content_id | title          | Kids_content  | content_type  |
-- +------------+----------------+---------------+---------------+
-- | 1          | Leetcode Movie | N             | Movies        |
-- | 2          | Alg. for Kids  | Y             | Series        |
-- | 3          | Database Sols  | N             | Series        |
-- | 4          | Aladdin        | Y             | Movies        |
-- | 5          | Cinderella     | Y             | Movies        |
-- +------------+----------------+---------------+---------------+

-- Result table:
-- +--------------+
-- | title        |
-- +--------------+
-- | Aladdin      |
-- +--------------+
-- "Leetcode Movie" is not a content for kids.
-- "Alg. for Kids" is not a movie.
-- "Database Sols" is not a movie
-- "Alladin" is a movie, content for kids and was streamed in June 2020.
-- "Cinderella" was not streamed in June 2020.

-- Solution
select distinct title
from
(select content_id, title
from content
where kids_content = 'Y' and content_type = 'Movies') a
join
tvprogram using (content_id)
where month(program_date) = 6
-- Question 3
-- Table: Activity

-- +--------------+---------+
-- | Column Name  | Type    |
-- +--------------+---------+
-- | player_id    | int     |
-- | device_id    | int     |
-- | event_date   | date    |
-- | games_played | int     |
-- +--------------+---------+
-- (player_id, event_date) is the primary key of this table.
-- This table shows the activity of players of some game.
-- Each row is a record of a player who logged in and played a number of games (possibly 0) before logging out on some day using some device.
 

-- Write an SQL query that reports the first login date for each player.

-- The query result format is in the following example:

-- Activity table:
-- +-----------+-----------+------------+--------------+
-- | player_id | device_id | event_date | games_played |
-- +-----------+-----------+------------+--------------+
-- | 1         | 2         | 2016-03-01 | 5            |
-- | 1         | 2         | 2016-05-02 | 6            |
-- | 2         | 3         | 2017-06-25 | 1            |
-- | 3         | 1         | 2016-03-02 | 0            |
-- | 3         | 4         | 2018-07-03 | 5            |
-- +-----------+-----------+------------+--------------+

-- Result table:
-- +-----------+-------------+
-- | player_id | first_login |
-- +-----------+-------------+
-- | 1         | 2016-03-01  |
-- | 2         | 2017-06-25  |
-- | 3         | 2016-03-02  |
-- +-----------+-------------+

-- Solution
Select player_id, min(event_date) as first_login
from Activity
Group by player_id
-- Question 9 
-- Table: Activity

-- +--------------+---------+
-- | Column Name  | Type    |
-- +--------------+---------+
-- | player_id    | int     |
-- | device_id    | int     |
-- | event_date   | date    |
-- | games_played | int     |
-- +--------------+---------+
-- (player_id, event_date) is the primary key of this table.
-- This table shows the activity of players of some game.
-- Each row is a record of a player who logged in and played a number of games (possibly 0) before logging out on some day using some device.
 

-- Write a SQL query that reports the device that is first logged in for each player.

-- The query result format is in the following example:

-- Activity table:
-- +-----------+-----------+------------+--------------+
-- | player_id | device_id | event_date | games_played |
-- +-----------+-----------+------------+--------------+
-- | 1         | 2         | 2016-03-01 | 5            |
-- | 1         | 2         | 2016-05-02 | 6            |
-- | 2         | 3         | 2017-06-25 | 1            |
-- | 3         | 1         | 2016-03-02 | 0            |
-- | 3         | 4         | 2018-07-03 | 5            |
-- +-----------+-----------+------------+--------------+

-- Result table:
-- +-----------+-----------+
-- | player_id | device_id |
-- +-----------+-----------+
-- | 1         | 2         |
-- | 2         | 3         |
-- | 3         | 1         |
-- +-----------+-----------+


-- Solution
With table1 as
(
   Select player_id, device_id,
   Rank() OVER(partition by player_id
               order by event_date) as rk
   From Activity
)
Select t.player_id, t.device_id
from table1 as t
where t.rk=1
-- Question 62
-- Table: Activity

-- +--------------+---------+
-- | Column Name  | Type    |
-- +--------------+---------+
-- | player_id    | int     |
-- | device_id    | int     |
-- | event_date   | date    |
-- | games_played | int     |
-- +--------------+---------+
-- (player_id, event_date) is the primary key of this table.
-- This table shows the activity of players of some game.
-- Each row is a record of a player who logged in and played a number of games (possibly 0) before logging out on some day using some device.
 

-- Write an SQL query that reports for each player and date, how many games played so far by the player. That is, the total number of games played by the player until that date. Check the example for clarity.

-- The query result format is in the following example:

-- Activity table:
-- +-----------+-----------+------------+--------------+
-- | player_id | device_id | event_date | games_played |
-- +-----------+-----------+------------+--------------+
-- | 1         | 2         | 2016-03-01 | 5            |
-- | 1         | 2         | 2016-05-02 | 6            |
-- | 1         | 3         | 2017-06-25 | 1            |
-- | 3         | 1         | 2016-03-02 | 0            |
-- | 3         | 4         | 2018-07-03 | 5            |
-- +-----------+-----------+------------+--------------+

-- Result table:
-- +-----------+------------+---------------------+
-- | player_id | event_date | games_played_so_far |
-- +-----------+------------+---------------------+
-- | 1         | 2016-03-01 | 5                   |
-- | 1         | 2016-05-02 | 11                  |
-- | 1         | 2017-06-25 | 12                  |
-- | 3         | 2016-03-02 | 0                   |
-- | 3         | 2018-07-03 | 5                   |
-- +-----------+------------+---------------------+
-- For the player with id 1, 5 + 6 = 11 games played by 2016-05-02, and 5 + 6 + 1 = 12 games played by 2017-06-25.
-- For the player with id 3, 0 + 5 = 5 games played by 2018-07-03.
-- Note that for each player we only care about the days when the player logged in.

-- Solution
select player_id, event_date, 
sum(games_played) over(partition by player_id order by event_date) as games_played_so_far
from activity
order by 1,2
-- Question 91
-- Table: Activity

-- +--------------+---------+
-- | Column Name  | Type    |
-- +--------------+---------+
-- | player_id    | int     |
-- | device_id    | int     |
-- | event_date   | date    |
-- | games_played | int     |
-- +--------------+---------+
-- (player_id, event_date) is the primary key of this table.
-- This table shows the activity of players of some game.
-- Each row is a record of a player who logged in and played a number of games (possibly 0) 
-- before logging out on some day using some device.
 

-- Write an SQL query that reports the fraction of players that logged in again 
-- on the day after the day they first logged in, rounded to 2 decimal places. 
-- In other words, you need to count the number of players that logged in for at least two consecutive 
-- days starting from their first login date, then divide that number by the total number of players.

-- The query result format is in the following example:

-- Activity table:
-- +-----------+-----------+------------+--------------+
-- | player_id | device_id | event_date | games_played |
-- +-----------+-----------+------------+--------------+
-- | 1         | 2         | 2016-03-01 | 5            |
-- | 1         | 2         | 2016-03-02 | 6            |
-- | 2         | 3         | 2017-06-25 | 1            |
-- | 3         | 1         | 2016-03-02 | 0            |
-- | 3         | 4         | 2018-07-03 | 5            |
-- +-----------+-----------+------------+--------------+

-- Result table:
-- +-----------+
-- | fraction  |
-- +-----------+
-- | 0.33      |
-- +-----------+
-- Only the player with id 1 logged back in after the first day he had logged in so the answer is 1/3 = 0.33

-- Solution
With t as 
(select player_id,
 min(event_date) over(partition by player_id) as min_event_date,
 case when event_date- min(event_date) over(partition by player_id) = 1 then 1 
 else 0 
 end as s
 from Activity)

select round(sum(t.s)/count(distinct t.player_id),2) as fraction 
from t
-- Question 111
-- Table: Activity

-- +--------------+---------+
-- | Column Name  | Type    |
-- +--------------+---------+
-- | player_id    | int     |
-- | device_id    | int     |
-- | event_date   | date    |
-- | games_played | int     |
-- +--------------+---------+
-- (player_id, event_date) is the primary key of this table.
-- This table shows the activity of players of some game.
-- Each row is a record of a player who logged in and played a number of games (possibly 0) before logging out on some day using some device.
 

-- We define the install date of a player to be the first login day of that player.

-- We also define day 1 retention of some date X to be the number of players whose install date is X and they logged back in on the day right after X, divided by the number of players whose install date is X, rounded to 2 decimal places.

-- Write an SQL query that reports for each install date, the number of players that installed the game on that day and the day 1 retention.

-- The query result format is in the following example:

-- Activity table:
-- +-----------+-----------+------------+--------------+
-- | player_id | device_id | event_date | games_played |
-- +-----------+-----------+------------+--------------+
-- | 1         | 2         | 2016-03-01 | 5            |
-- | 1         | 2         | 2016-03-02 | 6            |
-- | 2         | 3         | 2017-06-25 | 1            |
-- | 3         | 1         | 2016-03-01 | 0            |
-- | 3         | 4         | 2016-07-03 | 5            |
-- +-----------+-----------+------------+--------------+

-- Result table:
-- +------------+----------+----------------+
-- | install_dt | installs | Day1_retention |
-- +------------+----------+----------------+
-- | 2016-03-01 | 2        | 0.50           |
-- | 2017-06-25 | 1        | 0.00           |
-- +------------+----------+----------------+
-- Player 1 and 3 installed the game on 2016-03-01 but only player 1 logged back in on 2016-03-02 so the
-- day 1 retention of 2016-03-01 is 1 / 2 = 0.50
-- Player 2 installed the game on 2017-06-25 but didn't log back in on 2017-06-26 so the day 1 retention of 2017-06-25 is 0 / 1 = 0.00

-- Solution
with t1 as(
select *,
row_number() over(partition by player_id order by event_date) as rnk,
min(event_date) over(partition by player_id) as install_dt,
lead(event_date,1) over(partition by player_id order by event_date) as nxt
from Activity)

select distinct install_dt,
count(distinct player_id) as installs,
round(sum(case when nxt=event_date+1 then 1 else 0 end)/count(distinct player_id),2) as Day1_retention
from t1
where rnk = 1
group by 1
order by 1
-- Question 86
-- Get the highest answer rate question from a table survey_log with these columns: id, action, question_id, answer_id, q_num, timestamp.

-- id means user id; action has these kind of values: "show", "answer", "skip"; answer_id is not null when action column is "answer", 
-- while is null for "show" and "skip"; q_num is the numeral order of the question in current session.

-- Write a sql query to identify the question which has the highest answer rate.

-- Example:

-- Input:
-- +------+-----------+--------------+------------+-----------+------------+
-- | id   | action    | question_id  | answer_id  | q_num     | timestamp  |
-- +------+-----------+--------------+------------+-----------+------------+
-- | 5    | show      | 285          | null       | 1         | 123        |
-- | 5    | answer    | 285          | 124124     | 1         | 124        |
-- | 5    | show      | 369          | null       | 2         | 125        |
-- | 5    | skip      | 369          | null       | 2         | 126        |
-- +------+-----------+--------------+------------+-----------+------------+
-- Output:
-- +-------------+
-- | survey_log  |
-- +-------------+
-- |    285      |
-- +-------------+
-- Explanation:
-- question 285 has answer rate 1/1, while question 369 has 0/1 answer rate, so output 285.
 

-- Note: The highest answer rate meaning is: answer number's ratio in show number in the same question.

-- Solution
with t1 as(
select a.question_id, coalesce(b.answer/a.show_1,0) as rate
from 
(select question_id, coalesce(count(*),0) as show_1
from survey_log
where action != 'answer'
group by question_id) a
left join
(select question_id, coalesce(count(*),0) as answer
from survey_log
where action = 'answer'
group by question_id) b
on a.question_id = b.question_id)

select a.question_id as survey_log
from 
( select t1.question_id,
rank() over(order by rate desc) as rk
from t1) a
where a.rk = 1
-- Question 109
-- Table: UserActivity

-- +---------------+---------+
-- | Column Name   | Type    |
-- +---------------+---------+
-- | username      | varchar |
-- | activity      | varchar |
-- | startDate     | Date    |
-- | endDate       | Date    |
-- +---------------+---------+
-- This table does not contain primary key.
-- This table contain information about the activity performed of each user in a period of time.
-- A person with username performed a activity from startDate to endDate.

-- Write an SQL query to show the second most recent activity of each user.

-- If the user only has one activity, return that one. 

-- A user can't perform more than one activity at the same time. Return the result table in any order.

-- The query result format is in the following example:

-- UserActivity table:
-- +------------+--------------+-------------+-------------+
-- | username   | activity     | startDate   | endDate     |
-- +------------+--------------+-------------+-------------+
-- | Alice      | Travel       | 2020-02-12  | 2020-02-20  |
-- | Alice      | Dancing      | 2020-02-21  | 2020-02-23  |
-- | Alice      | Travel       | 2020-02-24  | 2020-02-28  |
-- | Bob        | Travel       | 2020-02-11  | 2020-02-18  |
-- +------------+--------------+-------------+-------------+

-- Result table:
-- +------------+--------------+-------------+-------------+
-- | username   | activity     | startDate   | endDate     |
-- +------------+--------------+-------------+-------------+
-- | Alice      | Dancing      | 2020-02-21  | 2020-02-23  |
-- | Bob        | Travel       | 2020-02-11  | 2020-02-18  |
-- +------------+--------------+-------------+-------------+

-- The most recent activity of Alice is Travel from 2020-02-24 to 2020-02-28, before that she was dancing from 2020-02-21 to 2020-02-23.
-- Bob only has one record, we just take that one.

-- Solution
select username, activity, startdate, enddate
from
(select *,
rank() over(partition by username order by startdate desc) as rk,
count(username) over(partition by username) as cnt
from useractivity) a
where a.rk = 2 or cnt = 1

-- Question 116
-- Table Activities:

-- +-------------+---------+
-- | Column Name | Type    |
-- +-------------+---------+
-- | sell_date   | date    |
-- | product     | varchar |
-- +-------------+---------+
-- There is no primary key for this table, it may contains duplicates.
-- Each row of this table contains the product name and the date it was sold in a market.
 

-- Write an SQL query to find for each date, the number of distinct products sold and their names.

-- The sold-products names for each date should be sorted lexicographically. 

-- Return the result table ordered by sell_date.

-- The query result format is in the following example.

-- Activities table:
-- +------------+-------------+
-- | sell_date  | product     |
-- +------------+-------------+
-- | 2020-05-30 | Headphone   |
-- | 2020-06-01 | Pencil      |
-- | 2020-06-02 | Mask        |
-- | 2020-05-30 | Basketball  |
-- | 2020-06-01 | Bible       |
-- | 2020-06-02 | Mask        |
-- | 2020-05-30 | T-Shirt     |
-- +------------+-------------+

-- Result table:
-- +------------+----------+------------------------------+
-- | sell_date  | num_sold | products                     |
-- +------------+----------+------------------------------+
-- | 2020-05-30 | 3        | Basketball,Headphone,T-shirt |
-- | 2020-06-01 | 2        | Bible,Pencil                 |
-- | 2020-06-02 | 1        | Mask                         |
-- +------------+----------+------------------------------+
-- For 2020-05-30, Sold items were (Headphone, Basketball, T-shirt), we sort them lexicographically and separate them by comma.
-- For 2020-06-01, Sold items were (Pencil, Bible), we sort them lexicographically and separate them by comma.
-- For 2020-06-02, Sold item is (Mask), we just return it.

-- Solution
select sell_date, count(distinct product) as num_sold, group_concat(distinct product) as products
from activities
group by 1
order by 1
-- Question 63
-- Table: Enrollments

-- +---------------+---------+
-- | Column Name   | Type    |
-- +---------------+---------+
-- | student_id    | int     |
-- | course_id     | int     |
-- | grade         | int     |
-- +---------------+---------+
-- (student_id, course_id) is the primary key of this table.

-- Write a SQL query to find the highest grade with its corresponding course for each student. In case of a tie, you should find the course with the smallest course_id. The output must be sorted by increasing student_id.

-- The query result format is in the following example:

-- Enrollments table:
-- +------------+-------------------+
-- | student_id | course_id | grade |
-- +------------+-----------+-------+
-- | 2          | 2         | 95    |
-- | 2          | 3         | 95    |
-- | 1          | 1         | 90    |
-- | 1          | 2         | 99    |
-- | 3          | 1         | 80    |
-- | 3          | 2         | 75    |
-- | 3          | 3         | 82    |
-- +------------+-----------+-------+

-- Result table:
-- +------------+-------------------+
-- | student_id | course_id | grade |
-- +------------+-----------+-------+
-- | 1          | 2         | 99    |
-- | 2          | 2         | 95    |
-- | 3          | 3         | 82    |
-- +------------+-----------+-------+

-- Solution
select student_id, course_id, grade
from(
select student_id, course_id, grade,
rank() over(partition by student_id order by grade desc, course_id) as rk
from enrollments) a
where a.rk = 1
-- Question 99
-- X city built a new stadium, each day many people visit it and the stats are saved as these columns: id, visit_date, people

-- Please write a query to display the records which have 3 or more consecutive rows and the amount of people more than 100(inclusive).

-- For example, the table stadium:
-- +------+------------+-----------+
-- | id   | visit_date | people    |
-- +------+------------+-----------+
-- | 1    | 2017-01-01 | 10        |
-- | 2    | 2017-01-02 | 109       |
-- | 3    | 2017-01-03 | 150       |
-- | 4    | 2017-01-04 | 99        |
-- | 5    | 2017-01-05 | 145       |
-- | 6    | 2017-01-06 | 1455      |
-- | 7    | 2017-01-07 | 199       |
-- | 8    | 2017-01-08 | 188       |
-- +------+------------+-----------+
-- For the sample data above, the output is:

-- +------+------------+-----------+
-- | id   | visit_date | people    |
-- +------+------------+-----------+
-- | 5    | 2017-01-05 | 145       |
-- | 6    | 2017-01-06 | 1455      |
-- | 7    | 2017-01-07 | 199       |
-- | 8    | 2017-01-08 | 188       |
-- +------+------------+-----------+
-- Note:
-- Each day only have one row record, and the dates are increasing with id increasing.

-- Solution
WITH t1 AS (
            SELECT id, 
                   visit_date,
                   people,
                   id - ROW_NUMBER() OVER(ORDER BY visit_date) AS dates
              FROM stadium
            WHERE people >= 100) 
            
SELECT t1.id, 
       t1.visit_date,
       t1people
FROM t1
LEFT JOIN (
            SELECT dates, 
                   COUNT(*) as total
              FROM t1
            GROUP BY dates) AS b
USING (dates)
WHERE b.total > 2

-- Question 82
-- Table: Delivery

-- +-----------------------------+---------+
-- | Column Name                 | Type    |
-- +-----------------------------+---------+
-- | delivery_id                 | int     |
-- | customer_id                 | int     |
-- | order_date                  | date    |
-- | customer_pref_delivery_date | date    |
-- +-----------------------------+---------+
-- delivery_id is the primary key of this table.
-- The table holds information about food delivery to customers that make orders at some date and specify a preferred delivery date (on the same order date or after it).
 

-- If the preferred delivery date of the customer is the same as the order date then the order is called immediate otherwise it's called scheduled.

-- The first order of a customer is the order with the earliest order date that customer made. It is guaranteed that a customer has exactly one first order.

-- Write an SQL query to find the percentage of immediate orders in the first orders of all customers, rounded to 2 decimal places.

-- The query result format is in the following example:

-- Delivery table:
-- +-------------+-------------+------------+-----------------------------+
-- | delivery_id | customer_id | order_date | customer_pref_delivery_date |
-- +-------------+-------------+------------+-----------------------------+
-- | 1           | 1           | 2019-08-01 | 2019-08-02                  |
-- | 2           | 2           | 2019-08-02 | 2019-08-02                  |
-- | 3           | 1           | 2019-08-11 | 2019-08-12                  |
-- | 4           | 3           | 2019-08-24 | 2019-08-24                  |
-- | 5           | 3           | 2019-08-21 | 2019-08-22                  |
-- | 6           | 2           | 2019-08-11 | 2019-08-13                  |
-- | 7           | 4           | 2019-08-09 | 2019-08-09                  |
-- +-------------+-------------+------------+-----------------------------+

-- Result table:
-- +----------------------+
-- | immediate_percentage |
-- +----------------------+
-- | 50.00                |
-- +----------------------+
-- The customer id 1 has a first order with delivery id 1 and it is scheduled.
-- The customer id 2 has a first order with delivery id 2 and it is immediate.
-- The customer id 3 has a first order with delivery id 5 and it is scheduled.
-- The customer id 4 has a first order with delivery id 7 and it is immediate.
-- Hence, half the customers have immediate first orders.

-- Solution
select 
round(avg(case when order_date = customer_pref_delivery_date then 1 else 0 end)*100,2) as
immediate_percentage
from 
(select *,
 rank() over(partition by customer_id order by order_date) as rk
from delivery) a
where a.rk=1
-- Question 38
-- Table: Delivery

-- +-----------------------------+---------+
-- | Column Name                 | Type    |
-- +-----------------------------+---------+
-- | delivery_id                 | int     |
-- | customer_id                 | int     |
-- | order_date                  | date    |
-- | customer_pref_delivery_date | date    |
-- +-----------------------------+---------+
-- delivery_id is the primary key of this table.
-- The table holds information about food delivery to customers that make orders at some date and specify a preferred delivery date (on the same order date or after it).
 

-- If the preferred delivery date of the customer is the same as the order date then the order is called immediate otherwise it's called scheduled.

-- Write an SQL query to find the percentage of immediate orders in the table, rounded to 2 decimal places.

-- The query result format is in the following example:

-- Delivery table:
-- +-------------+-------------+------------+-----------------------------+
-- | delivery_id | customer_id | order_date | customer_pref_delivery_date |
-- +-------------+-------------+------------+-----------------------------+
-- | 1           | 1           | 2019-08-01 | 2019-08-02                  |
-- | 2           | 5           | 2019-08-02 | 2019-08-02                  |
-- | 3           | 1           | 2019-08-11 | 2019-08-11                  |
-- | 4           | 3           | 2019-08-24 | 2019-08-26                  |
-- | 5           | 4           | 2019-08-21 | 2019-08-22                  |
-- | 6           | 2           | 2019-08-11 | 2019-08-13                  |
-- +-------------+-------------+------------+-----------------------------+

-- Result table:
-- +----------------------+
-- | immediate_percentage |
-- +----------------------+
-- | 33.33                |
-- +----------------------+
-- The orders with delivery id 2 and 3 are immediate while the others are scheduled.

-- Solution
Select 
Round(avg(case when order_date=customer_pref_delivery_date then 1 else 0 end)*100,2) as immediate_percentage
from delivery
-- Question 96
-- Write a query to print the sum of all total investment values in 2016 (TIV_2016), to a scale of 2 decimal places, for all policy holders who meet the following criteria:

-- Have the same TIV_2015 value as one or more other policyholders.
-- Are not located in the same city as any other policyholder (i.e.: the (latitude, longitude) attribute pairs must be unique).
-- Input Format:
-- The insurance table is described as follows:

-- | Column Name | Type          |
-- |-------------|---------------|
-- | PID         | INTEGER(11)   |
-- | TIV_2015    | NUMERIC(15,2) |
-- | TIV_2016    | NUMERIC(15,2) |
-- | LAT         | NUMERIC(5,2)  |
-- | LON         | NUMERIC(5,2)  |
-- where PID is the policyholder's policy ID, TIV_2015 is the total investment value in 2015, TIV_2016 is the total investment value in 2016, LAT is the latitude of the policy holder's city, and LON is the longitude of the policy holder's city.

-- Sample Input

-- | PID | TIV_2015 | TIV_2016 | LAT | LON |
-- |-----|----------|----------|-----|-----|
-- | 1   | 10       | 5        | 10  | 10  |
-- | 2   | 20       | 20       | 20  | 20  |
-- | 3   | 10       | 30       | 20  | 20  |
-- | 4   | 10       | 40       | 40  | 40  |
-- Sample Output

-- | TIV_2016 |
-- |----------|
-- | 45.00    |
-- Explanation

-- The first record in the table, like the last record, meets both of the two criteria.
-- The TIV_2015 value '10' is as the same as the third and forth record, and its location unique.

-- The second record does not meet any of the two criteria. Its TIV_2015 is not like any other policyholders.

-- And its location is the same with the third record, which makes the third record fail, too.

-- So, the result is the sum of TIV_2016 of the first and last record, which is 45.

-- Solution
select sum(TIV_2016) TIV_2016
from 
(select *, count(*) over (partition by TIV_2015) as c1, count(*) over (partition by LAT, LON) as c2
from insurance ) t
where c1 > 1 and c2 = 1; 
-- Question 68
-- Table: Queue

-- +-------------+---------+
-- | Column Name | Type    |
-- +-------------+---------+
-- | person_id   | int     |
-- | person_name | varchar |
-- | weight      | int     |
-- | turn        | int     |
-- +-------------+---------+
-- person_id is the primary key column for this table.
-- This table has the information about all people waiting for an elevator.
-- The person_id and turn columns will contain all numbers from 1 to n, where n is the number of rows in the table.
 

-- The maximum weight the elevator can hold is 1000.

-- Write an SQL query to find the person_name of the last person who will fit in the elevator without exceeding the weight limit. It is guaranteed that the person who is first in the queue can fit in the elevator.

-- The query result format is in the following example:

-- Queue table
-- +-----------+-------------------+--------+------+
-- | person_id | person_name       | weight | turn |
-- +-----------+-------------------+--------+------+
-- | 5         | George Washington | 250    | 1    |
-- | 3         | John Adams        | 350    | 2    |
-- | 6         | Thomas Jefferson  | 400    | 3    |
-- | 2         | Will Johnliams    | 200    | 4    |
-- | 4         | Thomas Jefferson  | 175    | 5    |
-- | 1         | James Elephant    | 500    | 6    |
-- +-----------+-------------------+--------+------+

-- Result table
-- +-------------------+
-- | person_name       |
-- +-------------------+
-- | Thomas Jefferson  |
-- +-------------------+

-- Queue table is ordered by turn in the example for simplicity.
-- In the example George Washington(id 5), John Adams(id 3) and Thomas Jefferson(id 6) will enter the elevator as their weight sum is 250 + 350 + 400 = 1000.
-- Thomas Jefferson(id 6) is the last person to fit in the elevator because he has the last turn in these three people.
-- Solution
With t1 as
(
select *,
sum(weight) over(order by turn) as cum_weight
from queue
order by turn)

select t1.person_name
from t1
where turn = (select max(turn) from t1 where t1.cum_weight<=1000)


-- Question 45
-- Table: Products

-- +------------------+---------+
-- | Column Name      | Type    |
-- +------------------+---------+
-- | product_id       | int     |
-- | product_name     | varchar |
-- | product_category | varchar |
-- +------------------+---------+
-- product_id is the primary key for this table.
-- This table contains data about the company's products.
-- Table: Orders

-- +---------------+---------+
-- | Column Name   | Type    |
-- +---------------+---------+
-- | product_id    | int     |
-- | order_date    | date    |
-- | unit          | int     |
-- +---------------+---------+
-- There is no primary key for this table. It may have duplicate rows.
-- product_id is a foreign key to Products table.
-- unit is the number of products ordered in order_date.
 

-- Write an SQL query to get the names of products with greater than or equal to 100 units ordered in February 2020 and their amount.

-- Return result table in any order.

-- The query result format is in the following example:

 

-- Products table:
-- +-------------+-----------------------+------------------+
-- | product_id  | product_name          | product_category |
-- +-------------+-----------------------+------------------+
-- | 1           | Leetcode Solutions    | Book             |
-- | 2           | Jewels of Stringology | Book             |
-- | 3           | HP                    | Laptop           |
-- | 4           | Lenovo                | Laptop           |
-- | 5           | Leetcode Kit          | T-shirt          |
-- +-------------+-----------------------+------------------+

-- Orders table:
-- +--------------+--------------+----------+
-- | product_id   | order_date   | unit     |
-- +--------------+--------------+----------+
-- | 1            | 2020-02-05   | 60       |
-- | 1            | 2020-02-10   | 70       |
-- | 2            | 2020-01-18   | 30       |
-- | 2            | 2020-02-11   | 80       |
-- | 3            | 2020-02-17   | 2        |
-- | 3            | 2020-02-24   | 3        |
-- | 4            | 2020-03-01   | 20       |
-- | 4            | 2020-03-04   | 30       |
-- | 4            | 2020-03-04   | 60       |
-- | 5            | 2020-02-25   | 50       |
-- | 5            | 2020-02-27   | 50       |
-- | 5            | 2020-03-01   | 50       |
-- +--------------+--------------+----------+

-- Result table:
-- +--------------------+---------+
-- | product_name       | unit    |
-- +--------------------+---------+
-- | Leetcode Solutions | 130     |
-- | Leetcode Kit       | 100     |
-- +--------------------+---------+

-- Products with product_id = 1 is ordered in February a total of (60 + 70) = 130.
-- Products with product_id = 2 is ordered in February a total of 80.
-- Products with product_id = 3 is ordered in February a total of (2 + 3) = 5.
-- Products with product_id = 4 was not ordered in February 2020.
-- Products with product_id = 5 is ordered in February a total of (50 + 50) = 100.

-- Solution
Select a.product_name, a.unit
from
(select p.product_name, sum(unit) as unit
from orders o 
join products p
on o.product_id = p.product_id
where month(order_date)=2 and year(order_date) = 2020
group by o.product_id) a
where a.unit>=100
-- Question 75
-- The Employee table holds all employees including their managers. Every employee has an Id, and there is also a column for the manager Id.

-- +------+----------+-----------+----------+
-- |Id    |Name 	  |Department |ManagerId |
-- +------+----------+-----------+----------+
-- |101   |John 	  |A 	      |null      |
-- |102   |Dan 	  |A 	      |101       |
-- |103   |James 	  |A 	      |101       |
-- |104   |Amy 	  |A 	      |101       |
-- |105   |Anne 	  |A 	      |101       |
-- |106   |Ron 	  |B 	      |101       |
-- +------+----------+-----------+----------+
-- Given the Employee table, write a SQL query that finds out managers with at least 5 direct report. For the above table, your SQL query should return:

-- +-------+
-- | Name  |
-- +-------+
-- | John  |
-- +-------+
-- Note:
-- No one would report to himself.

-- Solution
with t1 as
(
    select managerid, count(name) as total
    from employee
    group by managerid
)

select e.name
from t1
join employee e
on t1.managerid = e.id
where t1.total>=5
-- Question 69
-- Table: Users

-- +----------------+---------+
-- | Column Name    | Type    |
-- +----------------+---------+
-- | user_id        | int     |
-- | join_date      | date    |
-- | favorite_brand | varchar |
-- +----------------+---------+
-- user_id is the primary key of this table.
-- This table has the info of the users of an online shopping website where users can sell and buy items.
-- Table: Orders

-- +---------------+---------+
-- | Column Name   | Type    |
-- +---------------+---------+
-- | order_id      | int     |
-- | order_date    | date    |
-- | item_id       | int     |
-- | buyer_id      | int     |
-- | seller_id     | int     |
-- +---------------+---------+
-- order_id is the primary key of this table.
-- item_id is a foreign key to the Items table.
-- buyer_id and seller_id are foreign keys to the Users table.
-- Table: Items

-- +---------------+---------+
-- | Column Name   | Type    |
-- +---------------+---------+
-- | item_id       | int     |
-- | item_brand    | varchar |
-- +---------------+---------+
-- item_id is the primary key of this table.
 

-- Write an SQL query to find for each user, the join date and the number of orders they made as a buyer in 2019.

-- The query result format is in the following example:

-- Users table:
-- +---------+------------+----------------+
-- | user_id | join_date  | favorite_brand |
-- +---------+------------+----------------+
-- | 1       | 2018-01-01 | Lenovo         |
-- | 2       | 2018-02-09 | Samsung        |
-- | 3       | 2018-01-19 | LG             |
-- | 4       | 2018-05-21 | HP             |
-- +---------+------------+----------------+

-- Orders table:
-- +----------+------------+---------+----------+-----------+
-- | order_id | order_date | item_id | buyer_id | seller_id |
-- +----------+------------+---------+----------+-----------+
-- | 1        | 2019-08-01 | 4       | 1        | 2         |
-- | 2        | 2018-08-02 | 2       | 1        | 3         |
-- | 3        | 2019-08-03 | 3       | 2        | 3         |
-- | 4        | 2018-08-04 | 1       | 4        | 2         |
-- | 5        | 2018-08-04 | 1       | 3        | 4         |
-- | 6        | 2019-08-05 | 2       | 2        | 4         |
-- +----------+------------+---------+----------+-----------+

-- Items table:
-- +---------+------------+
-- | item_id | item_brand |
-- +---------+------------+
-- | 1       | Samsung    |
-- | 2       | Lenovo     |
-- | 3       | LG         |
-- | 4       | HP         |
-- +---------+------------+

-- Result table:
-- +-----------+------------+----------------+
-- | buyer_id  | join_date  | orders_in_2019 |
-- +-----------+------------+----------------+
-- | 1         | 2018-01-01 | 1              |
-- | 2         | 2018-02-09 | 2              |
-- | 3         | 2018-01-19 | 0              |
-- | 4         | 2018-05-21 | 0              |
-- +-----------+------------+----------------+

-- Solution
select user_id as buyer_id, join_date, coalesce(a.orders_in_2019,0)
from users
left join
(
select buyer_id, coalesce(count(*), 0) as orders_in_2019
from orders o
join users u
on u.user_id = o.buyer_id
where extract('year'from order_date) = 2019
group by buyer_id) a
on users.user_id = a.buyer_id
-- Question 103
-- Table: Users

-- +----------------+---------+
-- | Column Name    | Type    |
-- +----------------+---------+
-- | user_id        | int     |
-- | join_date      | date    |
-- | favorite_brand | varchar |
-- +----------------+---------+
-- user_id is the primary key of this table.
-- This table has the info of the users of an online shopping website where users can sell and buy items.
-- Table: Orders

-- +---------------+---------+
-- | Column Name   | Type    |
-- +---------------+---------+
-- | order_id      | int     |
-- | order_date    | date    |
-- | item_id       | int     |
-- | buyer_id      | int     |
-- | seller_id     | int     |
-- +---------------+---------+
-- order_id is the primary key of this table.
-- item_id is a foreign key to the Items table.
-- buyer_id and seller_id are foreign keys to the Users table.
-- Table: Items

-- +---------------+---------+
-- | Column Name   | Type    |
-- +---------------+---------+
-- | item_id       | int     |
-- | item_brand    | varchar |
-- +---------------+---------+
-- item_id is the primary key of this table.
 

-- Write an SQL query to find for each user, whether the brand of the second item (by date) they sold is their favorite brand. If a user sold less than two items, report the answer for that user as no.

-- It is guaranteed that no seller sold more than one item on a day.

-- The query result format is in the following example:

-- Users table:
-- +---------+------------+----------------+
-- | user_id | join_date  | favorite_brand |
-- +---------+------------+----------------+
-- | 1       | 2019-01-01 | Lenovo         |
-- | 2       | 2019-02-09 | Samsung        |
-- | 3       | 2019-01-19 | LG             |
-- | 4       | 2019-05-21 | HP             |
-- +---------+------------+----------------+

-- Orders table:
-- +----------+------------+---------+----------+-----------+
-- | order_id | order_date | item_id | buyer_id | seller_id |
-- +----------+------------+---------+----------+-----------+
-- | 1        | 2019-08-01 | 4       | 1        | 2         |
-- | 2        | 2019-08-02 | 2       | 1        | 3         |
-- | 3        | 2019-08-03 | 3       | 2        | 3         |
-- | 4        | 2019-08-04 | 1       | 4        | 2         |
-- | 5        | 2019-08-04 | 1       | 3        | 4         |
-- | 6        | 2019-08-05 | 2       | 2        | 4         |
-- +----------+------------+---------+----------+-----------+

-- Items table:
-- +---------+------------+
-- | item_id | item_brand |
-- +---------+------------+
-- | 1       | Samsung    |
-- | 2       | Lenovo     |
-- | 3       | LG         |
-- | 4       | HP         |
-- +---------+------------+

-- Result table:
-- +-----------+--------------------+
-- | seller_id | 2nd_item_fav_brand |
-- +-----------+--------------------+
-- | 1         | no                 |
-- | 2         | yes                |
-- | 3         | yes                |
-- | 4         | no                 |
-- +-----------+--------------------+

-- The answer for the user with id 1 is no because they sold nothing.
-- The answer for the users with id 2 and 3 is yes because the brands of their second sold items are their favorite brands.
-- The answer for the user with id 4 is no because the brand of their second sold item is not their favorite brand.

-- Solution
with t1 as(
select user_id, 
case when favorite_brand = item_brand then "yes"
else "no"
end as 2nd_item_fav_brand
from users u left join
(select o.item_id, seller_id, item_brand, rank() over(partition by seller_id order by order_date) as rk
from orders o join items i
using (item_id)) a
on u.user_id = a.seller_id
where a.rk = 2)

select u.user_id as seller_id, coalesce(2nd_item_fav_brand,"no") as 2nd_item_fav_brand
from users u left join t1
using(user_id)
-- Question 105
-- The Employee table holds all employees. The employee table has three columns: Employee Id, Company Name, and Salary.

-- +-----+------------+--------+
-- |Id   | Company    | Salary |
-- +-----+------------+--------+
-- |1    | A          | 2341   |
-- |2    | A          | 341    |
-- |3    | A          | 15     |
-- |4    | A          | 15314  |
-- |5    | A          | 451    |
-- |6    | A          | 513    |
-- |7    | B          | 15     |
-- |8    | B          | 13     |
-- |9    | B          | 1154   |
-- |10   | B          | 1345   |
-- |11   | B          | 1221   |
-- |12   | B          | 234    |
-- |13   | C          | 2345   |
-- |14   | C          | 2645   |
-- |15   | C          | 2645   |
-- |16   | C          | 2652   |
-- |17   | C          | 65     |
-- +-----+------------+--------+
-- Write a SQL query to find the median salary of each company. Bonus points if you can solve it without using any built-in SQL functions.

-- +-----+------------+--------+
-- |Id   | Company    | Salary |
-- +-----+------------+--------+
-- |5    | A          | 451    |
-- |6    | A          | 513    |
-- |12   | B          | 234    |
-- |9    | B          | 1154   |
-- |14   | C          | 2645   |
-- +-----+------------+--------+

-- Solution
select id, company, salary
from
(select *, 
row_number() over(partition by company order by salary) as rn,
count(*) over(partition by company) as cnt
from employee) a
where rn between cnt/2 and cnt/2+1
-- Question 95
-- Table: Transactions

-- +----------------+---------+
-- | Column Name    | Type    |
-- +----------------+---------+
-- | id             | int     |
-- | country        | varchar |
-- | state          | enum    |
-- | amount         | int     |
-- | trans_date     | date    |
-- +----------------+---------+
-- id is the primary key of this table.
-- The table has information about incoming transactions.
-- The state column is an enum of type ["approved", "declined"].
-- Table: Chargebacks

-- +----------------+---------+
-- | Column Name    | Type    |
-- +----------------+---------+
-- | trans_id       | int     |
-- | charge_date    | date    |
-- +----------------+---------+
-- Chargebacks contains basic information regarding incoming chargebacks from some transactions placed in Transactions table.
-- trans_id is a foreign key to the id column of Transactions table.
-- Each chargeback corresponds to a transaction made previously even if they were not approved.
 

-- Write an SQL query to find for each month and country, the number of approved transactions and their total amount, the number of chargebacks and their total amount.

-- Note: In your query, given the month and country, ignore rows with all zeros.

-- The query result format is in the following example:

-- Transactions table:
-- +------+---------+----------+--------+------------+
-- | id   | country | state    | amount | trans_date |
-- +------+---------+----------+--------+------------+
-- | 101  | US      | approved | 1000   | 2019-05-18 |
-- | 102  | US      | declined | 2000   | 2019-05-19 |
-- | 103  | US      | approved | 3000   | 2019-06-10 |
-- | 104  | US      | approved | 4000   | 2019-06-13 |
-- | 105  | US      | approved | 5000   | 2019-06-15 |
-- +------+---------+----------+--------+------------+

-- Chargebacks table:
-- +------------+------------+
-- | trans_id   | trans_date |
-- +------------+------------+
-- | 102        | 2019-05-29 |
-- | 101        | 2019-06-30 |
-- | 105        | 2019-09-18 |
-- +------------+------------+

-- Result table:
-- +----------+---------+----------------+-----------------+-------------------+--------------------+
-- | month    | country | approved_count | approved_amount | chargeback_count  | chargeback_amount  |
-- +----------+---------+----------------+-----------------+-------------------+--------------------+
-- | 2019-05  | US      | 1              | 1000            | 1                 | 2000               |
-- | 2019-06  | US      | 3              | 12000           | 1                 | 1000               |
-- | 2019-09  | US      | 0              | 0               | 1                 | 5000               |
-- +----------+---------+----------------+-----------------+-------------------+--------------------+

-- Solution
with t1 as
(select country, extract('month' from trans_date), state, count(*) as approved_count, sum(amount) as approved_amount
from transactions
where state = 'approved'
group by 1, 2, 3),
t2 as(
select t.country, extract('month' from c.trans_date), sum(amount) as chargeback_amount, count(*) as chargeback_count
from chargebacks c left join transactions t 
on trans_id = id
group by t.country, extract('month' from c.trans_date)),

t3 as(
select t2.date_part, t2.country, coalesce(approved_count,0) as approved_count, coalesce(approved_amount,0) as approved_amount, coalesce(chargeback_count,0) as chargeback_count, coalesce(chargeback_amount,0) as chargeback_amount
from t2 left join t1 
on t2.date_part = t1.date_part and t2.country = t1.country),

t4 as(
select t1.date_part, t1.country, coalesce(approved_count,0) as approved_count, coalesce(approved_amount,0) as approved_amount, coalesce(chargeback_count,0) as chargeback_count, coalesce(chargeback_amount,0) as chargeback_amount
from t2 right join t1 
on t2.date_part = t1.date_part and t2.country = t1.country)

select *
from t3
union
select *
from t4
-- Question 83
-- Table: Transactions

-- +---------------+---------+
-- | Column Name   | Type    |
-- +---------------+---------+
-- | id            | int     |
-- | country       | varchar |
-- | state         | enum    |
-- | amount        | int     |
-- | trans_date    | date    |
-- +---------------+---------+
-- id is the primary key of this table.
-- The table has information about incoming transactions.
-- The state column is an enum of type ["approved", "declined"].
 

-- Write an SQL query to find for each month and country, the number of transactions and their total amount, the number of approved transactions and their total amount.

-- The query result format is in the following example:

-- Transactions table:
-- +------+---------+----------+--------+------------+
-- | id   | country | state    | amount | trans_date |
-- +------+---------+----------+--------+------------+
-- | 121  | US      | approved | 1000   | 2018-12-18 |
-- | 122  | US      | declined | 2000   | 2018-12-19 |
-- | 123  | US      | approved | 2000   | 2019-01-01 |
-- | 124  | DE      | approved | 2000   | 2019-01-07 |
-- +------+---------+----------+--------+------------+

-- Result table:
-- +----------+---------+-------------+----------------+--------------------+-----------------------+
-- | month    | country | trans_count | approved_count | trans_total_amount | approved_total_amount |
-- +----------+---------+-------------+----------------+--------------------+-----------------------+
-- | 2018-12  | US      | 2           | 1              | 3000               | 1000                  |
-- | 2019-01  | US      | 1           | 1              | 2000               | 2000                  |
-- | 2019-01  | DE      | 1           | 1              | 2000               | 2000                  |
-- +----------+---------+-------------+----------------+--------------------+-----------------------+

-- Solution
with t1 as(
select DATE_FORMAT(trans_date,'%Y-%m') as month, country, count(state) as trans_count, sum(amount) as trans_total_amount
from transactions
group by country, month(trans_date)),

t2 as (
Select DATE_FORMAT(trans_date,'%Y-%m') as month, country, count(state) as approved_count, sum(amount) as approved_total_amount
from transactions
where state = 'approved'
group by country, month(trans_date))

select t1.month, t1.country, coalesce(t1.trans_count,0) as trans_count, coalesce(t2.approved_count,0) as approved_count, coalesce(t1.trans_total_amount,0) as trans_total_amount, coalesce(t2.approved_total_amount,0) as approved_total_amount
from t1 left join t2
on t1.country = t2.country and t1.month = t2.month
-- Question 59
-- Table: Movies

-- +---------------+---------+
-- | Column Name   | Type    |
-- +---------------+---------+
-- | movie_id      | int     |
-- | title         | varchar |
-- +---------------+---------+
-- movie_id is the primary key for this table.
-- title is the name of the movie.
-- Table: Users

-- +---------------+---------+
-- | Column Name   | Type    |
-- +---------------+---------+
-- | user_id       | int     |
-- | name          | varchar |
-- +---------------+---------+
-- user_id is the primary key for this table.
-- Table: Movie_Rating

-- +---------------+---------+
-- | Column Name   | Type    |
-- +---------------+---------+
-- | movie_id      | int     |
-- | user_id       | int     |
-- | rating        | int     |
-- | created_at    | date    |
-- +---------------+---------+
-- (movie_id, user_id) is the primary key for this table.
-- This table contains the rating of a movie by a user in their review.
-- created_at is the user's review date. 
 

-- Write the following SQL query:

-- Find the name of the user who has rated the greatest number of the movies.
-- In case of a tie, return lexicographically smaller user name.

-- Find the movie name with the highest average rating in February 2020.
-- In case of a tie, return lexicographically smaller movie name.

-- Query is returned in 2 rows, the query result format is in the folowing example:

-- Movies table:
-- +-------------+--------------+
-- | movie_id    |  title       |
-- +-------------+--------------+
-- | 1           | Avengers     |
-- | 2           | Frozen 2     |
-- | 3           | Joker        |
-- +-------------+--------------+

-- Users table:
-- +-------------+--------------+
-- | user_id     |  name        |
-- +-------------+--------------+
-- | 1           | Daniel       |
-- | 2           | Monica       |
-- | 3           | Maria        |
-- | 4           | James        |
-- +-------------+--------------+

-- Movie_Rating table:
-- +-------------+--------------+--------------+-------------+
-- | movie_id    | user_id      | rating       | created_at  |
-- +-------------+--------------+--------------+-------------+
-- | 1           | 1            | 3            | 2020-01-12  |
-- | 1           | 2            | 4            | 2020-02-11  |
-- | 1           | 3            | 2            | 2020-02-12  |
-- | 1           | 4            | 1            | 2020-01-01  |
-- | 2           | 1            | 5            | 2020-02-17  | 
-- | 2           | 2            | 2            | 2020-02-01  | 
-- | 2           | 3            | 2            | 2020-03-01  |
-- | 3           | 1            | 3            | 2020-02-22  | 
-- | 3           | 2            | 4            | 2020-02-25  | 
-- +-------------+--------------+--------------+-------------+

-- Result table:
-- +--------------+
-- | results      |
-- +--------------+
-- | Daniel       |
-- | Frozen 2     |
-- +--------------+

-- Daniel and Maria have rated 3 movies ("Avengers", "Frozen 2" and "Joker") but Daniel is smaller lexicographically.
-- Frozen 2 and Joker have a rating average of 3.5 in February but Frozen 2 is smaller lexicographically.

-- Solution
select name as results
from(
(select a.name
from(
select name, count(*),
rank() over(order by count(*) desc) as rk
from movie_rating m
join users u 
on m.user_id = u.user_id
group by name, m.user_id
order by rk, name) a
limit 1)
union
(select title
from(
select title, round(avg(rating),1) as rnd
from movie_rating m
join movies u
on m.movie_id = u.movie_id
where month(created_at) = 2
group by title
order by rnd desc, title) b
limit 1)) as d
-- Question 92
-- Table: Traffic

-- +---------------+---------+
-- | Column Name   | Type    |
-- +---------------+---------+
-- | user_id       | int     |
-- | activity      | enum    |
-- | activity_date | date    |
-- +---------------+---------+
-- There is no primary key for this table, it may have duplicate rows.
-- The activity column is an ENUM type of ('login', 'logout', 'jobs', 'groups', 'homepage').
 

-- Write an SQL query that reports for every date within at most 90 days from today, 
-- the number of users that logged in for the first time on that date. Assume today is 2019-06-30.

-- The query result format is in the following example:

-- Traffic table:
-- +---------+----------+---------------+
-- | user_id | activity | activity_date |
-- +---------+----------+---------------+
-- | 1       | login    | 2019-05-01    |
-- | 1       | homepage | 2019-05-01    |
-- | 1       | logout   | 2019-05-01    |
-- | 2       | login    | 2019-06-21    |
-- | 2       | logout   | 2019-06-21    |
-- | 3       | login    | 2019-01-01    |
-- | 3       | jobs     | 2019-01-01    |
-- | 3       | logout   | 2019-01-01    |
-- | 4       | login    | 2019-06-21    |
-- | 4       | groups   | 2019-06-21    |
-- | 4       | logout   | 2019-06-21    |
-- | 5       | login    | 2019-03-01    |
-- | 5       | logout   | 2019-03-01    |
-- | 5       | login    | 2019-06-21    |
-- | 5       | logout   | 2019-06-21    |
-- +---------+----------+---------------+

-- Result table:
-- +------------+-------------+
-- | login_date | user_count  |
-- +------------+-------------+
-- | 2019-05-01 | 1           |
-- | 2019-06-21 | 2           |
-- +------------+-------------+
-- Note that we only care about dates with non zero user count.
-- The user with id 5 first logged in on 2019-03-01 so he's not counted on 2019-06-21.

-- Solution
with t1 as
(
    select user_id, min(activity_date) as login_date
    from Traffic
    where activity = 'login'
    group by user_id
)

select login_date, count(distinct user_id) as user_count
from t1
where login_date between '2019-04-01' and '2019-06-30'
group by login_date
-- Question 6
-- X city opened a new cinema, many people would like to go to this cinema.
-- The cinema also gives out a poster indicating the movies’ ratings and descriptions.
-- Please write a SQL query to output movies with an odd numbered ID and a description that is not 'boring'.
-- Order the result by rating.

 

-- For example, table cinema:

-- +---------+-----------+--------------+-----------+
-- |   id    | movie     |  description |  rating   |
-- +---------+-----------+--------------+-----------+
-- |   1     | War       |   great 3D   |   8.9     |
-- |   2     | Science   |   fiction    |   8.5     |
-- |   3     | irish     |   boring     |   6.2     |
-- |   4     | Ice song  |   Fantacy    |   8.6     |
-- |   5     | House card|   Interesting|   9.1     |
-- +---------+-----------+--------------+-----------+
-- For the example above, the output should be:
-- +---------+-----------+--------------+-----------+
-- |   id    | movie     |  description |  rating   |
-- +---------+-----------+--------------+-----------+
-- |   5     | House card|   Interesting|   9.1     |
-- |   1     | War       |   great 3D   |   8.9     |
-- +---------+-----------+--------------+-----------+


-- Solution
Select *
from cinema
where id%2=1 and description not in ('boring')
order by rating desc
-- Question 54
-- Table: NPV

-- +---------------+---------+
-- | Column Name   | Type    |
-- +---------------+---------+
-- | id            | int     |
-- | year          | int     |
-- | npv           | int     |
-- +---------------+---------+
-- (id, year) is the primary key of this table.
-- The table has information about the id and the year of each inventory and the corresponding net present value.
 

-- Table: Queries

-- +---------------+---------+
-- | Column Name   | Type    |
-- +---------------+---------+
-- | id            | int     |
-- | year          | int     |
-- +---------------+---------+
-- (id, year) is the primary key of this table.
-- The table has information about the id and the year of each inventory query.
 

-- Write an SQL query to find the npv of all each query of queries table.

-- Return the result table in any order.

-- The query result format is in the following example:

-- NPV table:
-- +------+--------+--------+
-- | id   | year   | npv    |
-- +------+--------+--------+
-- | 1    | 2018   | 100    |
-- | 7    | 2020   | 30     |
-- | 13   | 2019   | 40     |
-- | 1    | 2019   | 113    |
-- | 2    | 2008   | 121    |
-- | 3    | 2009   | 12     |
-- | 11   | 2020   | 99     |
-- | 7    | 2019   | 0      |
-- +------+--------+--------+

-- Queries table:
-- +------+--------+
-- | id   | year   |
-- +------+--------+
-- | 1    | 2019   |
-- | 2    | 2008   |
-- | 3    | 2009   |
-- | 7    | 2018   |
-- | 7    | 2019   |
-- | 7    | 2020   |
-- | 13   | 2019   |
-- +------+--------+

-- Result table:
-- +------+--------+--------+
-- | id   | year   | npv    |
-- +------+--------+--------+
-- | 1    | 2019   | 113    |
-- | 2    | 2008   | 121    |
-- | 3    | 2009   | 12     |
-- | 7    | 2018   | 0      |
-- | 7    | 2019   | 0      |
-- | 7    | 2020   | 30     |
-- | 13   | 2019   | 40     |
-- +------+--------+--------+

-- The npv value of (7, 2018) is not present in the NPV table, we consider it 0.
-- The npv values of all other queries can be found in the NPV table.

-- Solution
select q.id, q.year, coalesce(n.npv,0) as npv
from queries q
left join npv n
on q.id = n.id and q.year=n.year

-- Question 50
-- Write a SQL query to get the nth highest salary from the Employee table.

-- +----+--------+
-- | Id | Salary |
-- +----+--------+
-- | 1  | 100    |
-- | 2  | 200    |
-- | 3  | 300    |
-- +----+--------+
-- For example, given the above Employee table, the nth highest salary where n = 2 is 200. If there is no nth highest salary, then the query should return null.

-- +------------------------+
-- | getNthHighestSalary(2) |
-- +------------------------+
-- | 200                    |
-- +------------------------+

-- Solution 
CREATE FUNCTION getNthHighestSalary(N INT) RETURNS INT
BEGIN
  RETURN (
      # Write your MySQL query statement below.
      select distinct a.salary
      from
      (select salary, 
      dense_rank() over(order by salary desc) as rk
      from Employee) a
      where a.rk = N
  );
END
-- Question 31
-- Table: Submissions

-- +---------------+----------+
-- | Column Name   | Type     |
-- +---------------+----------+
-- | sub_id        | int      |
-- | parent_id     | int      |
-- +---------------+----------+
-- There is no primary key for this table, it may have duplicate rows.
-- Each row can be a post or comment on the post.
-- parent_id is null for posts.
-- parent_id for comments is sub_id for another post in the table.
 

-- Write an SQL query to find number of comments per each post.

-- Result table should contain post_id and its corresponding number_of_comments, 
-- and must be sorted by post_id in ascending order.

-- Submissions may contain duplicate comments. You should count the number of unique comments per post.

-- Submissions may contain duplicate posts. You should treat them as one post.

-- The query result format is in the following example:

-- Submissions table:
-- +---------+------------+
-- | sub_id  | parent_id  |
-- +---------+------------+
-- | 1       | Null       |
-- | 2       | Null       |
-- | 1       | Null       |
-- | 12      | Null       |
-- | 3       | 1          |
-- | 5       | 2          |
-- | 3       | 1          |
-- | 4       | 1          |
-- | 9       | 1          |
-- | 10      | 2          |
-- | 6       | 7          |
-- +---------+------------+

-- Result table:
-- +---------+--------------------+
-- | post_id | number_of_comments |
-- +---------+--------------------+
-- | 1       | 3                  |
-- | 2       | 2                  |
-- | 12      | 0                  |
-- +---------+--------------------+

-- The post with id 1 has three comments in the table with id 3, 4 and 9. The comment with id 3 is 
-- repeated in the table, we counted it only once.
-- The post with id 2 has two comments in the table with id 5 and 10.
-- The post with id 12 has no comments in the table.
-- The comment with id 6 is a comment on a deleted post with id 7 so we ignored it.

-- Solution
Select a.sub_id as post_id, coalesce(b.number_of_comments,0) as number_of_comments
from(
select distinct sub_id from submissions where parent_id is null) a
left join(
select parent_id, count(distinct(sub_id)) as number_of_comments
from submissions
group by parent_id
having parent_id = any(select sub_id from submissions where parent_id is null)) b
on a.sub_id = b.parent_id
order by post_id
--Question 101
-- Table: Visits

-- +---------------+---------+
-- | Column Name   | Type    |
-- +---------------+---------+
-- | user_id       | int     |
-- | visit_date    | date    |
-- +---------------+---------+
-- (user_id, visit_date) is the primary key for this table.
-- Each row of this table indicates that user_id has visited the bank in visit_date.
 

-- Table: Transactions

-- +------------------+---------+
-- | Column Name      | Type    |
-- +------------------+---------+
-- | user_id          | int     |
-- | transaction_date | date    |
-- | amount           | int     |
-- +------------------+---------+
-- There is no primary key for this table, it may contain duplicates.
-- Each row of this table indicates that user_id has done a transaction of amount in transaction_date.
-- It is guaranteed that the user has visited the bank in the transaction_date.(i.e The Visits table contains (user_id, transaction_date) in one row)
 

-- A bank wants to draw a chart of the number of transactions bank visitors did in one visit to the bank and the corresponding number of visitors who have done this number of transaction in one visit.

-- Write an SQL query to find how many users visited the bank and didn't do any transactions, how many visited the bank and did one transaction and so on.

-- The result table will contain two columns:

-- transactions_count which is the number of transactions done in one visit.
-- visits_count which is the corresponding number of users who did transactions_count in one visit to the bank.
-- transactions_count should take all values from 0 to max(transactions_count) done by one or more users.

-- Order the result table by transactions_count.

-- The query result format is in the following example:

-- Visits table:
-- +---------+------------+
-- | user_id | visit_date |
-- +---------+------------+
-- | 1       | 2020-01-01 |
-- | 2       | 2020-01-02 |
-- | 12      | 2020-01-01 |
-- | 19      | 2020-01-03 |
-- | 1       | 2020-01-02 |
-- | 2       | 2020-01-03 |
-- | 1       | 2020-01-04 |
-- | 7       | 2020-01-11 |
-- | 9       | 2020-01-25 |
-- | 8       | 2020-01-28 |
-- +---------+------------+
-- Transactions table:
-- +---------+------------------+--------+
-- | user_id | transaction_date | amount |
-- +---------+------------------+--------+
-- | 1       | 2020-01-02       | 120    |
-- | 2       | 2020-01-03       | 22     |
-- | 7       | 2020-01-11       | 232    |
-- | 1       | 2020-01-04       | 7      |
-- | 9       | 2020-01-25       | 33     |
-- | 9       | 2020-01-25       | 66     |
-- | 8       | 2020-01-28       | 1      |
-- | 9       | 2020-01-25       | 99     |
-- +---------+------------------+--------+
-- Result table:
-- +--------------------+--------------+
-- | transactions_count | visits_count |
-- +--------------------+--------------+
-- | 0                  | 4            |
-- | 1                  | 5            |
-- | 2                  | 0            |
-- | 3                  | 1            |
-- +--------------------+--------------+
-- * For transactions_count = 0, The visits (1, "2020-01-01"), (2, "2020-01-02"), (12, "2020-01-01") and (19, "2020-01-03") did no transactions so visits_count = 4.
-- * For transactions_count = 1, The visits (2, "2020-01-03"), (7, "2020-01-11"), (8, "2020-01-28"), (1, "2020-01-02") and (1, "2020-01-04") did one transaction so visits_count = 5.
-- * For transactions_count = 2, No customers visited the bank and did two transactions so visits_count = 0.
-- * For transactions_count = 3, The visit (9, "2020-01-25") did three transactions so visits_count = 1.
-- * For transactions_count >= 4, No customers visited the bank and did more than three transactions so we will stop at transactions_count = 3

-- Solution
WITH RECURSIVE t1 AS(
                    SELECT visit_date,
                           COALESCE(num_visits,0) as num_visits,
                           COALESCE(num_trans,0) as num_trans
                    FROM ((
                          SELECT visit_date, user_id, COUNT(*) as num_visits
                          FROM visits
                          GROUP BY 1, 2) AS a
                         LEFT JOIN
                          (
                           SELECT transaction_date,
                                 user_id,
                                 count(*) as num_trans
                            FROM transactions
                          GROUP BY 1, 2) AS b
                         ON a.visit_date = b.transaction_date and a.user_id = b.user_id)
                      ),

              t2 AS (
                      SELECT MAX(num_trans) as trans
                        FROM t1
                      UNION ALL
                      SELECT trans-1 
                        FROM t2
                      WHERE trans >= 1)

SELECT trans as transactions_count, 
       COALESCE(visits_count,0) as visits_count
  FROM t2 LEFT JOIN (
                    SELECT num_trans as transactions_count, COALESCE(COUNT(*),0) as visits_count
                    FROM t1 
                    GROUP BY 1
                    ORDER BY 1) AS a
ON a.transactions_count = t2.trans
ORDER BY 1

-- Question 84
-- Table: Friendship

-- +---------------+---------+
-- | Column Name   | Type    |
-- +---------------+---------+
-- | user1_id      | int     |
-- | user2_id      | int     |
-- +---------------+---------+
-- (user1_id, user2_id) is the primary key for this table.
-- Each row of this table indicates that there is a friendship relation between user1_id and user2_id.
 

-- Table: Likes

-- +-------------+---------+
-- | Column Name | Type    |
-- +-------------+---------+
-- | user_id     | int     |
-- | page_id     | int     |
-- +-------------+---------+
-- (user_id, page_id) is the primary key for this table.
-- Each row of this table indicates that user_id likes page_id.
 

-- Write an SQL query to recommend pages to the user with user_id = 1 using the pages that your friends liked. It should not recommend pages you already liked.

-- Return result table in any order without duplicates.

-- The query result format is in the following example:

-- Friendship table:
-- +----------+----------+
-- | user1_id | user2_id |
-- +----------+----------+
-- | 1        | 2        |
-- | 1        | 3        |
-- | 1        | 4        |
-- | 2        | 3        |
-- | 2        | 4        |
-- | 2        | 5        |
-- | 6        | 1        |
-- +----------+----------+
 
-- Likes table:
-- +---------+---------+
-- | user_id | page_id |
-- +---------+---------+
-- | 1       | 88      |
-- | 2       | 23      |
-- | 3       | 24      |
-- | 4       | 56      |
-- | 5       | 11      |
-- | 6       | 33      |
-- | 2       | 77      |
-- | 3       | 77      |
-- | 6       | 88      |
-- +---------+---------+

-- Result table:
-- +------------------+
-- | recommended_page |
-- +------------------+
-- | 23               |
-- | 24               |
-- | 56               |
-- | 33               |
-- | 77               |
-- +------------------+
-- User one is friend with users 2, 3, 4 and 6.
-- Suggested pages are 23 from user 2, 24 from user 3, 56 from user 3 and 33 from user 6.
-- Page 77 is suggested from both user 2 and user 3.
-- Page 88 is not suggested because user 1 already likes it.

-- Solution
select distinct page_id as recommended_page
from likes
where user_id = 
any(select user2_id as id
from friendship
where user1_id = 1 or user2_id = 1 and user2_id !=1
union all
select user1_id
from friendship
where user2_id = 1) 
and page_id != all(select page_id from likes where user_id = 1)
-- Question 67
-- Table: Products

-- +---------------+---------+
-- | Column Name   | Type    |
-- +---------------+---------+
-- | product_id    | int     |
-- | new_price     | int     |
-- | change_date   | date    |
-- +---------------+---------+
-- (product_id, change_date) is the primary key of this table.
-- Each row of this table indicates that the price of some product was changed to a new price at some date.
 

-- Write an SQL query to find the prices of all products on 2019-08-16. Assume the price of all products before any change is 10.

-- The query result format is in the following example:

-- Products table:
-- +------------+-----------+-------------+
-- | product_id | new_price | change_date |
-- +------------+-----------+-------------+
-- | 1          | 20        | 2019-08-14  |
-- | 2          | 50        | 2019-08-14  |
-- | 1          | 30        | 2019-08-15  |
-- | 1          | 35        | 2019-08-16  |
-- | 2          | 65        | 2019-08-17  |
-- | 3          | 20        | 2019-08-18  |
-- +------------+-----------+-------------+

-- Result table:
-- +------------+-------+
-- | product_id | price |
-- +------------+-------+
-- | 2          | 50    |
-- | 1          | 35    |
-- | 3          | 10    |
-- +------------+-------+

-- Solution
with t1 as (
select a.product_id, new_price
from(
Select product_id, max(change_date) as date
from products
where change_date<='2019-08-16'
group by product_id) a
join products p
on a.product_id = p.product_id and a.date = p.change_date),

t2 as (
select distinct product_id
	from products)
	
select t2.product_id, coalesce(new_price,10) as price
from t2 left join t1
on t2.product_id = t1.product_id
order by price desc
-- Question 30
-- Table: Sales

-- +-------------+-------+
-- | Column Name | Type  |
-- +-------------+-------+
-- | sale_id     | int   |
-- | product_id  | int   |
-- | year        | int   |
-- | quantity    | int   |
-- | price       | int   |
-- +-------------+-------+
-- (sale_id, year) is the primary key of this table.
-- product_id is a foreign key to Product table.
-- Note that the price is per unit.
-- Table: Product

-- +--------------+---------+
-- | Column Name  | Type    |
-- +--------------+---------+
-- | product_id   | int     |
-- | product_name | varchar |
-- +--------------+---------+
-- product_id is the primary key of this table.
 

-- Write an SQL query that reports all product names of the products in the Sales table along with their selling year and price.

-- For example:

-- Sales table:
-- +---------+------------+------+----------+-------+
-- | sale_id | product_id | year | quantity | price |
-- +---------+------------+------+----------+-------+ 
-- | 1       | 100        | 2008 | 10       | 5000  |
-- | 2       | 100        | 2009 | 12       | 5000  |
-- | 7       | 200        | 2011 | 15       | 9000  |
-- +---------+------------+------+----------+-------+

-- Product table:
-- +------------+--------------+
-- | product_id | product_name |
-- +------------+--------------+
-- | 100        | Nokia        |
-- | 200        | Apple        |
-- | 300        | Samsung      |
-- +------------+--------------+

-- Result table:
-- +--------------+-------+-------+
-- | product_name | year  | price |
-- +--------------+-------+-------+
-- | Nokia        | 2008  | 5000  |
-- | Nokia        | 2009  | 5000  |
-- | Apple        | 2011  | 9000  |
-- +--------------+-------+-------+

-- Solution
Select a.product_name, b.year, b.price
from product as a
join
sales as b
on a.product_id = b.product_id
-- Question 29
-- Table: Sales

-- +-------------+-------+
-- | Column Name | Type  |
-- +-------------+-------+
-- | sale_id     | int   |
-- | product_id  | int   |
-- | year        | int   |
-- | quantity    | int   |
-- | price       | int   |
-- +-------------+-------+
-- sale_id is the primary key of this table.
-- product_id is a foreign key to Product table.
-- Note that the price is per unit.
-- Table: Product

-- +--------------+---------+
-- | Column Name  | Type    |
-- +--------------+---------+
-- | product_id   | int     |
-- | product_name | varchar |
-- +--------------+---------+
-- product_id is the primary key of this table.
 

-- Write an SQL query that reports the total quantity sold for every product id.

-- The query result format is in the following example:

-- Sales table:
-- +---------+------------+------+----------+-------+
-- | sale_id | product_id | year | quantity | price |
-- +---------+------------+------+----------+-------+ 
-- | 1       | 100        | 2008 | 10       | 5000  |
-- | 2       | 100        | 2009 | 12       | 5000  |
-- | 7       | 200        | 2011 | 15       | 9000  |
-- +---------+------------+------+----------+-------+

-- Product table:
-- +------------+--------------+
-- | product_id | product_name |
-- +------------+--------------+
-- | 100        | Nokia        |
-- | 200        | Apple        |
-- | 300        | Samsung      |
-- +------------+--------------+

-- Result table:
-- +--------------+----------------+
-- | product_id   | total_quantity |
-- +--------------+----------------+
-- | 100          | 22             |
-- | 200          | 15             |
-- +--------------+----------------+

-- Solution
Select a.product_id, sum(a.quantity) as total_quantity
from sales a
join
product b
on a.product_id = b.product_id
group by a.product_id
-- Question 90
-- Table: Sales

-- +-------------+-------+
-- | Column Name | Type  |
-- +-------------+-------+
-- | sale_id     | int   |
-- | product_id  | int   |
-- | year        | int   |
-- | quantity    | int   |
-- | price       | int   |
-- +-------------+-------+
-- sale_id is the primary key of this table.
-- product_id is a foreign key to Product table.
-- Note that the price is per unit.
-- Table: Product

-- +--------------+---------+
-- | Column Name  | Type    |
-- +--------------+---------+
-- | product_id   | int     |
-- | product_name | varchar |
-- +--------------+---------+
-- product_id is the primary key of this table.
 

-- Write an SQL query that selects the product id, year, quantity, and price for the first year of every product sold.

-- The query result format is in the following example:

-- Sales table:
-- +---------+------------+------+----------+-------+
-- | sale_id | product_id | year | quantity | price |
-- +---------+------------+------+----------+-------+ 
-- | 1       | 100        | 2008 | 10       | 5000  |
-- | 2       | 100        | 2009 | 12       | 5000  |
-- | 7       | 200        | 2011 | 15       | 9000  |
-- +---------+------------+------+----------+-------+

-- Product table:
-- +------------+--------------+
-- | product_id | product_name |
-- +------------+--------------+
-- | 100        | Nokia        |
-- | 200        | Apple        |
-- | 300        | Samsung      |
-- +------------+--------------+

-- Result table:
-- +------------+------------+----------+-------+
-- | product_id | first_year | quantity | price |
-- +------------+------------+----------+-------+ 
-- | 100        | 2008       | 10       | 5000  |
-- | 200        | 2011       | 15       | 9000  |
-- +------------+------------+----------+-------+

-- Solution
select a.product_id, a.year as first_year, a.quantity, a.price
from
( select product_id, quantity, price, year,
  rank() over(partition by product_id order by year) as rk
  from sales
) a
where a.rk = 1
-- Question 26
-- Table: Project

-- +-------------+---------+
-- | Column Name | Type    |
-- +-------------+---------+
-- | project_id  | int     |
-- | employee_id | int     |
-- +-------------+---------+
-- (project_id, employee_id) is the primary key of this table.
-- employee_id is a foreign key to Employee table.
-- Table: Employee

-- +------------------+---------+
-- | Column Name      | Type    |
-- +------------------+---------+
-- | employee_id      | int     |
-- | name             | varchar |
-- | experience_years | int     |
-- +------------------+---------+
-- employee_id is the primary key of this table.
 

-- Write an SQL query that reports the average experience years of all the employees for each project, rounded to 2 digits.

-- The query result format is in the following example:

-- Project table:
-- +-------------+-------------+
-- | project_id  | employee_id |
-- +-------------+-------------+
-- | 1           | 1           |
-- | 1           | 2           |
-- | 1           | 3           |
-- | 2           | 1           |
-- | 2           | 4           |
-- +-------------+-------------+

-- Employee table:
-- +-------------+--------+------------------+
-- | employee_id | name   | experience_years |
-- +-------------+--------+------------------+
-- | 1           | Khaled | 3                |
-- | 2           | Ali    | 2                |
-- | 3           | John   | 1                |
-- | 4           | Doe    | 2                |
-- +-------------+--------+------------------+

-- Result table:
-- +-------------+---------------+
-- | project_id  | average_years |
-- +-------------+---------------+
-- | 1           | 2.00          |
-- | 2           | 2.50          |
-- +-------------+---------------+
-- The average experience years for the first project is (3 + 2 + 1) / 3 = 2.00 and for the second project is (3 + 2) / 2 = 2.50

-- Solution
Select a.project_id, round(sum(b.experience_years)/count(b.employee_id),2) as average_years
from project as a
join
employee as b
on a.employee_id=b.employee_id
group by a.project_id
-- Question 28
-- Table: Project

-- +-------------+---------+
-- | Column Name | Type    |
-- +-------------+---------+
-- | project_id  | int     |
-- | employee_id | int     |
-- +-------------+---------+
-- (project_id, employee_id) is the primary key of this table.
-- employee_id is a foreign key to Employee table.
-- Table: Employee

-- +------------------+---------+
-- | Column Name      | Type    |
-- +------------------+---------+
-- | employee_id      | int     |
-- | name             | varchar |
-- | experience_years | int     |
-- +------------------+---------+
-- employee_id is the primary key of this table.
 

-- Write an SQL query that reports all the projects that have the most employees.

-- The query result format is in the following example:

-- Project table:
-- +-------------+-------------+
-- | project_id  | employee_id |
-- +-------------+-------------+
-- | 1           | 1           |
-- | 1           | 2           |
-- | 1           | 3           |
-- | 2           | 1           |
-- | 2           | 4           |
-- +-------------+-------------+

-- Employee table:
-- +-------------+--------+------------------+
-- | employee_id | name   | experience_years |
-- +-------------+--------+------------------+
-- | 1           | Khaled | 3                |
-- | 2           | Ali    | 2                |
-- | 3           | John   | 1                |
-- | 4           | Doe    | 2                |
-- +-------------+--------+------------------+

-- Result table:
-- +-------------+
-- | project_id  |
-- +-------------+
-- | 1           |
-- +-------------+
-- The first project has 3 employees while the second one has 2.

-- Solution
select a.project_id
from(
select project_id,
rank() over(order by count(employee_id) desc) as rk
from project
group by project_id) a
where a.rk = 1
-- Question 85
-- Table: Project

-- +-------------+---------+
-- | Column Name | Type    |
-- +-------------+---------+
-- | project_id  | int     |
-- | employee_id | int     |
-- +-------------+---------+
-- (project_id, employee_id) is the primary key of this table.
-- employee_id is a foreign key to Employee table.
-- Table: Employee

-- +------------------+---------+
-- | Column Name      | Type    |
-- +------------------+---------+
-- | employee_id      | int     |
-- | name             | varchar |
-- | experience_years | int     |
-- +------------------+---------+
-- employee_id is the primary key of this table.
 

-- Write an SQL query that reports the most experienced employees in each project. 
-- In case of a tie, report all employees with the maximum number of experience years.

-- The query result format is in the following example:

-- Project table:
-- +-------------+-------------+
-- | project_id  | employee_id |
-- +-------------+-------------+
-- | 1           | 1           |
-- | 1           | 2           |
-- | 1           | 3           |
-- | 2           | 1           |
-- | 2           | 4           |
-- +-------------+-------------+

-- Employee table:
-- +-------------+--------+------------------+
-- | employee_id | name   | experience_years |
-- +-------------+--------+------------------+
-- | 1           | Khaled | 3                |
-- | 2           | Ali    | 2                |
-- | 3           | John   | 3                |
-- | 4           | Doe    | 2                |
-- +-------------+--------+------------------+

-- Result table:
-- +-------------+---------------+
-- | project_id  | employee_id   |
-- +-------------+---------------+
-- | 1           | 1             |
-- | 1           | 3             |
-- | 2           | 1             |
-- +-------------+---------------+
-- Both employees with id 1 and 3 have the 
-- most experience among the employees of the first project. For the second project, the employee with id 1 has the most experience.

-- Solution
with t1 as(
select p.project_id, p.employee_id, e.experience_years,
rank() over(partition by project_id order by experience_years desc) as rk
from project p
join employee e
on p.employee_id = e.employee_id)

select t1.project_id, t1.employee_id
from t1
where t1.rk = 1

-- Question 41
-- Table: Queries

-- +-------------+---------+
-- | Column Name | Type    |
-- +-------------+---------+
-- | query_name  | varchar |
-- | result      | varchar |
-- | position    | int     |
-- | rating      | int     |
-- +-------------+---------+
-- There is no primary key for this table, it may have duplicate rows.
-- This table contains information collected from some queries on a database.
-- The position column has a value from 1 to 500.
-- The rating column has a value from 1 to 5. Query with rating less than 3 is a poor query.
 

-- We define query quality as:

-- The average of the ratio between query rating and its position.

-- We also define poor query percentage as:

-- The percentage of all queries with rating less than 3.

-- Write an SQL query to find each query_name, the quality and poor_query_percentage.

-- Both quality and poor_query_percentage should be rounded to 2 decimal places.

-- The query result format is in the following example:

-- Queries table:
-- +------------+-------------------+----------+--------+
-- | query_name | result            | position | rating |
-- +------------+-------------------+----------+--------+
-- | Dog        | Golden Retriever  | 1        | 5      |
-- | Dog        | German Shepherd   | 2        | 5      |
-- | Dog        | Mule              | 200      | 1      |
-- | Cat        | Shirazi           | 5        | 2      |
-- | Cat        | Siamese           | 3        | 3      |
-- | Cat        | Sphynx            | 7        | 4      |
-- +------------+-------------------+----------+--------+

-- Result table:
-- +------------+---------+-----------------------+
-- | query_name | quality | poor_query_percentage |
-- +------------+---------+-----------------------+
-- | Dog        | 2.50    | 33.33                 |
-- | Cat        | 0.66    | 33.33                 |
-- +------------+---------+-----------------------+

-- Dog queries quality is ((5 / 1) + (5 / 2) + (1 / 200)) / 3 = 2.50
-- Dog queries poor_ query_percentage is (1 / 3) * 100 = 33.33

-- Cat queries quality equals ((2 / 5) + (3 / 3) + (4 / 7)) / 3 = 0.66
-- Cat queries poor_ query_percentage is (1 / 3) * 100 = 33.33

-- Solution
Select query_name, round(sum(rating/position)/count(*),2) as quality, 
round(avg(case when rating<3 then 1 else 0 end)*100,2) as poor_query_percentage
from queries
group by query_name
-- Question 51
-- Write a SQL query to rank scores.
-- If there is a tie between two scores, both should have the same ranking. 
-- Note that after a tie, the next ranking number should be the next consecutive integer value. 
-- In other words, there should be no "holes" between ranks.

-- +----+-------+
-- | Id | Score |
-- +----+-------+
-- | 1  | 3.50  |
-- | 2  | 3.65  |
-- | 3  | 4.00  |
-- | 4  | 3.85  |
-- | 5  | 4.00  |
-- | 6  | 3.65  |
-- +----+-------+
-- For example, given the above Scores table, your query should generate the following report (order by highest score):

-- +-------+---------+
-- | score | Rank    |
-- +-------+---------+
-- | 4.00  | 1       |
-- | 4.00  | 1       |
-- | 3.85  | 2       |
-- | 3.65  | 3       |
-- | 3.65  | 3       |
-- | 3.50  | 4       |
-- +-------+---------+
-- Important Note: For MySQL solutions, to escape reserved words used as column names, 
-- you can use an apostrophe before and after the keyword. For example `Rank`.

-- Solution
select Score,
dense_rank() over(order by score desc) as "Rank"
from scores
-- Question 79
-- Table: Points

-- +---------------+---------+
-- | Column Name   | Type    |
-- +---------------+---------+
-- | id            | int     |
-- | x_value       | int     |
-- | y_value       | int     |
-- +---------------+---------+
-- id is the primary key for this table.
-- Each point is represented as a 2D Dimensional (x_value, y_value).
-- Write an SQL query to report of all possible rectangles which can be formed by any two points of the table. 

-- Each row in the result contains three columns (p1, p2, area) where:

-- p1 and p2 are the id of two opposite corners of a rectangle and p1 < p2.
-- Area of this rectangle is represented by the column area.
-- Report the query in descending order by area in case of tie in ascending order by p1 and p2.

-- Points table:
-- +----------+-------------+-------------+
-- | id       | x_value     | y_value     |
-- +----------+-------------+-------------+
-- | 1        | 2           | 8           |
-- | 2        | 4           | 7           |
-- | 3        | 2           | 10          |
-- +----------+-------------+-------------+

-- Result table:
-- +----------+-------------+-------------+
-- | p1       | p2          | area        |
-- +----------+-------------+-------------+
-- | 2        | 3           | 6           |
-- | 1        | 2           | 2           |
-- +----------+-------------+-------------+

-- p1 should be less than p2 and area greater than 0.
-- p1 = 1 and p2 = 2, has an area equal to |2-4| * |8-7| = 2.
-- p1 = 2 and p2 = 3, has an area equal to |4-2| * |7-10| = 6.
-- p1 = 1 and p2 = 3 It's not possible because the rectangle has an area equal to 0.

-- Solution
select p1.id as p1, p2.id as p2, abs(p1.x_value-p2.x_value)*abs(p1.y_value-p2.y_value) as area
from points p1 cross join points p2
where p1.x_value!=p2.x_value and p1.y_value!=p2.y_value and p1.id<p2.id
order by area desc, p1, p2
-- Question 44
-- Table: Department

-- +---------------+---------+
-- | Column Name   | Type    |
-- +---------------+---------+
-- | id            | int     |
-- | revenue       | int     |
-- | month         | varchar |
-- +---------------+---------+
-- (id, month) is the primary key of this table.
-- The table has information about the revenue of each department per month.
-- The month has values in ["Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"].
 

-- Write an SQL query to reformat the table such that there is a department id column and a revenue column for each month.

-- The query result format is in the following example:

-- Department table:
-- +------+---------+-------+
-- | id   | revenue | month |
-- +------+---------+-------+
-- | 1    | 8000    | Jan   |
-- | 2    | 9000    | Jan   |
-- | 3    | 10000   | Feb   |
-- | 1    | 7000    | Feb   |
-- | 1    | 6000    | Mar   |
-- +------+---------+-------+

-- Result table:
-- +------+-------------+-------------+-------------+-----+-------------+
-- | id   | Jan_Revenue | Feb_Revenue | Mar_Revenue | ... | Dec_Revenue |
-- +------+-------------+-------------+-------------+-----+-------------+
-- | 1    | 8000        | 7000        | 6000        | ... | null        |
-- | 2    | 9000        | null        | null        | ... | null        |
-- | 3    | null        | 10000       | null        | ... | null        |
-- +------+-------------+-------------+-------------+-----+-------------+

-- Note that the result table has 13 columns (1 for the department id + 12 for the months).

-- Solution
select id,
sum(if(month='Jan',revenue,null)) as Jan_Revenue,
sum(if(month='Feb',revenue,null)) as Feb_Revenue,
sum(if(month='Mar',revenue,null)) as Mar_Revenue,
sum(if(month='Apr',revenue,null)) as Apr_Revenue,
sum(if(month='May',revenue,null)) as May_Revenue,
sum(if(month='Jun',revenue,null)) as Jun_Revenue,
sum(if(month='Jul',revenue,null)) as Jul_Revenue,
sum(if(month='Aug',revenue,null)) as Aug_Revenue,
sum(if(month='Sep',revenue,null)) as Sep_Revenue,
sum(if(month='Oct',revenue,null)) as Oct_Revenue,
sum(if(month='Nov',revenue,null)) as Nov_Revenue,
sum(if(month='Dec',revenue,null)) as Dec_Revenue
from Department
group by id
-- Question 48
-- Table: Employees

-- +---------------+---------+
-- | Column Name   | Type    |
-- +---------------+---------+
-- | id            | int     |
-- | name          | varchar |
-- +---------------+---------+
-- id is the primary key for this table.
-- Each row of this table contains the id and the name of an employee in a company.
 

-- Table: EmployeeUNI

-- +---------------+---------+
-- | Column Name   | Type    |
-- +---------------+---------+
-- | id            | int     |
-- | unique_id     | int     |
-- +---------------+---------+
-- (id, unique_id) is the primary key for this table.
-- Each row of this table contains the id and the corresponding unique id of an employee in the company.
 

-- Write an SQL query to show the unique ID of each user, If a user doesn't have a unique ID replace just show null.

-- Return the result table in any order.

-- The query result format is in the following example:

-- Employees table:
-- +----+----------+
-- | id | name     |
-- +----+----------+
-- | 1  | Alice    |
-- | 7  | Bob      |
-- | 11 | Meir     |
-- | 90 | Winston  |
-- | 3  | Jonathan |
-- +----+----------+

-- EmployeeUNI table:
-- +----+-----------+
-- | id | unique_id |
-- +----+-----------+
-- | 3  | 1         |
-- | 11 | 2         |
-- | 90 | 3         |
-- +----+-----------+

-- EmployeeUNI table:
-- +-----------+----------+
-- | unique_id | name     |
-- +-----------+----------+
-- | null      | Alice    |
-- | null      | Bob      |
-- | 2         | Meir     |
-- | 3         | Winston  |
-- | 1         | Jonathan |
-- +-----------+----------+

-- Alice and Bob don't have a unique ID, We will show null instead.
-- The unique ID of Meir is 2.
-- The unique ID of Winston is 3.
-- The unique ID of Jonathan is 1.

-- Solution
select unique_id, name
from employees e
left join
employeeuni u
on e.id = u.id
order by e.id
-- Question 104
-- Table: Failed

-- +--------------+---------+
-- | Column Name  | Type    |
-- +--------------+---------+
-- | fail_date    | date    |
-- +--------------+---------+
-- Primary key for this table is fail_date.
-- Failed table contains the days of failed tasks.
-- Table: Succeeded

-- +--------------+---------+
-- | Column Name  | Type    |
-- +--------------+---------+
-- | success_date | date    |
-- +--------------+---------+
-- Primary key for this table is success_date.
-- Succeeded table contains the days of succeeded tasks.
 

-- A system is running one task every day. Every task is independent of the previous tasks. The tasks can fail or succeed.

-- Write an SQL query to generate a report of period_state for each continuous interval of days in the period from 2019-01-01 to 2019-12-31.

-- period_state is 'failed' if tasks in this interval failed or 'succeeded' if tasks in this interval succeeded. Interval of days are retrieved as start_date and end_date.

-- Order result by start_date.

-- The query result format is in the following example:

-- Failed table:
-- +-------------------+
-- | fail_date         |
-- +-------------------+
-- | 2018-12-28        |
-- | 2018-12-29        |
-- | 2019-01-04        |
-- | 2019-01-05        |
-- +-------------------+

-- Succeeded table:
-- +-------------------+
-- | success_date      |
-- +-------------------+
-- | 2018-12-30        |
-- | 2018-12-31        |
-- | 2019-01-01        |
-- | 2019-01-02        |
-- | 2019-01-03        |
-- | 2019-01-06        |
-- +-------------------+


-- Result table:
-- +--------------+--------------+--------------+
-- | period_state | start_date   | end_date     |
-- +--------------+--------------+--------------+
-- | succeeded    | 2019-01-01   | 2019-01-03   |
-- | failed       | 2019-01-04   | 2019-01-05   |
-- | succeeded    | 2019-01-06   | 2019-01-06   |
-- +--------------+--------------+--------------+

-- The report ignored the system state in 2018 as we care about the system in the period 2019-01-01 to 2019-12-31.
-- From 2019-01-01 to 2019-01-03 all tasks succeeded and the system state was "succeeded".
-- From 2019-01-04 to 2019-01-05 all tasks failed and system state was "failed".
-- From 2019-01-06 to 2019-01-06 all tasks succeeded and system state was "succeeded".

-- Solution
with t1 as(
select min(success_date) as start_date, max(success_date) as end_date, state
from(
select *, date_sub(success_date, interval row_number() over(order by success_date) day) as diff, 1 as state
from succeeded
where success_date between "2019-01-01" and "2019-12-31") a
group by diff),

t2 as(
select min(fail_date) as start_date, max(fail_date) as end_date, state
from(
select *, date_sub(fail_date, interval row_number() over(order by fail_date) day) as diff, 0 as state
from failed
where fail_date between "2019-01-01" and "2019-12-31") b
group by diff)


select 
case when c.state = 1 then "succeeded"
else "failed"
end as period_state,start_date, end_date
from(
select *
from t1
union all
select *
from t2) c
order by start_date
-- Question 73
-- Table: Actions

-- +---------------+---------+
-- | Column Name   | Type    |
-- +---------------+---------+
-- | user_id       | int     |
-- | post_id       | int     |
-- | action_date   | date    |
-- | action        | enum    |
-- | extra         | varchar |
-- +---------------+---------+
-- There is no primary key for this table, it may have duplicate rows.
-- The action column is an ENUM type of ('view', 'like', 'reaction', 'comment', 'report', 'share').
-- The extra column has optional information about the action such as a reason for report or a type of reaction. 
-- Table: Removals

-- +---------------+---------+
-- | Column Name   | Type    |
-- +---------------+---------+
-- | post_id       | int     |
-- | remove_date   | date    | 
-- +---------------+---------+
-- post_id is the primary key of this table.
-- Each row in this table indicates that some post was removed as a result of being reported or as a result of an admin review.
 

-- Write an SQL query to find the average for daily percentage of posts that got removed after being reported as spam, rounded to 2 decimal places.

-- The query result format is in the following example:

-- Actions table:
-- +---------+---------+-------------+--------+--------+
-- | user_id | post_id | action_date | action | extra  |
-- +---------+---------+-------------+--------+--------+
-- | 1       | 1       | 2019-07-01  | view   | null   |
-- | 1       | 1       | 2019-07-01  | like   | null   |
-- | 1       | 1       | 2019-07-01  | share  | null   |
-- | 2       | 2       | 2019-07-04  | view   | null   |
-- | 2       | 2       | 2019-07-04  | report | spam   |
-- | 3       | 4       | 2019-07-04  | view   | null   |
-- | 3       | 4       | 2019-07-04  | report | spam   |
-- | 4       | 3       | 2019-07-02  | view   | null   |
-- | 4       | 3       | 2019-07-02  | report | spam   |
-- | 5       | 2       | 2019-07-03  | view   | null   |
-- | 5       | 2       | 2019-07-03  | report | racism |
-- | 5       | 5       | 2019-07-03  | view   | null   |
-- | 5       | 5       | 2019-07-03  | report | racism |
-- +---------+---------+-------------+--------+--------+

-- Removals table:
-- +---------+-------------+
-- | post_id | remove_date |
-- +---------+-------------+
-- | 2       | 2019-07-20  |
-- | 3       | 2019-07-18  |
-- +---------+-------------+

-- Result table:
-- +-----------------------+
-- | average_daily_percent |
-- +-----------------------+
-- | 75.00                 |
-- +-----------------------+
-- The percentage for 2019-07-04 is 50% because only one post of two spam reported posts was removed.
-- The percentage for 2019-07-02 is 100% because one post was reported as spam and it was removed.
-- The other days had no spam reports so the average is (50 + 100) / 2 = 75%
-- Note that the output is only one number and that we do not care about the remove dates.

-- Solution
with t1 as(
select a.action_date, (count(distinct r.post_id)+0.0)/(count(distinct a.post_id)+0.0) as result
from (select action_date, post_id
from actions
where extra = 'spam' and action = 'report') a
left join
removals r
on a.post_id = r.post_id
group by a.action_date)

select round(avg(t1.result)*100,2) as  average_daily_percent
from t1

-- Question 43
-- Table: Actions

-- +---------------+---------+
-- | Column Name   | Type    |
-- +---------------+---------+
-- | user_id       | int     |
-- | post_id       | int     |
-- | action_date   | date    | 
-- | action        | enum    |
-- | extra         | varchar |
-- +---------------+---------+
-- There is no primary key for this table, it may have duplicate rows.
-- The action column is an ENUM type of ('view', 'like', 'reaction', 'comment', 'report', 'share').
-- The extra column has optional information about the action such as a reason for report or a type of reaction. 
 

-- Write an SQL query that reports the number of posts reported yesterday for each report reason. Assume today is 2019-07-05.

-- The query result format is in the following example:

-- Actions table:
-- +---------+---------+-------------+--------+--------+
-- | user_id | post_id | action_date | action | extra  |
-- +---------+---------+-------------+--------+--------+
-- | 1       | 1       | 2019-07-01  | view   | null   |
-- | 1       | 1       | 2019-07-01  | like   | null   |
-- | 1       | 1       | 2019-07-01  | share  | null   |
-- | 2       | 4       | 2019-07-04  | view   | null   |
-- | 2       | 4       | 2019-07-04  | report | spam   |
-- | 3       | 4       | 2019-07-04  | view   | null   |
-- | 3       | 4       | 2019-07-04  | report | spam   |
-- | 4       | 3       | 2019-07-02  | view   | null   |
-- | 4       | 3       | 2019-07-02  | report | spam   |
-- | 5       | 2       | 2019-07-04  | view   | null   |
-- | 5       | 2       | 2019-07-04  | report | racism |
-- | 5       | 5       | 2019-07-04  | view   | null   |
-- | 5       | 5       | 2019-07-04  | report | racism |
-- +---------+---------+-------------+--------+--------+

-- Result table:
-- +---------------+--------------+
-- | report_reason | report_count |
-- +---------------+--------------+
-- | spam          | 1            |
-- | racism        | 2            |
-- +---------------+--------------+ 
-- Note that we only care about report reasons with non zero number of reports.

-- Solution
Select extra as report_reason, count(distinct post_id) as report_count
from actions
where action_date = DATE_SUB("2019-07-5", INTERVAL 1 DAY) and action='report'
group by extra
-- Question 71
-- Table: Customer

-- +---------------+---------+
-- | Column Name   | Type    |
-- +---------------+---------+
-- | customer_id   | int     |
-- | name          | varchar |
-- | visited_on    | date    |
-- | amount        | int     |
-- +---------------+---------+
-- (customer_id, visited_on) is the primary key for this table.
-- This table contains data about customer transactions in a restaurant.
-- visited_on is the date on which the customer with ID (customer_id) have visited the restaurant.
-- amount is the total paid by a customer.
 

-- You are the restaurant owner and you want to analyze a possible expansion (there will be at least one customer every day).

-- Write an SQL query to compute moving average of how much customer paid in a 7 days window (current day + 6 days before) .

-- The query result format is in the following example:

-- Return result table ordered by visited_on.

-- average_amount should be rounded to 2 decimal places, all dates are in the format ('YYYY-MM-DD').

 

-- Customer table:
-- +-------------+--------------+--------------+-------------+
-- | customer_id | name         | visited_on   | amount      |
-- +-------------+--------------+--------------+-------------+
-- | 1           | Jhon         | 2019-01-01   | 100         |
-- | 2           | Daniel       | 2019-01-02   | 110         |
-- | 3           | Jade         | 2019-01-03   | 120         |
-- | 4           | Khaled       | 2019-01-04   | 130         |
-- | 5           | Winston      | 2019-01-05   | 110         | 
-- | 6           | Elvis        | 2019-01-06   | 140         | 
-- | 7           | Anna         | 2019-01-07   | 150         |
-- | 8           | Maria        | 2019-01-08   | 80          |
-- | 9           | Jaze         | 2019-01-09   | 110         | 
-- | 1           | Jhon         | 2019-01-10   | 130         | 
-- | 3           | Jade         | 2019-01-10   | 150         | 
-- +-------------+--------------+--------------+-------------+

-- Result table:
-- +--------------+--------------+----------------+
-- | visited_on   | amount       | average_amount |
-- +--------------+--------------+----------------+
-- | 2019-01-07   | 860          | 122.86         |
-- | 2019-01-08   | 840          | 120            |
-- | 2019-01-09   | 840          | 120            |
-- | 2019-01-10   | 1000         | 142.86         |
-- +--------------+--------------+----------------+

-- 1st moving average from 2019-01-01 to 2019-01-07 has an average_amount of (100 + 110 + 120 + 130 + 110 + 140 + 150)/7 = 122.86
-- 2nd moving average from 2019-01-02 to 2019-01-08 has an average_amount of (110 + 120 + 130 + 110 + 140 + 150 + 80)/7 = 120
-- 3rd moving average from 2019-01-03 to 2019-01-09 has an average_amount of (120 + 130 + 110 + 140 + 150 + 80 + 110)/7 = 120
-- 4th moving average from 2019-01-04 to 2019-01-10 has an average_amount of (130 + 110 + 140 + 150 + 80 + 110 + 130 + 150)/7 = 142.86

-- Solution
select visited_on, sum(amount) over(order by visited_on rows 6 preceding),
round(avg(amount) over(order by visited_on rows 6 preceding),2)
from 
(
	select visited_on, sum(amount) as amount
	from customer
	group by visited_on
	order by visited_on
) a
order by visited_on offset 6 rows
-- Question 12
-- Given a Weather table, write a SQL query to find all dates' Ids with higher temperature compared to its previous (yesterday's) dates.

-- +---------+------------------+------------------+
-- | Id(INT) | RecordDate(DATE) | Temperature(INT) |
-- +---------+------------------+------------------+
-- |       1 |       2015-01-01 |               10 |
-- |       2 |       2015-01-02 |               25 |
-- |       3 |       2015-01-03 |               20 |
-- |       4 |       2015-01-04 |               30 |
-- +---------+------------------+------------------+
-- For example, return the following Ids for the above Weather table:

-- +----+
-- | Id |
-- +----+
-- |  2 |
-- |  4 |
-- +----+

-- Solution
select a.Id
from weather a, weather b
where a.Temperature>b.Temperature and  datediff(a.recorddate,b.recorddate)=1
-- Question 76
-- Table: Scores

-- +---------------+---------+
-- | Column Name   | Type    |
-- +---------------+---------+
-- | player_name   | varchar |
-- | gender        | varchar |
-- | day           | date    |
-- | score_points  | int     |
-- +---------------+---------+
-- (gender, day) is the primary key for this table.
-- A competition is held between females team and males team.
-- Each row of this table indicates that a player_name and with gender has scored score_point in someday.
-- Gender is 'F' if the player is in females team and 'M' if the player is in males team.
 

-- Write an SQL query to find the total score for each gender at each day.

-- Order the result table by gender and day

-- The query result format is in the following example:

-- Scores table:
-- +-------------+--------+------------+--------------+
-- | player_name | gender | day        | score_points |
-- +-------------+--------+------------+--------------+
-- | Aron        | F      | 2020-01-01 | 17           |
-- | Alice       | F      | 2020-01-07 | 23           |
-- | Bajrang     | M      | 2020-01-07 | 7            |
-- | Khali       | M      | 2019-12-25 | 11           |
-- | Slaman      | M      | 2019-12-30 | 13           |
-- | Joe         | M      | 2019-12-31 | 3            |
-- | Jose        | M      | 2019-12-18 | 2            |
-- | Priya       | F      | 2019-12-31 | 23           |
-- | Priyanka    | F      | 2019-12-30 | 17           |
-- +-------------+--------+------------+--------------+
-- Result table:
-- +--------+------------+-------+
-- | gender | day        | total |
-- +--------+------------+-------+
-- | F      | 2019-12-30 | 17    |
-- | F      | 2019-12-31 | 40    |
-- | F      | 2020-01-01 | 57    |
-- | F      | 2020-01-07 | 80    |
-- | M      | 2019-12-18 | 2     |
-- | M      | 2019-12-25 | 13    |
-- | M      | 2019-12-30 | 26    |
-- | M      | 2019-12-31 | 29    |
-- | M      | 2020-01-07 | 36    |
-- +--------+------------+-------+
-- For females team:
-- First day is 2019-12-30, Priyanka scored 17 points and the total score for the team is 17.
-- Second day is 2019-12-31, Priya scored 23 points and the total score for the team is 40.
-- Third day is 2020-01-01, Aron scored 17 points and the total score for the team is 57.
-- Fourth day is 2020-01-07, Alice scored 23 points and the total score for the team is 80.
-- For males team:
-- First day is 2019-12-18, Jose scored 2 points and the total score for the team is 2.
-- Second day is 2019-12-25, Khali scored 11 points and the total score for the team is 13.
-- Third day is 2019-12-30, Slaman scored 13 points and the total score for the team is 26.
-- Fourth day is 2019-12-31, Joe scored 3 points and the total score for the team is 29.
-- Fifth day is 2020-01-07, Bajrang scored 7 points and the total score for the team is 36.

-- Solution
select gender, day, 
sum(score_points) over(partition by gender order by day) as total
from scores
group by 1,2
order by 1,2
-- Question 27
-- Table: Product

-- +--------------+---------+
-- | Column Name  | Type    |
-- +--------------+---------+
-- | product_id   | int     |
-- | product_name | varchar |
-- | unit_price   | int     |
-- +--------------+---------+
-- product_id is the primary key of this table.
-- Table: Sales

-- +-------------+---------+
-- | Column Name | Type    |
-- +-------------+---------+
-- | seller_id   | int     |
-- | product_id  | int     |
-- | buyer_id    | int     |
-- | sale_date   | date    |
-- | quantity    | int     |
-- | price       | int     |
-- +------ ------+---------+
-- This table has no primary key, it can have repeated rows.
-- product_id is a foreign key to Product table.
 

-- Write an SQL query that reports the best seller by total sales price, If there is a tie, report them all.

-- The query result format is in the following example:

-- Product table:
-- +------------+--------------+------------+
-- | product_id | product_name | unit_price |
-- +------------+--------------+------------+
-- | 1          | S8           | 1000       |
-- | 2          | G4           | 800        |
-- | 3          | iPhone       | 1400       |
-- +------------+--------------+------------+

-- Sales table:
-- +-----------+------------+----------+------------+----------+-------+
-- | seller_id | product_id | buyer_id | sale_date  | quantity | price |
-- +-----------+------------+----------+------------+----------+-------+
-- | 1         | 1          | 1        | 2019-01-21 | 2        | 2000  |
-- | 1         | 2          | 2        | 2019-02-17 | 1        | 800   |
-- | 2         | 2          | 3        | 2019-06-02 | 1        | 800   |
-- | 3         | 3          | 4        | 2019-05-13 | 2        | 2800  |
-- +-----------+------------+----------+------------+----------+-------+

-- Result table:
-- +-------------+
-- | seller_id   |
-- +-------------+
-- | 1           |
-- | 3           |
-- +-------------+
-- Both sellers with id 1 and 3 sold products with the most total price of 2800.

-- Solution
Select a.seller_id
from
(select seller_id, 
rank() over(order by sum(price) desc) as rk
from sales
group by seller_id) a
where a.rk=1
-- Question 33
-- Table: Product

-- +--------------+---------+
-- | Column Name  | Type    |
-- +--------------+---------+
-- | product_id   | int     |
-- | product_name | varchar |
-- | unit_price   | int     |
-- +--------------+---------+
-- product_id is the primary key of this table.
-- Table: Sales

-- +-------------+---------+
-- | Column Name | Type    |
-- +-------------+---------+
-- | seller_id   | int     |
-- | product_id  | int     |
-- | buyer_id    | int     |
-- | sale_date   | date    |
-- | quantity    | int     |
-- | price       | int     |
-- +------ ------+---------+
-- This table has no primary key, it can have repeated rows.
-- product_id is a foreign key to Product table.
 

-- Write an SQL query that reports the buyers who have bought S8 but not iPhone. Note that S8 and iPhone are products present in the Product table.

-- The query result format is in the following example:

-- Product table:
-- +------------+--------------+------------+
-- | product_id | product_name | unit_price |
-- +------------+--------------+------------+
-- | 1          | S8           | 1000       |
-- | 2          | G4           | 800        |
-- | 3          | iPhone       | 1400       |
-- +------------+--------------+------------+

-- Sales table:
-- +-----------+------------+----------+------------+----------+-------+
-- | seller_id | product_id | buyer_id | sale_date  | quantity | price |
-- +-----------+------------+----------+------------+----------+-------+
-- | 1         | 1          | 1        | 2019-01-21 | 2        | 2000  |
-- | 1         | 2          | 2        | 2019-02-17 | 1        | 800   |
-- | 2         | 1          | 3        | 2019-06-02 | 1        | 800   |
-- | 3         | 3          | 3        | 2019-05-13 | 2        | 2800  |
-- +-----------+------------+----------+------------+----------+-------+

-- Result table:
-- +-------------+
-- | buyer_id    |
-- +-------------+
-- | 1           |
-- +-------------+
-- The buyer with id 1 bought an S8 but didn't buy an iPhone. The buyer with id 3 bought both.

-- Solution
Select distinct a.buyer_id
from sales a join
product b
on a.product_id = b.product_id
where a.buyer_id in
(Select a.buyer_id from sales a join product b on a.product_id = b.product_id where b.product_name = 'S8') 
and
a.buyer_id not in (Select a.buyer_id from sales a join product b on a.product_id = b.product_id where b.product_name = 'iPhone')
-- Question 34
-- Table: Product

-- +--------------+---------+
-- | Column Name  | Type    |
-- +--------------+---------+
-- | product_id   | int     |
-- | product_name | varchar |
-- | unit_price   | int     |
-- +--------------+---------+
-- product_id is the primary key of this table.
-- Table: Sales

-- +-------------+---------+
-- | Column Name | Type    |
-- +-------------+---------+
-- | seller_id   | int     |
-- | product_id  | int     |
-- | buyer_id    | int     |
-- | sale_date   | date    |
-- | quantity    | int     |
-- | price       | int     |
-- +------ ------+---------+
-- This table has no primary key, it can have repeated rows.
-- product_id is a foreign key to Product table.
 

-- Write an SQL query that reports the products that were only sold in spring 2019. That is, between 2019-01-01 and 2019-03-31 inclusive.

-- The query result format is in the following example:

-- Product table:
-- +------------+--------------+------------+
-- | product_id | product_name | unit_price |
-- +------------+--------------+------------+
-- | 1          | S8           | 1000       |
-- | 2          | G4           | 800        |
-- | 3          | iPhone       | 1400       |
-- +------------+--------------+------------+

-- Sales table:
-- +-----------+------------+----------+------------+----------+-------+
-- | seller_id | product_id | buyer_id | sale_date  | quantity | price |
-- +-----------+------------+----------+------------+----------+-------+
-- | 1         | 1          | 1        | 2019-01-21 | 2        | 2000  |
-- | 1         | 2          | 2        | 2019-02-17 | 1        | 800   |
-- | 2         | 2          | 3        | 2019-06-02 | 1        | 800   |
-- | 3         | 3          | 4        | 2019-05-13 | 2        | 2800  |
-- +-----------+------------+----------+------------+----------+-------+

-- Result table:
-- +-------------+--------------+
-- | product_id  | product_name |
-- +-------------+--------------+
-- | 1           | S8           |
-- +-------------+--------------+
-- The product with id 1 was only sold in spring 2019 while the other two were sold after.

-- Solution
select distinct a.product_id, product_name from sales a join product b on a.product_id = b.product_id where a.product_id 
in
(select product_id from sales where sale_date >= '2019-01-01' and sale_date <= '2019-03-31')
and
a.product_id not in 
(select product_id from sales where sale_date > '2019-03-31' or sale_date < '2019-01-01')
-- Question 112
-- Table: Orders

-- +---------------+---------+
-- | Column Name   | Type    |
-- +---------------+---------+
-- | order_id      | int     |
-- | customer_id   | int     |
-- | order_date    | date    | 
-- | item_id       | varchar |
-- | quantity      | int     |
-- +---------------+---------+
-- (ordered_id, item_id) is the primary key for this table.
-- This table contains information of the orders placed.
-- order_date is the date when item_id was ordered by the customer with id customer_id.
 

-- Table: Items

-- +---------------------+---------+
-- | Column Name         | Type    |
-- +---------------------+---------+
-- | item_id             | varchar |
-- | item_name           | varchar |
-- | item_category       | varchar |
-- +---------------------+---------+
-- item_id is the primary key for this table.
-- item_name is the name of the item.
-- item_category is the category of the item.
 

-- You are the business owner and would like to obtain a sales report for category items and day of the week.

-- Write an SQL query to report how many units in each category have been ordered on each day of the week.

-- Return the result table ordered by category.

-- The query result format is in the following example:

 

-- Orders table:
-- +------------+--------------+-------------+--------------+-------------+
-- | order_id   | customer_id  | order_date  | item_id      | quantity    |
-- +------------+--------------+-------------+--------------+-------------+
-- | 1          | 1            | 2020-06-01  | 1            | 10          |
-- | 2          | 1            | 2020-06-08  | 2            | 10          |
-- | 3          | 2            | 2020-06-02  | 1            | 5           |
-- | 4          | 3            | 2020-06-03  | 3            | 5           |
-- | 5          | 4            | 2020-06-04  | 4            | 1           |
-- | 6          | 4            | 2020-06-05  | 5            | 5           |
-- | 7          | 5            | 2020-06-05  | 1            | 10          |
-- | 8          | 5            | 2020-06-14  | 4            | 5           |
-- | 9          | 5            | 2020-06-21  | 3            | 5           |
-- +------------+--------------+-------------+--------------+-------------+

-- Items table:
-- +------------+----------------+---------------+
-- | item_id    | item_name      | item_category |
-- +------------+----------------+---------------+
-- | 1          | LC Alg. Book   | Book          |
-- | 2          | LC DB. Book    | Book          |
-- | 3          | LC SmarthPhone | Phone         |
-- | 4          | LC Phone 2020  | Phone         |
-- | 5          | LC SmartGlass  | Glasses       |
-- | 6          | LC T-Shirt XL  | T-Shirt       |
-- +------------+----------------+---------------+

-- Result table:
-- +------------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+
-- | Category   | Monday    | Tuesday   | Wednesday | Thursday  | Friday    | Saturday  | Sunday    |
-- +------------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+
-- | Book       | 20        | 5         | 0         | 0         | 10        | 0         | 0         |
-- | Glasses    | 0         | 0         | 0         | 0         | 5         | 0         | 0         |
-- | Phone      | 0         | 0         | 5         | 1         | 0         | 0         | 10        |
-- | T-Shirt    | 0         | 0         | 0         | 0         | 0         | 0         | 0         |
-- +------------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+
-- On Monday (2020-06-01, 2020-06-08) were sold a total of 20 units (10 + 10) in the category Book (ids: 1, 2).
-- On Tuesday (2020-06-02) were sold a total of 5 units  in the category Book (ids: 1, 2).
-- On Wednesday (2020-06-03) were sold a total of 5 units in the category Phone (ids: 3, 4).
-- On Thursday (2020-06-04) were sold a total of 1 unit in the category Phone (ids: 3, 4).
-- On Friday (2020-06-05) were sold 10 units in the category Book (ids: 1, 2) and 5 units in Glasses (ids: 5).
-- On Saturday there are no items sold.
-- On Sunday (2020-06-14, 2020-06-21) were sold a total of 10 units (5 +5) in the category Phone (ids: 3, 4).
-- There are no sales of T-Shirt.

-- Solution
with t1 as(
select distinct item_category,
case when dayname(order_date)='Monday' then sum(quantity) over(partition by item_category,dayname(order_date)) else 0 end as Monday,
Case when dayname(order_date)='Tuesday' then sum(quantity) over(partition by item_category,dayname(order_date)) else 0 end as Tuesday,
Case when dayname(order_date)='Wednesday' then sum(quantity) over(partition by item_category,dayname(order_date)) else 0 end as Wednesday,
Case when dayname(order_date)='Thursday' then sum(quantity) over(partition by item_category,dayname(order_date)) else 0 end as Thursday,
Case when dayname(order_date)='Friday' then sum(quantity) over(partition by item_category,dayname(order_date)) else 0 end as Friday,
Case when dayname(order_date)='Saturday' then sum(quantity) over(partition by item_category,dayname(order_date)) else 0 end as Saturday,
Case when dayname(order_date)='Sunday' then sum(quantity) over(partition by item_category,dayname(order_date)) else 0 end as Sunday
from orders o
right join items i
using (item_id))

select item_category as category, sum(Monday) as Monday, sum(Tuesday) as Tuesday, sum(Wednesday) Wednesday, sum(Thursday) Thursday,
sum(Friday) Friday, sum(Saturday) Saturday, sum(Sunday) Sunday
from t1
group by item_category
-- Question 12
-- Description

-- Given three tables: salesperson, company, orders.
-- Output all the names in the table salesperson, who didn’t have sales to company 'RED'.

-- Example
-- Input

-- Table: salesperson

-- +----------+------+--------+-----------------+-----------+
-- | sales_id | name | salary | commission_rate | hire_date |
-- +----------+------+--------+-----------------+-----------+
-- |   1      | John | 100000 |     6           | 4/1/2006  |
-- |   2      | Amy  | 120000 |     5           | 5/1/2010  |
-- |   3      | Mark | 65000  |     12          | 12/25/2008|
-- |   4      | Pam  | 25000  |     25          | 1/1/2005  |
-- |   5      | Alex | 50000  |     10          | 2/3/2007  |
-- +----------+------+--------+-----------------+-----------+
-- The table salesperson holds the salesperson information. Every salesperson has a sales_id and a name.
-- Table: company

-- +---------+--------+------------+
-- | com_id  |  name  |    city    |
-- +---------+--------+------------+
-- |   1     |  RED   |   Boston   |
-- |   2     | ORANGE |   New York |
-- |   3     | YELLOW |   Boston   |
-- |   4     | GREEN  |   Austin   |
-- +---------+--------+------------+
-- The table company holds the company information. Every company has a com_id and a name.
-- Table: orders

-- +----------+------------+---------+----------+--------+
-- | order_id | order_date | com_id  | sales_id | amount |
-- +----------+------------+---------+----------+--------+
-- | 1        |   1/1/2014 |    3    |    4     | 100000 |
-- | 2        |   2/1/2014 |    4    |    5     | 5000   |
-- | 3        |   3/1/2014 |    1    |    1     | 50000  |
-- | 4        |   4/1/2014 |    1    |    4     | 25000  |
-- +----------+----------+---------+----------+--------+
-- The table orders holds the sales record information, salesperson and customer company are represented by sales_id and com_id.
-- output

-- +------+
-- | name | 
-- +------+
-- | Amy  | 
-- | Mark | 
-- | Alex |
-- +------+
-- Explanation

-- According to order '3' and '4' in table orders, it is easy to tell only salesperson 'John' and 'Pam' have sales to company 'RED',
-- so we need to output all the other names in the table salesperson.

-- Solution
# Takes higher time
# Select distinct a.name  
# from(
# select s.sales_id as sales, name
# from salesperson s left join orders o
# on s.sales_id = o.sales_id) a
# where a.sales != all(select distinct sales_id from orders o join company c on o.com_id = c.com_id where o.com_id = any (select com_id from company where name = 'RED'))

# Faster solution
SELECT name
FROM salesperson
WHERE sales_id NOT IN (SELECT DISTINCT sales_id
FROM orders
WHERE com_id = (SELECT com_id
FROM company
WHERE name = 'RED')) ;
-- Question 70
-- In facebook, there is a follow table with two columns: followee, follower.

-- Please write a sql query to get the amount of each follower’s follower if he/she has one.

-- For example:

-- +-------------+------------+
-- | followee    | follower   |
-- +-------------+------------+
-- |     A       |     B      |
-- |     B       |     C      |
-- |     B       |     D      |
-- |     D       |     E      |
-- +-------------+------------+
-- should output:
-- +-------------+------------+
-- | follower    | num        |
-- +-------------+------------+
-- |     B       |  2         |
-- |     D       |  1         |
-- +-------------+------------+
-- Explaination:
-- Both B and D exist in the follower list, when as a followee, B's follower is C and D, and D's follower is E. A does not exist in follower list.
 

-- Note:
-- Followee would not follow himself/herself in all cases.
-- Please display the result in follower's alphabet order.

-- Solution
select followee as follower, count(distinct(follower)) as num
from follow
where followee = any(select follower from follow)
group by followee
order by followee
-- Question 15
-- Write a SQL query to get the second highest salary from the Employee table.

-- +----+--------+
-- | Id | Salary |
-- +----+--------+
-- | 1  | 100    |
-- | 2  | 200    |
-- | 3  | 300    |
-- +----+--------+
-- For example, given the above Employee table, the query should return 200 as the second highest salary. 
-- If there is no second highest salary, then the query should return null.

-- +---------------------+
-- | SecondHighestSalary |
-- +---------------------+
-- | 200                 |
-- +---------------------+


-- Solution
select max(salary) as SecondHighestSalary
from employee
where salary ! = (Select max(salary)
                   from employee)
-- Question 89
-- Table point_2d holds the coordinates (x,y) of some unique points (more than two) in a plane.
 

-- Write a query to find the shortest distance between these points rounded to 2 decimals.
 

-- | x  | y  |
-- |----|----|
-- | -1 | -1 |
-- | 0  | 0  |
-- | -1 | -2 |
 

-- The shortest distance is 1.00 from point (-1,-1) to (-1,2). So the output should be:
 

-- | shortest |
-- |----------|
-- | 1.00     |
 

-- Note: The longest distance among all the points are less than 10000.

-- Solution
select round(a.shortest,2) as shortest
from(
select sqrt(pow((p1.x-p2.x),2)+pow((p1.y-p2.y),2)) as shortest
from point_2d p1
cross join point_2d p2
where p1.x!=p2.x or p1.y!=p2.y
order by sqrt(pow((p1.x-p2.x),2)+pow((p1.y-p2.y),2))
limit 1) a
-- Question 25
-- Table point holds the x coordinate of some points on x-axis in a plane, which are all integers.
 

-- Write a query to find the shortest distance between two points in these points.
 

-- | x   |
-- |-----|
-- | -1  |
-- | 0   |
-- | 2   |
 

-- The shortest distance is '1' obviously, which is from point '-1' to '0'. So the output is as below:
 

-- | shortest|
-- |---------|
-- | 1       |
 

-- Note: Every point is unique, which means there is no duplicates in table point

-- Solution
select min(abs(abs(a.x)-abs(a.next_closest))) as shortest
from(
select *,
lead(x) over(order by x) as next_closest
from point) a 
-- Question 23
-- Table: Students

-- +---------------+---------+
-- | Column Name   | Type    |
-- +---------------+---------+
-- | student_id    | int     |
-- | student_name  | varchar |
-- +---------------+---------+
-- student_id is the primary key for this table.
-- Each row of this table contains the ID and the name of one student in the school.
 

-- Table: Subjects

-- +--------------+---------+
-- | Column Name  | Type    |
-- +--------------+---------+
-- | subject_name | varchar |
-- +--------------+---------+
-- subject_name is the primary key for this table.
-- Each row of this table contains the name of one subject in the school.
 

-- Table: Examinations

-- +--------------+---------+
-- | Column Name  | Type    |
-- +--------------+---------+
-- | student_id   | int     |
-- | subject_name | varchar |
-- +--------------+---------+
-- There is no primary key for this table. It may contain duplicates.
-- Each student from the Students table takes every course from Subjects table.
-- Each row of this table indicates that a student with ID student_id attended the exam of subject_name.
 

-- Write an SQL query to find the number of times each student attended each exam.

-- Order the result table by student_id and subject_name.

-- The query result format is in the following example:

-- Students table:
-- +------------+--------------+
-- | student_id | student_name |
-- +------------+--------------+
-- | 1          | Alice        |
-- | 2          | Bob          |
-- | 13         | John         |
-- | 6          | Alex         |
-- +------------+--------------+
-- Subjects table:
-- +--------------+
-- | subject_name |
-- +--------------+
-- | Math         |
-- | Physics      |
-- | Programming  |
-- +--------------+
-- Examinations table:
-- +------------+--------------+
-- | student_id | subject_name |
-- +------------+--------------+
-- | 1          | Math         |
-- | 1          | Physics      |
-- | 1          | Programming  |
-- | 2          | Programming  |
-- | 1          | Physics      |
-- | 1          | Math         |
-- | 13         | Math         |
-- | 13         | Programming  |
-- | 13         | Physics      |
-- | 2          | Math         |
-- | 1          | Math         |
-- +------------+--------------+
-- Result table:
-- +------------+--------------+--------------+----------------+
-- | student_id | student_name | subject_name | attended_exams |
-- +------------+--------------+--------------+----------------+
-- | 1          | Alice        | Math         | 3              |
-- | 1          | Alice        | Physics      | 2              |
-- | 1          | Alice        | Programming  | 1              |
-- | 2          | Bob          | Math         | 1              |
-- | 2          | Bob          | Physics      | 0              |
-- | 2          | Bob          | Programming  | 1              |
-- | 6          | Alex         | Math         | 0              |
-- | 6          | Alex         | Physics      | 0              |
-- | 6          | Alex         | Programming  | 0              |
-- | 13         | John         | Math         | 1              |
-- | 13         | John         | Physics      | 1              |
-- | 13         | John         | Programming  | 1              |
-- +------------+--------------+--------------+----------------+
-- The result table should contain all students and all subjects.
-- Alice attended Math exam 3 times, Physics exam 2 times and Programming exam 1 time.
-- Bob attended Math exam 1 time, Programming exam 1 time and didn't attend the Physics exam.
-- Alex didn't attend any exam.
-- John attended Math exam 1 time, Physics exam 1 time and Programming exam 1 time.

-- Solution
Select a.student_id as student_id, a.student_name as student_name, a.subject_name as subject_name, coalesce(attended_exams,0) as attended_exams
from(
select *
from students
cross join subjects
group by student_id, student_name, subject_name) a
left join 
(Select e.student_id, student_name, subject_name, count(*) as attended_exams
from examinations e join students s
on e.student_id = s.student_id
group by e.student_id, student_name, subject_name) b
on a.student_id = b.student_id and a.subject_name =b.subject_name
order by a.student_id asc, a.subject_name asc
-- Question 105
-- A U.S graduate school has students from Asia, Europe and America. The students' location information are stored in table student as below.
 

-- | name   | continent |
-- |--------|-----------|
-- | Jack   | America   |
-- | Pascal | Europe    |
-- | Xi     | Asia      |
-- | Jane   | America   |
 

-- Pivot the continent column in this table so that each name is sorted alphabetically and displayed underneath its corresponding continent. The output headers should be America, Asia and Europe respectively. It is guaranteed that the student number from America is no less than either Asia or Europe.
 

-- For the sample input, the output is:
 

-- | America | Asia | Europe |
-- |---------|------|--------|
-- | Jack    | Xi   | Pascal |
-- | Jane    |      |        |

-- Solution
select min(case when continent = 'America' then name end) as America,
min(case when continent = 'Asia' then name end) as Asia,
min(case when continent = 'Europe' then name end) as Europe
from 
(select *, row_number() over(partition by continent order by name) as rn
from student) a
group by rn
-- Question 36
-- Table: Departments

-- +---------------+---------+
-- | Column Name   | Type    |
-- +---------------+---------+
-- | id            | int     |
-- | name          | varchar |
-- +---------------+---------+
-- id is the primary key of this table.
-- The table has information about the id of each department of a university.
 

-- Table: Students

-- +---------------+---------+
-- | Column Name   | Type    |
-- +---------------+---------+
-- | id            | int     |
-- | name          | varchar |
-- | department_id | int     |
-- +---------------+---------+
-- id is the primary key of this table.
-- The table has information about the id of each student at a university and the id of the department he/she studies at.
 

-- Write an SQL query to find the id and the name of all students who are enrolled in departments that no longer exists.

-- Return the result table in any order.

-- The query result format is in the following example:

-- Departments table:
-- +------+--------------------------+
-- | id   | name                     |
-- +------+--------------------------+
-- | 1    | Electrical Engineering   |
-- | 7    | Computer Engineering     |
-- | 13   | Bussiness Administration |
-- +------+--------------------------+

-- Students table:
-- +------+----------+---------------+
-- | id   | name     | department_id |
-- +------+----------+---------------+
-- | 23   | Alice    | 1             |
-- | 1    | Bob      | 7             |
-- | 5    | Jennifer | 13            |
-- | 2    | John     | 14            |
-- | 4    | Jasmine  | 77            |
-- | 3    | Steve    | 74            |
-- | 6    | Luis     | 1             |
-- | 8    | Jonathan | 7             |
-- | 7    | Daiana   | 33            |
-- | 11   | Madelynn | 1             |
-- +------+----------+---------------+

-- Result table:
-- +------+----------+
-- | id   | name     |
-- +------+----------+
-- | 2    | John     |
-- | 7    | Daiana   |
-- | 4    | Jasmine  |
-- | 3    | Steve    |
-- +------+----------+

-- John, Daiana, Steve and Jasmine are enrolled in departments 14, 33, 74 and 77 respectively. 
-- department 14, 33, 74 and 77 doesn't exist in the Departments table.

-- Solution
Select s.id, s.name
from students s left join
departments d
on s.department_id = d.id
where d.name is null
-- Question 22
-- Given a table salary, such as the one below, that has m=male and f=female values. 
-- Swap all f and m values (i.e., change all f values to m and vice versa) with 
-- a single update statement and no intermediate temp table.

-- Note that you must write a single update statement, DO NOT write any select statement for this problem.


-- Example:

-- | id | name | sex | salary |
-- |----|------|-----|--------|
-- | 1  | A    | m   | 2500   |
-- | 2  | B    | f   | 1500   |
-- | 3  | C    | m   | 5500   |
-- | 4  | D    | f   | 500    |
-- After running your update statement, the above salary table should have the following rows:
-- | id | name | sex | salary |
-- |----|------|-----|--------|
-- | 1  | A    | f   | 2500   |
-- | 2  | B    | m   | 1500   |
-- | 3  | C    | f   | 5500   |
-- | 4  | D    | m   | 500    |

-- Solution
Update salary
set sex = Case when sex = 'm' then 'f' 
when sex = 'f' then 'm'
end;
-- Question 53
-- Table: Teams

-- +---------------+----------+
-- | Column Name   | Type     |
-- +---------------+----------+
-- | team_id       | int      |
-- | team_name     | varchar  |
-- +---------------+----------+
-- team_id is the primary key of this table.
-- Each row of this table represents a single football team.
-- Table: Matches

-- +---------------+---------+
-- | Column Name   | Type    |
-- +---------------+---------+
-- | match_id      | int     |
-- | host_team     | int     |
-- | guest_team    | int     | 
-- | host_goals    | int     |
-- | guest_goals   | int     |
-- +---------------+---------+
-- match_id is the primary key of this table.
-- Each row is a record of a finished match between two different teams. 
-- Teams host_team and guest_team are represented by their IDs in the teams table (team_id) and they scored host_goals and guest_goals goals respectively.
 

-- You would like to compute the scores of all teams after all matches. Points are awarded as follows:
-- A team receives three points if they win a match (Score strictly more goals than the opponent team).
-- A team receives one point if they draw a match (Same number of goals as the opponent team).
-- A team receives no points if they lose a match (Score less goals than the opponent team).
-- Write an SQL query that selects the team_id, team_name and num_points of each team in the tournament after all described matches. Result table should be ordered by num_points (decreasing order). In case of a tie, order the records by team_id (increasing order).

-- The query result format is in the following example:

-- Teams table:
-- +-----------+--------------+
-- | team_id   | team_name    |
-- +-----------+--------------+
-- | 10        | Leetcode FC  |
-- | 20        | NewYork FC   |
-- | 30        | Atlanta FC   |
-- | 40        | Chicago FC   |
-- | 50        | Toronto FC   |
-- +-----------+--------------+

-- Matches table:
-- +------------+--------------+---------------+-------------+--------------+
-- | match_id   | host_team    | guest_team    | host_goals  | guest_goals  |
-- +------------+--------------+---------------+-------------+--------------+
-- | 1          | 10           | 20            | 3           | 0            |
-- | 2          | 30           | 10            | 2           | 2            |
-- | 3          | 10           | 50            | 5           | 1            |
-- | 4          | 20           | 30            | 1           | 0            |
-- | 5          | 50           | 30            | 1           | 0            |
-- +------------+--------------+---------------+-------------+--------------+

-- Result table:
-- +------------+--------------+---------------+
-- | team_id    | team_name    | num_points    |
-- +------------+--------------+---------------+
-- | 10         | Leetcode FC  | 7             |
-- | 20         | NewYork FC   | 3             |
-- | 50         | Toronto FC   | 3             |
-- | 30         | Atlanta FC   | 1             |
-- | 40         | Chicago FC   | 0             |
-- +------------+--------------+---------------+

-- Solution
with t1 as(
Select c.host_id, c.host_name, c.host_points
from(
select a.match_id, a.team_id as host_id, a.team_name as host_name, b.team_id as guest_id, b.team_name as guest_name, a.host_goals, a.guest_goals,
case
when a.host_goals > a.guest_goals then 3
when a.host_goals = a.guest_goals then 1
else 0
end as host_points,
case
when a.host_goals < a.guest_goals then 3
when a.host_goals = a.guest_goals then 1
else 0
end as guest_points
from(
select *
from matches m
join teams t
on t.team_id = m.host_team) a
join
(select *
from matches m
join teams t
on t.team_id = m.guest_team) b
on a.match_id = b.match_id) c
union all
Select d.guest_id, d.guest_name, d.guest_points
from(
select a.match_id, a.team_id as host_id, a.team_name as host_name, b.team_id as guest_id, b.team_name as guest_name, a.host_goals, a.guest_goals,
case
when a.host_goals > a.guest_goals then 3
when a.host_goals = a.guest_goals then 1
else 0
end as host_points,
case
when a.host_goals < a.guest_goals then 3
when a.host_goals = a.guest_goals then 1
else 0
end as guest_points
from(
select *
from matches m
join teams t
on t.team_id = m.host_team) a
join
(select *
from matches m
join teams t
on t.team_id = m.guest_team) b
on a.match_id = b.match_id) d)

Select team_id, team_name, coalesce(total,0) as num_points
from teams t2
left join(
select host_id, host_name, sum(host_points) as total
from t1
group by host_id, host_name) e
on t2.team_id = e.host_id
order by num_points desc, team_id

-- Question 1
-- Table: Users

-- +---------------+---------+
-- | Column Name   | Type    |
-- +---------------+---------+
-- | id            | int     |
-- | name          | varchar |
-- +---------------+---------+
-- id is the primary key for this table.
-- name is the name of the user.
 

-- Table: Rides

-- +---------------+---------+
-- | Column Name   | Type    |
-- +---------------+---------+
-- | id            | int     |
-- | user_id       | int     |
-- | distance      | int     |
-- +---------------+---------+
-- id is the primary key for this table.
-- user_id is the id of the user who travelled the distance "distance".
 

-- Write an SQL query to report the distance travelled by each user.

-- Return the result table ordered by travelled_distance in descending order, 
-- if two or more users travelled the same distance, order them by their name in ascending order.

-- The query result format is in the following example.

 

-- Users table:
-- +------+-----------+
-- | id   | name      |
-- +------+-----------+
-- | 1    | Alice     |
-- | 2    | Bob       |
-- | 3    | Alex      |
-- | 4    | Donald    |
-- | 7    | Lee       |
-- | 13   | Jonathan  |
-- | 19   | Elvis     |
-- +------+-----------+

-- Rides table:
-- +------+----------+----------+
-- | id   | user_id  | distance |
-- +------+----------+----------+
-- | 1    | 1        | 120      |
-- | 2    | 2        | 317      |
-- | 3    | 3        | 222      |
-- | 4    | 7        | 100      |
-- | 5    | 13       | 312      |
-- | 6    | 19       | 50       |
-- | 7    | 7        | 120      |
-- | 8    | 19       | 400      |
-- | 9    | 7        | 230      |
-- +------+----------+----------+

-- Result table:
-- +----------+--------------------+
-- | name     | travelled_distance |
-- +----------+--------------------+
-- | Elvis    | 450                |
-- | Lee      | 450                |
-- | Bob      | 317                |
-- | Jonathan | 312                |
-- | Alex     | 222                |
-- | Alice    | 120                |
-- | Donald   | 0                  |
-- +----------+--------------------+
-- Elvis and Lee travelled 450 miles, Elvis is the top traveller as his name is alphabetically smaller than Lee.
-- Bob, Jonathan, Alex and Alice have only one ride and we just order them by the total distances of the ride.
-- Donald didn't have any rides, the distance travelled by him is 0.

-- Solution
Select U.name as name, coalesce(sum(R.distance),0) as travelled_distance 
from Users U left join Rides R
on R.user_id = U.id
group by name
Order by travelled_distance desc, name

-- Question 114
-- Table: Product

-- +---------------+---------+
-- | Column Name   | Type    |
-- +---------------+---------+
-- | product_id    | int     |
-- | product_name  | varchar |
-- +---------------+---------+
-- product_id is the primary key for this table.
-- product_name is the name of the product.
 

-- Table: Sales

-- +---------------------+---------+
-- | Column Name         | Type    |
-- +---------------------+---------+
-- | product_id          | int     |
-- | period_start        | varchar |
-- | period_end          | date    |
-- | average_daily_sales | int     |
-- +---------------------+---------+
-- product_id is the primary key for this table. 
-- period_start and period_end indicates the start and end date for sales period, both dates are inclusive.
-- The average_daily_sales column holds the average daily sales amount of the items for the period.

-- Write an SQL query to report the Total sales amount of each item for each year, with corresponding product name, product_id, product_name and report_year.

-- Dates of the sales years are between 2018 to 2020. Return the result table ordered by product_id and report_year.

-- The query result format is in the following example:


-- Product table:
-- +------------+--------------+
-- | product_id | product_name |
-- +------------+--------------+
-- | 1          | LC Phone     |
-- | 2          | LC T-Shirt   |
-- | 3          | LC Keychain  |
-- +------------+--------------+

-- Sales table:
-- +------------+--------------+-------------+---------------------+
-- | product_id | period_start | period_end  | average_daily_sales |
-- +------------+--------------+-------------+---------------------+
-- | 1          | 2019-01-25   | 2019-02-28  | 100                 |
-- | 2          | 2018-12-01   | 2020-01-01  | 10                  |
-- | 3          | 2019-12-01   | 2020-01-31  | 1                   |
-- +------------+--------------+-------------+---------------------+

-- Result table:
-- +------------+--------------+-------------+--------------+
-- | product_id | product_name | report_year | total_amount |
-- +------------+--------------+-------------+--------------+
-- | 1          | LC Phone     |    2019     | 3500         |
-- | 2          | LC T-Shirt   |    2018     | 310          |
-- | 2          | LC T-Shirt   |    2019     | 3650         |
-- | 2          | LC T-Shirt   |    2020     | 10           |
-- | 3          | LC Keychain  |    2019     | 31           |
-- | 3          | LC Keychain  |    2020     | 31           |
-- +------------+--------------+-------------+--------------+
-- LC Phone was sold for the period of 2019-01-25 to 2019-02-28, and there are 35 days for this period. Total amount 35*100 = 3500. 
-- LC T-shirt was sold for the period of 2018-12-01 to 2020-01-01, and there are 31, 365, 1 days for years 2018, 2019 and 2020 respectively.
-- LC Keychain was sold for the period of 2019-12-01 to 2020-01-31, and there are 31, 31 days for years 2019 and 2020 respectively.

-- Solution
SELECT
    b.product_id,
    a.product_name,
    a.yr AS report_year,
    CASE 
        WHEN YEAR(b.period_start)=YEAR(b.period_end) AND a.yr=YEAR(b.period_start) THEN DATEDIFF(b.period_end,b.period_start)+1
        WHEN a.yr=YEAR(b.period_start) THEN DATEDIFF(DATE_FORMAT(b.period_start,'%Y-12-31'),b.period_start)+1
        WHEN a.yr=YEAR(b.period_end) THEN DAYOFYEAR(b.period_end) 
        WHEN a.yr>YEAR(b.period_start) AND a.yr<YEAR(b.period_end) THEN 365
        ELSE 0
    END * average_daily_sales AS total_amount
FROM
    (SELECT product_id,product_name,'2018' AS yr FROM Product
    UNION
    SELECT product_id,product_name,'2019' AS yr FROM Product
    UNION
    SELECT product_id,product_name,'2020' AS yr FROM Product) a
    JOIN 
    Sales b
    ON a.product_id=b.product_id  
HAVING total_amount > 0
ORDER BY b.product_id,a.yr
-- Question 109
-- Table: Players

-- +-------------+-------+
-- | Column Name | Type  |
-- +-------------+-------+
-- | player_id   | int   |
-- | group_id    | int   |
-- +-------------+-------+
-- player_id is the primary key of this table.
-- Each row of this table indicates the group of each player.
-- Table: Matches

-- +---------------+---------+
-- | Column Name   | Type    |
-- +---------------+---------+
-- | match_id      | int     |
-- | first_player  | int     |
-- | second_player | int     | 
-- | first_score   | int     |
-- | second_score  | int     |
-- +---------------+---------+
-- match_id is the primary key of this table.
-- Each row is a record of a match, first_player and second_player contain the player_id of each match.
-- first_score and second_score contain the number of points of the first_player and second_player respectively.
-- You may assume that, in each match, players belongs to the same group.
 

-- The winner in each group is the player who scored the maximum total points within the group. In the case of a tie, 
-- the lowest player_id wins.

-- Write an SQL query to find the winner in each group.

-- The query result format is in the following example:

-- Players table:
-- +-----------+------------+
-- | player_id | group_id   |
-- +-----------+------------+
-- | 15        | 1          |
-- | 25        | 1          |
-- | 30        | 1          |
-- | 45        | 1          |
-- | 10        | 2          |
-- | 35        | 2          |
-- | 50        | 2          |
-- | 20        | 3          |
-- | 40        | 3          |
-- +-----------+------------+

-- Matches table:
-- +------------+--------------+---------------+-------------+--------------+
-- | match_id   | first_player | second_player | first_score | second_score |
-- +------------+--------------+---------------+-------------+--------------+
-- | 1          | 15           | 45            | 3           | 0            |
-- | 2          | 30           | 25            | 1           | 2            |
-- | 3          | 30           | 15            | 2           | 0            |
-- | 4          | 40           | 20            | 5           | 2            |
-- | 5          | 35           | 50            | 1           | 1            |
-- +------------+--------------+---------------+-------------+--------------+

-- Result table:
-- +-----------+------------+
-- | group_id  | player_id  |
-- +-----------+------------+ 
-- | 1         | 15         |
-- | 2         | 35         |
-- | 3         | 40         |
-- +-----------+------------+

-- Solution
with t1 as(
select first_player, sum(first_score) as total
from
(select first_player, first_score
from matches
union all
select second_player, second_score
from matches) a
group by 1),

t2 as(
select *, coalesce(total,0) as score
from players p left join t1
on p.player_id = t1.first_player)

select group_id, player_id
from 
(select *, row_number() over(partition by group_id order by group_id, score desc) as rn
from t2) b
where b.rn = 1
-- Question 58
-- Given a table tree, id is identifier of the tree node and p_id is its parent node's id.

-- +----+------+
-- | id | p_id |
-- +----+------+
-- | 1  | null |
-- | 2  | 1    |
-- | 3  | 1    |
-- | 4  | 2    |
-- | 5  | 2    |
-- +----+------+
-- Each node in the tree can be one of three types:
-- Leaf: if the node is a leaf node.
-- Root: if the node is the root of the tree.
-- Inner: If the node is neither a leaf node nor a root node.
 

-- Write a query to print the node id and the type of the node. Sort your output by the node id. The result for the above sample is:
 

-- +----+------+
-- | id | Type |
-- +----+------+
-- | 1  | Root |
-- | 2  | Inner|
-- | 3  | Leaf |
-- | 4  | Leaf |
-- | 5  | Leaf |
-- +----+------+
 

-- Explanation

 

-- Node '1' is root node, because its parent node is NULL and it has child node '2' and '3'.
-- Node '2' is inner node, because it has parent node '1' and child node '4' and '5'.
-- Node '3', '4' and '5' is Leaf node, because they have parent node and they don't have child node.

-- And here is the image of the sample tree as below:
 

-- 			  1
-- 			/   \
--           2       3
--         /   \
--       4       5
-- Note

-- If there is only one node on the tree, you only need to output its root attributes.

-- Solution
select id,
case when p_id is null then 'Root'
when id not in (select p_id from tree where p_id is not null group by p_id) then 'Leaf'
else 'Inner'
end as Type
from tree
order by id
-- Question 16
-- A pupil Tim gets homework to identify whether three line segments could possibly form a triangle.
 

-- However, this assignment is very heavy because there are hundreds of records to calculate.
 

-- Could you help Tim by writing a query to judge whether these three sides can form a triangle, 
-- assuming table triangle holds the length of the three sides x, y and z.
 

-- | x  | y  | z  |
-- |----|----|----|
-- | 13 | 15 | 30 |
-- | 10 | 20 | 15 |
-- For the sample data above, your query should return the follow result:
-- | x  | y  | z  | triangle |
-- |----|----|----|----------|
-- | 13 | 15 | 30 | No       |
-- | 10 | 20 | 15 | Yes      |


-- Solution
select x, y, z,
case 
when x+y > z and x+z > y and  y+z > x  then 'Yes' 
when x=y and y=z then 'Yes'
else 'No'
end as Triangle
from triangle
-- Question 98
-- The Trips table holds all taxi trips. Each trip has a unique Id, while Client_Id and Driver_Id are both foreign keys to the Users_Id at the Users table. Status is an ENUM type of (‘completed’, ‘cancelled_by_driver’, ‘cancelled_by_client’).

-- +----+-----------+-----------+---------+--------------------+----------+
-- | Id | Client_Id | Driver_Id | City_Id |        Status      |Request_at|
-- +----+-----------+-----------+---------+--------------------+----------+
-- | 1  |     1     |    10     |    1    |     completed      |2013-10-01|
-- | 2  |     2     |    11     |    1    | cancelled_by_driver|2013-10-01|
-- | 3  |     3     |    12     |    6    |     completed      |2013-10-01|
-- | 4  |     4     |    13     |    6    | cancelled_by_client|2013-10-01|
-- | 5  |     1     |    10     |    1    |     completed      |2013-10-02|
-- | 6  |     2     |    11     |    6    |     completed      |2013-10-02|
-- | 7  |     3     |    12     |    6    |     completed      |2013-10-02|
-- | 8  |     2     |    12     |    12   |     completed      |2013-10-03|
-- | 9  |     3     |    10     |    12   |     completed      |2013-10-03| 
-- | 10 |     4     |    13     |    12   | cancelled_by_driver|2013-10-03|
-- +----+-----------+-----------+---------+--------------------+----------+
-- The Users table holds all users. Each user has an unique Users_Id, and Role is an ENUM type of (‘client’, ‘driver’, ‘partner’).

-- +----------+--------+--------+
-- | Users_Id | Banned |  Role  |
-- +----------+--------+--------+
-- |    1     |   No   | client |
-- |    2     |   Yes  | client |
-- |    3     |   No   | client |
-- |    4     |   No   | client |
-- |    10    |   No   | driver |
-- |    11    |   No   | driver |
-- |    12    |   No   | driver |
-- |    13    |   No   | driver |
-- +----------+--------+--------+
-- Write a SQL query to find the cancellation rate of requests made by unbanned users (both client and driver must be unbanned) between Oct 1, 2013 and Oct 3, 2013. The cancellation rate is computed by dividing the number of canceled (by client or driver) requests made by unbanned users by the total number of requests made by unbanned users.

-- For the above tables, your SQL query should return the following rows with the cancellation rate being rounded to two decimal places.

-- +------------+-------------------+
-- |     Day    | Cancellation Rate |
-- +------------+-------------------+
-- | 2013-10-01 |       0.33        |
-- | 2013-10-02 |       0.00        |
-- | 2013-10-03 |       0.50        |
-- +------------+-------------------+
-- Credits:
-- Special thanks to @cak1erlizhou for contributing this question, writing the problem description and adding part of the test cases.

-- Solution
with t1 as(
select request_at, count(status) as total
from trips 
where client_id = any(select users_id
from users
where banned != 'Yes') 
and driver_id = any(select users_id
from users
where banned != 'Yes')
and request_at between '2013-10-01' and '2013-10-03'
group by request_at),

t2 as
( select request_at, count(status) as cancel
from trips 
where client_id = any(select users_id
from users
where banned != 'Yes') 
and driver_id = any(select users_id
from users
where banned != 'Yes')
and request_at between '2013-10-01' and '2013-10-03'
and status != 'completed'
group by request_at
)

select request_at as Day, coalesce(round((cancel+0.00)/(total+0.00),2),0) as "Cancellation Rate"
from t1 left join t2
using(request_at)

-- Question 64
-- Table: Books

-- +----------------+---------+
-- | Column Name    | Type    |
-- +----------------+---------+
-- | book_id        | int     |
-- | name           | varchar |
-- | available_from | date    |
-- +----------------+---------+
-- book_id is the primary key of this table.
-- Table: Orders

-- +----------------+---------+
-- | Column Name    | Type    |
-- +----------------+---------+
-- | order_id       | int     |
-- | book_id        | int     |
-- | quantity       | int     |
-- | dispatch_date  | date    |
-- +----------------+---------+
-- order_id is the primary key of this table.
-- book_id is a foreign key to the Books table.
 

-- Write an SQL query that reports the books that have sold less than 10 copies in the last year, excluding books that have been available for less than 1 month from today. Assume today is 2019-06-23.

-- The query result format is in the following example:

-- Books table:
-- +---------+--------------------+----------------+
-- | book_id | name               | available_from |
-- +---------+--------------------+----------------+
-- | 1       | "Kalila And Demna" | 2010-01-01     |
-- | 2       | "28 Letters"       | 2012-05-12     |
-- | 3       | "The Hobbit"       | 2019-06-10     |
-- | 4       | "13 Reasons Why"   | 2019-06-01     |
-- | 5       | "The Hunger Games" | 2008-09-21     |
-- +---------+--------------------+----------------+

-- Orders table:
-- +----------+---------+----------+---------------+
-- | order_id | book_id | quantity | dispatch_date |
-- +----------+---------+----------+---------------+
-- | 1        | 1       | 2        | 2018-07-26    |
-- | 2        | 1       | 1        | 2018-11-05    |
-- | 3        | 3       | 8        | 2019-06-11    |
-- | 4        | 4       | 6        | 2019-06-05    |
-- | 5        | 4       | 5        | 2019-06-20    |
-- | 6        | 5       | 9        | 2009-02-02    |
-- | 7        | 5       | 8        | 2010-04-13    |
-- +----------+---------+----------+---------------+

-- Result table:
-- +-----------+--------------------+
-- | book_id   | name               |
-- +-----------+--------------------+
-- | 1         | "Kalila And Demna" |
-- | 2         | "28 Letters"       |
-- | 5         | "The Hunger Games" |
-- +-----------+--------------------+

-- Solution
select b.book_id, name
from
(select *
from books
where available_from < '2019-05-23') b
left join
(select *
from orders 
where dispatch_date > '2018-06-23') a
on a.book_id = b.book_id
group by b.book_id, name
having coalesce(sum(quantity),0)<10
-- Question 40
-- Table: Activity

-- +---------------+---------+
-- | Column Name   | Type    |
-- +---------------+---------+
-- | user_id       | int     |
-- | session_id    | int     |
-- | activity_date | date    |
-- | activity_type | enum    |
-- +---------------+---------+
-- There is no primary key for this table, it may have duplicate rows.
-- The activity_type column is an ENUM of type ('open_session', 'end_session', 'scroll_down', 'send_message').
-- The table shows the user activities for a social media website. 
-- Note that each session belongs to exactly one user.
 

-- Write an SQL query to find the daily active user count for a period of 30 days ending 2019-07-27 inclusively. A user was active on some day if he/she made at least one activity on that day.

-- The query result format is in the following example:

-- Activity table:
-- +---------+------------+---------------+---------------+
-- | user_id | session_id | activity_date | activity_type |
-- +---------+------------+---------------+---------------+
-- | 1       | 1          | 2019-07-20    | open_session  |
-- | 1       | 1          | 2019-07-20    | scroll_down   |
-- | 1       | 1          | 2019-07-20    | end_session   |
-- | 2       | 4          | 2019-07-20    | open_session  |
-- | 2       | 4          | 2019-07-21    | send_message  |
-- | 2       | 4          | 2019-07-21    | end_session   |
-- | 3       | 2          | 2019-07-21    | open_session  |
-- | 3       | 2          | 2019-07-21    | send_message  |
-- | 3       | 2          | 2019-07-21    | end_session   |
-- | 4       | 3          | 2019-06-25    | open_session  |
-- | 4       | 3          | 2019-06-25    | end_session   |
-- +---------+------------+---------------+---------------+

-- Result table:
-- +------------+--------------+ 
-- | day        | active_users |
-- +------------+--------------+ 
-- | 2019-07-20 | 2            |
-- | 2019-07-21 | 2            |
-- +------------+--------------+ 
-- Note that we do not care about days with zero active users.

-- Solution
Select activity_date as day, count(distinct user_id) as active_users
from activity
where activity_date > '2019-06-26' and activity_date < '2019-07-27'
group by activity_date
-- Question 35
-- Table: Activity

-- +---------------+---------+
-- | Column Name   | Type    |
-- +---------------+---------+
-- | user_id       | int     |
-- | session_id    | int     |
-- | activity_date | date    |
-- | activity_type | enum    |
-- +---------------+---------+
-- There is no primary key for this table, it may have duplicate rows.
-- The activity_type column is an ENUM of type ('open_session', 'end_session', 'scroll_down', 'send_message').
-- The table shows the user activities for a social media website.
-- Note that each session belongs to exactly one user.
 

-- Write an SQL query to find the average number of sessions per user for a period of 30 days ending 2019-07-27 inclusively, rounded to 2 decimal places. The sessions we want to count for a user are those with at least one activity in that time period.

-- The query result format is in the following example:

-- Activity table:
-- +---------+------------+---------------+---------------+
-- | user_id | session_id | activity_date | activity_type |
-- +---------+------------+---------------+---------------+
-- | 1       | 1          | 2019-07-20    | open_session  |
-- | 1       | 1          | 2019-07-20    | scroll_down   |
-- | 1       | 1          | 2019-07-20    | end_session   |
-- | 2       | 4          | 2019-07-20    | open_session  |
-- | 2       | 4          | 2019-07-21    | send_message  |
-- | 2       | 4          | 2019-07-21    | end_session   |
-- | 3       | 2          | 2019-07-21    | open_session  |
-- | 3       | 2          | 2019-07-21    | send_message  |
-- | 3       | 2          | 2019-07-21    | end_session   |
-- | 3       | 5          | 2019-07-21    | open_session  |
-- | 3       | 5          | 2019-07-21    | scroll_down   |
-- | 3       | 5          | 2019-07-21    | end_session   |
-- | 4       | 3          | 2019-06-25    | open_session  |
-- | 4       | 3          | 2019-06-25    | end_session   |
-- +---------+------------+---------------+---------------+

-- Result table:
-- +---------------------------+ 
-- | average_sessions_per_user |
-- +---------------------------+ 
-- | 1.33                      |
-- +---------------------------+ 
-- User 1 and 2 each had 1 session in the past 30 days while user 3 had 2 sessions so the average is (1 + 1 + 2) / 3 = 1.33.


-- Solution
select ifnull(round(avg(a.num),2),0) as average_sessions_per_user
from (
select count(distinct session_id) as num
from activity
where activity_date between '2019-06-28' and '2019-07-27'
group by user_id) a
-- Question 113
-- Table: Spending

-- +-------------+---------+
-- | Column Name | Type    |
-- +-------------+---------+
-- | user_id     | int     |
-- | spend_date  | date    |
-- | platform    | enum    | 
-- | amount      | int     |
-- +-------------+---------+
-- The table logs the spendings history of users that make purchases from an online shopping website which has a desktop and a mobile application.
-- (user_id, spend_date, platform) is the primary key of this table.
-- The platform column is an ENUM type of ('desktop', 'mobile').
-- Write an SQL query to find the total number of users and the total amount spent using mobile only, desktop only and both mobile and desktop together for each date.

-- The query result format is in the following example:

-- Spending table:
-- +---------+------------+----------+--------+
-- | user_id | spend_date | platform | amount |
-- +---------+------------+----------+--------+
-- | 1       | 2019-07-01 | mobile   | 100    |
-- | 1       | 2019-07-01 | desktop  | 100    |
-- | 2       | 2019-07-01 | mobile   | 100    |
-- | 2       | 2019-07-02 | mobile   | 100    |
-- | 3       | 2019-07-01 | desktop  | 100    |
-- | 3       | 2019-07-02 | desktop  | 100    |
-- +---------+------------+----------+--------+

-- Result table:
-- +------------+----------+--------------+-------------+
-- | spend_date | platform | total_amount | total_users |
-- +------------+----------+--------------+-------------+
-- | 2019-07-01 | desktop  | 100          | 1           |
-- | 2019-07-01 | mobile   | 100          | 1           |
-- | 2019-07-01 | both     | 200          | 1           |
-- | 2019-07-02 | desktop  | 100          | 1           |
-- | 2019-07-02 | mobile   | 100          | 1           |
-- | 2019-07-02 | both     | 0            | 0           |
-- +------------+----------+--------------+-------------+ 
-- On 2019-07-01, user 1 purchased using both desktop and mobile, user 2 purchased using mobile only and user 3 purchased using desktop only.
-- On 2019-07-02, user 2 purchased using mobile only, user 3 purchased using desktop only and no one purchased using both platforms.

-- Solution
SELECT p.spend_date, p.platform, IFNULL(SUM(amount), 0) total_amount, COUNT(DISTINCT u.user_id) total_users
FROM
(
SELECT DISTINCT(spend_date), 'desktop' platform FROM Spending
UNION
SELECT DISTINCT(spend_date), 'mobile' platform FROM Spending
UNION
SELECT DISTINCT(spend_date), 'both' platform FROM Spending
) p LEFT JOIN

(SELECT user_id, spend_date, SUM(amount) amount, (CASE WHEN COUNT(DISTINCT platform)>1 THEN "both" ELSE platform END) platform
FROM Spending
GROUP BY spend_date, user_id) u

ON p.platform = u.platform AND p.spend_date=u.spend_date

GROUP BY p.spend_date, p.platform

-- Question 46
-- Table: Countries

-- +---------------+---------+
-- | Column Name   | Type    |
-- +---------------+---------+
-- | country_id    | int     |
-- | country_name  | varchar |
-- +---------------+---------+
-- country_id is the primary key for this table.
-- Each row of this table contains the ID and the name of one country.
 

-- Table: Weather

-- +---------------+---------+
-- | Column Name   | Type    |
-- +---------------+---------+
-- | country_id    | int     |
-- | weather_state | varchar |
-- | day           | date    |
-- +---------------+---------+
-- (country_id, day) is the primary key for this table.
-- Each row of this table indicates the weather state in a country for one day.
 

-- Write an SQL query to find the type of weather in each country for November 2019.

-- The type of weather is Cold if the average weather_state is less than or equal 15, Hot if the average weather_state is greater than or equal 25 and Warm otherwise.

-- Return result table in any order.

-- The query result format is in the following example:

-- Countries table:
-- +------------+--------------+
-- | country_id | country_name |
-- +------------+--------------+
-- | 2          | USA          |
-- | 3          | Australia    |
-- | 7          | Peru         |
-- | 5          | China        |
-- | 8          | Morocco      |
-- | 9          | Spain        |
-- +------------+--------------+
-- Weather table:
-- +------------+---------------+------------+
-- | country_id | weather_state | day        |
-- +------------+---------------+------------+
-- | 2          | 15            | 2019-11-01 |
-- | 2          | 12            | 2019-10-28 |
-- | 2          | 12            | 2019-10-27 |
-- | 3          | -2            | 2019-11-10 |
-- | 3          | 0             | 2019-11-11 |
-- | 3          | 3             | 2019-11-12 |
-- | 5          | 16            | 2019-11-07 |
-- | 5          | 18            | 2019-11-09 |
-- | 5          | 21            | 2019-11-23 |
-- | 7          | 25            | 2019-11-28 |
-- | 7          | 22            | 2019-12-01 |
-- | 7          | 20            | 2019-12-02 |
-- | 8          | 25            | 2019-11-05 |
-- | 8          | 27            | 2019-11-15 |
-- | 8          | 31            | 2019-11-25 |
-- | 9          | 7             | 2019-10-23 |
-- | 9          | 3             | 2019-12-23 |
-- +------------+---------------+------------+
-- Result table:
-- +--------------+--------------+
-- | country_name | weather_type |
-- +--------------+--------------+
-- | USA          | Cold         |
-- | Austraila    | Cold         |
-- | Peru         | Hot          |
-- | China        | Warm         |
-- | Morocco      | Hot          |
-- +--------------+--------------+
-- Average weather_state in USA in November is (15) / 1 = 15 so weather type is Cold.
-- Average weather_state in Austraila in November is (-2 + 0 + 3) / 3 = 0.333 so weather type is Cold.
-- Average weather_state in Peru in November is (25) / 1 = 25 so weather type is Hot.
-- Average weather_state in China in November is (16 + 18 + 21) / 3 = 18.333 so weather type is Warm.
-- Average weather_state in Morocco in November is (25 + 27 + 31) / 3 = 27.667 so weather type is Hot.
-- We know nothing about average weather_state in Spain in November 
-- so we don't include it in the result table. 

-- Solution
Select c.country_name, 
case when avg(w.weather_state)<=15 then 'Cold'
     when avg(w.weather_state)>=25 then 'Hot'
else 'Warm'
end as weather_type
from weather w join
countries c
on w.country_id = c.country_id
where month(day) = 11
group by c.country_name

-- Question 88
-- Table: Candidate

-- +-----+---------+
-- | id  | Name    |
-- +-----+---------+
-- | 1   | A       |
-- | 2   | B       |
-- | 3   | C       |
-- | 4   | D       |
-- | 5   | E       |
-- +-----+---------+  
-- Table: Vote

-- +-----+--------------+
-- | id  | CandidateId  |
-- +-----+--------------+
-- | 1   |     2        |
-- | 2   |     4        |
-- | 3   |     3        |
-- | 4   |     2        |
-- | 5   |     5        |
-- +-----+--------------+
-- id is the auto-increment primary key,
-- CandidateId is the id appeared in Candidate table.
-- Write a sql to find the name of the winning candidate, the above example will return the winner B.

-- +------+
-- | Name |
-- +------+
-- | B    |
-- +------+
-- Notes:

-- You may assume there is no tie, in other words there will be only one winning candidate

-- Solution
with t1 as (
select *, rank() over(order by b.votes desc) as rk
from candidate c
join 
(select candidateid, count(*) as votes
from vote
group by candidateid) b
on c.id = b.candidateid)

select t1.name
from t1
where t1.rk=1
