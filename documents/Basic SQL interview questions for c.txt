Basic SQL interview questions for beginners (0 to 2 years of experience) 
Basic SQL data types and simple SELECT query
Question: Write a SQL query that retrieves the `first_name`, `last_name`, and `email` columns from a table named `users`, where the `email` domain is “example.com”. Assume that `email` is a `VARCHAR` type.

Example Answer:

SELECT first_name, last_name, email

FROM users

WHERE email LIKE '%@example.com';
Explanation: This query selects the `first_name`, `last_name`, and `email` columns from the `users` table and filters the rows to include only those with an email domain of “example.com”. The `LIKE` operator is used with a wildcard (`%`) to match any characters before “@example.com”.

SQL joins and relationships
Question: Write a SQL query to retrieve the `order_id` and `order_date` from an `orders` table and the `product_name` from a `products` table for all orders. Assume that the `orders` table has a `product_id` foreign key that references the `product_id` in the `products` table.

Example Answer:

SELECT o.order_id, o.order_date, p.product_name

FROM orders o

JOIN products p ON o.product_id = p.product_id;
Explanation: This query retrieves data from both the `orders` and `products` tables using an `INNER JOIN`. The `JOIN` is performed on the `product_id` column, which is common between the two tables, allowing the query to combine rows from each table where there is a matching `product_id`.

Basic data manipulation
Question: Write a SQL query to update the `salary` column in the `employees` table, increasing it by 10% for all employees who work in the “Sales” department. Assume the `department` column is of type `VARCHAR`.

Example Answer:

UPDATE employees

SET salary = salary * 1.10

WHERE department = 'Sales';
Explanation: This query updates the `salary` field in the `employees` table by multiplying the current salary by 1.10 (a 10% increase) for all employees in the “Sales” department. The `WHERE` clause ensures that only rows where the `department` is “Sales” are affected.

Learning tip: Want to review SQL basics before your next interview? Journey into SQL with Taylor Swift is a fun and accessible learning path in CodeSignal Learn where you’ll practice key querying skills using Taylor Swift’s discography as your database.

Intermediate SQL interview questions (2 to 5 years of experience) 
Complex SQL queries and subqueries
Question: Write a SQL query to find the top 3 customers with the highest total `order_amount` from the `orders` table. Assume that each order is linked to a customer via a `customer_id` column, and the `order_amount` is a numeric column.

Example Answer:

SELECT customer_id, SUM(order_amount) AS total_spent

FROM orders

GROUP BY customer_id

ORDER BY total_spent DESC

LIMIT 3;
Explanation: This query calculates the total `order_amount` spent by each customer using the `SUM()` function and groups the results by `customer_id`. The `ORDER BY` clause sorts the results in descending order of total spent, and the `LIMIT` clause restricts the output to the top 3 customers. This type of query is essential for analyzing customer behavior and identifying high-value customers.

Subqueries and data integrity
Question: Write a SQL query to find all employees in the `employees` table whose `salary` is greater than the average salary in their department. Assume that the table has `employee_id`, `department_id`, and `salary` columns.

Example Answer:

SELECT employee_id, department_id, salary

FROM employees e

WHERE salary > (

    SELECT AVG(salary)

    FROM employees

    WHERE department_id = e.department_id

);
Explanation: This query uses a subquery to calculate the average salary within each department. The main query then selects employees whose salary exceeds the average salary of their respective department. The use of correlated subqueries (where the subquery references a column from the outer query) is a powerful technique for comparing data within grouped contexts.

Indexes, performance, and transaction control
Question: Suppose you need to delete a large number of records from the `transactions` table where the `transaction_date` is older than one year. Write a SQL script that includes steps to ensure the deletion is efficient and doesn’t affect the performance of the database during the operation. Assume an index exists on the `transaction_date` column.

Example Answer:

BEGIN;

SET TRANSACTION ISOLATION LEVEL READ COMMITTED;

DELETE FROM transactions

WHERE transaction_date < NOW() - INTERVAL '1 year';

COMMIT;
Explanation: This script begins with a `BEGIN` statement to start a transaction. The `SET TRANSACTION ISOLATION LEVEL` command ensures that the operation uses the appropriate isolation level to prevent reading data that has been modified but not committed by other transactions (dirty reads), improving performance during the deletion. The `DELETE` operation then removes records older than one year, leveraging the existing index on `transaction_date` for faster execution. Finally, the `COMMIT` statement ensures that all changes are saved permanently, maintaining data integrity and consistency.

Learning tip: Refresh your SQL scripting skills before your next interview or assessment with the Learning SQL Scripting with Leo Messi learning path in CodeSignal Learn. Practice joins, functions, conditional logic, and more using stats from soccer star Lionel Messi’s career as your database. 

Advanced SQL interview questions (5 years experience or more)
SQL optimization techniques and handling large datasets
Question: You have a table `large_sales` with millions of rows and a composite index on `(customer_id, sale_date) named `idx_customer_date`. Write an optimized SQL query to retrieve the total sales amount for each `customer_id` in the year 2023, considering the potential performance impact due to the dataset size.

Example Answer:

SELECT customer_id, SUM(sale_amount) AS total_sales

FROM large_sales

WHERE sale_date BETWEEN '2023-01-01' AND '2023-12-31'

GROUP BY customer_id

USE INDEX (idx_customer_date);
Explanation: This query retrieves the total sales amount for each `customer_id` for the year 2023 from a very large dataset. By specifying the `USE INDEX` hint, the query explicitly directs the database to utilize the composite index on `(customer_id, sale_date)` to optimize the filtering and grouping operations instead of an index on just `sale_date`. This is crucial for maintaining performance when dealing with large datasets, as it minimizes the amount of data scanned.

Advanced data modeling and stored procedures
Question: Design a stored procedure named `UpdateEmployeeDepartment` that transfers an employee to a new department while ensuring that the new department’s `budget` is not exceeded. Assume that `employees` and `departments` tables exist, with `employees` containing `employee_id`, `department_id`, and `salary`, and `departments` containing `department_id`, `budget`, and `current_expenditure`.

Example Answer:

DELIMITER //

CREATE PROCEDURE UpdateEmployeeDepartment(IN emp_id INT, IN new_dept_id INT)

BEGIN

    DECLARE emp_salary DECIMAL(10,2);

    DECLARE current_expenditure DECIMAL(10,2);

    DECLARE dept_budget DECIMAL(10,2);

    SELECT salary INTO emp_salary FROM employees WHERE employee_id = emp_id;

    SELECT current_expenditure, budget INTO current_expenditure, dept_budget 

    FROM departments WHERE department_id = new_dept_id;

    IF current_expenditure + emp_salary <= dept_budget THEN

        UPDATE employees SET department_id = new_dept_id WHERE employee_id = emp_id;

        UPDATE departments SET current_expenditure = current_expenditure + emp_salary 

        WHERE department_id = new_dept_id;

    ELSE

        SIGNAL SQLSTATE '45000' SET MESSAGE_TEXT = 'Budget exceeded for the new department';

    END IF;

END //

DELIMITER ;
Explanation: This stored procedure first retrieves the salary of the employee being transferred and the budget and current expenditure of the target department. It then checks if adding the employee’s salary to the department’s current expenditure would exceed the department’s budget. If not, the employee is transferred, and the department’s expenditure is updated. If the budget would be exceeded, the procedure raises an error, ensuring budget constraints are respected. This approach demonstrates advanced data modeling by handling complex relationships between entities in the database.

Database architecture considerations and triggers
Question: Write a trigger named `CheckInventoryBeforeInsert` that prevents the insertion of a new order in the `orders` table if the total quantity of items ordered exceeds the available stock in the `inventory` table. Assume the `orders` table has `product_id` and `quantity` columns, and the `inventory` table has `product_id` and `stock_quantity` columns.

Example Answer:

DELIMITER //

CREATE TRIGGER CheckInventoryBeforeInsert

BEFORE INSERT ON orders

FOR EACH ROW

BEGIN

    DECLARE available_stock INT;

    SELECT stock_quantity INTO available_stock 

    FROM inventory 

    WHERE product_id = NEW.product_id;

    IF NEW.quantity > available_stock THEN

        SIGNAL SQLSTATE '45000' SET MESSAGE_TEXT = 'Insufficient stock for the product';

    END IF;

END //

DELIMITER ;
Explanation: This trigger executes before a new order is inserted into the `orders` table. It checks if the quantity being ordered exceeds the available stock in the `inventory` table. If the order quantity is greater than the available stock, the trigger prevents the insert operation by raising an error. This ensures that the database maintains data integrity and consistency, crucial for systems where inventory management is critical. It also reflects an understanding of how triggers can enforce business rules at the database level, which is a key consideration in robust database architecture.

Hard SQL server interview questions for senior developers (10+ years of experience)
High-availability solutions and disaster recovery strategies
Question: Can you describe a high-availability solution for an SQL Server environment, and how you would implement a disaster recovery plan to minimize downtime and data loss?

Example Answer: I would use Always On Availability Groups for high availability, setting up primary and secondary replicas across different servers, ideally in separate geographic locations. The primary replica handles transactions, while secondary replicas are kept in sync.

For disaster recovery, I’d configure a secondary replica in a remote data center with automatic failover. This setup ensures minimal downtime and no data loss if the primary server fails. I’d also establish regular backups and test the failover process to ensure reliability.

Performance tuning complex systems
Question: Can you walk me through your approach to diagnosing and resolving performance issues in a complex SQL Server system with multiple large databases?

Example Answer: I start by analyzing wait statistics to find bottlenecks like CPU or I/O issues. Then, I examine query execution plans to spot inefficiencies, such as unnecessary table scans.

For optimization, I may tune indexes, rewrite queries, or partition large tables. I also check system configurations, such as memory and I/O settings, and ensure regular maintenance tasks like index rebuilding are in place to keep performance stable.

Security best practices in SQL server management
Question: What are some of the security best practices you follow when setting up and managing SQL Server databases?

Example Answer: I follow the principle of least privilege, assigning minimal permissions needed for tasks. I integrate SQL Server with Active Directory for secure authentication and use encryption for sensitive data with tools like Transparent Data Encryption (TDE).

I also ensure SQL Server is regularly patched and perform security audits to monitor for unauthorized access. Regular reviews of activity logs help me quickly detect and respond to any security issues.

SQL performance tuning interview questions
Query optimization and execution plans analysis
Question: How do you approach optimizing a slow-running query in SQL Server, and what role do execution plans play in this process?

Example Answer: When optimizing a slow query, I start by analyzing its execution plan to identify bottlenecks like full table scans or expensive joins. The execution plan shows how SQL Server processes the query, helping me spot inefficiencies.

Based on the plan, I might rewrite the query, add or modify indexes, or adjust the query structure to reduce processing time. I continually review the updated execution plan to ensure the changes improve performance.

Index management and query optimization
Question: Can you explain your process for managing indexes to ensure efficient query performance in SQL Server?

Example Answer: I regularly monitor index usage to identify underutilized or missing indexes. If a query is slow, I check the execution plan to see if an index could improve performance.

I also evaluate existing indexes to ensure they are not redundant or overlapping, which could cause unnecessary overhead. Periodically, I perform index maintenance, such as rebuilding or reorganizing fragmented indexes, to keep the database performing optimally.

SQL server profiler and database tuning advisor
Question: How do you use SQL Server Profiler and Database Tuning Advisor to enhance database performance?

Example Answer: I use SQL Server Profiler to capture and analyze slow-running queries or resource-intensive operations. The trace data helps me identify patterns and specific queries that need optimization.

Then, I run these queries through the Database Tuning Advisor, which provides recommendations for indexing, partitioning, and other optimizations. This combination allows me to make data-driven decisions to enhance performance while avoiding guesswork.

Role-based SQL interview questions
SQL developer interview questions
Development environment setup and debugging SQL scripts
Question: Write a SQL script that sets up a development environment by creating a new schema named `dev_environment`, and within that schema, create a table `test_data` with columns `id` (INT, primary key) and `value` (VARCHAR). Then, include a statement to debug by inserting a sample record into the `test_data` table and verifying that the record was correctly inserted.

Example Answer:

CREATE SCHEMA dev_environment;

CREATE TABLE dev_environment.test_data (

    id INT PRIMARY KEY,

    value VARCHAR(100)

);

INSERT INTO dev_environment.test_data (id, value)

VALUES (1, 'Sample Data');

-- Debugging step: Check the inserted record

SELECT * FROM dev_environment.test_data WHERE id = 1;
Explanation: This script first creates a new schema named `dev_environment` to organize the development environment. It then creates a `test_data` table within that schema with an `id` column as the primary key and a `value` column for storing text data. The script includes a sample `INSERT` statement to add a record to the `test_data` table and a `SELECT` statement to verify that the insertion was successful. This approach helps in setting up a consistent development environment while also incorporating basic debugging practices.

Code versioning in SQL and best practices in database schema design
Question: Write a SQL script to create a version-controlled stored procedure that adds a new column `email` (VARCHAR) to an existing `users` table. Include comments that explain the purpose of the changes and a method to rollback the change if needed.

Example Answer:

-- Version 1.1: Adding an email column to users table

-- Purpose: To store email addresses of users

ALTER TABLE users

ADD email VARCHAR(255);

-- Rollback script: Remove the email column if the change needs to be undone

-- Version 1.1 Rollback

-- Purpose: To rollback the addition of the email column in case of issues

-- ALTER TABLE users

-- DROP COLUMN email;
Explanation: This script demonstrates best practices in code versioning and schema design. It includes an `ALTER TABLE` statement to add an `email` column to the `users` table, following a versioning format in the comments to track changes. The comments clearly explain the purpose of the update. Additionally, the script provides a rollback mechanism (commented out) to remove the `email` column if the change needs to be undone, promoting safe and controlled schema changes.

SQL interview questions for data analysts
SQL for data extraction and analytical functions in SQL
Question: Write a SQL query that extracts the total sales and calculates the average sales per month for each product in the `sales` table. The table contains `product_id`, `sale_date`, and `sale_amount` columns. Use SQL analytical functions to achieve this.

Example Answer:

WITH monthly_sales AS (

    SELECT 

        product_id, 

        EXTRACT(YEAR FROM sale_date) AS sale_year,

        EXTRACT(MONTH FROM sale_date) AS sale_month,

        SUM(sale_amount) AS monthly_total_sales

    FROM 

        sales

    GROUP BY 

        product_id, 

        EXTRACT(YEAR FROM sale_date), 

        EXTRACT(MONTH FROM sale_date)

)

SELECT 

    product_id,

    SUM(monthly_total_sales) AS total_sales,

    AVG(monthly_total_sales) AS avg_monthly_sales

FROM 

    monthly_sales

GROUP BY 

    product_id;
Explanation: This query uses SQL analytical functions to calculate the total sales and the average monthly sales for each product. The `SUM(sale_amount)` function aggregates the sales by `product_id`, month, and year. The `AVG()` function calculates the average of these monthly totals. This allows for a detailed analysis of sales patterns across products on a monthly basis.

Advanced reporting techniques and data visualization with SQL
Question: Write a SQL query to generate a report that shows the cumulative sales by month for the current year for each region. The `sales` table includes `region`, `sale_date`, and `sale_amount` columns. Ensure the report is ordered by region and month.

Example Answer:

SELECT 

    region, 

    EXTRACT(MONTH FROM sale_date) AS sale_month, 

    SUM(sale_amount) AS monthly_sales,

    SUM(SUM(sale_amount)) OVER (PARTITION BY region ORDER BY EXTRACT(MONTH FROM sale_date)) AS cumulative_sales

FROM 

    sales

WHERE 

    EXTRACT(YEAR FROM sale_date) = EXTRACT(YEAR FROM CURRENT_DATE)

GROUP BY 

    region, EXTRACT(MONTH FROM sale_date)

ORDER BY 

    region, sale_month;
Explanation: This query produces an advanced report that shows both monthly and cumulative sales by region for the current year. The `SUM(sale_amount)` function calculates the monthly sales per region. The cumulative sales are calculated using `SUM(SUM(sale_amount)) OVER (PARTITION BY region ORDER BY EXTRACT(MONTH FROM sale_date))`, which sums the monthly totals progressively. The report is ordered by region and then by month, making it useful for visualizations that track sales trends across regions over time.

SQL interview questions for data engineers
ETL processes and data quality + cleaning
Question: Write a SQL script that performs an ETL (Extract, Transform, Load) process to clean and load data from a `raw_sales` table into a `cleaned_sales` table. The `raw_sales` table contains `sale_id`, `sale_date`, `product_id`, `sale_amount`, and `customer_id`, where `sale_amount` may contain null or negative values. Clean the data by removing rows with null or negative `sale_amount`, and load the cleaned data into the `cleaned_sales` table.

Example Answer:

-- Step 1: Extract and Clean Data

INSERT INTO cleaned_sales (sale_id, sale_date, product_id, sale_amount, customer_id)

SELECT 

    sale_id, 

    sale_date, 

    product_id, 

    sale_amount, 

    customer_id

FROM 

    raw_sales

WHERE 

    sale_amount IS NOT NULL AND sale_amount > 0;

-- Step 2: Optional additional transformations can be applied here
Explanation: This script performs a basic ETL operation by extracting data from the `raw_sales` table, cleaning it by removing rows where `sale_amount` is null or negative, and then loading the cleaned data into the `cleaned_sales` table. This ensures that only valid sales data is stored in the `cleaned_sales` table, improving data quality for further analysis or reporting.

Data warehousing with SQL and SQL in data pipeline design
Question: Design a SQL query that aggregates daily sales data from a `daily_sales` table and loads it into a `monthly_sales_summary` table. The `daily_sales` table contains `sale_date`, `product_id`, and `sale_amount`. The `monthly_sales_summary` table should store `year`, `month`, `product_id`, and `total_sales`.

Example Answer:

-- Step 1: Aggregate Daily Sales into Monthly Totals

INSERT INTO monthly_sales_summary (year, month, product_id, total_sales)

SELECT 

    EXTRACT(YEAR FROM sale_date) AS year,

    EXTRACT(MONTH FROM sale_date) AS month,

    product_id,

    SUM(sale_amount) AS total_sales

FROM 

    daily_sales

GROUP BY 

    EXTRACT(YEAR FROM sale_date), EXTRACT(MONTH FROM sale_date), product_id;

-- Step 2: This data can now be used for reporting or further analysis
Explanation: This query aggregates daily sales data into monthly totals, which are then stored in the `monthly_sales_summary` table. The `EXTRACT(YEAR FROM sale_date)` and `EXTRACT(MONTH FROM sale_date)` functions are used to group the data by year and month. The `SUM(sale_amount)` function calculates the total sales per product for each month. This process is a common step in data warehousing, where data is aggregated and summarized for more efficient storage and faster querying.

Scenario-based SQL interview questions
Real-world problem-solving with SQL and handling corrupt data
Question: Can you describe how you would handle a situation where you find corrupt data in a critical production table, such as missing or invalid values in key columns?

Example Answer: If I encounter corrupt data in a production table, my first step would be to identify the extent of the corruption by running queries that check for anomalies like nulls in non-nullable columns or invalid data types. Once identified, I would create a backup of the affected data to ensure we have a recovery point.

Next, I’d isolate the problematic records and attempt to correct them, either by referencing backup data, if available, or by applying business rules to regenerate the correct values. If the corruption is widespread, I might consider restoring the table from a backup, followed by reapplying any subsequent valid changes. I would also investigate the root cause to prevent future occurrences, possibly by adding constraints or triggers to enforce data integrity.

Optimizing slow-running queries and simulating concurrency scenarios
Question: How would you approach optimizing a slow-running query in a high-traffic database, especially considering potential concurrency issues?

Example Answer: I would start by analyzing the query execution plan to identify inefficiencies like table scans, missing indexes, or suboptimal join operations. If the issue is related to indexing, I would add or adjust indexes to reduce the query’s execution time. Additionally, I’d consider query refactoring to eliminate unnecessary complexity.

Given the high-traffic environment, I’d also assess the query’s impact on concurrency. For example, I would check for locking or blocking issues that could be slowing down the system and might use techniques like query hints or isolation level adjustments to minimize contention. Finally, I would test the optimized query in a staging environment under simulated load to ensure that it performs well and doesn’t introduce new concurrency issues.

SQL for data migration tasks
Question: Can you walk me through your process for migrating large datasets from one SQL Server to another, ensuring minimal downtime and data integrity?

Example Answer: In a large-scale data migration, my first step is to plan and document the migration process, including identifying dependencies, assessing data volume, and estimating downtime. I usually start by performing a full backup of the source database to ensure we have a recovery point. 

To minimize downtime, I’d consider using techniques like log shipping or database mirroring to keep the target database up-to-date with changes made during the migration process. Before the final cutover, I’d perform a series of test migrations on a staging environment to verify that the data is correctly transferred and that the target environment functions as expected.

During the final migration, I’d carefully monitor the process, validating data integrity through checksums or row counts, and ensure that all necessary application connections are redirected to the new server. Post-migration, I’d run thorough tests to confirm everything is working correctly and that there are no data integrity issues.

Learning tip: Practice interview skills for behavioral interviews, recruiter screens, and panel interviews in CodeSignal Learn’s Behavioral Interview Practice for CS Students learning path. Engage in live mock interviews with an advanced AI agent and get immediate feedback on your performance from our AI tutor and guide, Cosmo.

Common SQL interview questions (if you have limited time to practice)
Essential SQL functions
Question: Write a SQL query to calculate the total number of orders and the average order amount from an `orders` table. The table contains columns `order_id`, `order_date`, and `order_amount`.

Example Answer:

SELECT 

    COUNT(order_id) AS total_orders, 

    AVG(order_amount) AS average_order_amount

FROM 

    orders;
Explanation: This query uses two essential SQL aggregate functions: `COUNT()` and `AVG()`. The `COUNT(order_id)` function calculates the total number of orders, while `AVG(order_amount)` calculates the average order amount across all orders. These functions are fundamental for summarizing data and generating insights from an SQL table.

SQL debugging
Question: You’ve written a query that doesn’t return the expected results. Describe how you would debug the issue, assuming you are dealing with a simple `SELECT` statement.

Example Answer:

-- Original query

SELECT * FROM customers WHERE last_name = 'Smith';

-- Debugging steps

-- 1. Check if the condition is too restrictive or misspelled

SELECT * FROM customers WHERE last_name LIKE '%Smith%';

-- 2. Verify the data

SELECT DISTINCT last_name FROM customers;

-- 3. Test a simplified version of the query

SELECT * FROM customers WHERE 1 = 1;

-- 4. Check for case sensitivity issues (if the database is case-sensitive) 

SELECT * FROM customers WHERE LOWER(last_name) = 'smith'; 

-- 5. Ensure there are no leading/trailing spaces 

SELECT * FROM customers WHERE TRIM(last_name) = 'Smith';
Explanation: The debugging process involves several steps. First, I’d check if the condition might be too restrictive or if there’s a typo by using a broader condition, like `LIKE`. Then, I’d verify the data by querying distinct values to see if the data matches the expected condition. Next, I’d run a simplified version of the query (`WHERE 1 = 1`) to confirm the basic query structure is sound. If your database is case-sensitive, Smith and smith would be treated differently. To avoid case sensitivity issues, you can use LOWER(last_name) = ‘smith’ or UPPER(last_name) = ‘SMITH’.  Finally, data might have leading or trailing spaces that affect the match. Using TRIM(last_name) = ‘Smith’ ensures that such spaces are removed before comparison. These steps help quickly identify common issues.  

Efficient query writing and key SQL clauses
Question: Write an efficient SQL query to retrieve all unique product names from a `products` table that has a `product_name` column, and ensure the results are sorted alphabetically.

Example Answer:

SELECT DISTINCT product_name

FROM products

ORDER BY product_name ASC;
Explanation: This query retrieves all unique product names using the `DISTINCT` clause, ensuring that no duplicates appear in the results. The `ORDER BY` clause sorts the product names alphabetically (`ASC`). Using `DISTINCT` in combination with `ORDER BY` is a common practice to write efficient queries that provide meaningful, well-organized results.

Critical performance factors
Question: Given a `sales` table with millions of records, write an optimized SQL query to retrieve the total sales amount for each `region` from the current year. The table includes `sale_id`, `region`, `sale_date`, and `sale_amount` columns.

Example Answer:

SELECT 

    region, 

    SUM(sale_amount) AS total_sales

FROM 

    sales

WHERE 

    EXTRACT(YEAR FROM sale_date) = EXTRACT(YEAR FROM CURRENT_DATE)

GROUP BY 

    region;
Explanation: This query efficiently calculates the total sales amount for each `region` by limiting the dataset to the current year using the `EXTRACT(YEAR FROM sale_date)` function in the `WHERE` clause. The `SUM(sale_amount)` function aggregates the sales for each `region`, and the `GROUP BY` clause organizes the results by region. This approach optimizes performance by reducing the data processed and ensures that the query scales well with large datasets.