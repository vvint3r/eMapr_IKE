SQL Interview Questions
1. What is Pattern Matching in SQL?
SQL pattern matching provides for pattern search in data if you have no clue as to what that word should be. This kind of SQL query uses wildcards to match a string pattern, rather than writing the exact word. The LIKE operator is used in conjunction with SQL Wildcards to fetch the required information.
1 Using the % wildcard to perform a simple search
The % wildcard matches zero or more characters of any type and can be used to define wildcards both before and after the pattern. Search a student in your database with first name beginning with the letter K:
SELECT * FROM students WHERE first_name LIKE 'K%'
1 Omitting the patterns using the NOT keyword
Use the NOT keyword to select records that don't match the pattern. This query returns all students whose first name does not begin with K.
SELECT * FROM students WHERE first_name NOT LIKE 'K%'
1 Matching a pattern anywhere using the % wildcard twice
Search for a student in the database where he/she has a K in his/her first name.
SELECT * FROM students WHERE first_name LIKE '%Q%'
1 Using the _ wildcard to match pattern at a specific position
The _ wildcard matches exactly one character of any type. It can be used in conjunction with % wildcard. This query fetches all students with letter K at the third position in their first name.
SELECT * FROM students WHERE first_name LIKE '__K%'
1 Matching patterns for a specific length
The _ wildcard plays an important role as a limitation when it matches exactly one character. It limits the length and position of the matched results. For example -
SELECT * /* Matches first names with three or more letters */ FROM students WHERE first_name LIKE '___%' SELECT * /* Matches first names with exactly four characters */ FROM students WHERE first_name LIKE '____'
2. How to create empty tables with the same structure as another table?
Creating empty tables with the same structure can be done smartly by fetching the records of one table into a new table using the INTO operator while fixing a WHERE clause to be false for all records. Hence, SQL prepares the new table with a duplicate structure to accept the fetched records but since no records get fetched due to the WHERE clause in action, nothing is inserted into the new table.
SELECT * INTO Students_copy FROM Students WHERE 1 = 2 ;
3. What is a Recursive Stored Procedure?
A stored procedure that calls itself until a boundary condition is reached, is called a recursive stored procedure. This recursive function helps the programmers to deploy the same set of code several times as and when required. Some SQL programming languages limit the recursion depth to prevent an infinite loop of procedure calls from causing a stack overflow, which slows down the system and may lead to system crashes.
DELIMITER $$ /* Set a new delimiter => $$ */ CREATE PROCEDURE calctotal( /* Create the procedure */ IN number INT , /* Set Input and Ouput variables */ OUT total INT ) BEGIN DECLARE score INT DEFAULT NULL ; /* Set the default value => "score" */ SELECT awards FROM achievements /* Update "score" via SELECT query */ WHERE id = number INTO score; IF score IS NULL THEN SET total = 0 ; /* Termination condition */ ELSE CALL calctotal(number + 1 ); /* Recursive call */ SET total = total + score; /* Action after recursion */ END IF; END $$ /* End of procedure */ DELIMITER ; /* Reset the delimiter */
4. What is a Stored Procedure?
A stored procedure is a subroutine available to applications that access a relational database management system (RDBMS). Such procedures are stored in the database data dictionary. The sole disadvantage of stored procedure is that it can be executed nowhere except in the database and occupies more memory in the database server. It also provides a sense of security and functionality as users who can't access the data directly can be granted access via stored procedures.
DELIMITER $$ CREATE PROCEDURE FetchAllStudents() BEGIN SELECT * FROM myDB.students; END $$ DELIMITER ;
5. What is Collation? What are the different types of Collation Sensitivity?
Collation refers to a set of rules that determine how data is sorted and compared. Rules defining the correct character sequence are used to sort the character data. It incorporates options for specifying case sensitivity, accent marks, kana character types, and character width. Below are the different types of collation sensitivity:
1 Case sensitivity:A and a are treated differently.
2 Accent sensitivity:a and á are treated differently.
3 Kana sensitivity: Japanese kana characters Hiragana and Katakana are treated differently.
4 Width sensitivity: Same character represented in single-byte (half-width) and double-byte (full-width) are treated differently.
6. What are the differences between OLTP and OLAP?
OLTP stands for Online Transaction Processing, is a class of software applications capable of supporting transaction-oriented programs. An important attribute of an OLTP system is its ability to maintain concurrency. OLTP systems often follow a decentralized architecture to avoid single points of failure. These systems are generally designed for a large audience of end-users who conduct short transactions. Queries involved in such databases are generally simple, need fast response times, and return relatively few records. A number of transactions per second acts as an effective measure for such systems.
OLAP stands for Online Analytical Processing, a class of software programs that are characterized by the relatively low frequency of online transactions. Queries are often too complex and involve a bunch of aggregations. For OLAP systems, the effectiveness measure relies highly on response time. Such systems are widely used for data mining or maintaining aggregated, historical data, usually in multi-dimensional schemas.
7. What is OLTP?
OLTP stands for Online Transaction Processing, is a class of software applications capable of supporting transaction-oriented programs. An essential attribute of an OLTP system is its ability to maintain concurrency. To avoid single points of failure, OLTP systems are often decentralized. These systems are usually designed for a large number of users who conduct short transactions. Database queries are usually simple, require sub-second response times, and return relatively few records. Here is an insight into the working of an OLTP system [ Note - The figure is not important for interviews ] -
8. What is User-defined function? What are its various types?
The user-defined functions in SQL are like functions in any other programming language that accept parameters, perform complex calculations, and return a value. They are written to use the logic repetitively whenever required. There are two types of SQL user-defined functions:
1 Scalar Function: As explained earlier, user-defined scalar functions return a single scalar value.
Table-Valued Functions: User-defined table-valued functions return a table as output.
1 Inline: returns a table data type based on a single SELECT statement.
2 Multi-statement: returns a tabular result-set but, unlike inline, multiple SELECT statements can be used inside the function body.
9. What is a UNIQUE constraint?
A UNIQUE constraint ensures that all values in a column are different. This provides uniqueness for the column(s) and helps identify each row uniquely. Unlike primary key, there can be multiple unique constraints defined per table. The code syntax for UNIQUE is quite similar to that of PRIMARY KEY and can be used interchangeably.
CREATE TABLE Students ( /* Create table with a single field as unique */ ID INT NOT NULL UNIQUE Name VARCHAR ( 255 ) ); CREATE TABLE Students ( /* Create table with multiple fields as unique */ ID INT NOT NULL LastName VARCHAR ( 255 ) FirstName VARCHAR ( 255 ) NOT NULL CONSTRAINT PK_Student UNIQUE (ID, FirstName) ); ALTER TABLE Students /* Set a column as unique */ ADD UNIQUE (ID); ALTER TABLE Students /* Set multiple columns as unique */ ADD CONSTRAINT PK_Student /* Naming a unique constraint */ UNIQUE (ID, FirstName);
10. What is a Query?
A query is a request for data or information from a database table or combination of tables. A database query can be either a select query or an action query.
SELECT fname, lname /* select query */ FROM myDb.students WHERE student_id = 1 ; UPDATE myDB.students /* action query */ SET fname = 'Captain' , lname = 'America' WHERE student_id = 1 ;
11. What is Data Integrity?
Data Integrity is the assurance of accuracy and consistency of data over its entire life-cycle and is a critical aspect of the design, implementation, and usage of any system which stores, processes, or retrieves data. It also defines integrity constraints to enforce business rules on the data when it is entered into an application or a database.
Practice Problems
Solve these problems to ace this concept
Engineers Joined Easy 25.6 Mins Solve Job Offer Hard 24.22 Mins Solve 12. What is the difference between Clustered and Non-clustered index?
As explained above, the differences can be broken down into three small factors -
1 Clustered index modifies the way records are stored in a database based on the indexed column. A non-clustered index creates a separate entity within the table which references the original table.
2 Clustered index is used for easy and speedy retrieval of data from the database, whereas, fetching records from the non-clustered index is relatively slower.
3 In SQL, a table can have a single clustered index whereas it can have multiple non-clustered indexes.
13. What is an Index? Explain its different types.
A database index is a data structure that provides a quick lookup of data in a column or columns of a table. It enhances the speed of operations accessing data from a database table at the cost of additional writes and memory to maintain the index data structure.
CREATE INDEX index_name /* Create Index */ ON table_name (column_1, column_2); DROP INDEX index_name; /* Drop Index */
There are different types of indexes that can be created for different purposes:
1 Unique and Non-Unique Index:
Unique indexes are indexes that help maintain data integrity by ensuring that no two rows of data in a table have identical key values. Once a unique index has been defined for a table, uniqueness is enforced whenever keys are added or changed within the index.
CREATE UNIQUE INDEX myIndex ON students (enroll_no);
Non-unique indexes, on the other hand, are not used to enforce constraints on the tables with which they are associated. Instead, non-unique indexes are used solely to improve query performance by maintaining a sorted order of data values that are used frequently.
1 Clustered and Non-Clustered Index:
Clustered indexes are indexes whose order of the rows in the database corresponds to the order of the rows in the index. This is why only one clustered index can exist in a given table, whereas, multiple non-clustered indexes can exist in the table.
The only difference between clustered and non-clustered indexes is that the database manager attempts to keep the data in the database in the same order as the corresponding keys appear in the clustered index.
Clustering indexes can improve the performance of most query operations because they provide a linear-access path to data stored in the database.
14. What is a Cross-Join?
Cross join can be defined as a cartesian product of the two tables included in the join. The table after join contains the same number of rows as in the cross-product of the number of rows in the two tables. If a WHERE clause is used in cross join then the query will work like an INNER JOIN.
SELECT stu.name, sub.subject FROM students AS stu CROSS JOIN subjects AS sub;
15. What is a Self-Join?
A self JOIN is a case of regular join where a table is joined to itself based on some relation between its own column(s). Self-join uses the INNER JOIN or LEFT JOIN clause and a table alias is used to assign different names to the table within the query.
SELECT A.emp_id AS "Emp_ID",A.emp_name AS "Employee", B.emp_id AS "Sup_ID",B.emp_name AS "Supervisor" FROM employee A, employee B WHERE A.emp_sup = B.emp_id;
16. What is a Join? List its different types.
The SQL Join clause is used to combine records (rows) from two or more tables in a SQL database based on a related column between the two.
There are four different types of JOINs in SQL:
1 (INNER) JOIN: Retrieves records that have matching values in both tables involved in the join. This is the widely used join for queries.
SELECT * FROM Table_A JOIN Table_B; SELECT * FROM Table_A INNER JOIN Table_B;
1 LEFT (OUTER) JOIN: Retrieves all the records/rows from the left and the matched records/rows from the right table.
SELECT * FROM Table_A A LEFT JOIN Table_B B ON A.col = B.col;
1 RIGHT (OUTER) JOIN: Retrieves all the records/rows from the right and the matched records/rows from the left table.
SELECT * FROM Table_A A RIGHT JOIN Table_B B ON A.col = B.col;
1 FULL (OUTER) JOIN: Retrieves all the records where there is a match in either the left or right table.
SELECT * FROM Table_A A FULL JOIN Table_B B ON A.col = B.col;
17. What is a Foreign Key?
A FOREIGN KEY comprises of single or collection of fields in a table that essentially refers to the PRIMARY KEY in another table. Foreign key constraint ensures referential integrity in the relation between two tables. The table with the foreign key constraint is labeled as the child table, and the table containing the candidate key is labeled as the referenced or parent table.
CREATE TABLE Students ( /* Create table with foreign key - Way 1 */ ID INT NOT NULL Name VARCHAR ( 255 ) LibraryID INT PRIMARY KEY (ID) FOREIGN KEY (Library_ID) REFERENCES Library(LibraryID) ); CREATE TABLE Students ( /* Create table with foreign key - Way 2 */ ID INT NOT NULL PRIMARY KEY Name VARCHAR ( 255 ) LibraryID INT FOREIGN KEY (Library_ID) REFERENCES Library(LibraryID) ); ALTER TABLE Students /* Add a new foreign key */ ADD FOREIGN KEY (LibraryID) REFERENCES Library (LibraryID);
18. What is a Subquery? What are its types?
A subquery is a query within another query, also known as a nested query or inner query. It is used to restrict or enhance the data to be queried by the main query, thus restricting or enhancing the output of the main query respectively. For example, here we fetch the contact information for students who have enrolled for the maths subject:
SELECT name, email, mob, address FROM myDb.contacts WHERE roll_no IN ( SELECT roll_no FROM myDb.students WHERE subject = 'Maths' );
There are two types of subquery - Correlated and Non-Correlated.
1 A correlated subquery cannot be considered as an independent query, but it can refer to the column in a table listed in the FROM of the main query.
2 A non-correlated subquery can be considered as an independent query and the output of the subquery is substituted in the main query.
Practice Problems
Solve these problems to ace this concept
Study Selection Medium 9.21 Mins Solve Job Offers 2.0 Hard 25.42 Mins Solve 19. What is a Primary Key?
The PRIMARY KEY constraint uniquely identifies each row in a table. It must contain UNIQUE values and has an implicit NOT NULL constraint. A table in SQL is strictly restricted to have one and only one primary key, which is comprised of single or multiple fields (columns).
CREATE TABLE Students ( /* Create table with a single field as primary key */ ID INT NOT NULL Name VARCHAR ( 255 ) PRIMARY KEY (ID) ); CREATE TABLE Students ( /* Create table with multiple fields as primary key */ ID INT NOT NULL LastName VARCHAR ( 255 ) FirstName VARCHAR ( 255 ) NOT NULL , CONSTRAINT PK_Student PRIMARY KEY (ID, FirstName) ); ALTER TABLE Students /* Set a column as primary key */ ADD PRIMARY KEY (ID); ALTER TABLE Students /* Set multiple columns as primary key */ ADD CONSTRAINT PK_Student /*Naming a Primary Key*/ PRIMARY KEY (ID, FirstName);
Practice Problems
Solve these problems to ace this concept
Student Query Easy 8.6 Mins Solve Country Filtration Easy 6.8 Mins Solve 20. What are Constraints in SQL?
Constraints are used to specify the rules concerning data in the table. It can be applied for single or multiple fields in an SQL table during the creation of the table or after creating using the ALTER TABLE command. The constraints are:
1 NOT NULL - Restricts NULL value from being inserted into a column.
2 CHECK - Verifies that all values in a field satisfy a condition.
3 DEFAULT - Automatically assigns a default value if no value has been specified for the field.
4 UNIQUE - Ensures unique values to be inserted into the field.
5 INDEX - Indexes a field providing faster retrieval of records.
6 PRIMARY KEY - Uniquely identifies each record in a table.
7 FOREIGN KEY - Ensures referential integrity for a record in another table.
21. What are Tables and Fields?
A table is an organized collection of data stored in the form of rows and columns. Columns can be categorized as vertical and rows as horizontal. The columns in a table are called fields while the rows can be referred to as records.
22. What is the difference between SQL and MySQL?
SQL is a standard language for retrieving and manipulating structured databases. On the contrary, MySQL is a relational database management system, like SQL Server, Oracle or IBM DB2, that is used to manage SQL databases.
23. What is SQL?
SQL stands for Structured Query Language. It is the standard language for relational database management systems. It is especially useful in handling organized data comprised of entities (variables) and relations between different entities of the data.
24. What is RDBMS? How is it different from DBMS?
RDBMS stands for Relational Database Management System. The key difference here, compared to DBMS, is that RDBMS stores data in the form of a collection of tables, and relations can be defined between the common fields of these tables. Most modern database management systems like MySQL, Microsoft SQL Server, Oracle, IBM DB2, and Amazon Redshift are based on RDBMS.
25. What is DBMS?
DBMS stands for Database Management System. DBMS is a system software responsible for the creation, retrieval, updation, and management of the database. It ensures that our data is consistent, organized, and is easily accessible by serving as an interface between the database and its end-users or application software.
26. What is Database?
A database is an organized collection of data, stored and retrieved digitally from a remote or local computer system. Databases can be vast and complex, and such databases are developed using fixed design and modeling approaches.
27. What is the SELECT statement?
SELECT operator in SQL is used to select data from a database. The data returned is stored in a result table, called the result-set.
SELECT * FROM myDB.students;
28. What are some common clauses used with SELECT query in SQL?
Some common SQL clauses used in conjuction with a SELECT query are as follows:
1 WHERE clause in SQL is used to filter records that are necessary, based on specific conditions.
2 ORDER BY clause in SQL is used to sort the records based on some field(s) in ascending (ASC) or descending order (DESC).
SELECT * FROM myDB.students WHERE graduation_year = 2019 ORDER BY studentID DESC ;
1 GROUP BY clause in SQL is used to group records with identical data and can be used in conjunction with some aggregation functions to produce summarized results from the database.
2 HAVING clause in SQL is used to filter records in combination with the GROUP BY clause. It is different from WHERE, since the WHERE clause cannot filter aggregated records.
SELECT COUNT (studentId), country FROM myDB.students WHERE country != "INDIA" GROUP BY country HAVING COUNT (studentID) > 5 ;
29. What are UNION, MINUS and INTERSECT commands?
The UNION operator combines and returns the result-set retrieved by two or more SELECT statements. The MINUS operator in SQL is used to remove duplicates from the result-set obtained by the second SELECT query from the result-set obtained by the first SELECT query and then return the filtered results from the first. The INTERSECT clause in SQL combines the result-set fetched by the two SELECT statements where records from one match the other and then returns this intersection of result-sets.
Certain conditions need to be met before executing either of the above statements in SQL -
1 Each SELECT statement within the clause must have the same number of columns
2 The columns must also have similar data types
3 The columns in each SELECT statement should necessarily have the same order
SELECT name FROM Students /* Fetch the union of queries */ UNION SELECT name FROM Contacts; SELECT name FROM Students /* Fetch the union of queries with duplicates*/ UNION ALL SELECT name FROM Contacts; SELECT name FROM Students /* Fetch names from students */ MINUS /* that aren't present in contacts */ SELECT name FROM Contacts; SELECT name FROM Students /* Fetch names from students */ INTERSECT /* that are present in contacts as well */ SELECT name FROM Contacts;
30. What is Cursor? How to use a Cursor?
A database cursor is a control structure that allows for the traversal of records in a database. Cursors, in addition, facilitates processing after traversal, such as retrieval, addition, and deletion of database records. They can be viewed as a pointer to one row in a set of rows.
Working with SQL Cursor:
1 DECLARE a cursor after any variable declaration. The cursor declaration must always be associated with a SELECT Statement.
2 Open cursor to initialize the result set. The OPEN statement must be called before fetching rows from the result set.
3 FETCH statement to retrieve and move to the next row in the result set.
4 Call the CLOSE statement to deactivate the cursor.
5 Finally use the DEALLOCATE statement to delete the cursor definition and release the associated resources.
DECLARE @name VARCHAR ( 50 ) /* Declare All Required Variables */ DECLARE db_cursor CURSOR FOR /* Declare Cursor Name*/ SELECT name FROM myDB.students WHERE parent_name IN ( 'Sara' , 'Ansh' ) OPEN db_cursor /* Open cursor and Fetch data into @name */ FETCH next FROM db_cursor INTO @name CLOSE db_cursor /* Close the cursor and deallocate the resources */ DEALLOCATE db_cursor
31. What are Entities and Relationships?
Entity: An entity can be a real-world object, either tangible or intangible, that can be easily identifiable. For example, in a college database, students, professors, workers, departments, and projects can be referred to as entities. Each entity has some associated properties that provide it an identity.
Relationships: Relations or links between entities that have something to do with each other. For example - The employee's table in a company's database can be associated with the salary table in the same database.
32. List the different types of relationships in SQL.
1 One-to-One - This can be defined as the relationship between two tables where each record in one table is associated with the maximum of one record in the other table.
2 One-to-Many & Many-to-One - This is the most commonly used relationship where a record in a table is associated with multiple records in the other table.
3 Many-to-Many - This is used in cases when multiple instances on both sides are needed for defining a relationship.
4 Self-Referencing Relationships - This is used when a table needs to define a relationship with itself.
33. What is an Alias in SQL?
An alias is a feature of SQL that is supported by most, if not all, RDBMSs. It is a temporary name assigned to the table or table column for the purpose of a particular SQL query. In addition, aliasing can be employed as an obfuscation technique to secure the real names of database fields. A table alias is also called a correlation name.
An alias is represented explicitly by the AS keyword but in some cases, the same can be performed without it as well. Nevertheless, using the AS keyword is always a good practice.
SELECT A.emp_name AS "Employee" /* Alias using AS keyword */ B.emp_name AS "Supervisor" FROM employee A, employee B /* Alias without AS keyword */ WHERE A.emp_sup = B.emp_id;
34. What is a View?
A view in SQL is a virtual table based on the result-set of an SQL statement. A view contains rows and columns, just like a real table. The fields in a view are fields from one or more real tables in the database.
35. What is Normalization?
Normalization represents the way of organizing structured data in the database efficiently. It includes the creation of tables, establishing relationships between them, and defining rules for those relationships. Inconsistency and redundancy can be kept in check based on these rules, hence, adding flexibility to the database.
36. What is Denormalization?
Denormalization is the inverse process of normalization, where the normalized schema is converted into a schema that has redundant information. The performance is improved by using redundancy and keeping the redundant data consistent. The reason for performing denormalization is the overheads produced in the query processor by an over-normalized structure.
37. What are the various forms of Normalization?
Normal Forms are used to eliminate or reduce redundancy in database tables. The different forms are as follows:
1 First Normal Form:A relation is in first normal form if every attribute in that relation is a single-valued attribute. If a relation contains a composite or multi-valued attribute, it violates the first normal form. Let's consider the following students table. Each student in the table, has a name, his/her address, and the books they issued from the public library -
Students Table
Student Address Books Issued Salutation Sara Amanora Park Town 94 Until the Day I Die (Emily Carpenter), Inception (Christopher Nolan) Ms. Ansh 62nd Sector A-10 The Alchemist (Paulo Coelho), Inferno (Dan Brown) Mr. Sara 24th Street Park Avenue Beautiful Bad (Annie Ward), Woman 99 (Greer Macallister) Mrs. Ansh Windsor Street 777 Dracula (Bram Stoker) Mr.
As we can observe, the Books Issued field has more than one value per record, and to convert it into 1NF, this has to be resolved into separate individual records for each book issued. Check the following table in 1NF form -
Students Table (1st Normal Form)
Student Address Books Issued Salutation Sara Amanora Park Town 94 Until the Day I Die (Emily Carpenter) Ms. Sara Amanora Park Town 94 Inception (Christopher Nolan) Ms. Ansh 62nd Sector A-10 The Alchemist (Paulo Coelho) Mr. Ansh 62nd Sector A-10 Inferno (Dan Brown) Mr. Sara 24th Street Park Avenue Beautiful Bad (Annie Ward) Mrs. Sara 24th Street Park Avenue Woman 99 (Greer Macallister) Mrs. Ansh Windsor Street 777 Dracula (Bram Stoker) Mr.
1 Second Normal Form:
A relation is in second normal form if it satisfies the conditions for the first normal form and does not contain any partial dependency. A relation in 2NF has no partial dependency, i.e., it has no non-prime attribute that depends on any proper subset of any candidate key of the table. Often, specifying a single column Primary Key is the solution to the problem. Examples -
Example 1 - Consider the above example. As we can observe, the Students Table in the 1NF form has a candidate key in the form of [Student, Address] that can uniquely identify all records in the table. The field Books Issued (non-prime attribute) depends partially on the Student field. Hence, the table is not in 2NF. To convert it into the 2nd Normal Form, we will partition the tables into two while specifying a new Primary Key attribute to identify the individual records in the Students table. The Foreign Key constraint will be set on the other table to ensure referential integrity.
Students Table (2nd Normal Form)
Student_ID Student Address Salutation 1 Sara Amanora Park Town 94 Ms. 2 Ansh 62nd Sector A-10 Mr. 3 Sara 24th Street Park Avenue Mrs. 4 Ansh Windsor Street 777 Mr.
Books Table (2nd Normal Form)
Student_ID Book Issued 1 Until the Day I Die (Emily Carpenter) 1 Inception (Christopher Nolan) 2 The Alchemist (Paulo Coelho) 2 Inferno (Dan Brown) 3 Beautiful Bad (Annie Ward) 3 Woman 99 (Greer Macallister) 4 Dracula (Bram Stoker)
Example 2 - Consider the following dependencies in relation to R(W,X,Y,Z)
WX - > Y [W and X together determine Y] XY - > Z [X and Y together determine Z]
Here, WX is the only candidate key and there is no partial dependency, i.e., any proper subset of WX doesn’t determine any non-prime attribute in the relation.
1 Third Normal Form
A relation is said to be in the third normal form, if it satisfies the conditions for the second normal form and there is no transitive dependency between the non-prime attributes, i.e., all non-prime attributes are determined only by the candidate keys of the relation and not by any other non-prime attribute.
Example 1 - Consider the Students Table in the above example. As we can observe, the Students Table in the 2NF form has a single candidate key Student_ID (primary key) that can uniquely identify all records in the table. The field Salutation (non-prime attribute), however, depends on the Student Field rather than the candidate key. Hence, the table is not in 3NF. To convert it into the 3rd Normal Form, we will once again partition the tables into two while specifying a new Foreign Key constraint to identify the salutations for individual records in the Students table. The Primary Key constraint for the same will be set on the Salutations table to identify each record uniquely.
Students Table (3rd Normal Form)
Student_ID Student Address Salutation_ID 1 Sara Amanora Park Town 94 1 2 Ansh 62nd Sector A-10 2 3 Sara 24th Street Park Avenue 3 4 Ansh Windsor Street 777 1
Books Table (3rd Normal Form)
Student_ID Book Issued 1 Until the Day I Die (Emily Carpenter) 1 Inception (Christopher Nolan) 2 The Alchemist (Paulo Coelho) 2 Inferno (Dan Brown) 3 Beautiful Bad (Annie Ward) 3 Woman 99 (Greer Macallister) 4 Dracula (Bram Stoker)
Salutations Table (3rd Normal Form)
Salutation_ID Salutation 1 Ms. 2 Mr. 3 Mrs.
Example 2 - Consider the following dependencies in relation to R(P,Q,R,S,T)
P - > QR [P together determine C] RS - > T [B and C together determine D] Q - > S T - > P
For the above relation to exist in 3NF, all possible candidate keys in the above relation should be {P, RS, QR, T}.
1 Boyce-Codd Normal Form
A relation is in Boyce-Codd Normal Form if satisfies the conditions for third normal form and for every functional dependency, Left-Hand-Side is super key. In other words, a relation in BCNF has non-trivial functional dependencies in form X –> Y, such that X is always a super key. For example - In the above example, Student_ID serves as the sole unique identifier for the Students Table and Salutation_ID for the Salutations Table, thus these tables exist in BCNF. The same cannot be said for the Books Table and there can be several books with common Book Names and the same Student_ID.
38. What are the TRUNCATE, DELETE and DROP statements?
DELETE statement is used to delete rows from a table.
DELETE FROM Candidates WHERE CandidateId > 1000 ;
TRUNCATE command is used to delete all the rows from the table and free the space containing the table.
TRUNCATE TABLE Candidates;
DROP command is used to remove an object from the database. If you drop a table, all the rows in the table are deleted and the table structure is removed from the database.
DROP TABLE Candidates;
39. What is the difference between DROP and TRUNCATE statements?
If a table is dropped, all things associated with the tables are dropped as well. This includes - the relationships defined on the table with other tables, the integrity checks and constraints, access privileges and other grants that the table has. To create and use the table again in its original form, all these relations, checks, constraints, privileges and relationships need to be redefined. However, if a table is truncated, none of the above problems exist and the table retains its original structure.
40. What is the difference between DELETE and TRUNCATE statements?
The TRUNCATE command is used to delete all the rows from the table and free the space containing the table. The DELETE command deletes only the rows from the table based on the condition given in the where clause or deletes all the rows from the table if no condition is specified. But it does not free the space containing the table.
41. What are Aggregate and Scalar functions?
An aggregate function performs operations on a collection of values to return a single scalar value. Aggregate functions are often used with the GROUP BY and HAVING clauses of the SELECT statement. Following are the widely used SQL aggregate functions:
1 AVG() - Calculates the mean of a collection of values.
2 COUNT() - Counts the total number of records in a specific table or view.
3 MIN() - Calculates the minimum of a collection of values.
4 MAX() - Calculates the maximum of a collection of values.
5 SUM() - Calculates the sum of a collection of values.
6 FIRST() - Fetches the first element in a collection of values.
7 LAST() - Fetches the last element in a collection of values.
Note: All aggregate functions described above ignore NULL values except for the COUNT function.
A scalar function returns a single value based on the input value. Following are the widely used SQL scalar functions:
1 LEN() - Calculates the total length of the given field (column).
2 UCASE() - Converts a collection of string values to uppercase characters.
3 LCASE() - Converts a collection of string values to lowercase characters.
4 MID() - Extracts substrings from a collection of string values in a table.
5 CONCAT() - Concatenates two or more strings.
6 RAND() - Generates a random collection of numbers of a given length.
7 ROUND() - Calculates the round-off integer value for a numeric field (or decimal point values).
8 NOW() - Returns the current date & time.
9 FORMAT() - Sets the format to display a collection of values.
PostgreSQL Interview Questions
42. What is PostgreSQL?
PostgreSQL was first called Postgres and was developed by a team led by Computer Science Professor Michael Stonebraker in 1986. It was developed to help developers build enterprise-level applications by upholding data integrity by making systems fault-tolerant. PostgreSQL is therefore an enterprise-level, flexible, robust, open-source, and object-relational DBMS that supports flexible workloads along with handling concurrent users. It has been consistently supported by the global developer community. Due to its fault-tolerant nature, PostgreSQL has gained widespread popularity among developers.
43. What is the capacity of a table in PostgreSQL?
The maximum size of PostgreSQL is 32TB.
44. What is the importance of the TRUNCATE statement?
TRUNCATE TABLE name_of_table statement removes the data efficiently and quickly from the table. The truncate statement can also be used to reset values of the identity columns along with data cleanup as shown below:
TRUNCATE TABLE name_of_table RESTART IDENTITY;
We can also use the statement for removing data from multiple tables all at once by mentioning the table names separated by comma as shown below:
TRUNCATE TABLE table_1, table_2, table_3;
45. Define tokens in PostgreSQL?
A token in PostgreSQL is either a keyword, identifier, literal, constant, quotes identifier, or any symbol that has a distinctive personality. They may or may not be separated using a space, newline or a tab. If the tokens are keywords, they are usually commands with useful meanings. Tokens are known as building blocks of any PostgreSQL code.
46. What are partitioned tables called in PostgreSQL?
Partitioned tables are logical structures that are used for dividing large tables into smaller structures that are called partitions. This approach is used for effectively increasing the query performance while dealing with large database tables. To create a partition, a key called partition key which is usually a table column or an expression, and a partitioning method needs to be defined. There are three types of inbuilt partitioning methods provided by Postgres:
1 Range Partitioning: This method is done by partitioning based on a range of values. This method is most commonly used upon date fields to get monthly, weekly or yearly data. In the case of corner cases like value belonging to the end of the range, for example: if the range of partition 1 is 10-20 and the range of partition 2 is 20-30, and the given value is 10, then 10 belongs to the second partition and not the first.
2 List Partitioning: This method is used to partition based on a list of known values. Most commonly used when we have a key with a categorical value. For example, getting sales data based on regions divided as countries, cities, or states.
3 Hash Partitioning: This method utilizes a hash function upon the partition key. This is done when there are no specific requirements for data division and is used to access data individually. For example, you want to access data based on a specific product, then using hash partition would result in the dataset that we require.
The type of partition key and the type of method used for partitioning determines how positive the performance and the level of manageability of the partitioned table are.
47. How can we start, restart and stop the PostgreSQL server?
1 To start the PostgreSQL server, we run:
service postgresql start
1 Once the server is successfully started, we get the below message:
Starting PostgreSQL: ok
1 To restart the PostgreSQL server, we run:
service postgresql restart
Once the server is successfully restarted, we get the message:
Restarting PostgreSQL: server stopped ok
1 To stop the server, we run the command:
service postgresql stop
Once stopped successfully, we get the message:
Stopping PostgreSQL: server stopped ok
48. What is the command used for creating a database in PostgreSQL?
The first step of using PostgreSQL is to create a database. This is done by using the createdb command as shown below: createdb db_name After running the above command, if the database creation was successful, then the below message is shown:
CREATE DATABASE
49. How will you change the datatype of a column?
This can be done by using the ALTER TABLE statement as shown below:
Syntax:
ALTER TABLE tname ALTER COLUMN col_name [SET DATA] TYPE new_data_type;
50. How do you define Indexes in PostgreSQL?
Indexes are the inbuilt functions in PostgreSQL which are used by the queries to perform search more efficiently on a table in the database. Consider that you have a table with thousands of records and you have the below query that only a few records can satisfy the condition, then it will take a lot of time to search and return those rows that abide by this condition as the engine has to perform the search operation on every single to check this condition. This is undoubtedly inefficient for a system dealing with huge data. Now if this system had an index on the column where we are applying search, it can use an efficient method for identifying matching rows by walking through only a few levels. This is called indexing.
Select * from some_table where table_col=120
51. Define sequence.
A sequence is a schema-bound, user-defined object which aids to generate a sequence of integers. This is most commonly used to generate values to identity columns in a table. We can create a sequence by using the CREATE SEQUENCE statement as shown below:
CREATE SEQUENCE serial_num START 100;
To get the next number 101 from the sequence, we use the nextval() method as shown below:
SELECT nextval('serial_num');
We can also use this sequence while inserting new records using the INSERT command:
INSERT INTO ib_table_name VALUES (nextval('serial_num'), 'interviewbit');
52. What are string constants in PostgreSQL?
They are character sequences bound within single quotes. These are using during data insertion or updation to characters in the database. There are special string constants that are quoted in dollars. Syntax: $tag$<string_constant>$tag$ The tag in the constant is optional and when we are not specifying the tag, the constant is called a double-dollar string literal.
53. How can you get a list of all databases in PostgreSQL?
This can be done by using the command \l -> backslash followed by the lower-case letter L.
54. How can you delete a database in PostgreSQL?
This can be done by using the DROP DATABASE command as shown in the syntax below:
DROP DATABASE database_name;
If the database has been deleted successfully, then the following message would be shown:
DROP DATABASE
55. What are ACID properties? Is PostgreSQL compliant with ACID?
ACID stands for Atomicity, Consistency, Isolation, Durability. They are database transaction properties which are used for guaranteeing data validity in case of errors and failures.
1 Atomicity: This property ensures that the transaction is completed in all-or-nothing way.
2 Consistency: This ensures that updates made to the database is valid and follows rules and restrictions.
3 Isolation: This property ensures integrity of transaction that are visible to all other transactions.
4 Durability: This property ensures that the committed transactions are stored permanently in the database.
PostgreSQL is compliant with ACID properties.
56. Can you explain the architecture of PostgreSQL?
1 The architecture of PostgreSQL follows the client-server model.
2 The server side comprises of background process manager, query processer, utilities and shared memory space which work together to build PostgreSQL’s instance that has access to the data. The client application does the task of connecting to this instance and requests data processing to the services. The client can either be GUI (Graphical User Interface) or a web application. The most commonly used client for PostgreSQL is pgAdmin.
57. What do you understand by multi-version concurrency control?
MVCC or Multi-version concurrency control is used for avoiding unnecessary database locks when 2 or more requests tries to access or modify the data at the same time. This ensures that the time lag for a user to log in to the database is avoided. The transactions are recorded when anyone tries to access the content.
For more information regarding this, you can refer here.
58. What do you understand by command enable-debug?
The command enable-debug is used for enabling the compilation of all libraries and applications. When this is enabled, the system processes get hindered and generally also increases the size of the binary file. Hence, it is not recommended to switch this on in the production environment. This is most commonly used by developers to debug the bugs in their scripts and help them spot the issues. For more information regarding how to debug, you can refer here.
59. How do you check the rows affected as part of previous transactions?
SQL standards state that the following three phenomena should be prevented whilst concurrent transactions. SQL standards define 4 levels of transaction isolations to deal with these phenomena.
1 Dirty reads: If a transaction reads data that is written due to concurrent uncommitted transaction, these reads are called dirty reads.
2 Phantom reads: This occurs when two same queries when executed separately return different rows. For example, if transaction A retrieves some set of rows matching search criteria. Assume another transaction B retrieves new rows in addition to the rows obtained earlier for the same search criteria. The results are different.
3 Non-repeatable reads: This occurs when a transaction tries to read the same row multiple times and gets different values each time due to concurrency. This happens when another transaction updates that data and our current transaction fetches that updated data, resulting in different values.
To tackle these, there are 4 standard isolation levels defined by SQL standards. They are as follows:
1 Read Uncommitted – The lowest level of the isolations. Here, the transactions are not isolated and can read data that are not committed by other transactions resulting in dirty reads.
2 Read Committed – This level ensures that the data read is committed at any instant of read time. Hence, dirty reads are avoided here. This level makes use of read/write lock on the current rows which prevents read/write/update/delete of that row when the current transaction is being operated on.
3 Repeatable Read – The most restrictive level of isolation. This holds read and write locks for all rows it operates on. Due to this, non-repeatable reads are avoided as other transactions cannot read, write, update or delete the rows.
4 Serializable – The highest of all isolation levels. This guarantees that the execution is serializable where execution of any concurrent operations are guaranteed to be appeared as executing serially.
The following table clearly explains which type of unwanted reads the levels avoid:
Isolation levels Dirty Reads Phantom Reads Non-repeatable reads Read Uncommitted Might occur Might occur Might occur Read Committed Won’t occur Might occur Might occur Repeatable Read Won’t occur Might occur Won’t occur Serializable Won’t occur Won’t occur Won’t occur
60. What can you tell about WAL (Write Ahead Logging)?
Write Ahead Logging is a feature that increases the database reliability by logging changes before any changes are done to the database. This ensures that we have enough information when a database crash occurs by helping to pinpoint to what point the work has been complete and gives a starting point from the point where it was discontinued.
For more information, you can refer here.
61. What is the main disadvantage of deleting data from an existing table using the DROP TABLE command?
DROP TABLE command deletes complete data from the table along with removing the complete table structure too. In case our requirement entails just remove the data, then we would need to recreate the table to store data in it. In such cases, it is advised to use the TRUNCATE command.
62. How do you perform case-insensitive searches using regular expressions in PostgreSQL?
To perform case insensitive matches using a regular expression, we can use POSIX (~*) expression from pattern matching operators. For example:
'interviewbit' ~* '.*INTervIewBit.*'
63. How will you take backup of the database in PostgreSQL?
We can achieve this by using the pg_dump tool for dumping all object contents in the database into a single file. The steps are as follows:
Step 1: Navigate to the bin folder of the PostgreSQL installation path.
C:\>cd C:\Program Files\PostgreSQL\10.0\bin
Step 2: Execute pg_dump program to take the dump of data to a .tar folder as shown below:
pg_dump -U postgres -W -F t sample_data > C:\Users\admin\pgbackup\sample_data.tar
The database dump will be stored in the sample_data.tar file on the location specified.
64. Does PostgreSQL support full text search?
Full-Text Search is the method of searching single or collection of documents stored on a computer in a full-text based database. This is mostly supported in advanced database systems like SOLR or ElasticSearch. However, the feature is present but is pretty basic in PostgreSQL.
65. What are parallel queries in PostgreSQL?
Parallel Queries support is a feature provided in PostgreSQL for devising query plans capable of exploiting multiple CPU processors to execute the queries faster.
66. Differentiate between commit and checkpoint.
The commit action ensures that the data consistency of the transaction is maintained and it ends the current transaction in the section. Commit adds a new record in the log that describes the COMMIT to the memory. Whereas, a checkpoint is used for writing all changes that were committed to disk up to SCN which would be kept in datafile headers and control files.
Conclusion:
SQL is a language for the database. It has a vast scope and robust capability of creating and manipulating a variety of database objects using commands like CREATE, ALTER, DROP, etc, and also in loading the database objects using commands like INSERT. It also provides options for Data Manipulation using commands like DELETE, TRUNCATE and also does effective retrieval of data using cursor commands like FETCH, SELECT, etc. There are many such commands which provide a large amount of control to the programmer to interact with the database in an efficient way without wasting many resources. The popularity of SQL has grown so much that almost every programmer relies on this to implement their application's storage functionalities thereby making it an exciting language to learn. Learning this provides the developer a benefit of understanding the data structures used for storing the organization's data and giving an additional level of control and in-depth understanding of the application.
PostgreSQL being an open-source database system having extremely robust and sophisticated ACID, Indexing, and Transaction supports has found widespread popularity among the developer community.
References and Resources:
1 PostgreSQL Download
2 PostgreSQL Tutorial
3 SQL Guide
4 SQL Server Interview Questions
5 SQL Query Interview Questions and Answers
6 SQL Interview Questions for Data Science
7 MySQL Interview Questions
8 DBMS Interview Questions
9 PL SQL Interview Questions
10 MongoDB Interview Questions
11 Database Testing Interview Questions
12 SQL Vs MySQL
13 PostgreSQL vs MySQL
14 Difference Between SQL and PLSQL
15 Difference between RDBMS and DBMS
16 SQL Vs NoSQL
17 SQL IDE
18 SQL Projects
19 MySQL Commands
20 SQL Books
21 OLTP vs OLAP
SQL MCQ
1.
An SQL query to delete a table from the database and memory while keeping the structure of the table intact?
2.
What is a pre-requisite for creating a database in PostgreSQL?To create a database in PostgreSQL, you must have the special CREATEDB privilege or
3.
Which of the following is known as a virtual table in SQL?
4.
What is the main advantage of a clustered index over a non-clustered index?
5.
SQL query used to fetch unique values from a field?
6.
Which statement is used to update data in the database?
7.
Which statement is false for the ORDER BY statement?
8.
What statement is used for adding data to PostgreSQL?
9.
Normalization which has neither composite values nor partial dependencies?
10.
What does SQL stand for?
11.
Which statement is true for a PRIMARY KEY constraint?
12.
What is the order of results shown by default if the ASC or DESC parameter is not specified with the ORDER BY command?
13.
What allows us to define how various tables are related to each other formally in a database?
14.
What is the name of the component that requests data to the PostgreSQL server?
15.
What languages are supported by PostgreSQL?
16.
What command is used for restoring the backup of PostgreSQL which was created using pg_dump?
17.
Query to select all records with "bar" in their name?
18.
Which command is used to tell PostgreSQL to make all changes made to the database permanent?
19.
Which statement is false for a FOREIGN KEY constraint?
20.
What is a Query?

---

SQL Interview Questions for Data Analysts
Entry-Level SQL Interview Questions for Data Analysts
These SQL interview questions for data analyst freshers tend to appear early in the hiring funnel—typically in recruiter screens, take-home challenges, or online assessments. While you might get sent a SQL interview questions for data analyst PDF as prep material, remember that companies are testing more than syntax memorization. They’re looking for clarity in how you approach data, how you structure logic, and how well you can generalize patterns across datasets.
Each of these basic SQL interview questions for data analyst roles targets specific foundational skills—whether it’s joining tables, applying aggregation, or extracting time-based insights. Mastering these builds a strong core for more advanced questions later in the process.
What query returns the largest salary in each department?
This exercise checks whether you can aggregate data and apply group-level filters. You need to GROUP BY department_id and use MAX(salary) to capture the highest value per group, then join to a dimension (or add a window function) if the interviewer also wants employee names. It reinforces the idea that every SELECT with a grouped aggregate must include only grouping columns or aggregates. Interviewers listen for discussion of NULL salaries and why casting to DECIMAL might be required when a table mixes currencies. Clear communication of these edge cases shows you understand how basic aggregation supports compensation dashboards.
How would you find the 2nd-highest salary in the engineering department?
A classic ranking problem that proves you can work with ordering and limits. Common answers use DENSE_RANK() or ROW_NUMBER() partitioned by department, filter on =2 , and add WHERE dept_name='engineering' . Candidates should point out that ties at the top push the “true” second value down, making DENSE_RANK() the safer choice. Interviewers like to hear how you’d handle departments with fewer than two employees, perhaps returning NULL or excluding them entirely. Knowing when to use a window function versus a correlated subquery highlights core SQL literacy.
Which neighborhoods have zero registered users?
This anti-join problem tests understanding of NULL handling and set logic. A straightforward LEFT JOIN users u ON n.neighborhood_id = u.neighborhood_id WHERE u.user_id IS NULL surfaces empty areas. Explaining why NOT IN or NOT EXISTS could behave differently when NULLs appear shows grasp of three-valued logic. The query is common in churn or vacancy analyses where managers need to see untouched market segments. Interviewers may ask how adding an index on neighborhood_id speeds results on large city-wide datasets.
How can you return one random car manufacturer with equal probability?
Selecting a uniform random row validates awareness of database-specific random functions ( ORDER BY RANDOM() in Postgres, TABLESAMPLE in BigQuery, etc.). Candidates should mention why adding LIMIT 1 is critical and discuss performance implications when the table grows—e.g., why full sorts can be expensive and how to use a precomputed sampling column. This maps to real features like “pick a random promo” or A/B bucket assignment. Good answers show you can translate business requests into performant SQL rather than relying on naïve approaches.
How much did 2022 sign-ups spend on every product?
Here you join users (filtered on registration_year = 2022 ) to purchases , then sum price * quantity per product_id . It reinforces join direction, date filtering, and grouped aggregation—bread-and-butter skills for any analyst. Explaining why you use an INNER JOIN versus LEFT JOIN (to exclude users with no purchases) demonstrates awareness of how join semantics affect totals. You might also discuss rounding currency and indexing on (user_id, product_id) to keep the query responsive.
How do you calculate the daily average downloads for free vs. paying accounts?
This question couples conditional aggregation with date grouping. You join accounts to downloads , apply COUNT(DISTINCT download_id) or simple COUNT(*) , and divide by distinct account count per plan to get averages—rounding to two decimals. Interviewers expect mention of grouping by download_date and plan_type and why accounts with zero downloads should be excluded per spec. It mirrors real SaaS KPIs like DAU/MAU or usage per subscription tier.
What query returns the maximum quantity bought for every product each year?
The task blends date extraction ( EXTRACT(YEAR FROM order_date) ) with per-product aggregation. You need to group by year and product_id , selecting MAX(quantity) as max_quantity , then order by those keys. Bringing up indexing on (product_id, order_date) and partitioning large fact tables on year helps show you think about scale even for basic metrics. Such year-over-year comparisons are staple requests in retail analytics.
How many days separate each user’s first and last session in 2020?
This problem evaluates use of MIN() and MAX() in one pass per user, or window functions if you prefer. You filter on YEAR(session_date)=2020 , compute the difference in days, and return user_id plus the gap. Candidates should highlight that results may be negative if data quality is bad and suggest placing a composite index on (user_id, session_date) to speed scanning billions of events. It echoes churn analytics where tenure length influences retention models.
How do you compute the average order value by gender?
Joining customer attributes to transaction totals and then grouping by gender tests basic join logic plus conditional counting (only users who have ever placed an order). You sum order_amount per user, divide by order count, and round to two decimals. Interviewers note whether you handle NULL genders and whether you use a subquery or CTE for clarity. Such demographic breakdowns appear daily in e-commerce BI work.
What share of Apple-platform actions ranked in the top-5 during November 2020?
You must filter on platform, restrict to November 2020, aggregate counts, then rank with DENSE_RANK . Handling ties properly and producing an ordered output shows mastery of grouping plus ranking logic in real engagement analyses.
How would you flag each purchase as either the customer’s first or a repeat in its product category?
Interviewers want to see whether you can leverage window functions ( ROW_NUMBER() or MIN(id) OVER (PARTITION BY user_id, category) ) to mark a “first” versus subsequent purchase, then cast that boolean into a tidy feature column. A good solution joins no extra tables, sorts by purchase time, and explains why session-level deduping isn’t needed. Mentioning that this repeat-purchase label later feeds retention analyses shows business awareness while keeping the SQL lightweight.
Given wireless packet logs, how can you return—per SSID—the largest number of packets any single device sent in the first ten minutes of 1 Jan 2022?
The query filters on the timestamp window, groups by both ssid and device_id , counts packets, then applies MAX() (or ROW_NUMBER() with DESC ordering) per SSID. Explaining that you choose an index on (ssid, created_at) to speed the time filter demonstrates practical sense, yet the core logic remains a straightforward aggregation—squarely entry-level.
What SQL statement gives each user’s total transaction cost, sorted from highest spender to lowest?
This test reinforces simple grouping ( SUM(amount) ) and ordering skills. Candidates should highlight that an INNER JOIN to a product table isn’t necessary unless unit prices live elsewhere, and that NULL amounts require COALESCE to keep sums correct. Such wallet-share rollups are daily fare for junior analysts in fintech or retail data teams.
How would you output, in one result set, the total transaction count, the number of distinct purchasers, the count of “paid” transactions ≥ $100, and the product with the highest paid revenue?
Interviewers are testing whether you can combine scalar subqueries or CTEs into a single select list. A neat answer uses four subqueries—each aggregating differently—while noting why unioning or multiple passes over the table would be less efficient. This “dashboard in one row” pattern appears often in recruiter screens.
Which five user actions ranked highest during Thanksgiving week 2020, and what were their ranks (ties allowed)?
The task mixes filtering on a date range, aggregating counts, and ranking with DENSE_RANK() . Candidates should explain tie handling and why ORDER BY action_count DESC before ranking is crucial. The scenario mirrors common engagement reporting—perfect for junior analysts who’ll build feature-usage tables.
How do you calculate the overall acceptance rate of friend requests, rounded to four decimals?
Solving requires counting total requests versus accepted ones—often via a join or a request_id IN (SELECT …) pattern. Key talking points include integer division pitfalls, the need to cast to DECIMAL , and whether to exclude self-friend edge cases. Simplicity keeps it entry-level, but the precision requirement checks attention to detail.
After discovering duplicate rows in employee_projects , how would you still identify the five priciest projects by budget-to-employee ratio?
A clean answer uses COUNT(DISTINCT employee_id) in the denominator, guarding against duplicates, then orders by the computed ratio and limits to five. The exercise spotlights practical data-quality thinking (deduping) without venturing into advanced optimization, making it a solid capstone basic query for new analysts.
SQL Interview Questions for Experienced Data Analysts
Once you’ve cleared the basics, most data analyst SQL interviews begin to probe deeper into query logic, edge-case reasoning, and optimization skills. For candidates with 3+ years of experience, the expectations go beyond just writing accurate queries. These are the types of SQL query interview questions for data analyst roles that assess how well you can translate business requests into accurate, testable SQL logic.
Expect a mix of hands-on live coding tasks and take-home SQL challenges—especially at companies like Meta, Amazon, or Netflix—where your ability to manipulate data at scale matters just as much as syntax fluency.
These types of SQL interview questions for experienced data analyst roles are especially common in companies handling terabytes of data daily—like e-commerce platforms, fintech firms, and data-driven marketplaces. If you’re looking to ace SQL interview questions for 3 years experience, focus on techniques like query planning, indexing strategy, and intelligent use of CTEs or window functions.
What is the last transaction recorded on each calendar day?
A banking table lists id , transaction_value , and created_at timestamps. Your goal is to pick, for every date, the single transaction with the latest timestamp and output its id, amount, and datetime. This entry-level task teaches the staple window-function pattern— ROW_NUMBER() OVER (PARTITION BY CAST(created_at AS DATE) ORDER BY created_at DESC) —and encourages candidates to consider tie-breakers when two entries share an identical timestamp. Interviewers gauge whether you can partition correctly, convert datetimes to dates, and deliver an ordered result set that business users can trust.
How would you pivot exam results so each student’s four test scores appear on a single row?
A table exam_scores records student-id, exam-id (1-4) and score. You’re asked to reshape the data into a wide format—one row per student, with separate columns for Exam 1 through Exam 4. The prompt reinforces essential entry-level skills: conditional aggregation (or filtered pivots) and null handling when scores are missing. Interviewers love it because it surfaces your mental model of grouping, selective aggregation with CASE, and output formatting for downstream dashboard use. Getting it right demonstrates that you can translate a reporting requirement into clean SQL without over-engineering the solution.
How would you pull a truly random row from a 100-million-row table without overloading the database?
A naïve ORDER BY RANDOM() causes full sorts, so seasoned beginners mention more efficient tricks—sampling by id range, using a random modulo predicate, or leveraging database sampling clauses like TABLESAMPLE BERNOULLI . The question pushes you to reason about performance trade-offs and estimate how long a query might lock. It also opens discussion on why approximate randomness is often “good enough” for dashboards or QA spot-checks.
Which customers have placed more than three transactions in both 2019 and 2020?
You aggregate by (user_id, year) with COUNT(*) , filter on >3 using a CTE, then GROUP BY user_id HAVING COUNT(DISTINCT year)=2 . The task ensures you understand grouping, HAVING filters, and how to pivot year-level conditions into a single pass. It’s the kind of simple cohort query recruiters expect juniors to nail quickly.
Which shipments were delivered during a customer’s membership period, and which were not?
You’re a data scientist on Amazon’s distribution team and must tag each shipment as Y (delivered while the customer was an active member) or N (delivered outside that window). The exercise checks your comfort with conditional joins and date-range logic: you’ll join a customers table that stores membership start and end dates to a shipments table, compare shipment dates against those ranges, and return a tidy report. A correct answer shows you can reason about inclusive vs. exclusive boundaries (edge-case shipments sent on the exact start or end date) and format a boolean output column. It’s a classic entry-level test of CASE expressions, simple joins, and clear communication of business rules. Mastering this pattern prepares you for common “flag-this-row” analytics tasks that pop up in day-to-day work.
How would you list only the duplicate rows in a users table?
Data-cleaning is core to analyst work, and this task checks your ability to spot duplicates using COUNT(*) > 1 in a grouped CTE or ROW_NUMBER() > 1 in a window. You must decide which columns define “duplicate”—often all columns except a surrogate key—and explain why hashing or concatenating fields can be handy. The interviewer looks for discussion of removing rather than just identifying duplicates, highlighting the importance of reproducible ETL pipelines. Mentioning how to add a composite unique index to prevent recurrence shows practical thinking.
Who are the top three highest-earning employees in each department?
Using employees and departments , build a ranked list of the three largest salaries per department, outputting employee full name, department name, and salary. The question probes intermediate query construction: joining reference tables, applying RANK() or DENSE_RANK() , and handling departments with fewer than three staff. A solid answer shows familiarity with window functions, tie-breaking rules, and ordering by multiple fields—skills that quickly separate candidates who only know basic aggregation from those who can craft polished reporting queries.
How many customers were upsold after their initial purchase?
Given a purchases table with timestamps, determine the number of users who bought additional products after their first purchase date (same-day multiple items don’t count). You’ll apply MIN(purchase_date) in a CTE, join back, and filter on later dates. The scenario tests logical thinking around customer behavior funnels and event ordering, plus competence with CTEs and date comparisons. It’s a favorite mid-screen question because the correct query is short yet requires careful reasoning about “first purchase” vs. “later purchase” semantics.
Create a January-2020 histogram of comments per user, with one-comment bins.
From an events table, count how many comments each user left in January 2020, then bucket those counts (0, 1, 2, …) and tally users per bucket. This query forces use of subqueries or CTEs for per-user counts followed by either a GROUP BY on that derived count or a windowed approach. Interviewers want to see if you understand grouping on aggregated results, generating missing buckets (optional), and rounding percentages if requested. It’s representative of product-analytics tasks like building engagement histograms.
What item did each user purchase third?
A transactions table records every order with user_id, item, timestamp, and id. You must return, for every user, the item (or full row) corresponding to their third chronological purchase, breaking timestamp ties with the lower id . The exercise highlights ranking functions ( ROW_NUMBER() ) and tie-handling logic—core abilities for analysts who work with event streams. It also demonstrates your understanding of why deterministic ordering matters when timestamps collide.
How would HR total regular pay, overtime, and overall compensation per role?
Group the payroll table by role_title , SUM(regular_salary) AS reg_pay , SUM(overtime_pay) AS ot_pay , and compute total_comp = reg_pay + ot_pay . Presenting both component and aggregated figures helps budget planning, and comparing each role’s share to company averages can reveal inequities. The paragraph stresses validating that overtime isn’t double-counted and explains how currency conversions or multi-country payrolls complicate roll-ups.
What query totals IT, HR, Marketing, and Other departmental spend by 2023 fiscal quarter?
Create fiscal_qtr = DATE_TRUNC('quarter', txn_date) (or custom fiscal logic), then sum amounts with conditional aggregation: SUM(CASE WHEN dept='IT' THEN amt END) AS it_spend , and group by fiscal_qtr . An “Other” column sums any department not explicitly listed. Finance uses this snapshot to spot over-budget units quickly. Including a quarter index or partition improves performance, and noting how fiscal calendars can differ from calendar quarters shows analyst diligence.
How do you compute a three-day rolling average of steps per user, excluding the first two days?
Partition by user_id , order by step_date , then AVG(steps) OVER (PARTITION BY user_id ORDER BY step_date ROWS BETWEEN 2 PRECEDING AND CURRENT ROW) gives the moving average. Filter out rows where ROW_NUMBER() ≤ 2. Rounding the result matches dashboard display needs. The explanation cautions about missing dates: if gaps exist, analysts might need a calendar table to fill them before windowing.
Which query extracts every user’s third purchase, breaking timestamp ties by the lower transaction ID?
Apply ROW_NUMBER() OVER (PARTITION BY user_id ORDER BY purchase_ts, id) and filter for row_number = 3 . Sorting by (user_id, id) afterwards yields a tidy list. Merchandisers watch this milestone because the third order often signals long-term retention. Highlight that users with fewer than three purchases won’t appear, and note that composite indexing on (user_id, purchase_ts, id) optimizes the window function.
How can you pivot worldwide branch sales so each year is a column?
Start by unifying all yearly branch tables—either with UNION ALL into a common-table expression or by querying a single long-format table if one already exists—so every row holds branch_id, year, total_sales . Apply a conditional aggregation: SUM(CASE WHEN year = 2021 THEN total_sales END) AS sales_2021 , repeating for each year you need, and group by branch_id . This returns one row per branch with sales split across columns, letting executives compare multi-year performance at a glance. Including a fallback “other_year” column or dynamically generating the year list future-proofs the report. A composite index on (branch_id, year) keeps scans fast even with millions of rows.
What query finds the median household income for every city?
Use a window function: partition incomes by city , order by income , then assign row numbers from both the top and bottom ( ROW_NUMBER() and COUNT(*) OVER ). For each city keep rows where the two row numbers meet in the middle; average them when the count is even. This calculation handles any sample size and avoids subqueries that scan the same city repeatedly. Presenting the median rather than the mean gives a better sense of typical earnings when outliers skew the data. Be sure to cast incomes to a numeric type with the right precision to avoid truncation.
How do you compute a 3-day weighted moving average of product sales (0.5, 0.3, 0.2)?
Partition rows by product_id , order by sale_date , and grab the three-day window with LAG() ; then calculate 0.5*curr + 0.3*prev1 + 0.2*prev2 only when both previous rows exist. Filtering with WHERE prev1 IS NOT NULL AND prev2 IS NOT NULL ensures you output dates that have two predecessors. This weighted view smooths volatility while still reacting quickly to recent shifts, which is why analysts prefer it for trend dashboards. Indexes on (product_id, sale_date) guarantee sequential access, and adding ROUND(value, 2) readies the figure for stakeholder slide decks.
How can you classify 2023 sales as Standard, Premium, or Promotional and sum them by region?
A CASE statement encodes the hierarchy: July overrides all as Promotional; East overrides the amount cut-off except in July; otherwise the 2 000 threshold splits Premium and Standard. Sum amount and COUNT(*) by region and the derived sale_type to build the report. Management uses these figures to tune pricing and seasonal promotions. Clearly documenting rule priority prevents logic drift, and partitioning by sale date enables parallel scans for year-long tables.
How would you calculate the number of unpurchased seats on every flight?
Joining flights , planes , and flight_purchases demands careful use of LEFT JOIN and COALESCE() to treat “no purchases” as zeros. Advanced candidates point out that a SKU-like key combining flight date, flight number, and seat number prevents double counting, and they’ll propose materialized views or incremental aggregation to keep the query sub-second for operations teams. Explaining how this metric feeds yield-management optimizations demonstrates business impact awareness.
What query reports 2022 total expenses and the company-wide average per department?
Senior analysts must aggregate at two levels in one result: per-department totals and an overall benchmark. A clean answer uses a window function ( AVG(total_expense) OVER () ) or a cross-join to a sub-aggregate, and discusses why aligning fiscal vs. calendar years matters. Performance commentary—such as partitioning the fact table on expense_date and compressing low-cardinality dept_id —signals experience with real finance datasets.
Which cities have the lowest-quality search results (all ratings < 3)?
The challenge blends grouping with Boolean tests across result sets: you must confirm that all rows per query meet a condition, not just the average. A typical pattern uses MIN(rating) and checks if it’s >=3 , then flips the logic. Interviewers expect discussion of the anti-join alternative and of clustering the table by (query, rating) so that the engine can skip irrelevant blocks—crucial for terabyte-scale search logs.
How many confirmation SMS responses do we receive by carrier and country on 28-Feb-2020?
Real-time marketing teams rely on this metric to detect deliverability issues, so latency and accuracy both matter. You join the latest “confirmation” message per phone number to the confirmations table, group by carrier, country , and count responses. Senior-level answers mention windowing to select “latest” per number, advocate a filtered index on type='confirmation' , and discuss why time-zone normalization is vital when messages span regions.
What share of comments on each ad occurs in the feed versus the “moments” surface?
Solving this requires UNIONing two comment tables, tagging the source, grouping by ad_id , and computing percentages. Advanced interviewees justify using COUNT(DISTINCT comment_id) to avoid duplicates, suggest bitmap indexes on ad_id , and highlight the importance of consistent UTC timestamps when ads run in multiple regions. They may even propose pre-aggregating hourly to power real-time advertiser dashboards.
How do you retrieve each employee’s current salary after an ETL bug inserted yearly updates as new rows?
The fix leverages ROW_NUMBER() over (employee_id ORDER BY salary_effective_date DESC) to isolate the latest row. You then join this CTE back to employees for a clean, deduplicated view. Experienced analysts discuss adding a surrogate key plus ON CONFLICT handling to prevent future drift and consider a covering index (employee_id, salary_effective_date DESC) to support both the query and payroll reports.
How would you build a monthly customer KPI report for 2020 showing user count, transactions, and GMV?
The query aggregates different measures from separate tables, aligns them on a generated date spine, and outputs tidy month-level rows suited for dashboards. Senior answers cover why using a calendar table avoids missing-month gaps, how to handle late-arriving transactions via incremental backfills, and ways to index on (order_date) + (user_signup_date) to keep nightly ETL light.
Which users performed ATM withdrawals exactly 10 seconds apart—indicating possible fraud?
You use LAG() over (user_id ORDER BY created_at) to compute time deltas, then confirm that all consecutive gaps equal 10 seconds. Edge-case handling (single-transaction users) and ordering in the final result ( ORDER BY user_id ) show professionalism. Discussing a composite index (user_id, created_at) and partitioning on created_at to shorten forensic look-backs moves the answer into staff-level territory.
Rank departments with ≥10 employees by percent earning > 100 K, keep top 3.
The query mixes conditional aggregation, filtering on department size, and percentage calculation. Interviewers check numeric precision, use of window functions for ranking, and thoughtful exclusion of small departments—common executive-level reporting nuances.
Find the three lowest-paid employees who have finished ≥2 projects.
The answer requires joining employee, project, and assignment tables, filtering completed projects, and ranking salaries. Interviewers judge your join ordering and ability to eliminate duplicates that inflate project counts.
Fix an ETL bug: retrieve each employee’s latest salary despite duplicate rows.
Seasoned analysts must deploy ROW_NUMBER() OVER (PARTITION BY employee_id ORDER BY salary_date DESC) or a MAX-by-GROUP approach, then discuss auditing steps to prevent future duplication. It tests data-quality instincts alongside SQL chops.
Which users post the same job repeatedly vs. only once?
Counting per-user per-job occurrences, then pivoting into single vs. multiple posters, forces nuanced grouping and CASE aggregation. Senior roles often own marketplace anti-spam metrics like this.
Who are the daily top-3 downloaders, using window RANK ?
Large download_fact tables demand efficient partitioning and thoughtful ordering; follow-ups often cover index or cluster key choices. Experienced analysts should anticipate scale-related pitfalls.
How would you integrate payment, behavior, and fraud logs to improve the system?
The prompt is open-ended: discuss data cleansing, schema unification, entity resolution, and choosing the right join keys. Interviewers want narrative structure—ingestion → validation → feature engineering → insight generation—illustrating that you can plan end-to-end analytics projects beyond a single SQL script.
Compute a weighted campaign score using 0.3 × open rate and 0.7 × click rate.
Beyond writing the SQL, strong candidates justify why click-heavy weighting matters, mention confidence intervals on small sends, and flag bias if opens are auto-filtered by email clients. It marries arithmetic SQL with marketing-domain reasoning.
Describe the data-model migration from a document DB to relational tables.
The scenario expects an architectural narrative: identifying entities (users, friendships, interactions), defining PK/FK constraints, planning backfill ETLs, and outlining read-pattern performance. It evaluates system-thinking more than pure query writing.
What insights and distribution metrics would you build on daily conversation counts?
After outlining potential KPIs (median conversations per user, power-law tails, churn predictors), you must deliver a SQL query producing the per-user daily conversation histogram. The question blends exploratory analytics reasoning with concrete querying—perfect for analysts who straddle data discovery and SQL execution
How would you calculate the total salary paid to employees who never finished a single assigned project ?
You must join employees to projects , flag unfinished work where end_dt IS NULL , and identify staff whose entire project set is unfinished. A CTE that counts finished projects per employee ( COUNT(end_dt) FILTER (WHERE end_dt IS NOT NULL) ) and filters on = 0 is a clean pattern. Summing their salaries shows the true cost of “slackers” and is often used in cost-reduction audits. Good answers also mention excluding employees with no projects and adding an index on (employee_id, end_dt) for speed.
What query returns the running (cumulative) sales total for every product, ordered by product and date?
This task highlights window functions: SUM(price) OVER (PARTITION BY product_id ORDER BY date ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) produces the cumulative field. Grouping isn’t needed—windowing keeps each purchase row intact—so analysts can trend the metric day by day. Discussing how to partition large sales tables (e.g., by month) shows scale awareness. Rounding and casting are minor but necessary finishing touches for reporting.
How many rows would each join type (INNER, LEFT, RIGHT, CROSS) return when you join all ads to the top-3 ads subquery?
You first build a CTE top_ads using ORDER BY popularity LIMIT 3 , then create four separate SELECT COUNT(*) blocks labeled by join type. The puzzle forces you to reason about join mechanics—INNER should give 3, LEFT gives N, RIGHT gives 3, and CROSS gives 3 × N—rather than just write syntax. Seasoned answers mention why RIGHT joins aren’t supported in some systems and how nulls affect counts. It doubles as a quick sanity-check of a candidate’s join intuition.
Which user has the highest average number of unique item categories per order?
You’ll need a two-stage aggregate: first count distinct categories inside each order, then average that count per user. Ordering the result and limiting to 1 returns the winning user. The question checks comfort with nested aggregation, COUNT(DISTINCT …) , and the difference between order-level and user-level granularity. Mentioning tie-handling and potential use of WITH TIES shows polish.
Which two students scored closest to each other on the SAT, and what was the score gap?
A neat approach self-joins the table on score differences or uses window functions with LAG() after ordering by score. You then pick the minimum absolute difference and, if ties remain, return the alphabetically higher pair. This tests ranking logic, tie-breaking, and string comparison. Explaining why an index on score speeds the inner difference scan gets bonus points.
What percentage of users held “Data Analyst” immediately before “Data Scientist”?
Using ROW_NUMBER() or LEAD() over each user’s job-history ordered by start date lets you compare adjacent titles. Count matches where the earlier row is “Data Analyst” and the next row is “Data Scientist,” divide by total distinct users, and round. The prompt tests your ability to work with ordered event data and compute conditional ratios. Call out pitfalls like overlapping date ranges or simultaneous titles.
On what earliest date did each user listen to their third unique song, and what was that song?
You partition by user_id , rank distinct song_id by first-play date, and filter for rank 3, leaving nulls for users below the threshold. Handling distinctness inside the window ( ROW_NUMBER() OVER … PARTITION BY user_id ORDER BY MIN(play_dt) ) shows finesse. Interviewers like hearing about edge cases—repeated plays of the same song—and why a surrogate key on (user_id, song_id) accelerates the query.
What fraction of 12/31/2019 active accounts closed on 1/1/2020?
You identify the active cohort on December 31, join to January 1 statuses, count those whose status changed to “closed,” and divide by the cohort total. Rounding to two decimals matches finance reporting standards. Discussing how to index (account_id, ds) for daily status snapshots demonstrates practical performance thinking. It also surfaces the need to guard against duplicate daily rows.
How would you label each user’s attribution as “paid” or “organic” based on prior Facebook or Google visits?
A subquery that checks EXISTS (SELECT 1 FROM visits WHERE user_id=… AND source IN ('facebook','google')) feeds a simple CASE statement. The exercise is tiny but reveals your clarity on boolean logic and set membership. Advanced candidates bring up deduplicating multi-channel visits and the importance of visit timing relative to conversion. It’s foundational for marketing analytics pipelines.
How do you assign session numbers to events when a session is ≤ 60 minutes of inactivity?
Use LAG(event_ts) per user to compute minute gaps, flag starts where gap > 60, then apply a running SUM() of those flags to generate session_id . This classic pattern tests mastery of window functions for stateful labeling. Mentioning timezone normalization and indexing (user_id, event_ts) shows real-world savvy. Edge-case awareness—like back-to-back identical timestamps—also impresses.
Which ongoing projects are forecast to go “over-budget” versus “within budget”?
You prorate each employee’s salary to project duration, sum per project, and compare to the budget. A CASE label outputs the status, making it useful for dashboards. The math forces candidates to convert annual salaries into daily costs and handle half-year examples correctly. Seasoned answers discuss assuming a 365-day divisor and suggest materializing salary snapshots for long projects.
What was the month-over-month revenue change for every month in 2019?
Aggregate revenue by month, then apply LAG(total_rev) to compute the change and ROUND(…,2) for presentation. Candidates must filter to 2019, handle January’s null prior month, and decide between absolute or percentage change. Performance-minded folks note that partitioning the transactions table by date keeps annual scans light.
Which products cost more than their own average transaction total?
A per-product CTE calculates AVG(price*quantity) as avg_total ; the outer query joins to products and filters on product_price > avg_total . Rounding both numeric columns to two decimals matches stakeholder expectations. The problem checks understanding of self-referential filters and grouped aggregates. Bringing up indexed materialized views for large SKU catalogs adds senior-level depth.
If you’re preparing offline, you can also use a downloadable SQL practice questions for data analyst interview PDF or worksheet. The best prep resources go beyond copy-paste queries—they explain why each query matters in a business context.
Let these questions guide your prep, but make sure to actually write and run them in a real SQL editor. Practice under realistic constraints is what separates candidates who pass from those who almost do.
Scenario-Based and Advanced SQL Questions
As you move into mid-level and senior data analyst roles, SQL interviews go beyond basic queries. Companies expect you to handle multi-table joins, create optimized CTEs, work with window functions, and debug slow queries in production. These advanced SQL interview questions for data analysts reflect business-critical scenarios where precision, scalability, and business logic all matter.
Many SQL scenario-based interview questions for data analyst roles also test your ability to reason about trade-offs—e.g., filtering before or after joins, dealing with nulls, or ensuring referential integrity. Below are examples that simulate these real-world demands.
How many minutes did each plane spend in the air on every calendar day?
This problem forces you to convert departure and arrival timestamps into minute-level duration, group by both plane_id and flight date, and round down fractional minutes. It rewards analysts who can spot hidden pitfalls—overnight flights that straddle dates, missing data, or daylight-saving jumps—and who suggest placing a composite index on (plane_id, departure_ts) to avoid full table scans during daily ETL. Interviewers also look for discussion of window vs. aggregation trade-offs when the same table powers multiple metrics.
Which SQL pulls the 2nd-longest flight for every city pair?
Normalize routes by sorting the two city names and storing them as city_a , city_b so A-B equals B-A, then compute duration_minutes . Apply ROW_NUMBER() OVER (PARTITION BY city_a, city_b ORDER BY duration_minutes DESC) and filter for rank = 2; if a pair lacks two flights no row returns. Sorting the final output by flight_id meets the spec. Airlines inspect this list to schedule backup aircraft for long hauls, so accuracy matters. A multi-column index on normalized cities plus duration speeds the ranking even in decade-long flight logs.
How would you calculate first-touch attribution for each converting shopper?
Join attribution to user_sessions , filter to rows where conversion = TRUE , and for every user_id pick the earliest session ( MIN(session_ts) ) then capture its channel with FIRST_VALUE(channel) over an ORDER BY session_ts . This isolates the discovery channel that led to the eventual purchase, informing marketing spend. Mention deduplicating multiple same-timestamp sessions and handling users who clear cookies. Partitioning by user_id and indexing on (user_id, session_ts) make the scan feasible when logs exceed a billion rows.
How do you calculate a three-day rolling average of deposits in a bank-transactions table?
You first filter to positive transaction_value s, group them by txn_date , then use AVG(daily_total) OVER (ORDER BY txn_date ROWS BETWEEN 2 PRECEDING AND CURRENT ROW) to smooth volatility in inflows. The explanation should mention why withdrawals are excluded, how to handle holidays that skip dates, and how indexing on txn_date keeps the query scalable for real-time dashboards.
How can you surface the five product pairs most frequently purchased together?
The technique explodes each transaction into product pairs via a self-join (enforcing p1 < p2 to avoid duplicates), counts occurrences, and orders by that count. The query illustrates market-basket analysis at scale—important for recommendation engines—so candidates should comment on deduplicating large intermediate joins (e.g., using sessionization or probabilistic sketches). Deterministic alphabetical ordering ( p2 ) ensures tie-breaking for reproducible BI dashboards.
Calculate a 3-day rolling average of step counts per user.
The query uses AVG() OVER(ORDER BY date ROWS BETWEEN 2 PRECEDING AND CURRENT ROW) while filtering out rows lacking two predecessors. It gauges ability to apply window frames, convert floating output to whole numbers, and think through the first-two-days exclusion. Such rolling metrics surface constantly in health-tech dashboards, making this a practical advanced exercise.
How many right-swipes does each feed-ranking variant average among users with ≥ 10, 50 and 100 total swipes?
You’ll join the swipes log to the variants table, bucket users by total-swipe count (≥10, ≥50, ≥100), and for each cohort compute the mean of is_right_swipe separately for variants A and B. The challenge mixes conditional aggregation with a HAVING filter on swipe thresholds and showcases why integer division of right/total swipes yields a clearer metric than simply averaging booleans. Good answers also explain how excluding users below each threshold avoids small-sample noise and why indexing (user_id, variant) keeps the query performant on billions of swipes. Interviewers listen for discussion of statistical significance and whether further stratification (e.g., by geography) might alter conclusions.
What cumulative distribution (CDF) of comment counts per user can you build, using one-comment buckets?
First, aggregate the comments table to get each user’s total comment count; then group these counts into integer buckets and apply a running SUM() window to produce the cumulative percentage. This pattern teaches how to turn raw frequency data into a CDF that product managers can read at a glance (e.g., “90 % of users leave ≤ 3 comments”). Candidates should mention handling sparse tails, ensuring the bucket list is complete, and ordering the window correctly so cumulative values stay monotonic. Indexing on user_id and pre-materialising daily comment tallies are good scale optimisations to note.
How many “likers’ likers” does each user have on a dating platform?
The task requires a self-join: first find everyone who liked you, then count how many people liked them in turn. Using DISTINCT in the inner set avoids double-counting duplicates, and grouping by the original user_id produces a tidy fan-out metric. The exercise checks fluency with aliasing, multi-layer joins, and reasoning about graph-like relationships in SQL. Interviewers value explanations of edge cases (users with no inbound likes) and performance tips such as indexing (liker_id) to accelerate the second-level lookup.
What annual-cohort retention does an annually-billed SaaS achieve?
In annual_payments each row marks a yearly renewal; cohort users by their first payment_year , then compute the share who renew in year +1 and year +2. You’ll pivot counts into retention percentages and order by cohort year for an executive-friendly table. The query demonstrates date extraction, cohort labelling, and conditional aggregation—all staples of subscription analytics. Strong answers discuss missing renewals (NULL pay dates), churn bias, and how to handle users who upgrade mid-cycle.
How many push notifications does a user receive before converting, segmented across the user base?
Join notification_deliveries to the users table, cap each user’s timeline at their conversion_date , and count notifications sent before that moment (or to today if they never convert). Aggregating these counts into a histogram—e.g., 0-3, 4-7, 8+—reveals whether over-messaging hurts conversions. The prompt tests temporal joins, NULL handling for non-converters, and the ability to craft business-ready distribution outputs. Savvy candidates will discuss window functions to pick “last pre-purchase notification” and the impact of time-zone normalisation.
What is the three-month cohort retention for each subscription plan?
Using subscriptions, derive the signup month, then for months +1, +2, +3 flag whether the user remains active ( end_date IS NULL or after the window). A UNION or conditional aggregation can roll these flags into retention rates per (start_month, plan_id, num_month) . The problem blends date math, cohort framing, and ratio calculation, mirroring dashboards built for product-led-growth teams. Interviewers look for clear ordering, thoughtful null checks, and commentary on why 3-month retention predicts lifetime value.
Does click-through rate rise with higher human-rated relevance in Facebook search results?
Combine search_results with search_events, group by discrete rating bands, and compute both impressions and clicks to output CTR per rating. A subsequent regression or chi-square test (outside SQL) can confirm significance, but the query itself must deliver clean aggregates: total rows, clicked rows, and CTR rounded to two decimals. The exercise checks ability to join fact tables correctly, avoid double counting when multiple results share a query, and reason about causality versus correlation in product metrics. Mentioning confidence intervals and exposure thresholds shows senior-level insight.
MySQL and Tool-Specific SQL Questions
If you’re preparing for a role that specifically uses MySQL, expect questions on syntax, functions, and performance nuances unique to this platform. Many MySQL data analyst interview questions focus on functions like STR_TO_DATE , DATEDIFF , GROUP_CONCAT , and window functions such as ROW_NUMBER() —all critical for date parsing, ranking, and summarizing large datasets.
What are common MySQL functions used in analytics?
In data analyst interviews focused on MySQL, you’ll often be asked to demonstrate fluency with built-in functions that are essential for performing analytical tasks. These types of MySQL interview questions for data analyst roles are especially common at smaller startups and SaaS companies that rely on MySQL for powering dashboards and reporting pipelines.
You may also be asked to debug queries involving MySQL-specific behaviors, such as how NULL values are sorted or how execution plans are generated. To stand out, go beyond just syntax—be ready to explain how MySQL handles indexing, temporary tables, and query optimization under the hood.
What does GROUP_CONCAT() do in MySQL, and when would you use it?
The GROUP_CONCAT() function in MySQL combines multiple values from grouped rows into a single comma-separated string. It’s especially useful in data analyst tasks when summarizing categories or labels into one field, such as combining all product names per customer. This function isn’t supported natively in all databases, making it a frequent question in MySQL data analyst interview questions. You may also be asked to handle truncation or ordering issues with this function.
How does MySQL handle NULL values when using ORDER BY ?
Unlike some databases that treat NULL as the lowest value, MySQL orders NULL s first in ascending order and last in descending order by default. This behavior can affect analytic reports or dashboards when sorting is involved, especially in rankings or priority queues. It’s a subtle point but commonly tested in MySQL interview questions for data analyst candidates who are expected to understand default behaviors and how to override them with IS NULL , IFNULL() , or COALESCE() .
Explain the difference between LIMIT in MySQL and TOP in SQL Server.
MySQL uses LIMIT to restrict the number of rows returned, while SQL Server uses TOP . For example, SELECT * FROM users LIMIT 10 is equivalent to SELECT TOP 10 * FROM users in SQL Server. Understanding this difference is crucial for analysts who switch between platforms or work with ETL pipelines spanning multiple systems. It’s also a common question in MySQL-focused interviews, testing adaptability across SQL dialects.
How would you use DATE_FORMAT() to bucket a timestamp column into calendar months while preserving the sort order?
Candidates should explain that DATE_FORMAT(ts,'%Y-%m') converts a timestamp to a year-month string that still sorts chronologically, enabling straightforward GROUP BY or ORDER BY without additional casts. A strong answer mentions edge cases such as time-zone inconsistencies, how %b %Y would break lexical ordering, and why using DATE_TRUNC() equivalents (available only in other SQL dialects) isn’t an option in pre-8.0 MySQL. Interviewers also appreciate discussion of indexing on the raw timestamp versus the formatted string.
When aggregating revenue, why might you choose COALESCE(col,0) over IFNULL(col,0) ?
Both functions replace NULL , but COALESCE is ANSI-standard and accepts more than two arguments, making queries portable and expressive. Analysts should note that in MySQL they perform identically on performance, yet COALESCE works inside window frames, nested selects, and casts without surprises. Highlighting subtle precedence rules and null-propagation pitfalls shows deeper fluency.
Explain how GROUP_CONCAT() can be used to create a “top products per user” string, and list two limitations of the function.
A complete answer covers concatenating sorted product names within a GROUP BY user_id , optionally controlling order with ORDER BY price DESC inside the function. Limitations include the default group_concat_max_len (often 1,024 bytes) and the inability to store more than one value per delimiter safely when downstream tools expect atomic columns. Mentioning a fallback to sub-queries or reporting tables demonstrates real-world experience.
Write a query that compares two dates in different time zones using CONVERT_TZ() and TIMESTAMPDIFF() . What could go wrong around daylight-saving changes?
Interviewers want to see CONVERT_TZ(order_ts,'UTC','America/New_York') followed by a TIMESTAMPDIFF(day, signup_ts, …) clause. A strong explanation calls out that DST gaps/overlaps may yield nulls if MySQL’s time-zone tables are stale, and that TIMESTAMPDIFF truncates toward zero. Acknowledging the need to refresh or mount mysql_tzinfo_to_sql shows operational maturity.
How can JSON_EXTRACT() (or > ) be leveraged to filter events stored in a JSON column, and why is a virtual column sometimes preferable?
Good answers demonstrate WHERE JSON_EXTRACT(meta,'$.event') = 'click' , discuss generated virtual columns for indexing, and note that MySQL cannot index deep keys directly without exposing them. They might also mention cost trade-offs of storing semi-structured data in JSON versus normalizing into relational tables.
Describe a scenario where LAG() combined with IFNULL() solves an analytics problem in MySQL 8.0.
Typical use-case: computing session gaps or day-over-day user metrics. The analyst must show LAG(val) OVER (PARTITION BY user ORDER BY ts) to fetch the prior value, then wrap in IFNULL() to set a default for the first row. Highlighting memory usage and the implications of window size indicates deeper expertise.
Why might SUBSTRING_INDEX() be safer than SUBSTRING() for parsing URLs into domain buckets?
SUBSTRING_INDEX(url,'/',3) grabs everything up to the third slash without relying on hard-coded positions, handling variable lengths robustly. Interviewers value commentary on search-engine tracking parameters, how leading “https://” changes offsets, and why pre-cleaning with LOWER() prevents case-sensitive mismatches.
Show how DENSE_RANK() differs from ROW_NUMBER() when extracting top-selling SKUs per month.
Candidates must articulate that DENSE_RANK preserves shared ranks for ties, whereas ROW_NUMBER forces uniqueness. Illustrating with revenue ties and explaining downstream dashboard impact (e.g., “top 3” may yield more than three rows) proves practical understanding.
What is the purpose of the CAST() function when joining a numeric user ID stored as a string to another table storing it as INT?
Beyond syntax, answers should discuss index usage: casting the smaller side (or using generated columns) maintains performance, whereas casting a bigint column in the WHERE clause negates indexes. Mentioning data-quality initiatives to align types highlights broader thinking.