Randomization Units in A/B Testing: Easy Explanation for Data Science Interviews
hey guys welcome back to my channel in this video let's talk about another common a testing question in data science interviews this video is a last video in the series of videos on top a testing questions in data science interviews in the previous videos I shared two topics one is about sample size estimation and the other one is about matric selection check them out if you haven't done yet in product case interviews you will need to be able to articulate the selection of a randomization unit from experiment so in this video we are going to talk about choosing a randomization unit from Ab test we'll go over what is a randomization unit what are some common use randomization units as well as what are some consideration when choosing a randomization unit let's dive right into it first of all what is a randomization unit the term may sound a bit intimidating but a randomization unit AKA unit of diversion is who or what we randomly assign to each variant or group of AB test selecting the right randomization unit is critical because it impacts both user experience and what metrics we can use in AB test you may think that randomization unit are simply users because in many experiments we refer to the randomization process as assign users to each group but this description is not accurate it's actually a bit more complicated than it initially appears and it will become clearer when we talk about different options of randomization unit let's start our discussion by defining the common use randomization units in practice user ID is one of the most Comm use randomization units it's fairly easy to see why right using the user ID ensures a consistent user experience and allows for long-term measurements such as user retention and users learning effects but there are other randomization units suitable for different scenarios in a test here are some examples a cookie an event a device ID or even IP address can also be used as randomization unit each of these options have some pros and cons let's look at them one by one user ID as we mentioned earlier are stable across time and platforms once the person reg on a website there will be a unique user ID attached to their account which is a huge Pro of using a user ID as a randomization unit but one thing to note is that a user ID can be used to review a person's identity consequently we need to be mindful of confidentiality and security issues when using user ID for identification purpose another issue to note is that the identification of registered users generally requires the user to log to their accounts which is the limitation of user ID as the randomization unit unlike user IDs cookies are pseudonymous IDs that are specific to a browser and a device if you don't know what web browser cookies are there are small footprints that are automatically assigned and stored in a browser when you log on to a website because cookies are Anonymous they are not identifiable but the users can clear cookies and cookies do not persist across browsers or platforms for example if you change your web browser from chrome to Firefox the cookie will change cookies can also expire so you can think of them as temporary user IDs so cookies are not persistent across platforms another kind of randomization unit is an event an event such as a page or session represents a finer level of granularity than a user ID in other words one user can be connected to many page views or sessions using page level randomization every page visit is considered a randomization unit this randomization method is simple because it does not require users to log on nor does it distinguish between actions from the same or different users session level randomization can also be used usually a session is defined as a continuous period of activities for instance you might log on to your Facebook account check out your friends activities and then close your browser typically the session will expire after 30 minutes of inactivity that's one session the next day if you log onto your Facebook account again it's considered a new session session level randomization treats every session as an independent occurrence accordingly one user can be assigned to different variants from time to time because there are typically more page views and sessions than users using an event as a randomization unit provides more units and gives us more power to detect smaller changes however it may also lead to inconsistent user experience so if the change is visible to users it's better to not use an event as a randomization unit another common use randomization unit is a device ID it is an immutable ID associated with a specific device device IDs are only available for mobile devices so they are mostly commonly used as randomization units for a testing of changes in mobile apps now we know some common use randomization units but how do we choose which one to use in a given scenario the first consideration is ensuring a consistent user experience you don't want users to experience One Design today and another design tomorrow because it will likely make users very frustrated therefore for changes that are visible to users like a changes in color button sizes and the layout of a page or other major design changes on website we should use a user ID or cookie as a randomization unit conversely if the change is not visible or easily noticed by users then the selection of a randomization unit depends on what we want to measure examples include website performance changes and changes in backand algorithms for instance if we want to compare the latency of loading web pages then we might use a page view as a randomization unit we can also consider session level or page level randomization page level randomization may be more powerful than user level randomization because it can reduce the variance in page level Matrix but if we want user to see what happens and track their experience over time a user ID is also a good option another related consideration is a comparison of the coarseness of the randomization unit and the unit of analysis as we have mentioned earlier the unit of analysis is the unit of your metric the general recommendation is that the randomization unit should be at least as a course as a unit of analysis let's consider a few examples to make this recommendation clear for example let's say we want to analyze clicks rate in which case the unit of analysis is a page view if we want the randomization unit to be a user let's ask ourselves whether a page view or user is a closer or bigger unit because many page view can be connected to us ID the US ID is a CER unit so we have met the conditions of this recommendation notice however that in this case we may have an issue with the previous consideration which is variability this example demonstrates the importance of all considerations in designing the randomization unit now suppose our randomization unit is a page view and we analyze user level matric such as average number of clicks per user here a page view is less course than the number of clicks per user and the user's experience is likely to include a mix of variants some pages in the control group and some pages in the trim groups that will make Computing user level metrics meaningless all right guys thank you so much for watching this video I hope you have found this video helpful remember I have a free product case interview cheat sheet to help you with interview preparation you can grab it by going to data interiew pro.com in this series of videos we talk about the top three AB testing questions in data science interviews they are sample size estimation metric selection and choosing a randomization unit understanding these questions and answers is important and is a good starting point but you need more than that to a data science interviews that's why I recommend you to subscribe to my channel so you don't miss future videos where I talk about how to approach data science interviews as well as how to learn data science in general so don't forget to subscribe to my channel so that you will see more content like this bye guys


Metric Selection in A/B Testing: Easy Explanation for Data Science Interviews
what's up guys welcome back to my channel in the last video we talked about estimating the symbol size for an a b test in this video let's discuss another topic related to a b testing which is the topic of metric selection metric selection is one of the most common ab testing questions in data science interviews interviewers often ask questions that require interviewees to select metrics for the purpose of ap testing for example what metrics would you use an experiment to understand a feature change given the importance of metric selection in the context of ap testing this video will discuss this topic in depth and focus on the following things what are driver metrics what are the attributes of driver metrics how to develop and select driver metrics i've use an a b test example from youtube to talk about how to select metrics let's dive in driver metrics also known as surrogate metrics are indirect or predictive metrics that are often used to measure short-term objectives these metrics align with the goal of the coming are sensitive to short-term progress and are actionable so that teams can be driven to work towards them in a nutshell driver metrics are the major metrics used for a b testing let's consider a simple ap test of an ad campaign the difference between the ad in the control and treatment group is its design the goal of the ad campaign is to increase total revenue from sales of items but given the goal of the campaign how do we select which metrics are best for the situation conversely which metrics are bad there are three overall criteria we can use to evaluate driver matrix first and most importantly a driver magic should be sensitive and timely in particular a driver metric should be sensitive enough to reflect the change made in the product the clicks rate is an ideal driver metric for this example because once we change the design of the ad the change will be reflected in the click-through rate we can also look at the conversion rate in this case it's a percentage of users who take the desired action such as making a purchase which is also an indicator that one ad is better than the other on the other hand the data active user of a particular product being advertised might not be a good driver metric because it can take time for people to purchase the product start using it and become a daily active user daily active user may be better suited as a success metric than driver magic because it can be impacted by multiple factors and may not be sensitive enough to the change in the ad this is not to say that bad driver metrics are unimportant for business they are but such metrics are not appropriate for running a b tests because they are not sensitive enough to measure the treatment effect driver metrics should also be measurable meaning we should be able to calculate the metric with the data collected during the experiment period intelco means most experiments are designed to run within a time frame of days or weeks so the measure with select should be suitable for such time frames using the example of the ad campaign the clix rate is very simple and easy to measure because the counts can be easily obtained even in real time the measure is simply the number of clicks divided by the number of impressions conversely the most active users and monthly user retention rates are bad ideas for metrics because they can now be calculated within the time frame of the test which is typically days or weeks finally a magic should be attributable in other words we must be able to attribute the change in magic to the experiment variant this requires us to be able to measure the metrics in the control and the treatment groups separately using the as example we can easily attribute the change in clicks rate to the design of the ads good design results in higher clicks rate and bad design results in a lower click-through rate these are the three attributes of good driver magics now let's talk about how do we come up with driver metrics in practice there are many ways to come up with ideas for magics and validate existing metrics metrics can also be developed by combining qualitative and quantitative methods qualitative investors include techniques such as user experience research focus groups and surveys to understand user's needs quantitative methods include analysis of data such as analysis of logs to see what users do and find patterns in the data but very often we don't have time to leverage all those methods to come up with metrics for an a b test especially during interviews time is very limited so what can we do is understand the motivation of an a b test and define metrics specifically for measuring the changes there are two ways i have found particularly helpful one way that i found helpful to come up with metrics is to fully understand the goal of a test is a goal about user growth is it to improve engagement or is it to increase revenue is it change about acquisition activation retention referral or revenue we want to be as specific as possible to fully understand the goal doing this will help you come up with a metric or two let's look at an example in 2021 youtube ran experiment to test hiding dislike counts on videos according to the company the goal was to better protect the creators from harassment help ensure small creators and those just getting started can thrive and create an inclusive and respectful environment where creators have the opportunity to succeed and feel safe to express themselves so what metric can we think of based on that goal assuming this feature change will help youtube achieve this goal we would expect to see small creators become more engaged on the platform post more videos and spend more time on the platform so two possible driver metrics for the goal are the average time spent on youtube per creator and the average number of videos published per creator ideally both metrics will be larger in the treatment group than in the control group other than focusing on the goal of the test another way to come up with metrics is to analyze user experiences consider the steps users in each group need to take to use a feature or product and think about some metrics to measure the difference between user experiences typically most products or features have a funnel that moves users towards taking key actions or desired outcomes that are meaningful to the business and changing user experiences may positively influence more users to get the desired outcome so the difference between user experiences can lead you to appropriate driver metrics for the youtube example what is the desired outcome well the desired outcome is the fever dislikes on videos on smaller channels in the experiment youtube assigned viewers to different groups the control group could still see the count of the number of dislikes for a video in the treatment group viewers could still dislike a video to share feedback with creators but they were not able to see the counts of dislikes given the difference between the control and the treatment groups as well as the desired outcome what metrics should we use for the experiment one metric is the average number of dislikes per viewer which will indicate if a user gives more of fewer dislikes also given that the goal of this experiment is to protect smaller creators we can measure the average number of dislikes for smaller creators ideally we will see a decrease in this number here are some experimental findings here by youtube viewers were less likely to target a video's dislike button to drive up the count in short our experiment data showed a reduction in dislike attacking behavior we have also heard directly from smaller creators and those just getting started with their youtube channel that they are unfairly targeted by dislike attacks our experiment data confirmed that this behavior does occur at a higher proportion on smaller channels although these findings do not mention the exact metric youtube has used we can tell that it was closely related to the company's original goal of the feature change which was to protect smaller creators to summarize we have talked about the two main strategies for developing driver metrics by considering the overall business goal and the difference in user experiences in the next video we'll go over another top a b testing question in a data science interview which is to choose a randomization unit from a b test if you like this video you may also want to grab my free product case interview cheat sheet to help you with editing product case interviews also make sure to subscribe to my channel so that you don't miss the next video thank you guys for watching i will see you in the next video


Sample Size Estimation in A/B Testing: Easy Explanation for Data Science Interviews
hey guys it's emma welcome back to my channel from today's video i want to start a series on the top a b testing questions i have seen in data science interviews this series of videos will include sample size estimation magic selection choosing randomization units simple since paradox and the network effect etc obviously there are lots of things i want to share with you and to make it easier for you to watch and learn i will focus on one topic in each video so if you are someone who is preparing for data science interviews or getting started to learn data science then this series of videos will be helpful for you before diving into those questions i want to mention that how ap testing questions appear in data science interviews this is a question i got from quite a few students who had never done a data science interview before so a b testing is not a stand alone interview it's rare to see any company have an ap testing interview round but if the comment appears in data science interviews especially product case interviews it means that a b testing questions are often asked together with product case or product sense questions for example what are some pros and cons of youtube removing dislikes of videos and how do you design a test to evaluate the effectiveness of this change as you can tell it requires not only product knowledge but also ap testing knowledge to answer this kind of question it's hard to provide an insightful and in-depth answer if you don't know much about a b testing now that you know where ap testing questions occur in data science interviews let's go over one of the top av testing questions that i have seen in design's interviews and that is to use power analysis to estimate the sample size needed for a test one last thing before diving into today's topic the questions in this series of videos require you to have some basic knowledge of a b testing if you are new to a b testing that's totally fine i have a playlist for you that you can start from there i have shared the link in the video description alright guys let's get started let's first review the general form of sample size estimation which is 2 sigma squared z alpha over 2 plus z beta squared over delta squared whereas sigma squared is an estimate of variance alpha is a significance level it's also the same as type 1 error or false positive rate z f over 2 is a z score such that the area to the right of z f over 2 is alpha over 2 under the standard normal curve beta is a type 2 error of force negative rate and the same as 1 minus power and z beta is the z-score such that the area to the right of z beta is beta under the standard normal curve delta is the difference between control and treatment so this is a general form of sample size estimation it's rare that you will get questions on how to derive this equation during an interview but if you are interested in learning about the derivation i have a link to a chapter of a book in the video description and it has details on the derivation even though it's not required to know exactly how to derive the equation it's helpful to understand how we obtain each component and how each component plays a part in estimating the sample size when sigma is 0.05 and beta is 0.2 we can get the rule of some formula which is 16 multiplied by sigma squared divided by delta squared if we want to be more conservative and lower the significance level alpha we can set alpha to be a smaller value for instance when alpha is 0.01 z alpha over 2 becomes 2.23 which is larger than 1.96 when alpha is 0.05 our coefficient increases from 16 to almost 19. so with decreasing alpha we need more samples intuitively this makes sense if we want to decrease type 1 error alpha we increase our confidence level of the estimation and we need more samples as our sample size increases the more information we have our uncertainty decreases and we have greater confidence in our estimation now let's look at how beta impacts sample size beta is type 2 error it's equal to 1 minus power so increasing beta means decreasing power and vice versa power is often set 0.8 in practice which means beta is 0.2 let's say we want to increase power to 0.9 then beta becomes 0.1 in this case z beta is 1.28 which is larger than 0.84 when beta is 0.2 and the coefficient increases from 16 to 21 it means that increasing our sample size can give us greater power to detect differences the smaller beta the greater power and more samples we need the next one is variance various estimation should be done before running the experiment it can be obtained from historical data generally speaking companies should have historical data such as system logs or user behavior data for data scientists to query to estimate variance for companies that have done some av tests before we can estimate the variance from previous eb tests and ae tests if no such data is available we can run an a test which is smaller experiment to get idea of the distribution of the data when there's no treatment aided and we should continue to improve estimations of variance with more data and experiments for future tests finally let's take a look at delta the difference between control and the treatment how do we know delta before running the test there's a reason to run the test to begin with right to know the difference between control and treatment the idea is to use the minimum detectable effect is also known as practical significance that is the minimum change that makes sense for the business for instance 10 milli increase in revenue or 10 000 increase in button click just because those values are noisy and can be impacted by many factors we need a minimum practical difference in order to conclude there is a meaningful impact to the business as you can tell when delta becomes smaller the resulting sample size will become larger it means when we want to detect smaller changes we will need more samples we need large enough samples to accurately estimate the difference because large sample sizes increase the probability of estimating the matrix we are interesting accurately so to summarize the lower the alpha the higher the contents level and the more samples we need the lower the beta the greater the power and the more samples we need the larger the variance the more samples needed to collect to run the experiment the smaller the variance the less samples needed to be collected to run the experiment also when we want to detect smaller changes that is when delta becomes smaller we will need more samples alright guys i hope the walkthrough of sample size estimation is helpful in the next video we will focus on metric selection for a b testing i will share with you some general rules that i have found helpful to select metrics for a b tests before i forget i also have a product case interview cheat sheet that you can find on dateinterviewpro.com so if you like this video you may also want to grab the cheat sheet to help you with product case interviews i offer practical knowledge on data science interviews and learning data science in general so don't forget to subscribe to this channel so that you will see more content like this i cannot wait to see you bye guys


A/B Testing Mistakes and Solutions: How to Excel in Your Data Science Interview!
hey guys welcome back to my channel in this video let's continue talking about common mistakes that the people make when it comes to interpreting every testing results and using those results to make product launch decisions in part one of the topic i shared misconceptions and misinterpretations of certain states concepts if you haven't watched it i highly recommend watching first before watching this video in this video we will dive into the other category of mistakes which is to assume the results are valid and reliable without doing sending checks and make launch decisions based on invalid results in reality there are so many factors that can make the results unreliable as a data scientist we should be at least aware of those problems and be able to explain to others why those problems exist and how to deal with them in this video we will cover the three most common issues let's get started one common issue that makes the results unreliable is sample ratio mismatch it refers to the instance that the sample ratio between control and treatment is not as designed for example if the experiment designed for ratio between the sample size of control and that of treatment is one to one after running the experiment you observe the ratio is 1.01 in other words the control group has more users than a treatment group then you use a t-test to test if the number of users in the control group is different from that in the treatment group and you obtain a p-value less than .05 you now realize that it's highly unlikely to observe such a ratio or more extreme condition on the design ratio there are many causes of sample ratio mismatch one is simply bugs or problems in assigning users to different groups while randomly assigning users based on the user id may sound simple and straightforward achieving proper randomization can be very challenging in reality for example many experiments have ramping up plans to make sure there's no risk when exposing users to a new feature the rampia plan typically starts with assigning only a small percentage of users in the treatment group then gradually roll out to more users and that could complicate the assignment of users things also get complicated when running multiple experiments in parallel and one user might be assigned to model experiments bugs and errors are more likely to occur than running one experiment at a time another potential issue is if we are looking at a particular segment of users and if the segmentation is based on some attributes that can change over time for example location people may move to a different geographic location so it will result in bias in allocating users to different groups for example you want to run experiments with the target population in the san francisco bay area so you assign users to the treatment group because his profile shows that he's in san francisco however he has already moved to a different location but he hasn't updated his profile yet during the experiment he updated his profile so when you get the results and do the analysis you need to filter the users who are not in the target region besides all these things it's also possible that the way the results are processed lead to sample ratio mismatch for example we have a data pipeline to filter out fraudulent users before analyzing the test results and there is a bug in the pipeline that causes the false positive rate to be different in different groups first positive means that we wrongly flag legitimate users as frosters say the first parting rate in the treatment group is higher this will cause a sample ratio mismatch we have covered a few factors that may lead to sample ratio mismatch next let's go over how to debug it if we observe such an issue i'm gonna summarize the recommendations for debugging from the book trustworthy online control experiments because it's pretty comprehensive and practical we can start with checking if there's any discrepancy upstream of the randomization point an example would be if our target users are those who enter checkout we want to see if there's any difference between controller treatment for users upstream of entering checkout in other words before entering checkout we can check if there's a gap between groups for user to land on the home page for users who put items in a shopping cart and all these steps before users starting the checkout process another thing to check is if the variant assignment is done correctly are users allocated to different variants properly if we use user id to assign users is the assignment truly random will any bias be introduced in this step for example if we find one group has many females and the other group has many males then the assignment is unbalanced and neither group represents the overall population so the result is likely to be inaccurate we could also look into the data processing pipeline a common source of sample ratio mismatch is about detection and filtering if there's any bug introduced in filtering bot traffic it will potentially cause sample ratio mismatch to further debug the issue we can check different segments of the population for example we could look into the ratios per day and see which days have anomalies we can also segment by other dimensions for example are the ratios different for new users versus returning users just to recap the first factor that makes testing results unreliable is sample ratio mismatch we have talked about different ways to debug it next let's go over another common problem that invalidates testing results and that is randomization units interfere with one another there's one important assumption of a b testing which is that randomization units are independent and there's no interaction between them this is called stable unit treatment value assumption if there are interference or spillovers between different groups then the result is definitely not reliable the estimated treatment effect could either underestimate or overestimate the true treatment effect in reality it's common that this assumption is violated for example in social networks such as facebook and linkedin users behaviors tend to be impacted by others in their social circles for example if a user's close friends all use facebook then she tends to use it more another violation of the stable unit treatment value assumption often happen in two sided markets such as ebay uber and lyft in these markets controlled treatment groups compete for the same resources if a new feature increases demand in the treatment group then the treatment group needs more supply to fulfill that demand and this will impact the supply in the control group because the resources are shared then what to do about it well we could not really avoid the interference between groups but we could predict where the interference will happen and taken into consideration in the experiment design phase if such a possibility exists we can change our design to isolate control and treatment units for example considering a treatment group in a completely different geographic location to avoid potential interference if predicting the interference is very challenging we should be able to monitor and detect it and once the mechanism of interference is well understood we could update the experiment design in a previous video on a b testing i provide more solutions to deal with interference between groups make sure to check it out the last common cause of unreliable results lies in changes in users behaviors users react to new features or products differently some favor new things and tend to use more when they see a new feature which is a novelty effect but others would hate it so they tend to use less which is called a primary effect now let me share with you a personal example i once used an online grocery shopping app and there was a new feature that every time you share a photo of the delivery you get a little cashback so that you can use it for your next purchase when i first saw this new feature i thought it was so cool i could get a little money back if i just take photos and share them so i use this feature for the very first few deliveries i got i took the pictures and shared them on the app after doing it a few times i realized the tiny cashback if i remember correctly is just a few cents it's not worth my time at all and i just stopped using it completely this is typical example of novelty effect as a user who sees a new feature i use it heavily at the beginning and then i just stop it at all note that both novelty and premise effects are not stable as you can tell from the example i just gave you they happen only during the initial period after users see a new feature or product while there's nothing we could do to prevent these effects what we could do and should do is to monitor if such effects exist and quantify them so that we could filter them out when evaluating the real treatment effect now at this point i pretty much shared with you a lot of mistakes people make when it comes to interpreting every testing results and using those results to make decisions as a data scientist who's leading the charge of ap testing it's our responsibility to educate others how to interpret data correctly and provide suggestions to deal with unreliable results so beyond everything else i have covered in this video i also have a whole playlist to help you learn advertising in depth i have the fundamentals of epi testing selecting the right metrics and examples to walk you through the whole process and the commonly asked interview questions and answers so if you want to learn advertising to prepare for your interviews or to expand your attacking knowledge make sure that you watch my videos that are in this playlist it's going to help you a lot anyways guys if you go to the end of this video thank you so much if you like it do let me know feel free to drop me a comment for any questions or feedback i post a lot more content surrounding ap testing and data science so stay tuned i will see you in the next video

A/B Testing Metrics: What You Need to Know About Success, Driver, and Guardrail Metrics!
hey guys welcome back to my channel in this video we will do a deep dive on matrix selecting the right metrics is super important to run an epitax in practice because we want to be clear about the goal as well as how to measure the results before running it not only these metric related questions often appear in data science interviews it can be a straightforward question that asks you about the pros and cons of a specific metric it can also be a question that asks you to formulate a metric for an online experiment so as a data scientist the knowledge of matrix is fundamental in this video we will start with business metrics including gold magic driver magic and guardrail magic then we'll talk about how to format metrics for online experiments the content of this video is based on not only my knowledge on metrics but also a few new things i've learned from reading the book trustworthy online control experiments i have learned quite a few things from reading that book and i recommend it to anyone who is interested in learning about a b testing okay let's get started there are three kinds of operational metrics that the companies use to major success and progress and to understand areas for improvements the first kind of magic is a goal magic it's also known as success magic true norse magic north star magic okr magic and the primary magic this kind of magic reflects a company's long-term vision and it always ties to a company's mission gold metrics are a small set of metrics that the company truly cares about i know it may sound abstract how do we translate such a mission or vision to a set of metrics let me give you an example facebook's mission is to give people the power to build the community and bring the world closer together and its goal metrics include advertising revenue daily active users and monthly active users while the transformation from its mission to its goal metrics isn't perfect the goal metrics do reflect what the companies ultimately care about and they are simple enough to be easily communicated to different stakeholders such as investors customers and employees the gold metric should also be stable over long periods of time to allow the whole organization to work towards improving it while gold metrics are critical to measure the overall success of a company they may not be suitable for online experiments because they can be difficult to major or may not be sensitive enough to product changes for example facebook hears about ad revenue but not every team could use it for a b testing there are teams focusing on improving user engagement and also teams focusing on website or native app performance for such teams what they do definitely contribute to the company's overall success but they don't use those commonly level goal metrics to major performance compared with goal metrics which are about the long-term vision we also need metrics to reflect short-term progress the driver metric also known as surrogate magic indirect or predict metric are often used to measure short-term objectives they align with the goal metrics but they are more sensitive and actionable to be able to major short-term progress and the drive teams to work on it that's also why they are better than the gold metrics to be used for a b testing now let me give you a concrete example of a driver metric a marketing team's goal is to acquire new users and one of the driver metrics could be the number of new users registered per day the distinction between the gold magic and the driver metric is actually something new i learned from the book before reading it i thought i knew what a success metric is i have developed such metrics in practice and have used them to run online experiments but after reading the book i realized that i was wrong what i thought of as success metrics were actually driver metrics in fact success magic is the same as the goal magic and it's about the long term vision while the driver metric is used to major short-term progress and is more suitable for online experiments check out this blog written by my friend rob and me it covers several metric frameworks which can be very helpful for you to understand what metrics are used in different business domains the last category of metrics is a guardrail magic as the name suggests government metrics guard us from harming the business and violating important experiments assumptions in this book it categorizes guardrail matrix into two groups which i think is very helpful to understand different roles of gallery metrics the first one is the organizational gallery metric if this kind of metric shifts to the negative direction the business will suffer significant loss for example if the loading time of a web page increased by a few million seconds there can be a significant loss of customers and revenue in practice the page loading latency is often used as a gallery metric when new features are developed and tested through every testing a few other commonly used organizational guide metrics include errors per page and client crisis the other kind of gallery magic is trustworthy related metrics they are used to monitor the trustworthiness of an experiment that is to check if there is any violation of its assumptions one of the common use metric is to check if randomization units assigned to each variant is truly random when the numbers in different groups are different the authors refer this as simple ratio mismatch we then need to perform a t-test or a chi-square test to check if the assignment ratio matches with what was designed now you know the definition of goal matrix driver metrics as well as gary metrics in practice we need to be clear about the contacts when talking about a specific metric because the same metric can be used differently for different teams one team's driver metric can be other teams garbo metric for example one front-end team's goal is to improve web performance so reducing latency is their goal and time to interactive tti can be one of their driver metrics a product team may use the same metric as a guardrail metric to make sure any product changes don't increase latency next let's look at what are the attributes of a good metric in the blog post i mentioned earlier it has a few general rules to formulate matrix a good magic should be simple it's easy to understand and calculate and people should be able to remember and discuss it easily if you cannot use one sentence to explain magic it's not simple secondly the definition of a good metric is clear and there is no ambiguity in interpretation thirdly a good metric should be actionable the metric can be shifted by changes in products and it offers insights on how you can improve it should not be easily gamed gaming means that a metric makes you feel like you are getting results but it offers no insights into actual business health or growth short-term revenue is an example of such a metric increasing prices of products may increase short-term revenue however the business may lose customers in the long run we have talked about operational metrics which are critical to major accounting or teams performance however not all of them are suitable for online controlled experiments next let's go over what are the requirements of metrics that can be used directly for experimentation in the book the authors summarize three attributes for metrics that are suitable for experimentation measurable we should be able to calculate those metrics with data collected during the experiment period attribute we should be able to attribute matrix values to the experiment variants it means that the metrics will be able to be calculated separately for different variants in the experiment sensitive and timely experiment metrics should be sensitive enough to detect changes in a timely manner in online experiments we typically select a few driver metrics as a key metrics as well as some government metrics to monitor impacts on other aspects of the business now i want to share one question i get constantly since we have multiple metrics for experiment how do we make the launch decision when one metric goes up and one metric goes down it's a very reasonable question and this scenario happens often in practice many organizations have a mental model of the trade-offs they are willing to accept when they see any particular results for example trade-offs between user acquisition and revenue how should the company strike the optimal balance between revenue maximization and user acquisition acquiring new users can always be down by expensive campaigns such as providing large discounts or gifts but it will degrade revenue this kind of trade-off is not something that can be determined by a single data scientist or a single data science team it's something that is discussed and aligned among various stakeholders such as product teams marketing teams engineering teams and the leadership team etc finally i want to share with you two practical suggestions from the book on formulating metrics for experiments one is to combine a few target metrics into an overall evaluation criteria a weighted combination of the most important driver metrics and use it as the only criteria for an experiment if coming up with such an oec is difficult the authors recommend choosing no more than five metrics as their target metrics there are two main disadvantages of having too many metrics one is that too many metrics may confuse people and it may possibly lead to ignoring the key metrics the other downside is including too many metrics may affect the decision-making process and increase the chances of having forced discoveries alright guys i hope you have learned something new about the metrics in this video in the next few videos we will continue talking about interesting topics about a testing stay tuned i will see you soon

A/B Testing Metrics: What You Need to Know About Success, Driver, and Guardrail Metrics!
hey guys welcome back to my channel in this video we will do a deep dive on matrix selecting the right metrics is super important to run an epitax in practice because we want to be clear about the goal as well as how to measure the results before running it not only these metric related questions often appear in data science interviews it can be a straightforward question that asks you about the pros and cons of a specific metric it can also be a question that asks you to formulate a metric for an online experiment so as a data scientist the knowledge of matrix is fundamental in this video we will start with business metrics including gold magic driver magic and guardrail magic then we'll talk about how to format metrics for online experiments the content of this video is based on not only my knowledge on metrics but also a few new things i've learned from reading the book trustworthy online control experiments i have learned quite a few things from reading that book and i recommend it to anyone who is interested in learning about a b testing okay let's get started there are three kinds of operational metrics that the companies use to major success and progress and to understand areas for improvements the first kind of magic is a goal magic it's also known as success magic true norse magic north star magic okr magic and the primary magic this kind of magic reflects a company's long-term vision and it always ties to a company's mission gold metrics are a small set of metrics that the company truly cares about i know it may sound abstract how do we translate such a mission or vision to a set of metrics let me give you an example facebook's mission is to give people the power to build the community and bring the world closer together and its goal metrics include advertising revenue daily active users and monthly active users while the transformation from its mission to its goal metrics isn't perfect the goal metrics do reflect what the companies ultimately care about and they are simple enough to be easily communicated to different stakeholders such as investors customers and employees the gold metric should also be stable over long periods of time to allow the whole organization to work towards improving it while gold metrics are critical to measure the overall success of a company they may not be suitable for online experiments because they can be difficult to major or may not be sensitive enough to product changes for example facebook hears about ad revenue but not every team could use it for a b testing there are teams focusing on improving user engagement and also teams focusing on website or native app performance for such teams what they do definitely contribute to the company's overall success but they don't use those commonly level goal metrics to major performance compared with goal metrics which are about the long-term vision we also need metrics to reflect short-term progress the driver metric also known as surrogate magic indirect or predict metric are often used to measure short-term objectives they align with the goal metrics but they are more sensitive and actionable to be able to major short-term progress and the drive teams to work on it that's also why they are better than the gold metrics to be used for a b testing now let me give you a concrete example of a driver metric a marketing team's goal is to acquire new users and one of the driver metrics could be the number of new users registered per day the distinction between the gold magic and the driver metric is actually something new i learned from the book before reading it i thought i knew what a success metric is i have developed such metrics in practice and have used them to run online experiments but after reading the book i realized that i was wrong what i thought of as success metrics were actually driver metrics in fact success magic is the same as the goal magic and it's about the long term vision while the driver metric is used to major short-term progress and is more suitable for online experiments check out this blog written by my friend rob and me it covers several metric frameworks which can be very helpful for you to understand what metrics are used in different business domains the last category of metrics is a guardrail magic as the name suggests government metrics guard us from harming the business and violating important experiments assumptions in this book it categorizes guardrail matrix into two groups which i think is very helpful to understand different roles of gallery metrics the first one is the organizational gallery metric if this kind of metric shifts to the negative direction the business will suffer significant loss for example if the loading time of a web page increased by a few million seconds there can be a significant loss of customers and revenue in practice the page loading latency is often used as a gallery metric when new features are developed and tested through every testing a few other commonly used organizational guide metrics include errors per page and client crisis the other kind of gallery magic is trustworthy related metrics they are used to monitor the trustworthiness of an experiment that is to check if there is any violation of its assumptions one of the common use metric is to check if randomization units assigned to each variant is truly random when the numbers in different groups are different the authors refer this as simple ratio mismatch we then need to perform a t-test or a chi-square test to check if the assignment ratio matches with what was designed now you know the definition of goal matrix driver metrics as well as gary metrics in practice we need to be clear about the contacts when talking about a specific metric because the same metric can be used differently for different teams one team's driver metric can be other teams garbo metric for example one front-end team's goal is to improve web performance so reducing latency is their goal and time to interactive tti can be one of their driver metrics a product team may use the same metric as a guardrail metric to make sure any product changes don't increase latency next let's look at what are the attributes of a good metric in the blog post i mentioned earlier it has a few general rules to formulate matrix a good magic should be simple it's easy to understand and calculate and people should be able to remember and discuss it easily if you cannot use one sentence to explain magic it's not simple secondly the definition of a good metric is clear and there is no ambiguity in interpretation thirdly a good metric should be actionable the metric can be shifted by changes in products and it offers insights on how you can improve it should not be easily gamed gaming means that a metric makes you feel like you are getting results but it offers no insights into actual business health or growth short-term revenue is an example of such a metric increasing prices of products may increase short-term revenue however the business may lose customers in the long run we have talked about operational metrics which are critical to major accounting or teams performance however not all of them are suitable for online controlled experiments next let's go over what are the requirements of metrics that can be used directly for experimentation in the book the authors summarize three attributes for metrics that are suitable for experimentation measurable we should be able to calculate those metrics with data collected during the experiment period attribute we should be able to attribute matrix values to the experiment variants it means that the metrics will be able to be calculated separately for different variants in the experiment sensitive and timely experiment metrics should be sensitive enough to detect changes in a timely manner in online experiments we typically select a few driver metrics as a key metrics as well as some government metrics to monitor impacts on other aspects of the business now i want to share one question i get constantly since we have multiple metrics for experiment how do we make the launch decision when one metric goes up and one metric goes down it's a very reasonable question and this scenario happens often in practice many organizations have a mental model of the trade-offs they are willing to accept when they see any particular results for example trade-offs between user acquisition and revenue how should the company strike the optimal balance between revenue maximization and user acquisition acquiring new users can always be down by expensive campaigns such as providing large discounts or gifts but it will degrade revenue this kind of trade-off is not something that can be determined by a single data scientist or a single data science team it's something that is discussed and aligned among various stakeholders such as product teams marketing teams engineering teams and the leadership team etc finally i want to share with you two practical suggestions from the book on formulating metrics for experiments one is to combine a few target metrics into an overall evaluation criteria a weighted combination of the most important driver metrics and use it as the only criteria for an experiment if coming up with such an oec is difficult the authors recommend choosing no more than five metrics as their target metrics there are two main disadvantages of having too many metrics one is that too many metrics may confuse people and it may possibly lead to ignoring the key metrics the other downside is including too many metrics may affect the decision-making process and increase the chances of having forced discoveries alright guys i hope you have learned something new about the metrics in this video in the next few videos we will continue talking about interesting topics about a testing stay tuned i will see you soon

A/B Testing Made Easy: Real-Life Example and Step-by-Step Walkthrough for Data Scientists!
I wake up at My Channel and video lessons And The people to drive into the whole process of morning ibis This part of the story is affecting import World of value Asus integrum recover the mezicef Vng art with love message to before the Masters to consider When going against in Viet rat it during the war III mint Milk to have to know that one hundred and it'll make your own the streets this video is for adults take after two of us pharmacy our country's problems When Jumping furmark dates are typical operation welcome I have a few days and spatial most save on test em Immortals end of Night When planning stand stood for you that into this video uses Anh Ni Micky Yoochun tht example you think about of pict know it Comes the web site and special force One Team prepared a new Fair to support of unique special product Line It's acumin might want to say something better Together Windows operating in the short end This passage is adapted to English National average is constructed in Singapore test mic I just met with me and Secret travel or woman is that a simple Touch Me Help our Eyes all Movies with one or more of children make up with me stay at Ford Explorer standard and data rate of of and your product weight is supplied are not the committed to draw ibis no rest for the problem that I would then eat  ch vong  dch phn quang linh dch whispers Of Justice and accurate What are the parents and Vietnamese equivalence forza balestra Christmas ever go to the warning system as in the different value then you Singapore the speaker which makes It is unflavor will notice of your trip of physical parental control and treatment with the number one Used in the country people in the same as that remains However The optional Love You raise me beautiful beautiful dress socola report for repair battlegroup We cannot Lie fresnel by the number of use and his troops and the magic cube YAN News play music now We mix make any support and come up with guarantor deferment the contours pulse tilt and then a pulse single speaker I have Michael Kors and make up and interact with the first time including its Asuma for the section on the great content just installed the Sugar to the war of land in the shopping cart and photos of Vanda Style apart and becomes her that in few hundred thousand Words and folders example We can see It Is Beautiful Life the principles recommendations khi nhn xt Trong phn bnh lun hai gip ham mun do to support you Republic of lodoss war war war is indicated by clicking the icon the first street on the Lord Will Go On The Theme and the one thing the first feminine ministers are Used in the impact of love you everything that love you move and comparisons work people Galant fortis payments nespa fellas We have enough money for IELTS for example We could love you See you There are gonna make you Nice And examples of things that We have is not Used for different now We're all over two agreement And The Woman is your name will you nights were very little Bride examine The Earth is the Weather Today or use of Pacific judgment of used to answer this question Last Light tr chi welcome I showed The file you understand on the web site does the proud of sport and instant Voice of the painted the motor Bike that come to the fastest easiest and healthcare products to make payments know that you're asking Battle For Speed the list of results for mature Brothers directed are the force no huyt perfume of installed operations on to what We want most suitable for workers were Used Used On The Top Of The Phone OS Toeic basic user and on the home page will the most in Use It will to show How are and prepare to you nostalgic art sullivan Nobody in parking apk Androi amorphous Love is to consider your Little Thing That's Who have intention to make nh em I miss the special force because of you see you think of all the speaker terminals Ok we have the lightest compilation of stunning the summer Style agreement with sweet home I Used To occur in traning improve their feelings to container first What is the practical chicken licken spondylus How much is that matters pulpits First of all because of sweetness makes the correct form of the humble how much in cleasing gel You Girl will be considered a When the cost English example of treatment is in Technology have a christophe increase of tourists in problem for users is the position Urban lovers for learning English way to get rid of At least once again to collect  Anh c Minutes To The Power of that Are The World Is suitable for ever and to calculate the samples were Used in the system is different from under five test English level with power and ever with love life the Formula still Want For Christmas with my desk work When Christmas in the division of the position and tourists efficiently manage and control if you the weather on how to drive from earthsea love you for the author's permission to know the value of system is love to see you the sikament Center for example to draw instinct colors Used in any job with sticks and unique user IS apparent YouTube air force and Homes are Used in Total Note If you want to attend a Smart and the one to introduce a a sample of positional over that to one person you also use English the sample Sites that many people learn English German YouTube sitepal on the market in the wishlist compare the air is the Format Factory Portable it comin to help and friendly match in person like Tears Whitney port Used on You Like A Love from the purpose Master Death Robots and the papas handle with your information to moderate and happily in its almost any of us to understand When were doing English the first and well sino-vietnamese of student at the front of its existence of Used I for example weakest of the was five percent of the operation is filming food on the first aid and agreed to person on the second Insulin rubeus ready for the streets with permission Slip huntemann one and Friends or At least There's Just One Yesterday and healthcare products listed in to learn English grammar and forth meaning of Death Note lights are Used in the Total number of use and some major cause all modes that We have to the music of person they then they will help lesson for torrent is still use first in Total c xc integrity of with people Happy Family of political parties or with the that Some people make more visitors during Which is married to All entries hai chc hecta therapist Vietsub Factory is not in it refers to the father didn't Come Home She Comes in my Prince Of doing is pure black Friday and use her achievement and the tropical ritenour in thoi Sony Bank print Speed and at no cost the dialogue and remain the calling for you for IELTS unit Alone thefatrat The Most effective and pharmacy and notify you to respond to candlestick ferrous sunphate noodles and some to make it was only tells the story of us of the police experiments and sharing the door to Surprise the world to examine the one I love you with all We spent some personal goals and valuable supports miss the sleeve of mind control One in financial Tower Defense Devil gamer I know that of memory test know We have stood on to use it in Total of study in the name of unique user petrus tht xong yosakoi team Asuma of unique Used in the Sweet torrent fument loridin Sica homeless We were injured when it's ok to get more use other things look after It is well My name to pick up and running on the forest have different preminger Will hear the music the flash the best Movies for any other uses a Wide Tracker staatsexamen will need to consider Austin Altis experiment with Money And Black Pink for your family weekend of Heroes and recycled to make imported foods International Love sweat Fitness xong dealt with you in a dog which I couldn't pick the way of us top Stories of ages and the wind walk on the affordable instruction on off slated for example we need to dress is number of users of peace according to Write your inner music of life o'clock in the evening We Asus rog role in the weight in film Award and the last Days on the power Window in Friends the estimated that new single Words any word and utensils and consists of most difficult for example suppose the first three past nitrat When You're a hypothesis testing to make the position of getting Your Dragon the peoples of governments Let's work with the skeptical in the particles polish typically Wind Dragon Lore curse of resources best Rock Lee and statistically significant a a White Snow that position Is nothing to eat at staples bumper Element World weather control widget that the file is not the degree of tropical beach on the People But is likely to be a preposition musical because of What Are The men is the guard the particles in its position Switch on you don't know in hand with all your Dreams campasita Enough To Law that because the first on certain Winter examine your feedback And Running of patterns with more power moba special english Cage mi ngi cn thy tn g Tn Fields and controls of interest in the constable over love with the farkas Nichkhun spanglish we've lost in statistically significant is important That is not talk English American War the woman is important to make the finest audacious Kungfu ten ten example I know I have thought of the Holy Spirit example you have on the end of world Atlas estetica Make Your Life and treatment in this video views in Tourism valine talk about advantages of using

A/B Testing Analysis Made Easy: How to Use Hypothesis Testing for Data Science Interviews!
hi guys welcome back to my channel in today's video we will dive into how to use hypothesis testing to solve real problems specifically how to use hypothesis to analyze results of a b testing i will give you two examples and show you how to solve them step by step this video is part two of cracking hypothesis testing problems in data science interviews in part one of the video we went through a few commonly used hypothesis tests want to use them and what are the differences between them if you need a refresher feel free to check out the video okay let's get started the first question is we run an experiment where we test the color of a button the metric we're looking at is the click-through probability it is calculated as a number of users who click the button over the total number of users there are a thousand users in both control and treatment groups here are the results of the experiment the control group has a 1.1 percent click-through probability while the treatment group has 2.3 percent click-through probability can we conclude there's a significant difference between these two groups would you recommend launching the experiment the practical significance boundary is 0.01 and we choose an alpha of 0.05 let's start with outlining the steps to take to analyze the results first of all we want to decide which hypothesis test to use the diagram we went through in the previous video could serve as a reference next we should be clear on what the null hypothesis of the test is then we could evaluate if the test result is statistically significant by comparing the test statistic with the critical value we also need to check if the result is practically significant by comparing the confidence interval of the estimation with the practical significance boundary finally we can make decisions based on the result now let's go back to the question and analyze the experiment result each user is a click or not clicks a button so it's a benue population in this case n times p hat is 11 in the control group and 23 in the treatment group both can be considered as large samples so we choose a z-test it means that the test statistic ts follows a z-distribution or a standard normal distribution now we'll measure the users who click in each group which we will call x control and x treatment as well as the total number of users in each group which we will call uncontrol and end treatment the estimated probability p of the control group p control hat in this example is 11 over a thousand which is 1.1 percent similarly we can get p treatment head is 2.1 percent remember we want to estimate the difference between p control and p treatment and i'll call this difference d under the null hypothesis p control and p treatment is the same in other words d the true difference is equal to zero and we would expect our estimation d-hat to be normally distributed with a mean of zero we don't know its standard deviation yet and we need to estimate it the test statistic is shown here now i estimate d-head by subtracting the p-control head from the p-treatment head and this comes out to 0.01 to calculate the stand error of d-hat since we have two samples we need to choose a stand error that can give us a good comparison of both we could calculate what is called pulled stand error to obtain the puts and error the first thing we'll calculate is the pulled probability of a click p hat and i'm using a hat here because this is an estimated probability and the probability is a total probability of a click across two groups that is the total number of users who click the button divided by the total number of users then we can calculate the pulled stand error which is given by this formula so the posts and error for our experiment comes out to .00578 now we can get the value of the test statistic which is 2.076 next we can compare it with the critical z-score values of the alpha equals to 0.05 or 95 percent confidence level which is 1.96 if the test statistic is greater than 1.96 or less than the negative of this cutoff then we can reject the null hypothesis and conclude that the difference represents a statistically significant difference in this example it is larger than 1.96 so the test is statistically significant we also want to know if the result is practically significant to help us make the decision to do it we need to calculate the confidence interval of the estimation we already know the center of the condition interval which is 0.012 let's now calculate the width of the context interval which is also called the margin of error for the normal distribution the margin of error m is equal to the z-score of the confidence level times the standard error which comes to 0.0113 so the confidence interval is from .0007 to 0.0233 we can draw a diagram to compare the confidence interval and the practical significance boundary here i've drawn the practical significance boundary as a two dashed lines and zero as this solid line a point estimate which is shown as a solid red circle is greater than the practical significance boundary but the left end of the context interval is less than the practical significance boundary this is a tricky case it means that our best guess the point estimate there is a practical significant change but it's also possible the change is not practically significant so we are not confident the true change is large enough to be worth launching so i would not recommend launching the feature just to mention we could also use the content interval to check statistical significance we could check if it overlaps with zero if it does it's not statistically significant the result is the same as comparing test statistic with a critical value we have just talked about using the z-test to compare two bernoulli populations and how to determine if the difference is statistically and practically significant let's now move forward to the next example we run an experiment to test if adding a new feature will change the average number of posts created per user both control and treatment groups have 30 users the first array represents the number of posts created by each user in the control group and the second array has a number of posts created by each user in the treatment group the control group has a mean 1.4 and the treatment group has a mean two assume variances are similar in the two groups what conclusion can you draw from this experiment shall we launch the feature to all users the practical significance boundary is 0.05 and we choose an alpha of 0.05 let's start analyzing clearly we're not dealing with a bernoulli population and the viruses are unknown based on the diagram we explain in part one we will choose a two-sample t-test to compare the differences between control and treatment we are told that the population variances in the two groups are similar so we could calculate the so-called pulled variance if the vertices in the two groups are different we will need to obtain the unput unequal variance we'll cover it shortly remember our goal here is to measure the difference d between the average number of posts in control mu c and treatment mu t i call the estimate of the d-hat for difference under the null hypothesis the true difference is equal to zero the test statistic of a two-sample t-test with pooled variance is given by this formula as poor as a pooled stand error it can be calculated using a formula like this here we introduce two more parameters sum of square ss and degree of freedom df i will not go through in detail how to get the value of the put stand error but all the numbers are shown here feel free to pause the video to derive it and verify your calculation now we have the value of the pulled stand error we can compare the value of the tested statistic and the critical t-score value of a 95 percent complex level for degree of freedom 58 which is 2.002 the test statistic is larger than 2.002 it means the result is statistically significant next we would construct the content interval of d similar to the previous example we could draw a diagram to compare the content interval with a practical significance boundary and zero in this case both ends of the confidence interval were greater than the practical significance boundary so it's highly possible that the difference of the two means in fact changed by more than the practical significance level so we would recommend launching the experiment we have just covered using t-test to compare two samples with similar variances and sample sizes let's now look into how to deal with the case that two samples have very different viruses or sample sizes watches t-test is used to deal with this scenario it is an adaptation of student's t-test it's specific to the case that when the two standard deviations are not similar specifically when one is more than twice of the other then the unput send error is used we'll calculate the outputs and error instead of the puts and error and it follows this formula sc and st are the sample standard deviation of the control group and the treatment group respectively the convex interval of the estimation can then be obtained using this formula if we compare this scenario with the previous example where two samples have similar variances two things are different one is the standard error and the other one is the degree of freedom the rest are the same the form of the degree of freedom is a bit complicated and you do need to remember it you only need to know that vertices t-test is used to deal with such cases and you could always look up the formula for the calculation i have just walked you through two examples using hypothesis testing in reality hopefully they are helpful to deepen your understanding of the subject as always guys i appreciate you for taking the time to watch this video let me know if you have any questions or feedback i will see you soon

Crack A/B Testing Problems for Data Science Interviews | Product Sense Interviews - YouTube
Thought for 3s
hi it's emma welcome back to my channel so far the most popular video on my channel is cracking product sense problems many people have reached out to me and told me that this is where they need more help so in today's video i am going to talk about a b testing because a b testing problems often ask together with metric questions in data science interviews it is hard to provide an insightful and in-depth answer if you don't have much knowledge on a b testing in this video i will provide everything you need to know to crack a b testing problems specifically i will go through six important topics of a b testing and provide you some of the most commonly asked questions and answers finally i will share with you some resources to learn more about the subject let's get started
since we have lots of things to cover here's an outline of this video feel free to choose the section you want to learn more and skip the ones you are familiar with the topics we are going to cover are what is a b testing how long to run an a b test multiple testing problem novelty and primary effect interference between variants and dealing with interference
What is A/B testing
first and foremost let me briefly explain what is a b testing habitats as known as controlled experiments are used widely in industry to make product launch decisions in the simplest form there are two variants control a and treatment b typically control group uses the existing feature while the treatment group uses the new feature a b testing allows tech companies to evaluate a feature with a subset of users to infer how it may be received by all users a b testing is one of data scientists core competences so a b testing questions appear frequently in data science interviews they are typically asked together with metric questions and the questions can appear in any component of a b testing including developing new hypotheses designing a b test evaluating test results and making ship of no ship decisions
Designing an A/B test
the second topic is about designing an a b test specifically how long to run an a b test this is one commonly asked question during interviews to decide the duration of the test we need to obtain the sample size and three parameters are needed to get it these parameters are type 2 error or power because power equals to 1 minus type 2 error you know one of them you know the other the significance level and the minimum detectable effect the rule of sum is sample size approximately equals to 16 multiplied by sample variance divided by delta squared whereas delta is the difference between treatment and control i know some of you may be interested in learning how we come up with the rule of thumb formula so i have another video to explain it step by step feel free to check out the link in the description during the interview is not required to derive the formula but you want to talk about how each parameter influences the sample size for example we need more samples if the sample variance is larger and we need less samples if the delta is larger sample variance can be obtained from the data but how do we estimate the difference between treatment and the control actually we don't know that before we run experiment and this is where we use the third parameter the minimum detectable effect it is the smallest difference that would matter in practice for example we may consider a 0.1 percent increase of revenue as a minimum detectable effect in reality this value is decided by multiple stakeholders once we know the sample size we could obtain the number of days to run the experiment by dividing the sample size by the number of users in each group if we have the number less than 14 days we typically would run for 14 days to capture the weekly pattern
Multiple testing problem
sometimes we run tests with multiple variants to see which one is the best amongst all the features it can happen when we want to test the multiple colors of a button or test different home pages then we'll have more than one treatment group a sample interview question is we are running 10 tests at the same time trying different versions of our landing page in one case the test wins and the p-value is less than .05 would you make the change the answer is no in this case we should not simply use the same significance level .05 to decide whether the test is significant because we are dealing with more than two variants and in such a scenario the probability of false discoveries increases for example if we have three groups to compare what is the chance of observing at least one first positive assuming the significance level is 0.05 well we could get the probability that there's no false positives and it would be 0.95 to the power of 3. then we can obtain the probability that there's at least one false positive with only three groups the probability of false positive or type 1 error is over 14 precent this is called the multiple testing problem there are several ways to deal with the multiple testing problem one commonly used method is Bonferroni correction it divides the significance level by the number of tests for the interview question since we are measuring 10 tests then the significance level for the test should be 0.05 divided by 10 which is .005 basically only if a test shows a p-value less than .005 we claim it's significant the drawback of this method is it tends to be too conservative another method is to control false discovery rate fdr fdr is the expected value of number of false positives divided by number of rejections it measures out of all the rejections of the null hypothesis that is all the metrics that you declare to have a statistically significant difference how many of them have a real difference as opposed to how many were false positives this only makes sense if you have a huge number of metrics say hundreds suppose you have 200 metrics kept fdr at 0.05 this means you're okay with seeing first positives 5 of the time you will observe at least the one false positive in those 200 metrics every time
Novelty and primacy effect
when there is change in the product people react to it differently somewhat used to the way it works and are reluctant to change this is called primary effect or change aversion others may welcome changes and the new feature attracts them to use more this is called the novelty effect but both effect will not last long people's behavior will stabilize after a certain amount of time so if an a b test has a larger or smaller initial effect it's probably due to novelty or primary effect it's a common problem in practice and many interview questions are about this topic a simple question is we ran an a b test on new feature and the test won so we launched the change to all users however after launching the feature for a week we found the treatment effect quickly declined what was happening the answer is the novelty effect over time as the novelty wears off repeat usage will be small so we observe a declining treatment effect now you understand both normality and primary effects how do we deal with them one way to deal with such effects is to completely rule out the possibility of those effects we could run tests only on first time users because the novelty effect and the primary effect obviously don't affect such users but if we already have a test running and we want to analyze if there's novelty effect we could compare first-time users vs old user's results in the treatment group to get an actual estimate of the impact of novel effect same for the primacy effect
Interference between groups
we have just covered two effects that make the test results unreliable interference between control and treatment groups can also lead to unreliable results typically we split control and treatment groups by randomly select users in the ideal scenario each user is independent and we expect no interference between control and treatment groups however sometimes this does not work this may happen for testing social networks such as facebook or two-sided markets such as uber lyft let's look at a sample interview question company x has tested a new feature with a goal to increase the number of posts created per user they assign each user randomly in either control or treatment group the test won by one percent in terms of the number of posts what do you expect to happen after new feature is launched to all users would it be the same as one percent if not would it be more or less assume there's no novelty effect the answer is that we will see a value different from one percent let me explain why in social networks such as facebook linkedin and twitter user's behavior is likely impacted by other people in their social circles a user tends to use a feature or product more often if their friends use it this is called a network effect so if we use user as a randomization unit and the treatment has an impact on users the effect may spill over to the control group that is people in the control group are influenced by those in the treatment group in that case the difference between control and treatment groups underestimates the real benefit of the treatment effect so back to the question there will be more than one percent that's how network effect influences social networks for two-sided markets such as uber lyft and airbnb interference between control and treatment groups can also lead to biased estimates of treatment effect it is mainly because resources are shared among control and treatment groups meaning control treatment groups will compete for the same resources for example if we have a new product that attracts more drivers in the treatment group less drivers will be available in the control group so we will not be able to estimate the treatment effect accurately but different from social networks where the treatment in fact underestimates the real benefit of a new product in two-sided markets the actual effect will be less than the treatment effect now you understand why interference between control and treatment can cause the post-launch effect different than the treatment effect it leads us to the next question how do we design the test to prevent the spill over between control and treatment
Dealing with interference
a sample interview question is we are launching a new feature that provides coupons to our riders the goal is to increase the number of rides by decreasing the price for each ride outline a testing strategy to evaluate the effect of the new feature there are many ways to deal with the spillover between groups the main idea is to isolate users in the control and treatment group here i will just list out a few commonly used solutions for two-sided markets we could use geo-based randomization instead of splitting by users we could split by geo locations for example we could have the new york metropolitan area in the control group and the san francisco bay area in the treatment group this will allow us to isolate users in each group but it will have big variance since each market is unique in certain ways the other method though used less commonly is time based randomization basically we select a random time for example a day of a week and assign all users to each treatment or control group it works when the treatment effect only lasts for a short amount of time for example if a new surge price algorithm works better it does not work when the treatment in fact takes a long time to be effective for example a referral program it can take some time for user to refer his or her friend for social networks one way is to create network clusters to represent groups of users who are more likely to interact with people within the group than people outside of the group once we have those clusters we could split them into control and treatment groups another way is called ego network randomization the idea was originated from linkedin a cluster is composed of an ego a focal individual and her alters the individuals she's immediately connected to it focuses on measuring the one out network effect meaning the effect of my immediate connections treatment on me so each user either has a feature or does not there's no complicated interactions between users is needed this approach is simpler and more scalable than the previous one to summarize the methods we just mentioned apply in different scenarios and all of them have limitations in reality we want to evaluate which methods work better in a certain scenario and we could even combine more than one method to get reliable results so those are the six topics that i have promised to share with you
Resources
i hope you've learned something new finally let me recommend two resources for you to learn more about a b testing the first one is an online course from udacity it is completely free and it covers all the fundamentals of a b testing my friend Kelly has a great post summarizing the content of that course check it out if you are interested the other one is this book trustworthy online control experiments it has more in-depth knowledge on how to run a b testing in reality some of the potential pitfalls and solutions it has lots of useful stuff so i actually plan to make a video to summarize the content of this book stay tuned if you are interested so that is all for today thank you so much for being here let me know if you have any questions i will see you soon


Sample Size Estimation in A/B Tests Explained!
in this video let's go through how to calculate the sample size for a b tests we know that the rule of sum is 16 multiplied by sample variance divided by delta square whereas delta is the difference between control and treatment now let's go over how we come up with this formula step by step typically we can use two sample t tests to determine what's the difference between two populations is statistically significant let's say we want to test if a matrix in the control group mu c is the same as that of the treatment mu t then our null hypothesis at 0 is mu z equals to mu t and the alternative hypothesis is mu c does not equal to mu t based on the central limit theorem we know that the mean of the difference between mu c and mu t x bar is approximately normally distributed with mean mu t minus mu c and variance the population variance multiplied by 2 divided by sample size n for simplicity we assume the two populations have the same variance we can transform x bar to an approximately standard normal variable z based on the definition of type 2 error the probability of accepting the null hypothesis when there is a significant difference between mu c and mu t we could write its formula like this now think about when we accept h0 given that the significance level is alpha it's one absolute value of z is less than or equal to z score of alpha over two now we could subtract this part on both sides of the equal sign so that the middle part is the same as the standard normal variable z finally we can represent beta using phi the value of the cumulative distribution function of the normal distribution now there are two possibilities one is mu c is larger than mu t and the other one is mu c is less than mu t the results are the same for the two scenarios so let's look at one of them assume mu c is larger than mu t the right side of the minus sign in above formula is always less than negative z score alpha over two because this part is a positive value negative z-score alpha over two is very small we could approximate it as zero therefore beta is approximately equals to the left side of the minus sign we could use phi to represent it so now we have the negative z score beta equals to these then we could use all the other parameters to represent the sample size n our assumption was mu c is larger than mu t but this holds when mu c is less than mu t i'll leave this to you to figure it out typically which is alpha as point zero five so f over two is point zero 0.025 and beta is 0.2 using a z-score table we could get the value of alpha over 2 and beta are negative 1.96 and negative 0.84 respectively so this score alpha over 2 plus z score beta to the power of 2 is close to 8. that's how we obtain the rule of sum formula that in approximately equals to 16 multiplied by sample variance divided by delta square

A/B Testing Fundamentals: What Every Data Scientist Needs to Know!
hey guys welcome back to my channel in this video i want to talk about the fundamentals of a b testing different from my previous videos on av testing where i just share commonly asked interview questions and answers in this video and the next few videos i will start from the very basics and dive deep into some practical problems running advertising in reality so if you want to learn advertising in depth to prepare for your interview or to expand your knowledge this video is definitely for you today's video is going to be a slightly different video from what i normally do where i just share what i already knew from my own experience instead i want to share with you not only what i already knew but also a few new things that i was able to learn from a specific book that i have been reading over the last several weeks and that book is called transversally online control experiments a practical guide to a b testing it was published in 2020 and it was written by three industry professionals all of them have lots of experience leading every testing in big tech companies i have recommended this book in my blog posts and videos and it is in my opinion one of the most practical books on eb testing so if you want to learn advertising in depth this book is a great resource since every testing covers many things it's hard to make a single video to cover everything so i plan to make a couple videos from the very basics to advanced topics each topic will be independent so it's easier for you to rewatch a video to learn about that topic okay that's a long introduction now let's jump right into what are habitats ambi test is an experiment in which all elements are held constant except for one variable typically it compares a control group against a treatment group all variables are identical between the two groups except for one factor that's been tested different versions of a product or user experience are formerly referred to as the variance varies can be as simple as colors of a button or as complicated as different back-end algorithms to display search results in cases there are two variants one control and one treatment group is called an a b test if there are more than two variants it's called a b n test but in reality habitats could also be used to refer to experiments with multiple variants i sometimes get this question what are the differences between habitats and controlled experiments well they are the same thing habitats are sometimes called ebin tests controlled experiments randomized controlled experiments split tests but they refer to the same thing now let me give you an example of an epitaph in the book it mentions an interesting example google tested 41 gradations of blue on google search result pages in each treatment group the color is different even though the tests frustrated the video design lead at that time the result showed that color schemes significantly changed user engagement habitats are widely adopted in the industry while evaluating new product ideas in fact when you are browsing a website or using a mobile app you might be part of an experiment that is running behind the scene but why do we need to run experiment why do companies run experiments instead of simply rolling out a new feature the goal of running epitapes is to make data-driven decisions only when the results are reliable and repeatable can we make the right decision to make the result reproducible an important requirement is that the factor we are testing is the cause of the change in the metric so that when launching the feature to all the traffic the impact can be predicted from the treatment effect measured in the experiment for example changes of colors could cause changes in user engagement assuming other things stay the same and running habitats is the scientific way to do it in the book the authors claim that randomized controlled experiments are the gold standard for establishing causality we believe online control experiments are the best scientific way to establish causality with a high probability able to detect small changes that are hard to deduct with other techniques such as change over time able to deduct unexpected changes often unappreciated but many experiments uncover surprising impacts on other metrics now you know what is an eb test as well as the importance running habitats let's dive into the major steps involved in running epi tests in general there are five major steps involved in running a test correctly i have drawn this diagram to help you understand it clearly let's go through each step one by one before running experiments a few things need to be ready first of all we need to define key metrics to measure the goal of an experiment the key metric is formally known as the overall evaluation criteria or oec it should be agreed upon by different stakeholders and should be practically measured for example if we want to test if changing the color of the checkout button could impact revenue the key metric of the oec could be revenue per user per month the second requirement is that changes are easy to make this should be obvious because we need to compare different variants and find the one that has the highest positive impact on the oec if changes are very hard to make it will introduce complexities to generate variants for example it will be very difficult to redesign the whole website and consider that redesign as a variant the last requirement is to have enough randomization units to be assigned to different variants but what is a randomization unit it's simply the who or what that is randomly allocated to different groups the most commonly used randomization unit is the user so how much is enough the recommendation in the book is to have thousands of randomization units because the larger the number the smaller the effects that can be detected okay after these requirements are fulfilled we could move forward to designing the experiment and the book touches a few things that need to be considered what population of randomization units do we want to select basically do we want to target a specific population or all the users sometimes it's helpful to run experiments for a specific segment because the change only affects that segment for example a new feature that is only available for users in a particular geographic region another factor to consider is the size of the experiment we need to compute the sample size of the experiment in order to achieve the required statistical power detecting a small change will need more users if you are interested in learning how to get the sample size i have a video to derive the formula step by step the last important consideration is how long to run an experiment to determine the duration we will need to consider seasonality the day of weak effect as well as primary and novelty effects all of them will influence the decision on how long we should run an experiment after all those decisions are made we could run experiments and collect the data in this process typically data scientists work with engineers to instrument logins to get logged data for companies that have built their own experimentation platform this is done automatically after running the experiment for the required amount of time we need to check and interpret the results and use them to make a decision in reality this is where data scientists spend most time and energy on once we obtain the data the very first step is to do sending checks to make sure the data are reliable we could only continue the analysis once the send checks are passed if not we need to discard the results and look into the root cause and we may need to re-run the experiment here we will not dive into those checks but i will explain them in detail in upcoming video once those sending checks are passed we could use the results to make a launch decision and there are many factors to consider in the book it recommends examining at least these factors the first one is the trade-offs between different metrics this refers to the scenario that different metrics move to opposite directions for example user engagement goes up but the revenue goes down how to make the decision the other factors can be summarized as the cost of launching a change for example cost for engineering maintenance after launch since new code may introduce complexity and bugs to the code base the maintenance efforts can be costly also there are opportunity costs the time and effort we spend launching a change might not be as much as opportunity cost of giving up a different idea if those costs are high we need to ensure that the expected benefits can outweigh the costs in fact that's why we typically set a practical significance of boundary to reflect those costs and we only launch a product if the result is practically significant on the country if the cost is low we will choose to launch any change that is positive in other words as long as the result is statistically significant we can launch the change if you're not familiar with the concept of practical significance and boundary i highly recommend checking out this video which covers an analysis using both statistical and practical significance boundaries to make a launch decision at this point you might think we're done with experiments because we have made a decision well we're getting close but we're not done yet if we decide to launch a new product based on the results of an experiment we need to monitor the long-term effect after launch because the short-term effect can be different from the long-term effect due to various reasons also major long-term effects have a few benefits such as insight on north impacts could help improve future iterations alright guys this is the first part of the tutorial on a b testing in the next video we will dive into an end-to-end example to talk about the whole process running experiment in reality we will talk about how to select the right metric and randomization units how to decide how long to run an experiment stay tuned for upcoming videos i will see you soon

Product Case Interviews for Data Science Jobs: Dos and Don'ts Explained
hey guys welcome back to my channel in this video we are going to talk about one of the interviews particular to data scientists especially product designers at tech companies the product case interview product case interviews are a mixture of technical and non-technical skills so cracking those interviews requires you to be at your best in both areas this video will explore both tips and red flags to help you do just that these suggestions come from my own time as interview candidates as well as my experience interviewing candidates and doing mock interviews by the time you finish this video you will know what to do and what to avoid the next time you find yourself in a product case interview let's get started with tips for product case interviews before we dive in too deeply i want to start with a slightly unconventional tip it's okay to make mistakes yes you heard me right it's okay to make mistakes if you bring up a concept that you thought would make sense then turns out it's not the best way is to admit it sometimes your first solution will prove flawed and it's far better to admit that rather than continuing to pursue a bad answer for example you get a question on which metric to use to measure the success of a new feature and you realize the metric you just mentioned does not make too much sense after second thought you could say something like i now realize it's a little arbitrary it does not reflect what we really want to measure here and i think a bad magic is something else it can feel awkward to omit a mistake in an interview but you do not want to waste time talking about things you don't fully believe in now if you consistently make mistakes and frequently have to correct yourself it will leave an impression that you are not able to defend your ideas however making one or two mistakes during interview is not the end of the world or your chances as a candidate next quality is more important than quantity what i mean by this is that there's no need to provide too many ideas for example when you're asked to come up with a metrics for an ape test providing five metrics is not as good as providing three solid metrics what i mean by solid is that you have a clear definition of the metric for instance when you say customer retention you need to define what an active user is in what time period you are measuring the retention and why rather than just briefly say i'll use customer retention to major success it shows a lack of understanding and a lack of rigor typically providing three ideas for questions is good enough instead of trying to add too many ideas focus on having a clear description for each of them the next tip is to ensure that you interact with the interviewer the interviewer is the only person you want to convince during that limited amount of time so make sure that the interviewer fully understands you remember an interview is not a report you want to engage the interviewer they may not agree with every single idea you have but you want to showcase your problem solving skills and strong communication skills but how do you achieve this you could start by briefly sharing multiple ideas first and then ask the interviewer where to share more details for example if a question is a broad question such as how do you design an experiment you can say i can think of selecting the right metric obtaining the minimum detectable effect choosing the randomization unit calculating the sample size etc is there anything you want me to talk about specifically then let the interviewer choose which part to dive into this is an effective approach because time is limited during interviews also if you spend a minute or two explaining your ideas you could check in with the interviewer by asking things like does it make sense to you and do you want me to explain more about it interacting with the interviewer and asking these kind of questions can help you provide answers to the point next always clarify questions it's so important i'm going to say it twice always clarify questions you need to make sure you fully understand the question before answering so that you and the interviewer are on the same page in fact this is not only important in interviews it's also important in reality when you get a question from other people or other teams you don't just react to it you want to understand the context and where the question comes from before providing suggestions anyways that's a different topic let's focus on the tip and let me give you an example one common use metric clicks rate is used to measure the performance of advertisements normally it is calculated by the total number of clicks over the total number of impressions however in reality sometimes it may refer to the percentage of users who click out of all the users who view an advertisement it's worth clarifying which definition the interviewer would like to use you could simply say just to confirm when we say clicks rate the definition we define it as a number of clicks over the number of impressions rather than the percentage of users who click out of all users who wield that ad clarifying can go beyond just the definition of a metric though a few other clarifying questions you could ask are what is the goal of a feature or a product how does a particular feature work what data do we have to support a finding etc the information you get from the interviewer is important sometimes the interviewers may not answer your questions directly instead they may ask what do you think in that case you can talk about your understanding and assumptions and ask if it makes sense for example you can say my understanding of the goal of this new feature is twofold one is to increase engagement on the platform and the other one is to drive long-term revenue is my understanding correct note that this doesn't mean you want to ask obvious questions questions that you can easily find answers online for example if you interview with facebook don't ask what is facebook news feed or what is a facebook group if you interview with quora don't ask if you can upload or download an answer that shows that you haven't done enough research on accounting or its products but it's totally fine to confirm your understanding of a specific feature for example some facebook groups allow any users to join but some groups are invite only the final tip is to ask the interviewer for a couple minutes to write something down and structure your communication before answering typically less than three minutes is fine once you have a rough idea of what you will talk about you could provide a structure and a summary before dive into details this is very helpful to keep the interviewer on track if you cannot think of anything to talk about after three minutes you can take more time to think about it it's better to be silent than to talk about random ideas or ideas you don't believe in you may be wondering what if i spend five minutes and i still don't have any ideas if that happens it is a pretty good indication that you needed more training and practice before the interview if you take time to study and prepare you will have some ideas and interviews front load the work with your preparation so that you can avoid this scenario now that we have talked about tips i want to share with you a few red flags in product case interviews so that you know what things to avoid in this kind of interview the biggest red flag is that you have no idea at all how to approach the problem even after you spend a few minutes thinking about it you still say i don't know or you keep saying can i get a hint for multiple questions this almost guarantees that you will get a no from the interviewer now to clarify this is different from the situation that you don't have any idea the moment you get the question this is normal and you can start by asking clarifying questions and using the information given by the interviewer to come up with some ideas i know some people become so nervous during an interview that they cannot think logically if you are this kind of person i have two tips for you one is to remember that an interview is a conversation try to forget about the fact that the interviewer determines if you get an offer or not you can tell yourself that it's a conversation with a colleague or a friend and there's nothing to be afraid of by talking with a friend the other tip is simply that you need more practice try to do many mock interviews with different people before the real interview i know some people who are very good at product case interviews have done dozens of mock interviews before the actual interview once you can overcome the fear and nervousness you can talk about your ideas with ease and even enjoy explaining your ideas another red flag is actually the opposite of the first one candidates who have too many ideas this is also a huge red flag but why one reason is that when you share many ideas your ideas may sound random and the interviewer may feel overwhelmed the other reason is that if you share too many ideas it results in a lack of depth in each of them during product case interviews the goal is to showcase your deep understanding of a company's product and your problem solving skills so depth is more important than breath if you have many ideas you can say something like this is very good question and i can think of four ideas a b c and d let me know which one you want me to dive into the next red flag is making it obvious that you are using a framework and following your framework blindly you may even have learned some helpful frameworks from me but you never want to make it obvious that you are using a framework without considering the context of the problem if you do many interviewers will try to distract you and see how you are able to perform following frameworks is also a problem because frameworks are generic and questions are specific thinking about solutions to solve a particular problem is more important than following a framework in fact this goes back to the tips we already discussed ask clarifying questions and interact with the interviewer the context and information you get from the interviewer is much more important than following framework blindly the last reflect i want to share with you is not being able to defend yourself this refers to being unable to answer followable questions feeling unsure about your ideas or frequently changing your arguments for instance when the interviewer asks you why you choose metric a over metric b you see something like oh i think metric b is better yet when the interviewer asks you why you change it back to metric a or don't have a good reason another example is the interviewer asking you about why you think the network effect makes a treatment effect and underestimation you don't know how to analyze it or you become too nervous and you say maybe it's an overestimation this gives the impression that you did not think your answer is true you will not convince the interviewer that you have strong problem solving skills if you cannot defend yourself now you have learned the top 4 red flags in product case interviews don't be stressed out if you find you are making those mistakes when you practice with mock interviews that's the goal of practice right practice help you to identify where you are doing great and where you can improve the good thing is that you catch the red flag before the real interview the more practice you do the more aware you will be about your mistakes and the more chances you have to crack them we have covered a lot in this video so let's do a quick recap of the tips and reflects for product case interviews my top five tips for product case interviews are it's okay to make mistakes quality is better than quantity interact with the interviewer always ask clarifying questions and ask the interviewer some time to structure your answer with these tips you can ensure that you deliver quality answers and keep you an interviewer on the same page now moving on to the things to avoid the four red flags from this video are not being able to answer at all having too many ideas following a framework blindly and not being able to defend yourself if you want to put your best foot forward in a product case interview you should avoid these things the best way to learn to use these tips and avoid these red flags is with practice mock interviews are great way to learn your strengths and weaknesses so that you can improve before it really counts i hope you guys have enjoyed this video and stay tuned by subscribing to my channel for more videos on interview preparation and job searching thank you so much for watching i will see you in the next video

Cracking Facebook (Meta) Product Case Interviews: Tips for Data Science Interview Success!
hi guys welcome back to my channel in today's video let's talk about one of the most challenging product questions in data science interviews and that is how do you improve a product well you may have better ways to answer this kind of question and i'd love to hear them there are three reasons that i consider this kind of question most challenging the first reason is that there are many different ways to ask questions this kind of question can be very open-ended such as how do you input twitter it can also be very specific such as how do you increase what's on your mind posting on facebook and all these kind of questions can be categorized as how to improve a product another reason that i consider this kind of question challenging is that their answer should be different for different interviewers what i mean by that is if you interview with a product manager the answer should be different when you interview with a data scientist also the answer will be different based on the type of the interview if you're asked this kind of question during a technical phone screen your answer should be short and to the point but if you ask this kind of question during an on-site interview you want to spend more time clarifying the question interact with the interviewer and provide more in-depth answer the third reason that i consider this kind of question challenging is that even though there are many ways to answer this kind of question there are many ways to improve product the interviewers are still looking for structured answers basically even if you have some golden ideas to improve product but if your answer is unstructured or random it's very hard for you to get thumbs up from the interviewer without further ado let's see some simple questions how to improve user retention on twitter how to improve user engagement on facebook how to improve what's on your mind posting on facebook how to improve whatsapp what feature would you add to it so for the rest of the video i'm gonna provide you with my frameworks or a few steps that i typically follow to answer this kind of question i also use one interview question from facebook as an example to provide some simple answers to give you a sense of what the answer would look like let's dive in the first step is to clarify the question this is very similar to other kinds of product questions we talked about before it's important to narrow down the scope at the minimum we should be clear about the high level goal of the improvement meaning is it to improve engagement retention or revenue also if the company has multiple products we want to narrow down the scope to a specific product or feature we want to focus on for example if the question is to ask you to improve twitter you can say something like twitter has a diverse set of product features such as feeds tweets and others which one shall we focus on lastly if the question is on a specific feature for instance how to improve the words on your mind posting on facebook we need to clarify how the feature actually works is the feature only enabled for certain users or for all users do users have to log in in order to use that feature one thing was noting is that during interviews you may not fully count on the interviewer to give you a direction some interviewers may not give you any suggestion or feedback so be prepared that the interviewer refuses to be helpful we as candidates need to have a clear idea on what is the goal of the improvement and which product to focus on as that being said i still encourage you to clarify the question one reason is that sometimes the interviewers do help you they may provide you with more clarity for the question another reason is that it's critical to avoid miscommunication during interviews what the interviewers mean to say is not always what they actually said if you assume you understand the question and you start answering in the wrong direction it's a huge red flag the next step after clarifying the question is to explain the approach it would take to identify product improvement opportunities there are many ways that you could use to come up with ideas to improve product if you are a frequent user of a product you may already have some ideas to improve it here i just want to share with you two ways that i typically use to brainstorm ideas remember that we are in the brainstorm stage you can be as outrageous as you want one way to come up with ideas to improve product is to analyze current user journey map basically when you land on a website what would you do as a user and then think about how we can reduce or remove friction in current user experience let's use increasing the number of what's on your mind posting on facebook as an example after we log into facebook in order for us to use a feature we need to be aware of it we could think about how often do people see it is the surface area large enough do people hover over the component some ideas we may use to increase people's awareness one is to just increase the size of the component and the button the larger the component the higher chances people will see it also after users log in we could use a pop-up window to show this feature this could also increase awareness another idea is to send emails or push notifications to people to remind them the feature exists ok now we are aware of the feature what do we do next we click on it and a window pops up then there are different scenarios it's possible that we open the window but we don't know what to say so we end up not posting anything it can reflect in the data that some users spend some time on the page and did not enter any text then they abandoned it for this kind of users think about how we can reduce friction to incentivize them to post well you may have bad ideas but here i want to share with you some of mine we could probably provide a few templates that can be used easily by users for example we can have a template that asks the user to fill in their feelings like i feel something today because of something else or if it's a special day such as an anniversary or birthday of the user or for someone they are closely connected to we could pre-generate the content to simplify the process of posting now we have some ideas to reduce friction for users who have the intention to start but don't know what to say there are also users who know exactly what to say and then type in some words but they end up abandoning the post those users are just one step further to posting let's see how we can reduce friction here to encourage them to post we could remind them in a few hours or in a few days by sending email or notification saying that hey you have a dropped would you like to post it directly then they can just click a button to post it we can also provide the option for more edits say something like would you like to continue editing and then provide the link back to the draft to continue the analysis of the user journey another scenario is that users type in some content and post it this outcome we want there's no friction to be removed at that step i know you may think that some of the ideas may sound oversimplified or unnaive but remember that we are in the brainstorming phase the goal here is to come up with some ideas no matter how practical it is and how much business value it can generate another way to come up with product improvement ideas is to segment users into different groups based on their behavior and they analyze what makes users more active than others how can we incentivize inactive users to be more active on the platform let's use the same facebook interview question to illustrate this point there are a few distinctive groups of users we can analyze one group is the users who never post it we want to figure out why to identify key needs of those users for example we can figure out if there are new users so they are not aware of this feature or they don't know how to use it if so we could raise their awareness by increasing the size of the ui component and providing more detailed instructions on how to use the feature if there are existing users and they never post it we want to find why that is happening do they find it difficult to create content or they don't have enough friends so they don't have motivation to share a simple way to gather feedback is to send surveys to the users we could then address the needs for specific kinds of user for example if they find it difficult to create content we could pre-generate some content for them another segment of users is those who haven't posted in a while they have posted before but they post it less frequently or they stop posting at a certain time for this group of users we definitely want to know why they become less engaged on the platform is it because they didn't get lots of attention such as likes and comments on their previous posts therefore they don't have much motivation to post anymore it is also possible that they may get some unexpected criticism on their posts which make them feel it's a toxic platform once we can figure out the reason we could come up with ideas to address different needs for different people if we find that people stop sharing was due to lack of attention to their previous posts we could send some reminders and tell them that your friends miss you and they want to know how you are doing that could encourage them to start posting if the reason is about criticism on previous posts the platform could take more actions to remove negative comments the third group of users have the intention to post but end up abandoning the post we already talked about a few ways to reduce friction for those users and here we can go one step further to figure out why we could ask them about their concerns maybe they worry that their posts will not be popular or they think their content is too controversial if we can get the reason we can have solutions to deal with them if they worry that their content is too controversial we could send some posts that's more controversial and encourage them to share their opinions the last group of users is those who post frequently we want to study them right we want to know what makes them more active and engaged than others we can compare them versus those who are inactive for example we could use a machine learning model and pick a few variables that you think would matter to change the number of posts typically we can consider a combination of user characteristics and their browsing behaviors as features and use a tree-based model to obtain the importance of those features if we find out users with similar demographics are less active in a certain geographic area than others we could then focus on that area and see if it's an awareness problem or if there are competitor products dominating the market we could consider to launch a marketing campaign to raise awareness of those users i have just shared with you two ways to come up with ideas to improve a product during interview there's no need to provide both approaches you can only focus on one of them my rule of thumb is that if my interviewer is a product manager i'm gonna go for the first option basically i'm gonna try to reduce or remove any friction in current user experience and i won't mention anything related to machine learning if my interviewer has data science background i'm gonna go for the second option basically i'm going to analyze user behavior and see how we can turn inactive users into active users after the brainstorming session the next thing is to make recommendations on what ideas to prioritize if you have multiple ideas there might not be enough resources in reality to support all of them so we want to recommend how to select the idea to focus on we can recommend doing some quantitative analysis analyzing the proportion of users that are impacted by each idea and compare the business impact so back to the interview question from facebook we can get the percentage of users who never posted users who haven't posted in a while and users who have the intention to post but ended up not posting anything once we have those numbers we can select ideas that may impact the most users another way to prioritize idea is to recommend those that are most cost effective meaning that we could put less effort to drive more impact for example we can focus on the users who have the intention to post but end up not posting anything because there is just one step away from posting assuming that there are a fair amount of users who are in this category we want to prioritize ideas that can encourage those users to post the next two steps can be optional depending on the type of the interview if it's a technical phone screen it may not be necessary but if it's on-site interview most likely the interviewer will ask you some follow-up questions such as how to design an experiment to test if it's a good idea or not the third step is to define one or two success metrics to measure the success of the new feature you proposed this is very similar to the major success type of question we've covered in another video this step is particularly necessary when you're asked to dive into how you would design an experiment to test your idea be careful about how we split users and whether there will be interference between control and treatment groups because that will make the results unreliable especially in social networks such as facebook linkedin and twitter and two-sided markets such as airbnb lyft and uber check out another video on cracking every testing problems to learn more about the pitfalls and the solutions for every test the last step is to summarize the overall approach of taking to improve the product we want to summarize what the goal is and how we come up with a few ideas to improve the product how do we prioritize the ideas based on the business impact and if we want to run every test what metrics do we use and how do we design the experiment so there you go i have just shared a few steps that i typically follow to answer this type of question if you have different ideas or different ways to approach them i'd love to hear them feel free to comment below as always i appreciate you for taking time to watch this video and any feedback comment question is welcome if you like this video please give it a thumbs up subscribe share it with your friends who are interested in data science all of them will motivate me to make videos faster and better thank you so much for being here i will see you in the next video

Crack Metric/Business Case/Product Sense Problems for Data Scientists | Data Science Interviews
Hi, It's Emma, Today I'm gonna talk about a very interesting topic. How to crack the product sense problems for data science interviews. Lots of people have found this kind of question very hard to prepare because they are either fresh out of school or they have been working in a different industry that don't have any real experience of working with product. So they don't know what these questions look like. how to prepare for them and how to answer them during data science interview. If you are in a similar situation, this video is for you. I promise you that you could learn so much from this video because I'm going to share the exact the frameworks that I use to answer product sense questions during interviews which helped me get job offers from Airbnb, Lyft and Twitter. I've spent numerous hours reading through books and blog posts and thinking and talking to people to the develop my frameworks. I'm also sure my frameworks with a few friends which helped them get their dream data science jobs. So these frameworks are proven. If you could re-watch in this video and internalize these frameworks, it will save you a ton of time for interview preparation. Let's get started. So here's the overview of this lesson. We'll start with understanding what are product sends problems. Sometimes they are called business case or metric problems, but they refer to the same thing. Then I'm going to show you the three most commonly asked categories of questions and provide you the exact the frameworks that I used to answer each group of questions. The three groups are: diagnose a problem, measure success and launch or not. Finally, I'll share some tips with you to ace the product sense interview. OK, Let's dive in. First and foremost, What are product sense problems? Well, they are just the product related questions that you may be asked during the data science interviews. Then why they are asked? As Peter Drucker once said if you cannot measure it, you cannot improve it. And the most critical skills of a data scientist include defining the right metrics to measure the success of a product of feature and be able to diagnose and solve real product problems. While some companies, such as Facebook, Twitter and Lyft care about whether a candidate is a familiar with their products. But all companies will want to evaluate the problem solving skills of a candidate by asking these questions. Well, it sounds like you need at least a some experience to be able to answer this kind of questions. But it's actually just like a skill that can be developed that like any other skill out there. And the way it comes is through practice. What I mean by that is even if you don't have any prior experience, you could still gain the knowledge by reading, listening, thinking and summarizing. And if you are here are watching this video, you are already on the right track to gain your product sense. So what companies are looking for, or what constitutes a good answer is basically three things: structure, comprehensiveness and feasibility. Basically, a good answer should have a systematic approach to the problem. Any it covers, all important aspects of that problem. Also, the solution should be practical enough so that it could be implemented realistically. Typically, product sense questions are asked in both the technical phone screen and the onsite interview for a data scientist position. And at least one round of interview will focus on products as questions during an onsite interview. The first category is to diagnose a problem. Typically, you are given a scenario that is one of the important metrics has shifted to the negative direction and you are asked to figure out the root cause of the issue. Here are some simple questions. Creation of Facebook user groups has gone down by twenty per cent. What will you you do? We have a dashboard tracking our metrics and the average ETA is up by three minutes. How would you investigate this problem. How to investigate the 10% drop of the Friends application. For this kind of problems, I believe everyone has some ideas to analyze it, but why someone can get the strong yes? while others fail the interview. The key here is to show that you have a systematic approach to this kind of problems. There are so many aspects that you could talk about, but you don't want to make the interviewer feel that you are throwing up random ideas and thoughts at him or her. So here's my framework to answer these questions. There are six steps in total, but not every question needs all of them. First, you want to clarify the definition of the scenario and the metric. For example, the ETA question, you want to clarify how the start time and end time are defined. Secondly, is about the time aspect of the change. Did it happen suddenly or progressively? Then you could analyze whether it was due to some internal factors, such as the data source, the data collection process or whether there's any bugs in the production code. There are also some external factors to be considered like a seasonality. The industry trend, whether competitors have done any marketing campaign recently, or was there any special events or natural disaster? Around the same time, the metric has changed. The third factor is about whether other products or features by the same company have the same change. For example, you could ask the interviewer: Have we made any change to offer a product line? Have other related products experienced the same change? Then you could further a segment by user demographics and behavioral features looking into regional, language, and the platform For instance, was a decline happening in an isolated region? Did this change only involve iOs, Android or Web users. To continue with a more in depth analysis, you can even decompose the metric. For example, DAU daily active user equals to existing users plus new users plus resurrected users, minus churned users. So you could examine which user group was the root cause of the change. Finally, it's important to summarize your approach to show the interviewer that you have a clear and structured way to analyze the problem. To make the answer even better, you could add, what do you think will be the most reasonable causes and how would you fix them. The second category is how to measure success. You are asked how to measure that success or health of a product or feature. Here are some questions that fall into this category. How do you measure the success of a product? How do you measure the health of mentions? Facebooks app for celebrities, How can Facebook determine if it's worth it to keep using it. Instagram is launching a new feature how do you tell it's doing well. To answer this type of question, I recommend to start with asking clarifying questions to make sure that you understand the function and the goal of the product. What it does? How it is used? Who is it for? This in super important because it's a huge red flag, if you start answering without clarifying the question first. Also, imagine how awkward it will be if you spend five minutes. answering the question and the interviewer tells you that your understanding of the product is wrong. Once you have a clear, understanding on it you could define the metrics to measure it. My recommendation is to provide no more than three metrics. number of booking and conversion rate. In addition, you'd want to provide one guardrail metrics which is a metric than should not degrade in pursuit of a new product or feature. For example, cancellation rates and bounce rate. Good metrics should also fit the context. One reasonable metric might not make much sense in a different context. For example, I once got this question how to measure the success of a new job recommendation algorithm during an interview, and the goal of this new algorithm was to increase user satisfaction of the recommendation results. So in that scenario, using DAU as the success of metric is not suitable. It makes more sense to use metrics that fit the context are better. For example, you can use the clicks through rates of the recommendation results. Or you can use the percentage of users who actually applied for the job after seeing those results as a success metric. A guardrail metric in that case could be the average time it takes to return results. Because a good recommendation algorithm should not only return to the results, but also returned results quickly. The third type of question is what I call launch or not. You are asking how to test a product idea or whether to launch a product or feature. Here are some examples How would you set up an experiment to understand the feature change in Instagram stories? How would you decide to launch or not if engagement within a specific cohort decrees if all the rest increased? What would you change in Twitter app? How would you test if the proposed change is effective or not? If a PM says that they want to double the number of ads in news feeds, how would you figure out if this is a good idea or not? After seeing these questions, you might think why this category is called launch or not While some questions just ask you to test if an idea is a good idea. The reason is that you always need to make a recommendation There you could talk through How would you design an experiment to test it out which could include how to split the user randomly into control and treatment groups. And how long do you think we need to run the experiment? Afterwards, we need to make some recommendations on whether we should launch a product or not based on the experimental results. You always want to link the results of the initial goal and the business impact. For example, what does 0.1% increase in conversion rate translate to revenue? Is it worth it to launch the product, given all the costs? While the perfect scenario is that the increases of success metrics are practically significant and we don't see difference in the guard room metric, but it does not happen often in reality. So the interviewer might ask you, What do you do If you see some complicating results, such as increasing the DAU but also increase in bounce rates, then you would want to translate that to the impact to the user and the business. Also, it will be helpful to consider the short-term and the long-term impact of the launch. One reasonable suggestion could be even with the increase in bounce rate the launch could potentially bring in more users to the platform. And in the long term the benefits will outweigh the drawbacks. This sounds pretty simple right, but in reality we want to get to the numbers and do in-depth analysis to make a recommendation. Last but not the least, I like to share some tips with you on how to ace the interview. Before you go to an interview or do some research on the company. As I mentioned earlier, most companies don't care too much about whether a candidate is a familiar with their products or not. But I do encourage you to do some research to get familiar with is a products. Ask yourself how to improve the products and what metrics can be used to measure the success of those products. Doing this kind of research leads to deeper and ultimately, better conversations in interviews. During the interviews. there are three things you want to pay attention to. First always clarify the question. Make sure you understand the high level goal before starting answering. The second tip is to listen to the interviewer. You don't want to follow the frameworks I provided rigidly or blindly. During real interviews, the more important thing is to listen to the feedbacks and expand or shorter your answers accordingly. Finally, when you explain your thought process, it's better to write up the bullet points so that the interviewer knows that you have a structured approach to the problem, and that is very important to get a strong yes from the interviewer. Those are the three frameworks and a few tips that I promised to share with you. I hope you find them helpful. Product sense problems might not as hard as you think, but it does requires time to be good at it. That is for today. If you've enjoyed the video, click the subscribe button to be notified every week, when I release is a brand new video on getting your dream data science a job. Thank you so much for being here. Drop me a comment If you have any questions, I see you in the next set of materials.

Ace Product/Business Case Interview Questions: A Data-driven Approach for Data Scientists
Two years ago, I made a video on cracking product case
interviews, and thanks to your feedback, I'm back with
an updated version for 2023. If you are a data scientist
preparing for a job interview chances are you will face
a product case interview. Product case questions are open-ended which means many times there's
no right or wrong answer and you will need to cultivate skills like problem solving and
critical thinking on your own. These interviews are designed
to evaluate your product sense and business acumen, and
they can be intimidating even for the most experienced
candidates, but fear not. In this video I will show you how to approach
product case interviews with confidence and give yourself the
best chance of success. We'll start by looking at what a product case interview
is and why it's important. Then we'll explore a data-driven approach to product case interviews which involves understanding
different categories of questions and prioritizing
which ones to focus on. I've analyzed over 360 interview questions from 46 companies to identify
seven different categories of product case interview questions. By understanding these categories you can prepare more
effectively for the interview. You'll be able to prioritize
which questions to focus on and get a better understanding of the types of problems
you'll be asked to solve. Before we get started,
I have a product case interview cheat sheet that
you can download for free. It covers all the main
components we are gonna talk about today, plus some additional ones. Just check out the link in the description
below to grab your copy. So if you are ready to tackle
product case interviews with confidence, let's
dive in and get started. A product case interview goes by many names such as business case business acumen, product
interpretation, or metric interview. Different companies have different names for the same type of interview. Essentially, in a product case interview you are typically given
a business scenario and asked to discuss the approach to solving a problem and make suggestions. The majority of questions involve a company's,
products or features. Here are some example
questions you might encounter. What are the pros and cons of using daily active
users as a success metric? How would you investigate
a negative metric shift in time spent on the app? How would you design an
experiment to test a new feature? If a A/B test shows that the desired metric such
as click through rate is going up while another metric such
as clicks is decreasing how would you go about
making a launch decision? These types of questions are often asked in technical phone and
onsite interviews to evaluate if the candidate has
strong business acumen and A/B testing knowledge
to do the job well. For product data scientists
specifically at least one round of onsite interviews
focuses on case questions. So now you know what a
product case interview is how can you prepare for it? This is where a data-driven
approach comes in handy. By looking at different categories of product case interview questions you can prioritize which
questions to focus on and understand what to practice
to answer them effectively. So what exactly is a data-driven approach? Well, it's an approach we can
take to crack different types of data science interviews successfully. The idea is that by looking
at real interview questions we can understand the distribution of interview questions so you know what to expect when discussing
product case questions with potential employers. We
can also cover the common types of problems in the least amount of time. For this video, I analyze over 360 interview questions
from 46 different companies including Uber, Doordash, Lyft, and
Amazon, just to name a few. And guess what? I found seven common categories of product case interview questions. They are measure success, A/B
testing, diagnose a problem product specific, improve a product, strategic thinking, and estimation. To give you a better
idea of these categories I've even created a pie chart
to show you the breakdown of interview questions by category. By understanding these
categories and practicing problems for each one, you'll
be much better prepared for your next product case interview. So are you ready to
dive in and learn more? Let's do this. In a product case interview, you may come across questions that ask
you to measure the success of a product or feature. Let's call this type of
question measure success. It's a common type of
question that accounts for about 23% of interview questions. So how do you measure the
success of a product or feature? One way is to examine metrics such as user engagement, conversion
rates, and retention. These metrics can help you
understand which features are resonating with users and
driving business outcomes. For example let's say you're asked how you
would measure the success of YouTube's story feature. What metrics would you look
at to see if it's doing well? Another version of the
measure success question is about products that
haven't been launched yet. In this case, you can identify
metrics that may be useful for evaluating the potential
impact of a new product. For instance if Uber is planning to launch
a referral program for riders what metrics would you
use to measure its success? Finally, it's important
to consider the pros and cons of different metrics
when measuring success. While conversion rates
can be a useful metric for evaluating the impact
of a specific feature they may not capture the full picture of user engagement and satisfaction. You may need to use a
combination of metrics to get a better understanding
of overall product success. By using the right metrics,
you can get valuable insights into the effectiveness
of different features and products and make informed decisions about future product development. Okay, let's move on. The second most common type of
question is about A/B testing. Basically this type of question asks you to
design an experiment and make decisions based on the results. There are two ways this type of question can be presented to you. First, you may be given an idea and asked to design an
experiment to test it. In this case, you need to think critically about the different components
involved in designing a test and come up with a design
to address potential issues. Alternatively, you may be asked
to come up with an idea first before designing an experiment to test it. This can be a bit more challenging but it also gives you
a chance to be creative and come up with something
practical and useful. Once you have proposed your idea you will need to design an
experiment that effectively tests its viability and
potential for success. Regardless of the scenario it's important to analyze the results of the experiments and make
informed decisions based on the data available to you. This can involve looking at
driver metrics, guardrail metrics, and cost to determine whether the proposed change
or idea is effective or not. Here are some sample questions. How would you set up experiment to test
new feature in Quora? If engagement decreased in one segment but increased in others,
how would you decide whether or not to launch a feature change? What changes would you make to TikTok app? How would you test if the proposed change
is effective or not? Another type of question you might get in a product case interview is
all about problem solving and figuring out what's causing an issue. This is called the
diagnose a problem question. It's all about investigating
the underlying factors that could be causing the problem. So how do you tackle it? Well, it's important to
remember that there could be multiple reasons for problem, so
having a structured approach that prioritizes
investigation steps is key. You need to know what to look
at first and what to look at later. Let's take some examples. Say the estimated time
of arrival ETA of Lyft or Uber rides has
increased by three minutes and you are asked to
explain why. There could be many factors contributing to the problem. For instance, you might
explore internal factors such as changes to the app's algorithm
or external factors such as changes in traffic
patterns or increased demand for rides. It is important to approach the
investigation systematically and prioritize factors based
on their potential impact on the issue. Another example question could
be about investigating a one person drop in daily
active users of Slack. Even though it may not be a
huge drop, it's essential to figure out the cause of this decline. It could be because of changes
to the platform's features or the holiday season or
even increased competition from other messaging apps. Again, having a structured
approach is essential to figure out the root cause of the issue. Finally, let's consider a
scenario where a referral program in DoorDash isn't generating
the expected response rate. To diagnose this problem,
you might want to take a look at each step of the
referral program funnel to understand why users
aren't engaging with it. This could involve looking
at the messaging and incentives used to promote the program how easy it is to refer someone and what rewards are being offered. So we've gone over the
three most common types of questions that you
are likely to encounter in a product case interview and those account for
over 60% of questions. So if you are short on time those are the ones you want to focus on but let's quickly over the other 40%. The next type of question is
a bit of a catch all category for product specific questions
that don't really fit into any other categories. These are specific to a
product, feature, or company and they require you to
have a deep understanding of the product. Here are some sample questions. How do you evaluate the impact
of fake news on Facebook? How do you determine the optimal
ratio between company posts and individual posts for LinkedIn feeds? These types of questions can be tricky because they require a deep understanding of the product as well as the industry and market it operates in. To answer them effectively,
you need to do your research and understand the underlying
factors that impact the product and its users. Another type of question is
called improve a product. As you may have guessed, it's
about improving a product. These questions aim to
evaluate a data sciencist's ability to think creatively and come up with solutions to problems. They challenge you to
think outside the box and consider how to enhance
a product's functionality user experience, and
overall value proposition. Whether it is a software
application, a website, or even a service the possibilities for
improvement are endless. Here are some sample questions. How would you improve user
engagement on LinkedIn? How would you improve TikTok and what new features would you add to it? How would you improve what's on your mind posting on Facebook? By asking these questions,
interviewers are looking for data scientists who can
demonstrate their creativity, problem solving skills,
and attention to detail. They want to see how you
approach complex problems and come up with practical
solutions that can make a meaningful
difference for end users. The next type of question
is about strategic thinking. The strategic thinking category is all about evaluating a data
scientist's ability to analyze complex scenarios and make informed decisions
that have a long-term impact on product or company. These questions require you to
consider the bigger picture. Here are some sample questions. Back in 2016, there was no
story feature on Instagram. How do you decide whether to
launch this feature or not? What should the hourly rate
for Instacart shoppers be? These questions are designed
to test your ability to identify potential
obstacles and opportunities, analyze data, and come up
with practical solutions. To excel in this category it's important to step
back from the details and consider the broad
implications of your decisions. This requires a deep
understanding of market trends, user behavior, and the
competitive landscape. You need to evaluate
the risks and benefits and develop a plan that
balances short-term and long-term goals. This can be challenging,
but it's an essential skill for any data scientist who wants
to make a meaningful impact on a product or a company. Now, the last type of question we are gonna cover
is what I call estimation. Are you someone who loves to think fast and make quick calculations
based on limited information? If so, you might enjoy the
estimation questions category. Often asked by banks and consulting firms like
Capital One, BCG, and Discover. While estimation questions
may not be as common in tech companies, they can
still be an essential part of the data science interview
process for other industries. These questions challenge
you to think on your feet and develop a quick estimate
based on information provided. For example, you might be
asked to calculate the profit for a credit card partnership based on existing users, revenue, cost, and potential joint marketing campaigns. To ace this type of question. you need to be comfortable with numbers and have a strong grasp of basic arithmetic
and financial concepts. You also need to be able to
quickly understand the context of the problem and identify the key pieces of information needed
to develop an estimate. All right, that's it for today's video. on product case interview questions. We identified seven types of questions that you might encounter during the interview using
a data-driven method. By focusing on these question types you can better prepare yourself
for the interview process and increase your chances of success. If you want to take your
preparation to the next level I have a product case
interview cheat sheet that you can download for free. It covers all the categories of questions we talk about
today, plus some additional ones. Just check out the link in the description
below to grab your copy. So go ahead and download the cheat sheet, practice your skills, and get
ready to crush that interview. Thanks for watching,
and don't forget to like and subscribe for more videos like this. See you soon.

How to Build Strong Product Sense for Data Scientists
As a data scientist, you may
know how crucial it is to have a strong product sense,
especially if you are working in a product data scientist
or data analytics role. If you are new to this field you may be working on
developing your product sense but how do you develop product sense? Well, the answer is simple. You can start by reading books
that help you bridge the gap between your technical skills and your ability to speak
the language of the product. In this video I want to recommend three
books that are suitable for both beginners and
seasoned data scientists. These books are not your
typical data scientist books but they are incredibly valuable in building your product sense and helping you excel in your job. The first two books provide a wealth of product knowledge while
the third one is highly technical and can benefit
any data scientist. They will help you understand
the importance of metrics the role of product managers and how to use data to
drive business growth. So if you are ready to
take your product sense to the next level, let's dive into the three books
that I highly recommend. The first book I like to
recommend is Lean Analytics by Alistair Croll and Benjamin Yoskovitz. Do you like to read books that
are packed with key studies and insights from over a
hundred business experts? If so, this book is a great choice. This book is actually
written for entrepreneurs but it can be super helpful
for data scientists too. So how can data scientists
effectively communicate with stakeholders according
to Lean Analytics? Well, the second part of the book called
Finding the Right Metric for Right Now provides a
practical framework to measure and optimize business operations
for data-driven growth. It covers commonly used
analytical frameworks and emphasizes the discipline of one metric that matters
to define metrics for success and communicate them
effectively to stakeholders. It also explains how analytics
can be applied to businesses of different stages and types. This book also covers
when to use metrics such as customer acquisition,
retention, revenue, and virality to measure success and optimize
products and marketing. This is especially useful
for data scientists to understand the specific needs
of different businesses. In the third part Line
in the Sand, the authors discuss several business
models such as e-commerce, SaaS mobile first businesses,
and two-sided marketplaces. It's all discussed in the
context of Lean Analytics which is a data driven
approach to developing and scaling a business. On top of that, the authors
provide case studies of businesses that have
successfully used analytics to grow and scale their operations. The examples offer practical
applications which data scientists can adapt to their own work. Overall, I find Lean Analytics
to be a super helpful book in building my knowledge of
metrics and understanding how data scientists can contribute
to the growth of a business. The case studies were especially
interesting and informative for me. So if you are a data scientist
looking to build your product sense and understand how to
use data to contribute to the success of a business I highly recommend Lean Analytics. This is an excellent resource
that can help bridge the gap between technical skills
and the ability to effectively communicate about the product. Another book I found incredibly helpful in building my product
sense as a data scientist is Cracking the Product Manager Interview by Gayle Laakmann McDowell,
and Jackie Bavaro. This book is a valuable
resource that provides guidance and strategies for landing
a job as a product manager. Now you might be thinking,
why should I care about the book on product
management as a data scientist? Well, let me tell you understanding product management
is crucial in this field. Firstly, data scientists work closely with product managers or PMs. They are key stakeholders
and decision makers when it comes to defining
the product roadmap and identifying
opportunities for improvement. Having a strong understanding of product management can
help you communicate more effectively with PMs and
make more informed decisions. Secondly, many companies have
PMs interview data scientists. This is because PMs are often responsible for overseeing the development
of new products and features. So they need to make sure
that data scientists they work with have a good understanding
of the product and its goals. That's why it's important that
you can speak their language and understand their
perspective. With 16 chapters the book covers a wide range of topics including product manager responsibilities how to gain the right experience crafting an effective resume
and honing technical skills. The authors provide practical tips and strategies for
answering common interview questions as well as
case study examples to help readers enhance their
problem solving skills. One of the things I found most
helpful about this book is in chapters 14 and 15 where
it provides analytical and metric frameworks such
as the 5Cs framework and the marketing mix to analyze and answer some case questions. This can be highly valuable for data scientists who want
to build their product sense and understand how to
use data to contribute to the success of a product. In conclusion, Cracking
the Product Manager Interview is an excellent
resource for any data scientist who wants to build
their product knowledge and understand how to work effectively with product managers. Now that you've got a solid understanding of product knowledge
from the previous books let's dive into A/B testing
a powerful tool used by companies to make
data-driven decisions. So what's the role of a data scientist in
A/B testing? In this field you likely play a crucial
part in this process. From defining metrics for
success to analyzing results and communicating insights to stakeholders you will be responsible for ensuring that the
experiment is properly designed to address the
businesses questions and that the results are reliable
and practically significant. Additionally, you may need
to perform data cleaning and processing to ensure
accuracy and completeness but perhaps most importantly a data scientist needs
to be able to interpret and communicate results to
stakeholders effectively. They need to work closely with
product managers, engineers and other stakeholders to ensure that the insights from
the experiment are being used to drive business decisions. So a data scientist's role
requires a deep understanding of statistical concepts and the ability to communicate
effectively with others. While the previous books
I recommended offer practical knowledge and
transferrable skills that apply to your role as a data scientist the next book is the most
technical of the three. I had to read it multiple times to fully understand the
technicalities, so don't worry if you don't get everything
on the first try. It's a book you can keep coming back to. It's highly technical, but
it's also very valuable. The book I want to
recommend is Trustworthy Online Controlled Experiments
by three co-authors and all of them are industry experts. It's a valuable resource for anyone looking to build
their A/B testing knowledge and acquire the practical
skills needed to succeed in a data scientist role. It is my go-to book for AB testing. While you can learn statistics
and machine learning in school, you probably
won't learn AB testing. That's why this book is a must have for every data scientist and here are some
benefits this book offers. You'll understand how A/B testing
works under the hood even if you don't have any actual
experience in A/B testing. This makes it especially helpful if you are new to this field. Even if you are an
experienced data scientist you can still learn a lot of practical knowledge
from this book, such as best practices in the
industry and the solutions to real problems data
scientists face in their jobs. When you have conversations
with colleagues and work on real problems in
practice, you can refer back to the book when faced with
problems you remember reading about. Now, let's dive into
what this book is all about. It provides a comprehensive
guide to conducting A/B tests, also known as online
experiments, in a reliable and ethical manner. The book is divided into five
parts, Introductory Topics for Everyone, Selected Topics
for Everyone, Complimentary and Alternative Techniques
to Controlled Experiments, Advanced Topics for Building
an Experimentation Platform, and Advanced Topics for
Analyzing Experiments. In the first two parts, the
authors provide a comprehensive introduction to the fundamentals
of online experimentation. They cover the importance
of experiments, how they are conducted, and
the statistical principles that underpin them. Additionally, the authors
supplement the theory with real world case studies,
providing practical examples that illustrate the concepts discussed. In the third part, the
authors present a range of complimentary techniques
to online experiments. These techniques include logs-based
analysis, user experience research, and observational causal studies which can augment online
experiments to provide more comprehensive insights. In the last two parts the authors explore
advanced topics in online experimentation such as
hypothesis testing and A/A tests. They also provide solutions
for common challenges such as multiple testing and the
leakage between variants. One of the book's greatest
things is its adaptability to different audiences,
including data scientists data science managers, product
managers, and engineers. The authors have thoughtfully
organized the content to make it easy to navigate and find the information that
is most relevant to each role. For instance, data scientists can start with chapters specifically
tailored to their needs. Overall Trustworthy Online Controlled Experiments
is an essential resource for anyone seeking to expand
their knowledge of A/B testing and acquire practical skills
to excel in this area. Now, to wrap up these three
books provide valuable insights and practical knowledge for data scientists looking
to build their product sense and make data-driven decisions. By understanding product
management, lean analytics and A/B testing, you can connect
your technical expertise with product insights,
which will allow you to better communicate with your
team and drive business impact. Remember, building your product
sense takes time and effort but it's a crucial skill
for data scientists to have. So keep reading and learning
and you will be well on your way to becoming a
valuable asset to any team. If you have any questions
or book recommendations of your own, feel free to leave
them in the comments below. Don't forget to like and subscribe for more content like this. Thanks for watching, and I
will see you in the next video.

What is Two-sample T-test? Easy Explanation for Data Science Interviews
hey guys in this lesson let's talk about two simple t-tests so we will cover how to use t-test for independent samples and dependent samples for independent samples there are two scenarios one is that the two samples have equal variants and the other scenario is that the two samples have different variances and we will cover both scenarios in this video let's start with some interview questions calculate the mean difference understand error of mean difference for two sample t-test Implement two sample T tests in python or r as you can tell you need to know how to calculate the test statistic for two simple t-tests so that you can Implement in python or R and in this lesson we'll cover all the things you need to know to Ace this kind of interview questions let's start with independent symbols so let's say X1 and X2 are random samples from two independent populations each population is a product approximately normally distributed the samples within each population are independent of each other so let me specify what independent samples mean independent samples or unpaired samples means that there's no relationship between the subjects in each sample subjects in the first group cannot also be in the second group and no subject in either group can influence subjects in the other group also no group can influence the other group so that's what independent samples or unpaired samples mean for two simple t-tests we have two high pulses the null hypothesis is that the mean of the first population is the same as the mean of the second population and alternative hypothesis is that the two are not the same let's see what are the steps involved to conduct independent t-tests we first calculate the difference amount sample means and then we sum the variances of the samples and calculates the equal or on equal viruses so there are two scenarios here the two groups may have equal variances or on equal variances and then we calculate the T statistic and compare it with the T critical value if the T statistic is larger than the t critical value we reject the null hypothesis these steps are pretty straightforward right now let's dive into two sample t-test with e-coverances this is scenario that the two groups of viruses are believed to be equal then we use a t-test based on pulled variance estimate so this is what the test statistic looks like it's X1 bar minus X2 bar over SP square root of 1 over and one plus one over and two x i bar is the sample mean of simple I for I is 1 or 2 and the pool depends simple virus as P Squared is defined by the sum of squares 1 plus sum of squares 2 over N1 plus N2 minus 2. sum of squares is the sum of squares of a data point x j to the sample mean x i bar under the null hypothesis the test statistic the T statistic follows a t distribution with degrees of freedom and 1 plus and 2 minus 2. essentially the numerator is the difference between these two simple means and the denominator is the simplest standard deviation of the difference we can also derive the context interval for two sample t-test with equal variances and the level 1 minus Alpha 100 come this interval for the difference between two means is X of 1 bar minus x 2 bar plus or minus t and 1 plus and 2 minus 2 Alpha over 2 multiplied by as p e square root of 1 over and one plus one over and two so this part is the point estimate and this part is the margin of error now let's look at how to use the vertex T Test with unequal variances if the two viruses are not similar one is more than twice of the other but the other assumptions for the two sample t-test hold then we can use Welch's t-test which employs an onput stand error so the on percent error is the square root of S1 squared over N1 plus as 2 squared over N2 our test statistic becomes x 1 bar minus x 2 bar over square root of S1 squared over N1 plus S2 squared over N2 and you can see the numerator is the same as the equivalence scenario but the denominator is different here as 1 and S2 are symbols stand deviations of the two groups and we calculate them separately the degrees of freedom is a little bit complicated it's S1 squared over N1 plus S2 squared over N2 squared over S1 squared over N1 squared over N1 minus 1 plus as 2 squared over N2 squared over N2 minus 1. so the difference between the equivalence scenario and the only equivalence scenarios are two things the denominator is different as well as the degrees of freedom now we call a lot of the series let's look at example to compare the mean height of men in Two Cities we have two symbols here one symbol has 19 data points and the other sample has 23 data points we simulate some data and run the test in Python right we assume the two populations have different balances for the first population the standard deviation is 1.97 and mu 1 is 172 so we can sample 19 data points from the population of course we don't have access to these parameters this is just for illustrative purposes for the second sample there are 23 data points the standard deviation is 2.2 and the mean is 169. use the Welches t-test we can test if the mean height difference is close to zero was 95 percent confidence level so mu 0 is 0 here outputs then error can be calculated by using the sample runs over N1 plus the simple variance of the second sample over N2 and then we take the square root of this quantity we can also get the observed t score using the difference between the two simple means minus zero which is Mu 0 here over the onput stand error the degrees of freedom is a little bit complicated here we need to plug in the equation and then calculate the critical t-score based on the degrees of freedom in this case The observed t-score is close to 4 but the critical t score is 2.02 so we learned in the rejection region and there's a statistical significant difference between the height of men in the two cities now let's say how we can derive the compass interval for the difference between two population means a level 1 minus Alpha 100 come this interval for the difference between two means is shown here it's x 1 bar minus x 2 bar the difference between the two sample means is the point estimate and then we have margin of error which is a critical T value multiplied by the output stand error the input instant error is a square root of S1 squared over N1 plus S2 squared over N2 as 1 and S2 are sample standard deviation and the degree of Freedom here is a little bit complicated is as 1 squared over N1 plus S2 squared over N2 and take the square of this quantity that's our numerator and the denominator is S1 squared over N1 squared over N1 minus 1 plus as 2 squared over N2 squared over N2 minus 1. let's say example to derive the context interval for the difference of height of men so we continue using the previous example we can get the margin of error which is a critical t-score multiplied by the output stand error and then we obtain the lower bound of the canvas interval is the difference between the two sample means minus margin of error and the upper bound of the confidence interval is the difference between the two simple means plus margin of error that's how we can obtain the company's interval using the point estimate which is the difference between the two simple means and the critical t-score as well as the output stand error finally let's look at using the t-test for dependent samples dependent on paired samples means that each data point in one sample is uniquely appeared to a data point in the second sample for example one to Major change over time in a longitudinal study we measure one variable at one point in time and measure the same variable of the same sample at a later point in time or we want to compare pre-test and post hat difference we want to see if there's a significant effect due to the treatment then we have two dependent samples there are some advantages of this kind of design either controls for individual differences but there are also some disadvantages like there's carry over in fact the second treatment can be affected by the first treatment and the other May influence the results which treatment is first can affect the results now let's look at the hypothesis for testing the difference between dependent samples let D1 D2 the N be a small random sample of the differences in pairs our null hypothesis is that mu D equals mu 0 and our alternative hypothesis is Mu D is not the same as Mu zero almost always mu 0 equals zero so basically we want to test if the difference between dependent samples is zero or not the steps to conduct dependency test is similar to conduct independent heat tests we first calculate the difference of the pairwise differences and then we obtain the standard deviation of the pairwise differences and then we can calculate the T statistic and compare the T statistic with the T critical value if the statistic is larger than the t critical value we reject the null hypothesis here's the form of the T statistic T is D Bar minus mu 0 over St over square root of n D Bar is the average of the differences mu 0 is the average of the differences under the null hypothesis typically it's zero and as D is the standard deviation of simple differences under the null hypothesis the test statistic follows a t distribution with n minus 1 degrees of freedom now let's say example to evaluate the effect of stretching on height let's say we measure the height of a group of people who do not stretch regularly then they start stretching regularly for a year then we measure their height again and take the differences you want to see if there's a statistically significant effect meaning that if the population mean differs from zero here we have a total of 20 data points and the sigma is 0.01 population mean is zero we sample the difference from this population but we don't have access to the sigma nor population mean here our mu zero here is zero because we want to see if the population mean differs from zero and we can calculate the observed t score which is a difference of the samples minus mu 0 over sample standard deviation divided by square root of N and we can compare the observed t score with a critical t-score here's the observed t score is a negative value and its absolute value is less than the critical t score so we fail to reject the null hypothesis so there's probably no change in height of the additional stretching finally we can derive the content interval for dependent samples a level 1 minus Alpha 100 comes interval for the mean difference mu D is given by D Bar plus or minus t n minus 1 Alpha over 2 multiplied by standard deviation of D over square root of n If the symbol size is large enough typically more than 30 data points this account is interval for the mean difference mu D is D Bar plus or minus z alpha over 2 as T over square root of n so we replace the T critical value with the Z critical value all right that's it for this lesson we went over using the t-test for independent samples as well as dependent samples for the independent samples there are two scenarios either the two groups have equal variances or they have only equivalences that's it for this lesson I will see you in the next one

What is One-sample T-test? Easy Explanation for Data Science Interviews
hey guys in this lesson let's focus on the t-test specifically we'll go over one sample t-test t-test is one of the top topics asked in statistics interviews so it's really important to have very good understanding of t-test before your interviews so that you can ask interview questions related to t-test let's start with looking at some interview questions what are the differences between the t-test and the z-test what are the assumptions of the t-test in this lesson we'll focus on the fundamentals of t-test we'll look at the difference between the t-test and the z-test assumptions of the t-test and we will look at the test statistic of t-tests and finally we'll go over using the t-test for one simple mean in the next lesson we will talk about t-test for two sample means which is more complicated and that build on top of what we will learn in this lesson so let's Dive Right into what's the difference between t-test and Zetas all right first of all let's look at the difference between the t-test and the z-test we learned from the previous person that zitas is used to pass the mean proportion of a population against a number or against the mean proportion of another population as long as some basic assumptions are satisfied so what are these assumptions the data are normally distributed with known virus or a large enough sample that we could invoke some well-known serum such as the central limit serum or the zlaskis serum to obtain a normally distributed test statistic so the key is that the test statistic is normally distributed why is it large enough samples it means that we have at least 30 simple points and in that case there's no need to assume normality as the central limit theorem implies that you can use a z-test for both cases so for large sample sizes the T test procedure gives a most identical p-values as a z-test procedure the t-test can be replaced by a z-test if we have over 30 samples and we know the variance of the population and also the data is not highly skilled similar to the zitas t-test also has one sample and two sample tests okay let's move on to the assumptions of the t-test the t-test accomplishes the same goal as Zetas but under a complementary set of assumptions the sample size is not too large nearly less than 30 and the population balance is unknown which is almost always the case in applications finally the data is normally distributed even though we can relax these assumptions slightly by requiring only that the simple mean be normally distributed and the sample variance be chi-squared distributed and independent of the sample mean there's an important note here we should not use a t-test if the sample contains outliers as that may mean some of the required assumptions like normality are in fact validated so these are the three assumptions of the t-test now let's move on to the T statistic to use the t-test the test statistic T equals simple mean minus mu 0 over sample standard deviation from the previous lesson you can tell that this form is the same as a z-test here mu 0 is a constant or the sample mean of a second population to which we would like to compare our population mean if the assumptions for the t-test are satisfied then the test statistic the T statistic follows the student T distribution what is a student T distribution let's compare it with Z distribution so that you can understand what it is here's a diagram showing three curves the blue curve in the Z distribution or standard normal curve the red curve and the green curve both of them are T distribution but they have different sample sizes the dependence of the T Distribution on the symbol size n is the Y is only parameter which would be called degrees of freedom the decrease of freedom is the number of pieces of information that can freely vary without violating any given restrictions that is the number of independent piece of information available to estimate another piece of information it's a number of independent input minus the number of intermediate input if this sounds too abstract let me give you an example the simple balance of ended points in the denominator of the test statistic depends on the sample mean right so we have n minus 1 degrees of freedom and in the sample size 1 means the sample mean is restricted so the test statistic has n minus 1 degrees of freedom once we understand the degrees of freedom let's go back to this diagram so from this diagram we can see that the shape of the T distribution differs for sample size much smaller density which is a green curve compared to sample size close to 30. the one with less simple points has heavier tails and if we compare the p-distribution with the Z distribution we can see that the T distribution has heavier Tails than the normal distribution at the sample size n increases the t-distribution better approximates the normal distribution for large number of sample points more than 30 the T distribution and the normal distribution become almost identical even though it's not shown on this diagram but that's just a fact that the T distribution approximates the standard normal curve once we have more sample points all right now we know the difference between the T distribution and the Z distribution let's look at the T test for one simple mean we use a t-test for one simple mean when we want to estimate the population mean from a small random sample with less than 30 data points here are the two hypothesis the null hypothesis is that mu the population mean is the same as Mu zero and the alternative hypothesis is that mu is not the same as Mu zero now let's look at the test statistic we can construct our T statistic and under the null hypothesis the test statistic is the sample mean minus mu 0 over s over square root of n x bar in the sample mean mu zero is a constant we want to compare our population made against and as it is simple standard deviation as in the square root of the sum of x i minus X bar squared over n minus 1. as some of you may know as squared is an unbiased estimator of the population variance under the null hypothesis we know that the test statistic follows a t distribution with n minus 1 degrees of freedom now let's look at the critical T value for two-sided tests let's say we set the significance level Alpha we Define t n minus 1 Alpha over 2 the positive real number to the left of which an observed t minus 1 random variable false with probability probability the test statistic larger than T and minus 1 Alpha over 2 is Alpha over 2 and the probability that the test statistic is less than negative t n minus 1 Alpha over 2 is also Alpha over 2. so this diagram summarizes this nicely so we can see that the distribution is symmetric and on either side if the test statistic is larger than the critical T value the area is Alpha over 2. okay now let's move an example to see how to use the t-test for one simple mean let's say we want to estimate the average height of women in the US we randomly sampled 10 women we guess the true value is around 160 centimeters here's how we can use Python to run the t-test so we know the total number of simple points is 10 and here we use Sigma 2.1 and the population mean 162 to simple 10 points from this population of course we don't have access to these two parameters otherwise there's no need to conduct the hypothesis test right so our hypothesis are null hypothesis specifically is Mu 0 is 160. so we can calculate our observed t score which is X bar minus mu 0 over simple standard deviation over square root of n We compare the observed t-score with the critical t-score and in this case these two are pretty close to each other but the observed t score is slightly larger than the critical t-score so we land in the rejection region we reject our null hypothesis in favor of the alternative and our conclusion is that the height is likely different from 160 centimeters finally let's look at how to derive small sample confidence interval for a population mean because it has statistic follows a t distribution we can get a 1 minus Alpha 100 context interval for the population mean mu is X bar plus or minus t n minus 1 Alpha over 2 s over square root of n if the sample size is large enough we can instead use x bar plus or minus z alpha over 2 s over square root of n so we replace the T critical value with the Z critical value if we have enough sample size so continue using our previous example to estimate the height of women in the U.S we can calculate the margin of error which is the critical t score multiply the simple standard deviation over square root of N and then we can obtain the lower bound of the canvas interval which is the point as made the sample mean minus margin of error and upper bound is the sample mean plus module of error so finally we can obtain the context interval of the height of women all right that's it for this lesson in this lesson we want over the difference between t-test and z-test assumptions of t-test and we also talk about the test statistic of T Test finally we look at how to use the T test for one simple mean in the next lesson we'll look at using the t-test for two sample means which is more complicated situation but with the fundamental knowledge we learned in this lesson we are good to go for the next lesson I see you in the next one

Ace Statistics Interviews: A Data-driven Approach For Data Scientists
let's be honest statistics can seem daunting especially when it comes up during a data science job interview sometimes it feels like no matter how much you have prepared there are those unexpected questions that catch you off guard so how can you get ready for them in this video we are going to explore the top statistics questions that often come up in these interviews using a data-driven approach and we present everything in a way that's easy to understand even if it's been a while since you last studied statistics the best part we are going to go over all of this in less than 15 minutes are you ready let's dive in hey this is professionals it's Emma from amazon.com we're all about helping you land your dream data scientist job we offer proactive tips and strategies online interviews preparing for interviews and negotiating offer so if you're new here consider subscribing now let's dive into this topic I'll be through my fair share of interviews where I stumbled upon statistics questions that left me is scratching my head let me tell you it wasn't a great feeling but as I gained more experience I started noticing a pattern some Concepts come up more frequently than others to back up my observations I decided to take a daily driven approach I'm gathered and analyzed over 300 statistics interview questions from over 50 different companies and guess what my findings confirmed what I suspected I'm really excited to share all of this with you firstly I discovered that the most common asked questions in this interviews focus on fundamental concepts these concepts are super important to grasp so in this video we are going to dive into the top five Concepts that come up most often they are p-value linear regression t-test correlation coefficient and types of Errors now out of all these Concepts p-value stands out as the most important one in data science interviews it appears over 10 percent end of the questions almost half of the companies out there asking dates to explain what a p-value is so it's crucial that you understand this concept inside out and how it's commonly used this will greatly increase your chances of easing your next interview now let's take closer look at each of these Concepts one by one let's dive into the concept of the p-value usually you will come across a question about how to explain it will provide a structured answer by breaking it down into a few steps we'll look at the definition the meaning of its values and its application to answer this question the p-value is a useful tool in hypothesis testing to help us make sense of our observations and draw conclusions simply put the p-value measures How likely we are to get results as Extreme as the ones we observed in our sample assuming that our initial assumption the null hypothesis is true when we say as extreme we mean results that provide enough evidence to support an alternative hypothesis a low p-value means that we have less support of the null hypothesis in practice we often use a cut value of 0.05 if the p-value is less than 0.05 it means we have strong evidence against the null hypothesis so we can reject it on the other hand if the p-value is greater than 0.05 it means we have weak evidence against the null hypothesis so we can't reject it one common application of the p-value is in a b testing imagine we have a treatment group and a control group and we want to determine if there's a difference between them in terms of a specific metric we run experiment and collect data from both groups the smaller the p-value the more content we can be that there's actually a difference between the two groups now we've discussed what the p-value is another common ask question is how to explain it to a non-technical audience a helpful method is to use Simple examples let's consider a scenario involving a productivity app called notion you and your colleagues want to improve efficiency and accomplish more tasks within the limited time you have to test effectiveness of notion you decide to run an experiment you divide your team into two groups one group uses the app to manage their tasks and attract their progress while the other group continues with their typical task management methods after a period of time you evaluate the productivity of each group now we can bring in the concept of the p-value to evaluate the significance of the results you calculate the p-value to determine if the difference in productivity between the two groups is statistically significant or if it could be due to chance a low P value would suggest that the app is likely effective in boosting productivity on the other hand a high P value would indicate that the observed differences in productivity could be due to random factors other than the app itself by considering the p-value we can make a more informed decision about whether notion is likely to be beneficial for improving productivity so now that we've covered p-value let's dive into another important topic linear regression another question we often encounter is what are the assumptions of linear regression don't worry if you don't know I'll break them down for you in a simple way there are four key assumptions that we need to consider and we can remember them easily with the acronym line let's start with the first assumption represented by the letter L it's all about the relationship between the independent variable let's call it X and the dependent variable which we'll call Y the sum is that the value on y changes in linear manner with X moving on to the second assumption represented by the letter i it stands for the Assumption of statistical Independence of the residuals residuals are the differences between the actual y values and the predicted y values from the regression model we want these residuals to be independent of each other meaning that the value of one residual does not influence the value of another now let's talk about the third assumption represented by the letter N it suggests that the residuals follow a normal distribution in similar terms if we were to plot the procedures on graph we'd want them to form a nice symmetric bell-shaped curve however in large samples this assumption becomes less critical finally we have the fourth assumption represented by the letter e it stands for equal variance we want the variability of the residuals to be consistent across different values of X this means that the spread of the residuals shouldn't change as X increases or decreases so to summarize the Assumption of linear regression can be remembered using the acronym line we have the L for the linear relationship between X and Y the I for the independence of residuals the N for normal distribution of residuals and the E for equal variance of residuals and that concludes our discussion of the first two statistics Concepts in intervals now if you are eager to delve deeper into this subject and want to discover more about the insights I gained from analyzing over 300 statistics interview questions I've got something special for you I'll put together a handy cheat sheet that covers the most frequently Asked statistics interview questions by familiarizing yourself with these questions you'll be equipped to answer more than 40 percent of the interview questions you may encounter best of all you can download this cheat sheet absolutely free just click the link provided in the video description below alright let's now proceed with our list of questions next let's dive into the topic of t-tests it's a concept that often comes up interviews and is really useful to understand so what are the assumptions of the t-test and one can actually use it let's break it down aditas is a statistical tool that helps us determine if two groups have different means and there are a few key assumptions we need to keep in mind when using it actually most of the assumptions of linear regression also apply to t-tests except for the one about linear relationship so we can simplify it using the acronym i and e to remember the assumptions let's go through them one by one the i in i and e stands for Independence it simply means that the observations within each group or sample should be independent of each other in other words the samples in one group shouldn't be influenced by other samples in the same group moving on to the end the i and e which stands for normality this assumption tells us that the data in each group or the differences between the groups should roughly follow a normal distribution it means that the data should kind of look like a bell curve however even if the data doesn't perfectly fit that shape the T Test can still handle well especially when we have a large sample size as the central limit theorem states that the sampling distribution of the sample means will be approximately normal regardless of the shape of the population distribution finally the e in i and e stands for equal variance if we are comparing the means of two independent groups its ideal is the variances of these groups are roughly the same this assumption is crucial for interpreting the results accurately but if the sample balances are significantly different we can use alternative versions of the t-test like a Welch's t-test these variants do not require equivalences and can provide valid outcomes now if you want to learn more about how to use a t-test for one sample or two simple tests I've got dedicated videos on those topics with implementation in Python you can find them in this video description below all right let's look at another popular Topic in statistics the correlation coefficient a question that often comes up is how to tell the difference between covariance and correlation coefficient here's a table that summarizes the main differences between them the correlation coefficient tells us how strong the linear relationship between two variables is while covariance focuses on the direction of the relationship to calculate the correlation coefficient you divide the covariance by the square root of the product of the viruses of the two variables now when it comes to the units of the measurement the correlation coefficient is unitless this means that even if you use different units for the original variables it won't affect the correlation coefficient as long as there's a linear relationship between the variables on the other hand covariance is obtained by multiplying the units of the two variables together here's another key difference the absolute value of the covariance cannot be greater than product of the standard deviations of the individual variables however the correlation coefficient always 4 between negative 1 and 1. so these two majors help us understand the strength and direction of the linear relationship between two variables I hope this helps explain the distinction between them for you now let's move on to discussing the fifth question that is about different types of errors in hypothesis testing there are two main types that we need to understand let's start with the first one the type 1 error type 1 error occurs when we mistakenly conclude that there is a difference between two groups even though in reality there isn't to make it clearer let's consider an example of a b testing imagine we are working for an e-commerce company and we want to find out if changing the color of the binary button on the app will increase the number of people who actually make a purchase so a Time One error occurs when we claim that changing the button color from Blue to Yellow will result in a significant difference in the conversion rate however in reality there's no actual difference in conversion rates between the two colors now let's move on to the second type of error known as a type 2 error this happens when we make the mistake of failing to reject a false null hypothesis in simpler terms it means that we conclude there is no significant difference between the groups when in fact there is a difference so going back to our previous example we might conclude that changing the button color won't have a significant impact on the conversion rate but the truth is it actually does make a difference so we fail to detect the difference between the two colors to help you remember this Concepts easily here's a handy tip that I found on stack exchange.com you can think of a type 2 error as a false negative or a false false by repeating the word false it becomes easier to remember on the other hand a type 1 error is a false positive because it only contains one instance of the word false now if you're interested in diving deeper into hypothesis testing I've got a video for you check out my video on AV testing analysis Made Easy how to use hypothesis testing for data science intervals in this one we take a closer look at how hypothesis testing applies to AP testing a topic that is often discussed intervals and don't forget to also watch my top 5 statistic Concepts in data science interviews video it's a fantastic resource where you can learn about other important Concepts that might show up in your next interview by the way I've put together a whole playlist dedicated to helping you crack those tough statistics interview questions it's not just beneficial for interviews but it will also boost your skills as a data scientist remember the more you know the more confident you will be so stay curious keep learning and I have no doubt that you will excel in your next interview thanks for watching and I will see you soon foreign

What is the Z-test for Proportions in Statistics? Easy Explanation for Data Science Interviews
hey guys in this video let's talk about using the z-test for proportions there are quite some interview questions related to this topic you might get interview questions to compare clicks rates on the advertisement of two groups of users another example is that the interviewer might ask you to derive accountants interval for the probability of getting heads from a series of coin tosses as you can see this for these questions you need to use the z-test for proportions to conduct the hypothesis test and the drug conclusions in this video we will focus on three topics the first one is why we use a z-test for promotions some people get confused by this so I want to clarify this point first and then we will talk about the one proportion Z test and two proportions z-test by the end of this video you will have a pretty good understanding of using the zitas for proportions let's get right into it alright the first topic I want to focus on is why we want to use z-test for proportions so some people get confused by this and and they don't know if we should use a z-test or t-test for proportions so I want to clarify this point first the typical one and two simple proportions tests are of this form the test statistic is D over s where D is the difference between a proportion and a constant or the difference between two proportions and as is an estimated standard deviation of d based on selaski's serum as long as the denominator s converges in probability to that unknown standard deviation Sigma D then D over as approximately normally distributed therefore we have some justification of treating the test statistic as a sympathetically normal but we don't really have any justification for treating it as t-distributed so theoretically we don't use thetas to test proportions and there's no good argument that t distribution should be better than the Z distribution as an approximation to the distribution of the intest physique but t-tests are sometimes used in practice to test proportions that is not entirely wrong because the results from a t-test is similar to that of a Zetas for large samples now let's move forward to one proportion z-test we use one proportion z-test when we want to compare a proportion of a population to a constant let's say p is the success rate or proportion of a large number and of independent Bernoulli trials and P hat is the observed success rate that is the number of observed successes over the total number of Trials when the sample contains at least 10 successes and 10 failures it will be reasonable to use a normal approximation of a binomial distribution so we know that NP hat follows a binomial distribution with n numbers of Trials and P in the success rate NP hat also follows a normal distribution with mean equals MP and variance is npq where Q is 1 minus p so P hat itself follows a normal distribution with mean p and variance is p 1 minus P Over N so we know all this even before we conduct any hypothesis test in terms of one proportion test the hypothesis look like this the null hypothesis is p equals P0 and the alternative hypothesis is p is not the same as P0 pretty simple right under our null hypothesis we know that the test statistic follows a standard normal distribution this is what the test statistic looks like it's p hats minus P0 over square root of p01 minus P0 over n based on this we can calculate the observed Z statistic and compare it with the Z critical value and observed this statistic is larger than the critical Z value then we can reject the null hypothesis now let's look at example using the z-test for one proportion test let's say we want to estimate the clicks rate P of users on us suppose that we have an algorithm for add selection and we like to estimate the quick rate P of users on the ads selected by this algorithm given that we have access to a thousand users and observed a clicks rate is p hat equals 0.2 we set a significant level Alpha of five percent our null hypothesis in this case is p equals point 15. here's how we can use Python to run this High post test it's pretty simple so we know our P0 in this case is 0.15 we have a total of a thousand observations and the P hat is 0.2 based on P0 we can calculate Sigma which is P0 multiplied by 1 minus P0 over n and then take the square root of this quantity so we can get observed z-score is p hat minus P0 over Sigma we can compare the observed this score in this case it's close to 4.43 to the critical z-score is 1.96 and obviously The observed this score is larger than the critical z-score so we reject the null hypothesis because the test statistic follows a standard normal distribution we can use it to derive the context interval for proportion a level one minus Alpha a hundred percent come this interval for proportion p is p hat plus or minus z alpha over 2 multiplied by Sigma which is p Hertz multiply 1 minus P hat Over N here we don't have any P0 we only need to use P hat to get the converse interval of the true proportion p let's say example to derive the confidence interval of clicks rate on ADD we'll continue to use the previous example and here we don't need to use n in P0 we can calculate Sigma using p hat in the P hat multiplied by 1 minus B hat over n and then we take the square root of this quantity and then we can get margin of error as the critical z-score multiply Sigma it's actually this part in this equation so we know the point as made is p hat and the margin of error is this quantity and then we can get the lower bound of this contents interval which is p hat minus margin of error and the upper bound is p hat plus money of error so that's how we can get the confidence interval of clicks rate on ads finally let's look at two proportions z-test we use two proportions Zetas when we want to compare one proportion with another proportion basically proportions P1 and P2 of two populations here's our two high pauses the null hypothesis is that P1 is the same as P2 and the alternative hypothesis is that P1 is not the same as P2 so under the null hypothesis the two proportions are the same similarly to the one proportion case we can get the distribution of P1 hat and P2 hat we know that P1 hat follows a normal distribution with mean P1 and variance is P1 1 minus P1 over N1 we can also get the distribution of P2 hat which is a normal distribution with mean P2 and variance is P2 multiplied by 1 minus P2 over N2 we can also get the difference between these two proportions which should also follow a normal distribution and the mean becomes P1 minus P2 and the variance is the sum of the two viruses under the null hypothesis P1 is the same as P2 so our test statistic follows a standard normal distribution here's what the test statistic looks like it's P1 has minus P2 hat over square root of P hat one minus P hat one over N1 plus 1 over N2 P hat is a pulled proportion it's basically a weighted mean of these two proportions P1 and P2 it's K1 plus K2 over N1 plus N2 it's basically the total number of successes over the total number of Trials across these two samples the reason that the test statistic follows a standard normal distribution is basically uh because of this we know that P1 hat minus P2 hat follows a normal distribution and we also know that based on the null hypothesis P1 is the same as P2 so this quantity follows a standard normal distribution under the null hypothesis one thing worth noting is that in many statistical programs the default is to estimate the two proportions separately meaning that they use unpooled proportions so then the test statistic becomes P1 hat minus P2 hat over square root of P1 has 1 minus V1 hat over N1 plus P2 has one minus P2 hat over N2 so they don't use put proportion both of this work you can use either one of this to conduct the hypothesis test but knowing that under the null hypothesis P1 in the same as P2 so preferably we should use the put proportion because the two proportions are the same under the null hypothesis now let's look an example to compare the clicks rate of two algorithms suppose that we have two algorithms that are using different strategies to show us and the first algorithm we have a total of 30 clicks among 900 Impressions the clicks rate is 0.033 for the second algorithm there were 20 clicks among a thousand Impressions so the clicks rate is two percent So based on the data we gathered we can now run a hypothesis test to see if the two proportions are the same in this case the first sample has 900 data points and observed propulsion is 0.033 and the second symbol has a thousand data points with the observed uh proportion 0.02 we can get the difference between the two observed proportions and calculate the pulled proportion which is the total number of clicks over the total number of impressions based on the pulled proportion we can also get the pulled standard deviation in this case is P1 minus P multiplied by 1 over an X plus one over NY and we take square root of this quantity we can also calculate the on-post standard deviation but again we should use the pulled proportion in this case because under the null hypothesis PX is the same as py so we can get observed this score using D over pull the standard deviation and in this case is 0.69 uh it's less than the critical z-score so we fail to reject the null hypothesis and we conclude there's no significant difference between the two clicks rates of algorithms similar to the one proportion z-test we can derive the complex interval for the difference between two proportions there's an important thing to note here typically we use input proportions instead of put estimate of proportions to derive the context interval the reason is that while the hypothesis testing procedure is based on the null hypothesis the contest interval approach is not based on that assumption so a level one minus Alpha 100 come this interval of P1 minus P2 is P1 Hertz minus P2 hat plus or minus the margin of error which is z alpha over 2 square root of P1 hat one minus P one hat over N1 plus P2 hat one minus P2 hat over N2 this part is basically the square root of the sum of variance of these two samples finally let's look at example how to derive the confidence interval of the difference between clicks rates of two algorithms so it's pretty simple we just continue using the previous example we can get the margin of error which is a critical z-score multiplied by the output stand deviation we should use the on-post standard deviation instead of the put standard deviation and then we obtain the lower bound of the Countess interval which is D the difference between the two observed clicks rate minus margin of error and the upper bound is the plus margin of error so that's how we can obtain the complex interval of the difference between two clicks rates awesome in this lesson you'll learn z-test for proportions we have talked about why using the Z test instead of t-test for proportions and also we talk about the test statistic and the testing procedure for one proportion Z test as well as two proportions z-test I hope you learned something new from this video I will see you in the next one

What is the Z-test for Means in Statistics? Easy Explanation for Data Science Interviews - YouTube
hey guys in this lesson let's talk about the z-test we'll focus on the Z test for means so in this lesson we will cover want to use z-test assumptions of the z-test basically the fundamentals of z-test and then we will look at some examples um how to use a z-test for one simple mean and how to use a z-test to compare the means of two populations this has often appears in interviews typically the interviewer will ask you some application questions meaning that you need to know how to use Zetas to draw conclusions from data for example you're giving the quick rate of an advertisement from two groups of users and you're asked to draw conclusions based on the data if there's any significant difference between the clicks rate of two groups of users the interviewer is likely to ask you to walk through the whole hypothesis testing process which means you need to know how to calculate the test statistic and how to draw conclusions from the data and we will cover this in this lesson so let's start with understanding when to use the z-test the z-test is used to infer properties of a population mean or proportion from a large enough sample of the population we can use a z-test to compare our population mean or proportion to a specific value or use a z-test to compare population means or proportions from two different samples now let's look at the assumptions of the z-test to use the z-test we need to make sure our data meets some assumptions and the one major assumption is the test statistic follow Z distribution or standard normal distribution typically the form of the test statistic looks like this it's a simple mean minus mu 0 over sample standard deviation where mu 0 is a constant or the sample mean of a second population to which we would like to compare our population mean there are multiple ways to ensure that this quantity Z is normally distributed so that we can use the z-test the easiest way is to assume that the population is normally distributed so our sample is also normally distributed and the population balance is known then the denominator of this quantity is constant and Z is normal in practice we really know the true variance of the population and that the sample points are exactly normally distributed so a more common scenario is that we do not know the exact distribution of each sample point but we do know they are observed independently of each other and that we have more than 30 simple points and that the population likely have a finite variance if these assumptions are met then by Central limit theorem the sample variance is the constant estimator of the true variance and this quantity the test statistic is approximately standard normal let's see some examples of using the z-test let's first look at z-test for one simple mean we use a z-test for one simple mean when we want to compare the population mean with a specific value suppose X is a large random sample of size n from a population with mean mu and the standard deviation Sigma so here are our hypothesis our null hypothesis is that mu the population mean in the same as Mu zero and our alternative hypothesis is that mu is not the same as Mu zero these are the two hypothesis of a z-test for one simple mean now let's look at the Z statistic based on the central limit theorem we know that the simple mean X bar follows a normal distribution with mean mu and variance is the population variance over n under the null hypothesis that mu is the same as Mu 0 we know the Z statistic follows a standard normal distribution if Sigma is unknown which is the case in almost all applications and the sample size is large we can replace it with a simple standard deviation s then the test statistic becomes simple mean minus mu 0 over s over square root of n based on our sample data we can obtain observed z-score and compare it with the critical z-score to draw a conclusion well Alpha the significance level is 0.05 we know the critical z-score is 1.96 so we can compare our observed z-score versus the critical z-score and we reject the null hypothesis if the observed this score is larger than the critical z-score otherwise we fail to reject the null hypothesis now let's look at example to use the z-test from one sample mean let's say we want to test the height of men suppose that we sample the height of 1000 Min we generate this data from a normal distribution with mean 172 centimeters and a standard deviation 4.5 centimeters but as expected in practice we'll assume that we do not have access to these parameters meaning that we do not know the true value of the population mean our null hypothesis in this case is that the population mean is 160 centimeters now let's look at the code we create a solid and simple points with the population mean 172 and sigma 4.5 but we don't have access to these parameters and we want to know if the mean is close to 160. so what we do here is we calculate the simplest standard deviation and then we want to know if the population mean is close to Mu 0 which is 160 centimeters so what we do is we first calculate the sample standard deviation which is the population standard deviation divided by square root of n then we can obtain the observed z-score which is the simple mean minus mu 0 divided by sample standard deviation in this case I've observed z-score is 84.76 which is much higher than the critical z-score which is 1.96 so we can reject the null hypothesis and conclude that our population mean is not equal to 160 centimeters that's how we can use a z-test to infer the property of a population mean now let's see how we can use a z-test to overturn the confidence interval for population mean suppose X1 X2 to xn are independent samples from a population with mean mu and suppose that n is larger than 30 so that we can apply the central limit theorem if the standard deviation of the population is known to us then a level 1 minus Alpha multiplied by a hundred percent come this interval for Mu is is X bar plus or minus z alpha over 2 Sigma over square root of n Alpha is a significant level for example Alpha is .05 then we can obtain a 95 complex interval for Mu here's the point estimate is the simple mean and the margin of error is z alpha over 2 Sigma over square root of n if Sigma is unknown to us which is typically the case we can use the sample standard deviation as now let's see how we can obtain the confidence interval from the height of men we will continue using our previous example and we already obtained the critical Z score which is 1.96 we can obtain the margin of error which is a critical z-score multiplied by Sigma over square root of N and then we can obtain the lower bound of the convince interval which is a point estimate minus the margin of error and the upper bound is the point estimate the sample mean plus margin of error that's how we can obtain the content interval from one sample mean all right let's move on to the next section how to use a z-test to compare the means of two populations let's say X1 is a large random sample of size M1 from a population with mean B1 and the standard deviation Sigma 1 and X2 is a large random sample of size N2 from a population with mean mu 2 and the standard deviation Sigma 2. we are interested in comparing the means of these two populations basically we want to know if mu 1 minus mu 2 is 0. here are our two hypotheses our null hypothesis is that mu 1 is the same as Mu 2 and our alternative hypothesis is Mu 1 is not the same as Mu 2. to conduct a hypothesis test we first need to compute a resist statistic in this case we know that the two samples are sampled from normal populations so the difference between these two samples should also follow a normal distribution with mean mu 1 minus mu 2 and variance is the sum of the two population balances now if we look at the distribution of the difference between between the simple means it follows a normal distribution with a mean mu 1 minus mu 2 but the variance becomes the variance of the first population over N1 plus the variance of the second population over N2 so we know this quantity follows a standard normal distribution and under the null hypothesis disease statistic follows the standard normal distribution let's look at example to calculate disease statistic let's say we want to compare the height of men from two different countries here we use some parameters to simulate two samples the first sample has one thousand data points and it's sampled from a normal distribution with mu 172 and sigma 4.5 the second sample has 9008 points it's sampled from another normal distribution which means 169 and sigma is 3. of course we don't have access to these parameters in real life applications but here we just use these parameters to simulate two samples and now we are interested in if the difference between the two population means is zero or not what we can do is we calculate the difference between these two sample means and then obtain its standard deviation the standard deviation of the difference is the square root of the variance of the first sample over the size of the first sample plus the variance of the second sample over the size of the second sample we can obtain to observed z-score which is D the sample difference minus mu 0 over standard deviation of d and we compared to the critical z-score which is 1.96 and obviously in this case I've observed this score is much larger than the critical z-score so we reject the null hypothesis similarly we can obtain the company's interval for the difference between two means the point as made in this case is the difference between the two sample means and the margin of error is d alpha over 2 multiplied by Sigma 1 squared over N1 plus Sigma 2 squared over N2 if Sigma 1 and sigma 2 are unknown to us we can estimate them using the sample centimetation as 1 and S2 respectively now finally let's say example to obtain the compass interval for the difference of height of men we just continue to use the previous example we can obtain the margin of error which is a critical z-score multiply Sigma D the standard deviation of the sample difference and then our lower bound of the confidence interval is D the simple mean difference minus module of error and the upper bound is D plus margin of error so now we can get the two sample on this interval from the point estimate D and the margin of error all right guys that's it for this lesson I will see you in the next one

What are Assumptions of Linear Regression? Easy Explanation for Data Science Interviews - YouTube
hey guys it's Emma welcome back to my channel in this video let's focus on linear regression linear regression is one of the top topics appearing in data science interviews especially statistics interviews and in this video we are going to focus on one fundamental question what are the assumptions of linear regression we will talk about not only the assumptions of linear regression but also how to diagnose violations of those assumptions as you will see violations of linear regression assumptions ranging importance from critical to lesson ideal but non-critical on top of that I will also give you a tip to remember the assumptions easily alright let's dig in the assumptions are as follows the true online relationship between X and Y the independent and the dependent variables is linear the residuals are independent the residuals are normally distributed and they have constant variance I.E hormones density so the first Assumption of linear regression is about the relationship between X and Y and the remaining three assumptions are about the residuals or errors you can easily remember this with the acronym line IO stands for linear relationship I means the residuals are independent and means the residuals are normally distributed and e means equivalence of residuals it's important to check these assumptions when performing linear regression as violations of these assumptions can have a substantial impact on the parameter estimates including the complex intervals and levels of statistical significance note that linear regression does not assume anything about the distributions of X and Y also you will sometimes see additional assumptions listed such as the sample is representative of the population or the variables are measured accurately Etc although these are important considerations for statistical modeling they are not necessary assumptions of linear regression now let's look at each assumption in more detail the first Assumption of linear relationship is that there is a linear relationship between X and Y the independent and dependent variables this is a critical assumption as we are estimating the linear parameters of the independent variable if the underlying relationship between X and Y is not linear the model won't be a good fit for the data and the parameter estimates will be meaningless in reality it is rare for X and Y to have a perfect linear relationship in other words our data to form perfectly on a straight line instead it's more common for data to appear as a cloud of points as you can see on the slide in each scenario the data for around a straight line but none of them for exactly on the line one common way to spot non-linearity between X and Y and verify if this assumption has been met is to use residual plots a residual plot is a plot of the residuals on the y-axis versus the predicted values of the dependent variable on the x-axis the points should be symmetrically distributed around the horizontal line with a roughly constant variance let's look at some simple plots the plot on the left does not show any obvious signs that the relationship between X and Y is non-linear the residuals appear to be randomly dispersed and the variance of the residues is the same all along the x-axis but the plot on the right shows potential issues in particular there appears to be a curved linear Trend in the residuals suggesting that the relationship between X and Y is non-linear we should not use a straight line to model this data instead a more advanced technique should be used the second Assumption of linear regression is that the residuals are independent this assumption is also critical but often violated when the data are collected over time such as time series data and successive residuals tend to be positively correlated for example if we collect successive data on the temperature in the city the temperatures are likely to be correlated within seasons and unlikely to be random now let's see how to diagnose if the data meet this assumption to check Independence we can check if the observations are collected in a sequence and if there's any connection between cases that are close to one another a pattern that is not random suggests lack of Independence on the slide the left plot shows the sample data with your best fitting lines and the right part shows their corresponding residuals it's clear that in this case successive observations are highly correlated so the independence assumption does not hold moving on to the third regression assumption which is that the residues should follow a normal distribution this is nice to have condition however it is not required here we benefit from the central limit theorem which holds that if the symbol size is a large and the data do not have lots of outliers then the parameter estimates and the predicted values of the dependent variable are approximately normally distributed even when the residuals are not however violation of normality for the residuals create a couple problems they make it difficult to both determine if model coefficients are significantly different from zero and the calculated complex intervals for predictions sometimes the residuals are non-normal because they are skewed by the presence of a few large outliers since primary estimation is based on the minimization of squared error a few extreme observations can have a disproportionate influence on parameter estimates therefore it is essential to check your procedures to see how they are distributed there are different ways to check the normality of the residuous a common method is to use a quantile quantile or QQ plot of the residuals cucoplas shows the quantile of the residual distribution versus the quantity of a normal distribution having the same mean and variance if the distribution of residuous is normal the points on such a plot should foreclose to the diagonal line as shown in the graph in this plot the residuals have a bow shaped pattern the deviations from a diagonal line indicate that the residuals have excessive skewness meaning that they are not symmetrically distributed and have too many log errors in the positive direction making for a long tail in this plus there's excessive skewness or a long tail in the negative Direction and the ball is turned in the other direction here's another example this is an example of an s-shaped distribution this plot shows a distribution that is under dispersed first relative to normal distribution it's more like a uniform distribution as shown in the histogram on the left the tails in such a distribution are virtually non-existent and there appear to be no outliers in the data in summary the bow shaped and the s-shaped distributions of the QQ plot indicate that the regression procedures are not normally distributed which is a violation of the normality Assumption of residuals moving on to the last Assumption of linear regression and that is sometimes referred to as homosetheticity of residuals homo statistic means equivalence the residuals should have approximately equivalence if not the regression model might not be a good fit for certain data points I.E those with large residuals in practice this assumption is often violated it is a less important consideration for inferences based on parameter estimates in terms of verifying this assumption we can also look at the residual plot the residual plot is useful for checking the homostasticity of residuous and in the case of Time series data you may want to look at a plot of residuals versus time time to ensure that residuals have equal variance let's look at some examples here are three plots that suggest that the variance of the residuals is not constant for the predicted values in the first case the variance is small to the left of the plot and large to the right the residuals become much larger as the fitted values increase in the second plot in the middle the variance of the residual is linear instead of constant in the last plot on the right the pattern is quadratic meaning that the residuals do not have equivalence for all these three plots the religious lack homocelasticity and violate the last Assumption of linear regression alright guys let's summarize what we have learned in this lesson we revealed the four major assumptions of linear regression they can be easily remembered using the acronym line the first two assumptions are critical and the last two are less important we also discovered how to diagnose each assumption it's important to check these assumptions when performing linear regression okay that's it for this lesson feel free to re-watch this lesson to review the assumptions of linear regression I will see you in the next lesson

Stratified Sampling In 3 Mins: Easy Explanation for Data Scientists
hey guys in this video let's talk about stratified sampling this method is often used in machine learning to create test datasets to evaluate models especially when the data is unbalanced you will learn what is stratified sampling and the pros and cons of this sampling technique let's dive in if your population is not a homogeneous group meaning that it can be divided into subpopulations or strata then you can use stratified sampling the idea is simple we divide the population into strata then use simple random or systematic sampling on each stratum making sure to keep the proportions in our sample similar to the ones in the entire population for instance in a survey of data scientists you may divide data scientists by level of seniority Junior data scientists senior designers data science managers and data science leaders suppose we know that is stratum represents 25 percent of the entire population of data scientists and we want to survey a symbol of 100 data scientists then we use simple random sampling to choose 25 data signage from each group the goal of stratified sampling is to ensure that all members are represented in the sample there are a few advantages of using this sampling method one is its accuracy stratified sampling can produce more precise estimates than simple random sampling when members of the strata are homogeneous but relative to the entire population it reduces variability and bias in our sample because the representativeness of the sample is increased due to the intentional sampling from each stratum stratified sampling can also make sample selection efficient and manageable by organizing a population into groups with similar characteristics we can better manage a sample than might otherwise be too large to analyze it also allows us to focus on important subpopulations and ignores irrelevant ones there are also a few drawbacks of stratified sampling first it can be difficult to organize a population into groups particularly if members fall into multiple groups for instance it is relatively easy to create Estrada for data scientists based on their seniority it is a much more complex task to try to create strata based on the skill sets many data scientists have a diverse set of skills so it will be difficult to organize them into one group second many strata may be small and have few members although stratified sampling aims for strata proportions that are representative of the entire population the lack of data within certain strata may make it difficult to create precise estimates for every stratum for instance if we are creating strata based on whether a user of a SAS company is a fraudulent or legitimate we are likely to have far fewer fraudulent cases than non-fraudulent cases it's entirely possible that we simply won't have enough fraudulent cases to provide meaningful estimates for them particularly if we consider other factors such as gender or age to create groups in addition to use stratified sampling we must ensure that every member of the population fits into only one stratum and that all strata collectively contain every member of the greater population this involves extra planning and information gathering which will make it hard to implement awesome you've learned stratified sampling in next video I will talk about the last assembly method in this video series on Sunday methods and that is cluster sampling stay tuned

Cluster Sampling In 5 Mins: Easy Explanation for Data Scientists
hey guys in this video let's look at a sampling method that is similar to stratified sampling cluster sampling this is the last video in our video series on sampling techniques we looked at three common self investors simple random sampling systematic sampling and stratified sampling if you haven't watched the previous videos yet you can find all the links in the video description let's dive into cluster sampling if your population is naturally divided into groups that are representative of the whole population then it may be more cost effective to select some of these groups using simple random sampling or systematic sampling and include all of the elements and individuals of the selected groups in our sample this is cluster sampling let's consider a simple example say we want to survey data scientists in the U.S and we found a few Tech hubs such as the Seattle Metropolitan Area the San Francisco Bay Area the New York metropolitan area Etc it means our population of Interest is divided into similar groups in different Geographic locations then we may simply randomly select some locations using simple random sampling and include all data scientists from those locations in our sample the underlying assumption is that data scientists from a randomly selected geographic area are representative of the whole population of data scientists in the US if there are too many data scientists in one area and we need to reduce the cost of sampling we can use simple random sampling or systematic sampling to assemble some comments in that area then we include all data scientists in those companies as our sample this will be called multi-stage cluster sampling because we first divide the population of data scientists by location and then by coming in selected locations let's look at the pros and cons of cluster sampling cluster sampling is practical and relatively easy to use this type of sampling process enables us to study large populations that would otherwise be too challenging or complicated to analyze class assembly has a high external validity which means that the results can be generalized to the whole population cost assembling is also cheap and efficient in the data science example it reduces expenses needed to cover large geographical populations we only need to focus on one or a few areas cluster sampling also has some drawbacks firstly there's an inherent complexity to Cluster sampling which comes in through the planning of the study this method often requires more attention because we need to determine how to divide up a larger population efficiently and probably and ensure that the Clusters represent every possible characteristic of the population also cluster sampling is associated with a high sampling error when the Clusters Do Not Mirror the population's characteristics or serve as a mini representation of the population as a whole there will be less statistical certainty and accuracy also sampling error increases as the number of clustering stages increases finally let's look at the differences between stratified sampling and cluster sampling although they appear to be similar there are different techniques in both techniques we start by dividing the population into groups but what we mean by group is most likely different in the two in stratified sampling a group is a a characteristic of the population such as the level of seniority of data scientists in cluster sampling a group is a subset of the population that has all of the characteristics of the whole population in other words a group is a miniature or smaller version of the population there is one additional difference between the two sampling methods in stratified sampling we randomly select members from every group ensuring that the proportions in the sample are the same as those in the population for the data scientist example we need to sample from all groups including Junior data scientists senior data scientists data science managers and data science leaders by comparison in cluster sampling we randomly select some groups and they include all members of the groups in the sample for the data scientific example we can simply roll a die to decide which of the locations we choose and then we keep all data scientists at those locations in our sample in this video series we looked at four common sub investors simple random sampling systematic sampling stratified sampling and cluster sampling each of these methods have pros and cons I hope this video series help you understand different sampling techniques and choose which one is appropriate for analysis and research thank you guys for watching I will see you in the next video bye guys

Systematic Sampling In 2 Mins: Easy Explanation for Data Scientists
hey guys in this video let's talk about another sampling technique systematic sampling you will learn what this mapping sampling is and the pros and cons of this sampling technique suppose all of a population's members are the same for one or more dimensions in other words a population is logically homogeneous then you could conduct systematic sampling by creating a list of all members of the population and selecting every individual for example with the easing of code with restrictions some countries decided to only test some people arriving from abroad the countries do not expect to see a significant difference between selecting a number of passengers uniformly at random and testing every first passenger who gets off the plan so they simply test every first person instead of everyone this sampling technique is easy to implement and efficient once we have selected a target population we only need to decide the sampling interval and a starting point then we are able to identify members of our sample however there are some drawbacks to this method one drawback is it is vulnerable to periodicities or patterns in the list for instance if the list of passengers alternates between men and women then selecting every first person may only capture women so the ordering of the list is important you should not use systematic sampling if your population is ordered cyclically or periodically as your resulting sample cannot be guaranteed to be representative all right we just learned systematic sampling we have two more assembly methods to go there are stratified assembly and the cluster sampling i will see you in the next video bye guys


Simple Random Sampling In 3 Mins: Easy Explanation for Data Scientists
one of the biggest challenges faced by data scientists is dealing with a massive amount of data a proactive way to study a population is to use sampling in this video series on sampling methods we view them into four common use sampling techniques which are simple random sampling systematic sampling stratified sampling and cluster sampling I will talk about what they are and the pros and cons of each sampling technique let's focus on simple random assembling in this video you'll be able to understand what is simple random sampling and its pros and cons let's get started the first semester is simple random sampling as simple as it may sound we just randomly select the elements from population intuitively we'd like to make sure that we do not favor any individual in the population When selecting elements that is we want to select our individuals uniformly at random with equal probability the hope is that by selecting our individuals from the larger population without preference we will likely get the sample that is representative of the larger population for example assuming we have a list of all data scientists in the US we can simply select 200 data scientists at random from the list for our sample there are some advantages of simple random sampling first it minimizes bias it is at least a biased sampling method because every member of the target population has an equal chance of being chosen therefore it is likely to have a high internal and external validity like any sampling technique there is a room for error in simple random sampling but this method is intended to provide an unbiased sampling approach simple random sampling can be cubism and the time consuming when something from a large Target population this becomes a bigger problem as the difficulty of determining every element or individual in the population increases imagine the population of interest is all data scientists in the US we can try to create as a complete a list of individuals as possible but it is nearly impossible not to miss some individuals this results in non-randomeness as not all individuals have an equal probability of being selected it simple random sampling can be vulnerable to sampling errors specifically the randomness of the selection may cause the sample to not resemble the population as a whole basically we have a sample that is not representative of the population for example if we choose a hundred data scientists from the entire population we may have 50 Junior data scientists by pure chance and they would compromise 50 percent of our sample when in reality Junior data scientists may only make up 25 percent of all these scientists the technique we will talk about the next attempt to overcome this problem by using information about the population to choose a more representative sample as promised we learned simple random sampling there are three more sampling methods I want to introduce to you and in the next video we will look at systematic sampling stay tuned


What are Bootstrap and Permutation Tests in Data Science? Easy Explanation for Beginners
what's up guys welcome back to my channel in this video we will talk about resembling you may have heard of the term recently before even if you haven't you might have used it in statistical analysis or predictive modeling in this video we will cover topics such as what is resampling and why we need resampling also we will introduce two resembling methods bootstrapping and the permutation tests finally we will compare these two methods our goal for this video is to understand what they are and I will try my best to explain them as intuitively as possible alright guys let's get started let's first recap what is simple and word example we talk about both Concepts in a previous video on sampling if you haven't watched it yet I highly recommend watching it first before diving into this video alright guys let's go back to this video a sample is a subset of the population sampling is the selection of a subset of units from within a population to estimate the current characteristics of the whole population resampling consists in sampling a simple the result is the sample of the original sample so in resampling the original sample is considered as the population resampling is a non-parametric method so why do we need resampling why is it useful at all suppose we are interested in estimating the properties of a population or in comparing two populations we have sampled our populations by conducting an experiment or an observational study and we have estimated statistics of Interest such as the mean using the simple mean in a single population case we would like to estimate the variability in our sample statistic by Computing the variance of its distribution or a countless interval in the two population case we'd like to test whether the means of the two populations are equal if it made sense to introduce assumptions on distribution of the data such as normality of the sample points or a video was easy to collect new data from the population IE samples the true population again then we would use techniques such as t-test z-test and complex intervals in parametric statistics was the normality assumption does not hold and we are not able to collect more data from the population we will use resampling for instance the data is collected by running an experiment the costs such as time and money of collecting new data may be too high for instance in a marking campaign a company issues coupons to customers to promote a specific product after the campaign is over and the coupons are expired it's hard to generate muted points about customers behavior in observation of study we may not know when we will next be able to observe new data points for example when the sample points are astronomical phenomena in reality there are many cases you may not have the whole population to assemble from anymore to solve these problems we will simply assemble our first sample again enough times to estimate the simple statistic distribution this is resampling we can then compute the variance of this distribution or its quantiles to estimate the variability and the compute complex intervals and p-values one of our non-underlying assumptions when using resampling is that simple statistics are good enough approximations of the population statistics that we may resample to estimate the stand errors for example if we are interested in the mean we are assuming that the standard deviation of the sample means is approximately equal to the standard deviation of the mean obtained by resampling okay now you know why resulting is useful let's discuss some common use with sampling methods there's a variety of reasonably methods and we'll focus on the two common used ones in practice bootstrapping and the permutation tests the first method is bootstrapping it is used for estimating the Precision of simple statistics by drawing randomly with replacement from the original sample the bootstrap method uses the gaming sample to create a new distribution called the bootstrap distribution that approximates the sampling distribution for the simple mean or other statistics the idea behind boost driving is that if the original sample is a representative of the population then the bootstrap distribution of the mean will look approximately like the sampling distribution of the mean that is have roughly the same spread and shape to find the bootstrap distribution of the mean we draw samples of size n with replacement from the original sample and then compute the mean of each resample these samples are called free samples or bootstrap samples in other words we now treat the original sample as the population know that drawing lots of bootstrap observations from the original You Know sample is not like joint observations from the online population because it does not create new data for most statistics bootstrap distributions approximate the spread bias and the shape of the actual sampling distribution the bootstrap is not used to get better parameter estimates because the bootstrap distributions are centered around statistics calculated from the original sample rather than the unknown population variables instead bootstrap sampling is useful for quantifying the behavior of a parameter estimate such as its stand error skewness and bias or for calculating complex intervals you may be wondering how many bootstrap samples are needed typically a thousand bootstrap samples are enough for rough approximations but more are needed for greater accuracy the rule of thumb is to use at least 10 000 bootstrap resembles and more when accuracy matters the other resembly method we are going to cover is the permutation test it's an non-parametric hypothesis test the reason is a resembly method is that it relies on resampling the original data assuming the null hypothesis based on every simple data it can be concluded How likely the original data is to occur under the null hypothesis your permutation test two or more samples are involved let's look at the two sample case as is similar to the case of three or more groups let's say we have two groups control and treatment the null hypothesis of a permutation test is two samples come from the same distribution if the null hypothesis is true it means the treatment to which the groups were exposed to do not have an effect then all the observations are drawn from the same distribution and we can mix them up this assumption is called exchangeability of the observations under the null hypothesis the exchangeability assumption is weaker than assuming that the sample points are independent and identically distributed to permute means to exchange labels on data points or change the order of a set of values so we combined observations from two groups then we create a permutation resample by drawing a certain number of observations without replacement from the poor data to be one sample and we leave the remaining observations to be the second sample we then calculate the statistic of interest for example the difference in means of the two samples we repeat this many times such as a thousand or more basically we test the hypothesis by random join groups from this combined set and seeing how much the two samples differ from one another in each resample to know whether to reject the null hypothesis we Reason by contradiction if the observe the mean difference from the two original samples is an unlikely realization of the simple mean difference distribution computed from all of the permutations then the two original samples were probably not from the same distribution the p-value is a fraction of times the random statistic exceeds the original statistic for example The observed mean difference from the two original samples is two and we have a total of a thousand permutations we simply count the number of sample means as or more extreme than our initial test statistic and the p-value is a count divided by the total number of tested statistics we calculated in this case it's turned over a thousand now you understood both post-trapping and permutation tests let's compare these two measures the permutation test is used to estimate the sampling distribution for the sample mean or other statistics we sample with replacement from the original sample by comparison the permutation test is used to perform a hypothesis test when we may assume that the put sample points are exchangeable under the null hypothesis we sample without replacement from the put data that is everything for this video If you like this video I have a whole playlist for you on statistics related Concepts and problems I talk a lot about the data science data science interviews as well as tips and strategies to help you crack data science interviews and keep your skills sharp so make sure to subscribe to my channel to get updates on future content thank you so much for watching I will see you in the next video bye


Sampling With and Without Replacement: Easy Explanation for Data Scientists
hey guys welcome back to my channel in this video i want to give you an overview of sampling a popular statistical concept that involves selecting a subset of units to estimate the characteristics of a population sampling is particularly useful when dealing with big data sets in practice in this video we will dive into why do we need sampling what is sampling and the replacement of units i will try to explain everything in 10 minutes by the end of this video you will have a good understanding of what the sampling is and the difference between sampling with replacement and without replacement let's get started let's start with a typical setting when we started a problem in practice before we can conduct an experiment such as eb testing or observational study we need to select some participants the word participant here is understood in a broad sense this could be people for example the visitors of our website or people having a certain medical condition but there could also be any other objects or events of interest ideally we have access to the whole population we want to study for example if we are interested in studying popular topics on twitter and understand the settlement of those topics then we ideally look at all of the tweets produced every day if we are interested in the lens of books on a certain topic they would ideally look at all the books ever written on that topic imagine if whenever we run experiments we needed to survey or measure the entire population of interest that would be enormously time intensive and costly process most of the time we do not have access to all possible observations this could be for many reasons for example it may be challenging to gather all observations together there are about 600 million tweets produced every day it is unnecessary to look at all of them to determine the topics that are discussed during the day nor it is efficient to look at all the tweets to determine the sentiment on each of the topics another possible reason that we do not have access to all observations is that more observations are expected to be made in the future an example is there are new tweets created every day also it may be difficult or expensive to make more observations so we run into a very common issue when designing an experiment we do not easily have access to the entire population due to cost privacy complexity etc this is where something comes in thankfully data scientists do not need to assemble the entire population but the assembly must be done correctly in all the files to make correct inferences so what exactly is sampling wikipedia puts it this way sampling is the selection of the subset of individuals from within a population to estimate characteristics of the whole population statisticians attempt to collect samples that are representative of the population in question so sampling allows us to make inferences about the population based on a much smaller subset of the population which we call a sample sampling enables the selection of the right sample from within the larger data set to estimate attributes of the population going back to our tweets example instead of looking at all of the tweets we will try to select a subset of them that we believe has the same proportions of topics as entire population of tweets that way we should be able to use statistical inference methods to infer the properties of the whole population of tweets now we know what assembly and why something is important in practice before diving into different sampling methods let's observe that we can sample from the population either with or without replacement for assembling with replacement it is one of sampling units is drawn from a population and is returned to that population after its characteristics have been recorded before the next unit is drawn using the tweet example when we sample with replacement each time we select the tweet we look at its content and record it then return it to the pool before selecting another tweet so when we sample with replacement we might end up selecting and measuring the same unit more than once the items in the samples are independently drawn from the population because one random draw is not affected by the previous draw in comparison while assembling unit is drawn from a population and is not returned to that population before the next unit is drawn the sampling is said to be without replacement it means that each draw is not independent as the size of the pool decreases after each draw therefore there's a distinction when sampling multiple times using something without replacement there's no independence among different draws for instance if you wish to draw a sample of size 2 from a population of 100 people then the probability of any one person being selected is 100 however after having selected the very first person the probability of any one of the many people being chosen is now one over 99 in many real situations for example in conducting surveys we do not want to have the same person pulled twice so we would sample without replacement in which case we will survey a different person each time and the same person will not be surveyed twice to summarize the difference between those two replacements of selected units is that when we sample with replacement each draw is independent practically this means that what we get on the first one doesn't affect what we get on the second one unit may appear multiple times if we sample with replacement white appears at most once if we sample without replacement however in cases where populations are very large compared to the sample size data collected on the sampling without replacement are reasonably approximated by data collected on the sampling with replacement because when the population is huge whether we put the selected unit back or not does not have a significant impact on the probability of one of the many units being selected let's summarize what we have learned in real world scenarios we really have access to the entire population we are interested in therefore if we want to learn the properties of this population using statistical inference we need to gather samples that are representative of the whole population this is the sampling we also mentioned two ways of replacement of selected units they are simply with and without replacement alright guys as promised that's simply in 10 minutes i hope you enjoyed this video in the next video we will talk about re-sampling specifically i've explained two common use reassembly methods bootstrapping and permutation tests make sure you subscribe and hit notification bell so you can stay updated on all my latest content stay tuned bye guys


Is This Coin Fair? Hypothesis Testing Questions and Answers in Data Science Interviews
what's up guys and welcome back to my channel in this video let's solve a common ask problem in data science interviews together the problem is how to test if a coin is fair or not there are a few different versions of the problem which is designed to test the understanding of the binomial distribution as well as hypothesis testing the core question being asked is the same in all versions of the problem and it is how to test the hypothesis that a coin is fair in this video we are going to discuss the most open-ended version of this problem which is the following how would you design an experiment to test if a coin is a fair coin or not by the end of this video you will have a clear idea on how to solve this problem maybe you already know how to solve it then you can compare your idea with mine let's get started before we discuss this version of the problem we need to be familiar with several concepts including the binomial distribution and hypothesis testing and you might be familiar with them at this point so let's do a quick recap the binomial distribution describes the number of successes in a sequence of n independent trials where k is the number of successful trials and p is the probability of success number of heads of multiple coin flips follow a binomial distribution because they have a binary outcome and each flip is independent we can refer to the number of leaves as the number of chops in and the probability of heads and the probability of success p now let's take a look at our question of how to design an experiment to test whether or not a coin is fair unlike many coin problems we are not given the outcome or number of tosses to test if a coin is fair or not we have two different tests we can use one option is to use a binomial test when we have small number of samples the other option is to use a z-test with a large sample size in this question there's no requirement on the number of flips so we can choose either a binomial test or a z-test to solve the problem but the better option is the z-test with a large amount of flips because the power is larger with a larger sample size it means that with large number of coin flips the chance the test correctly reject the null hypothesis is higher now let's go over the steps to solving the problem together the first step is to specify the null and alternative hypothesis in this case we assume the coin in the fear coin under the null hypothesis and set the null hypothesis to be p equals to 0.5 let's also set the significance level alpha to be 0.05 which is a common use significance level the second step is to determine the number of tosses we need to perform the z-test in other words how many tosses are needed to conduct a z-test with sufficient power giving our selected significance level for now let's set our number of tosses to be a solvent under the null hypothesis the test statistic approximately follows a standard normal distribution in the numerator we have n multiplied by p hat which is the observed number of successful trials the probability of success p represents the probability of success under the null hypothesis so unmultiplied by p is the number of heads under the null hypothesis in the denominator we have the standard deviation of the binomial distribution we can also use k to denote the observed number of successes and k the same as n multiplied by p hat now we can compute the mean and standard deviation of number of heads under the null hypothesis let's plug the mean and the standard deviation into the formula for the test statistic the next step is to get the threshold for the observed number of successes k we can define success as a head or a tail either way the threshold is a number of successes under the null hypothesis if we observe a number of successes within the threshold we fail to reject the null hypothesis and conclude the coin is fair otherwise we reject the null hypothesis to calculate the threshold we can find the critical value based on our choice of the significance level 0.05 the critical z-score values when using a 95 percent confidence level are negative 1.96 and positive 1.96 standard deviations because we are doing a two-sided test the critical z-score can be a negative value or a positive value now we are able to calculate the threshold of number of successes and we get k is larger than 469 and less than 530.98 therefore the number of observed heads should be no less than 469 and should not exceed 531 in order for us to be convinced that the coin is a fair coin this applies to the number of tails as well otherwise we reject the null hypothesis and we are convinced that the coin is biased now let's summarize what we have learned in this lesson we learned one of the three versions of the coin problems which is how to design an experiment to determine whether or not a coin is fair to determine whether a coin is fair or not we can design an experiment to toss a coin a thousand times if the coin is fair we should observe 469 to 531 heads and the same applies to tails otherwise we should reject the null hypothesis and be convinced the coin is biased alright that's everything for this video if you enjoyed it you may also want to check out this playlist on solving more statistics related problems i talk a lot about data science data science interviews as well as tips and strategies to land your dream job in the tech industry so make sure to subscribe to my channel to get updates about future content thank you so much for watching i will see you in the next video bye


Bayes' Theorem Made Easy: Intuitive Explanation for Data Scientists
what's up guys welcome back to my channel in this video let's talk about one of the most important theorems in probability theory and that is the base theorem or base rule to help you understand the concept i will try my best to explain it as simply as possible also instead of talking about all the theories that may be hard for you to understand and remember i will use visualization to illustrate b serum intuitively afterwards we'll look at example together and see how to use the base theorem to solve a real problem sounds exciting all right let's dive right into it base theorem is a bayesian approach dealing with conditional probabilities and it provides a mathematical rule for how you should change your existing beliefs when you're presented with new data or information it states that the conditional probability of event a based on the occurrence of another event b is equal to the likelihood of event b giving the event a multiplied the probability of event a to understand it intuitively let's use visualizations to derive the base theorem i'm going to borrow the idea presented in this amazing blog post by oscar bonilla this post does a great job of explaining the concept intuitively i have a link to the post in the video description feel free to check it out so the idea is to use the venn diagram to visualize probabilities let's say we have a universe with all the possible outcomes and we are interested in some subset of them namely some event for example we study the behaviors of users on website our universe is all the users on our website let's consider two possible outcomes for any particular user was the login today or not we use a in the venn diagram to show the event users who log in today so what is the probability that the random selected user logs in today it is just the number of elements in a divided by the number of elements of the universe right so it's simply a divided by u the universe now let's look at another event a user login yesterday let's call it event b similar to event a we can get the probability that the randomly selected user logged in yesterday as a number of elements in b divided by the number of elements of the universe now let's look at these two events together what if a user logged in both yesterday and today we can use the overlapped area to represent the event that users who log in both yesterday and today we can also get the probability of both events occurring or the joint probability of a and b which is the number of elements in the overlapped area over the total number of elements of the universe this is getting interesting let's see how we can represent the conditional probability probability of a given b from the diagram basically b becomes our new universe now and we want to know given b what is the probability of a in other words once we confine ourselves to just event b what is the probability of a from the diagram we can tell it's simply the probability of the intersection of a and b divided by the probability of b so we can represent the probability of a given b as the probability of a intersection b over the probability of b similarly we can get the conditional probability of b given a in this case a becomes our new universe and we try to find the probability of event b within the new universe it is equivalent to the probability of the intersection of a and b divided by probability of a now let's organize what we have learned so far we got two different conditional probabilities using a venn diagram the second equation tells us that p a intersection b is equivalent to pb given a multiplied pa it indicates another way to obtain the joint probability of events a and b therefore we can replace the joint probability of events a and b with pb given a multiplied by pa in the formula for pa given b doing so gives us base theorem the base theorem allows us to calculate the probability of a given b once we know the probability of b given a probability of a and probability of b simple right i hope these diagrams help you understand how to derive the base theorem now let's try applying base theorem to an example suppose we have the following situation each user on website has a daily login probability of 1 4. in other words the probability of any user login on any particular day is 1 4. if we use a login on a particular day and also logging on a previous day the probability of that user spending more than 5 minutes on web page 4 5th however if you use a login on a particular day and did not log in on a previous day the probability of that user spending more than 5 minutes on web page is one third the question is given that a user spent more than five minutes on web page today what is the probability that the user log in both today and yesterday well because a user has to log in today to spend time on the webpage we know they have logging today so the question is basically given that a user spent more than five minutes on web page today what is the probability that the user logging yesterday this is clearly a conditional probability problem let's organize information we have and write it using notation of conditional probabilities just to recap we know the probability of daily login is one fourth we also know that giving a user login both yesterday and today the probability that user spend more than five minutes on webpage is 4 5th and we know that given user login today and did not log in yesterday the probability of that user spending more than 5 minutes on web page is one third what we really want to obtain is the probability that the user login today and yesterday given the user has spent more than 5 minutes on web page now that we have organized what we know about this problem let's plug our information into base theorem in this example event a is a user logging both yesterday and today and event b is user spending over five minutes on web page the numerator is a conditional probability of b given a multiplied by the probability of a so we can calculate it as the probability of users spending more than 5 minutes on the web page given that they log in both yesterday and today multiply by the probability that the user log in both yesterday and today well the conditional probability is already known to us which is four fifths we only need to obtain the probability of the user login both yesterday and today since the user's daily logins are independent we can calculate the probability of a user login both yesterday and today as 1 over 4 squared so we can get the numerator easily next let's see how to calculate the denominator which is the probability of a user spending more than 5 minutes on weight page here we need to use the law of total probability which states that the decomposition of probability a is the sum of the probabilities conditioned on bi multiplied by probability of bi with events bi spanning all the cases of a so the denominator which is the probability of a user spending more than 5 minutes on web page can be decomposed into two parts the first part is the probability of a user spending more than five minutes on the web page and the user login both yesterday and today the second part is the probability of users spending more than five minutes on the web page but the user did not log in yesterday you may realize that the first part is exactly the same as the numerator we just calculated so we can reuse it for the second part we already know the conditional probability of a user login today but not yesterday is one third we can obtain the probability of a user login today and not easily as one over four multiply three over four because the probability of the user logging on any day is one over four multiplying the two gives us the joint probability now we got the probability of a user spending over five minutes on web page today since we have both the numerator and the denominator we can get the final probability which is four over nine it means that giving a user spend more than five minutes on the web page today the probability that user logging yesterday is less than one half that's how we can apply base theorem to solve a real problem great job working through this problem with me we made it in this video we covered one of the most commonly used theorems in probability theory and that is the base theorem which is often used to calculate conditional probabilities if you like this video make sure to subscribe to this channel to get updates about future content thank you so much for watching i will see you soon


Probability and Statistics Made Easy: Essential for Data Scientists
have you ever wondered what's the difference between probability and statistics why do they often appear together hey guys it's Emma welcome back to my channel in this video I really want to talk about topic which took me quite some time to understand and that is the difference between probability and statistics I was a little confused by the two terms when I first learned about them and I was curious to know why they often appear together I thought it would be helpful to make a video and talk about the difference and relationships between the two explicitly so in this video let's focus on the difference and the relationships between the two first I will explain the two areas of Statistics descriptive and inferential statistics then talk about the difference between probability and statistics finally we will touch the relationships between the two the goal of this video is really to give you a big picture of statistics and probability to help you understand those Core Concepts let's get started in statistics there are descriptive and inferential statistics as a name suggests descriptive statistics are used to describe a sample such statistics are very straightforward you simply decide on the sample that you are interested in obtain the data and then calculate the summary statistics of the data the summary statistics are often displayed visually in graphs using common charts you have seen many times like a bar chart and a line chart Etc different from descriptive statistics inferential statistics use data from a sample to make inferences about the population from which the sample was drawn another way to think about this is that the goal of inferential statistics is to draw conclusions based on Sample and generalize the conclusions to a population unlike descriptive statistics there is uncertainty involved in inferential statistics because you are not simply describing a sample but also trying to infer properties about a larger population the uncertainty in inferential statistics is typically reflected in a probability which serves to quantify our conclusion and acknowledge that we cannot State our conclusions with 100% confidence let's look at an example in this article published on towards AI the authors did a very good job analyzing data science job market trend for 2021 they obtained job data from several career portals and conducted descriptive statistics from over 3,000 data scientist job postings they look at things such as top companies in the US actively recruiting data scientists top locations hiring data scientists in the US and the trend of positions within different groups of experience level all of them are helpful for job Seekers to understand the job market there might be a little hard for you to see them clearly on the slide but I have the link to the article in the video description feel free to check it out now let's assume that from this sample You observe that the top two programming languages for data scientists is Python and SQL and you want to know if this finding also applies to the whole population of data scientist jobs and not just from those 3,000 job posts in other words you want to make conclusions about all job openings for data scientists in the US based on a sample of over 3,000 job posts now you will need to use inferential statistics which involve a level of uncertainty as you generalize from the sample to the population of all data scientist job openings this is a core difference between inferential and descriptive statistics next let's talk about probability I'm sure you have heard the term many times to put it simply it reflects the likelihood that particular event will occur the probability of an event is a number between zero and one as you can tell probability is simply the assignment of numbers between Z and one to possible events we can upend the probability of an event at a time such as flipping a con predicting the weather and predicting indices in the stock market Etc as we look at the world around us such as populations of human beings our social interactions financial markets and so on and we ask ourselves can deterministic models always capture the complexity in these systems the answer is no it becomes clear that we need a framework or language for us to talk about all models at once and that is probability Theory probability Theory provides a framework for modeling of complex systems although it is not possible to perfectly predict random events much can be said about their behavior for instance common intuition tells us that if we toss a fear con many times then roughly half of the time it will turn up heads and the other half it will turn up tails the more often the coin is flipped the more likely that the ratio of the number of heads to the number of Tails will approach one probability Theory provides a formal version of this intuitive idea known as the law of large numbers now we know what is probability and the probability Theory so what is the exact difference between probability and statistics what we really want to compare is probability Theory and inferential statistics because these two are often compared with each other so basically our question is what is the difference between probability Theory and inferential statistics to understand the difference between the two I think it's helpful to start with understanding their relationships with observed data as I mentioned earlier probability theory is a mathematical framework able to capture the uncertainty in the outcome of event we are trying to model we can use probability Theory along with knowledge of the rules of the experiment to design a model before the experiment has been taking place it means that probability solves problems even when there's no observed data it tries to discover and model Universal patterns an example is a law of large numbers I mentioned earlier on the other hand inferential statistics mostly comes into play after an experiment has taken place and an outcome of data has been observed the goal is often to recover a general probabilistic model from observed data to this end inferential statistics eer assumes a theoretical model and estimates its parameters from the data such as sample means and Sample variations or it could try to infer the general model directly essentially for INF statistics we are giving the data and ask what model generated the data for example we might be told that we conducted five coin tosses and obtained five heads in a row and asked what the likelihood is that the coin was a fear coin so in summary probability Theory aims to provide abstract framework to model seemly unpredictable system we can use the framework to reason from a population to a sample it is deductive reasoning and we don't need observed data to solve probability Problems by comparison inferential statistics use either collected or observed data to infer General properties about the population it Reasons from the sample to the population and we need data to make an inference so the problem considered by probability Theory and inferential statistics are kind of the inverse of each other now that you know the difference between probability Theory and inferential statistics if they are kind of like the inverse of each other why do we we often talk about them together next let's look at the relationships between probability Theory and inferential statistics the relationship between the two is that we apply probability theory in statistics to draw conclusions from data probability Theory provides a mathematical foundation for statistics and statistics in application of probability Theory to fix model to observe data so knowing the fundamentals of probability Theory can help you understand how to conduct statistical inference so probability Theory and statistics are closely related in fact there are two schools of inferential statistics basing inference and frequeny inference the difference between them is based on their interpretations of what the probability means the basing approach interpret the probability as a degree of belief in an event it is specifies that there's some prior probability the prior probability maybe based on prior knowledge about the event such as personal beliefs or assumptions about the event or the results of previous experiments it uses base theorem to compute and update probabilities after obtaining observed data any certain you again is a condition on having made the correct guess for the prior by comparison with frequencies approach probability has measures in the context of repeating a measurement as we measure something in a large number of times the frequency of a given value indicates the probability of measuring that value it views probability as a limit of the relative frequency of an event after many trials common use techniques such as hypothesis testing confidence interval and P values are all frequencies techniques so how to interpret probability is essential to choose what technique to use to conduct inferential statistics I hope this video give you guys some clarity on the difference between descriptive and inferential statistics as well as the difference between probability and statistics basically probability Reasons from the population to the sample it's deductive reasoning while statistics reason from the sample to the population they're also closely related and the probability theory is used to conduct statistics as always guys I appreciate you for watching this video If you like this video make sure to subscribe to my channel to get more content like this I talk about data science interviews and the learning data science in general the tips and strategies I share will be helpful for you especially if you are someone who is actively looking for jobs in the tech industry feel free to drop me a comment below if you have any questions or feedback stay tuned I will see you soon


Probability Distributions Made Easy: Top 3 to Know for Data Science Interviews
so as a data scientist there are lots of probability distributions you need to know but what are the most commonly used ones hey guys it's emma welcome back to my channel in this video i want to share with you the top three distributions in data science interviews i think people have different interview experience and their selections might be different from mine so i'm gonna talk about the three probability distributions that i have seen the most often in data science interviews and they are the normal distribution binomial distribution and the geometric distribution we will not only talk about these distributions but also show you some real life applications of these distributions we will cover questions such as why normal distribution is so commonly used what does success mean in a binomial distribution and how to use geometric distribution to obtain customer lifetime i hope this video serves as a refresher for you when you review your probability knowledge even better you can learn a few new things from this video without further ado let's get started with the normal distribution which is by far the most common distribution that i have seen in data science interviews the normal distribution or what is sometimes referred to as a gaussian distribution is a very common distribution for continuous data giving its resemblance to a bell the distribution is also known as a bale curve the main reason for the popularity of this distribution is the central limit theorem which states that the sampling distribution of the means follows a normal distribution no matter what the underlying distribution of the population is yes that's right our population can be as strange as possible but if we repeatedly draw samples from it the distribution of the sample means will be normally distributed like a bell curve the real sum is to draw at least 30 samples but if the population is very skewed we need a much larger sample size for the central limit serum to be accurate note that the normal distribution is not just a one curve it's a collection of curves each of which is determined by its mean and standard deviation the mean is the highest point in the normal distribution and the standard deviation measures the amount of variability or dispersion of a set of values this allows us to select the most appropriate distribution from the collection of normally distributed curves now let's consider an example let's say we want to estimate the average time spent per user per day on website so we randomly select 10 users and calculate the average time they spend per day on our website we iterate this process say a thousand times and plot the averages we obtain each time so if we look at it on a graph the x-axis will be the average time spent per day among all users and the y-axis is the frequency what is the distribution of the average time spent this is a normal distribution right this is because of central limit theorem which tells us that we do not need to know the distribution of the population if we draw repeated samples and take the average of them the distribution of the sample means will soon resemble a normal distribution so how about if instead of the average time spent per user per day we want to look at the total time spent by all users per day that is we sum up all the time spent across all users for a day so if we plot it on a graph the x-axis is the total time span per day across all users and the y-axis is a frequency is it a different distribution well as it turns out this also follows a normal distribution because the central limit theorem for sums tells us that if we keep drawing sufficiently large samples and take their sums the sums also follow a normal distribution because of the central limit theorem the normal distribution is widely used to model sampling distributions because again even when the underlying population does not follow a normal distribution the sample statistics do as that being said the underlying population really follows a normal distribution in reality in fact most raw data you see in reality folks a distribution that is more accurately described as a long-tailed it has a long tail on either the left side or the right side for example the distribution of time spent on social media is a long-tailed distribution according to a research from facebook very few people actually spend more than three hours on the site per day with a vast majority spending about one number now let's move forward to another common use distribution the binomial distribution it is for discrete data like the normal distribution the binomial distribution is a collection or family of curves the shape of their probability mass function is determined by two variables the probability of success p and the total number of trials in success means obtaining the outcome of interest and it can be defined however you choose to do as long as the event has a binary outcome and success refers to one of the outcomes a simple example of a binomial distribution would be the number of hats among a specific number of tosses of a coin now let's consider a more practical example an advertisement has a click-through rate that is calculated by the total number of clicks over the total number of impressions what is the distribution of the total number of clicks that is a simple question right it follows a binomial distribution because it has a binary outcome and we are counting the number of successes how about the clix rate what is the distribution of clicks rate it's also a binomial distribution but it's a normalized binomial distribution or a bernoulli distribution because it's not the total number of successes it's the success rate similar to clicking and not clicking if we have other binary outcomes such as purchasing or not purchasing a product or subscribing or not subscribing to a particular service we can use a binomial distribution to measure the total number of successes with success defined as occurrence of the event of interest we can consider a purchase as a success or a cancellation event as a success so the binary distribution is a very common use in reality to measure the total number event of interest the last distribution we are going to talk about today is geometric distribution before we talk about geometric distribution it's helpful to know the negative binomial distribution from its name you might guess it is related to the binomial distribution and that's right it is also a discrete distribution this distribution represents the number of successes before a specific number of failures occur again success and failure are arbitrary terms you can define them however you choose as long as they are binary outcomes a simple example of a negative binomial distribution is the number of heads that come up before the tens tail comes up the geometric distribution is a spatial case of the negative binomial distribution and it represents the distribution of the number of trials needed to get the first success sometimes it also refers to the number of trials needed before the first success the difference between the two is one includes the first success while the other excludes the first success both are geometric distributions and the choice of which to use depends on convenience and the context the one parameter for the geometric distribution is p the probability of success as you can see from the graph on the slide the two definitions share the same shape in terms of their probability mass function but the first one the one that includes the first success is shifted over one on the x-axis because it includes the first success the geometric distribution is commonly used in reality and one common use case is to calculate customer lifetime giving customer churn rate let's see how we can obtain it suppose the monthly churn rate is c it refers to the proportion of existing customers who churn in a month and that is constant over time what we want to find is on average how many months it takes for a customer to churn or the average customer lifetime in this case we define success as churning and our success rate is a monthly term rate c note that we will be using the first definition of geometric distribution to calculate the number of months up to and including the first month when a user actually turns what we want for the average customer lifetime is actually the expectation of a geometric distribution and that is 1 over c it means on average it takes 1 over 3 months for a customer to churn in other words the average customer lifetime is 1 over c so if c is 10 the customer lifetime is 10 month if c is 20 the customer lifetime is 5 months that's how we can use the geometric distribution to find customer lifetime assuming the churn rate is constant alright guys we have discussed three distributions the normal distribution the binomial distribution and the geometric distribution these three distributions are commonly in these science interviews and they are frequently used in practice as well i hope by now you have a better understanding of those distributions if you like this video make sure to subscribe to this channel to get updates about future content i will see you in the next video

Acing the Statistics Interview for Data Science Jobs
Hai Hai guys welcomeback to my channel three wakareno the interview data Heart soundtrack The sesi interview interview interview uh uh so in this video is worth unit-unit to be prepared to Ensure ninokuni2 kau bersih hypothesis testing and represent populasi khusus Yuchai biasa boat trouble type-c Conditioner probability and the problem problem inklusif action and combination for you and you need to be awesome and continued Under the Shining like a lot from the most interesting You're always in Unity Hai Endang prometric tachs and more detailed University interview with free question present at the end of the year out your so Proud of You Baby I'm not know what what you know and understand each school how to lose weight resep Rich kuintal in cucu Styles explaining cupcake Origins and explaining nonteknik before with time in two different approach for young lex much from its you English yes oke no problem of hypothesis is one of Us and show out If you would you like some stupid gue klikhost.com musuhin Club Ini unsur studed konsep Resort beach holiday dress up your answer is not a word you said you can hear the first one is the definition of you like about you about me KYT full tower aku sohail about of the tank nakhchivan you mean What do you mean What do you talking about how the rich chigga world-wide importance to side step out how with our free report Hai bebek Khair nice place the seimbang ini ichfa if you know how to answer this movie is no way out bitch students Who talk with an open heart you and your heart must not know bout you so much fun movieclips movie much for the example of king to come here and you my all out of love you so much for what do you know you have not give you more with your Gift of the night FW YouTube notulen obat sirichai also implemented the series free sample khai check out this video for more in-depth Look What You Like about Shout out to understand what you can do it is crucial for ease of course the beach Holiday I went to the days of Chairul speaking English subject of Knowles youtuber for interviews advances properti hypothesis testing and recreation you mean out without even if you like about how to make up with Authors yogs Khair under water beach friends and watch you fade away hope you have What you need at least for watching see you in the next video

Mastering Hypothesis Testing for Data Science Interviews: Binomial, Z-test, and T-test
hey guys welcome back to my channel in this video i want to dive into hypothesis testing problems in data science interviews the hypothesis test we are going to cover include binomial tests z-test and t-test by the end of this video you will be able to understand what are different kinds of interview questions on hypothesis testing what are the differences between those hypothesis tests and when to use each of them the knowledge you gain from this video will help you solve those interview problems easily and accurately let's get started there are three kinds of hypothesis testing questions in interviews the first kind of questions are very basic this kind of questions may appear in any part of a technical phone screen or an on-site interview to evaluate if a candidate has fundamental knowledge on hypothesis testing here are some examples what are the differences between a z-test and a t-test want to use a z-test versus a t-test giving the data how do you calculate the t-statistic or z-statistic the second type of hypothesis testing questions typically ask together with eb testing questions a typical example would be the interviewer gives you a test result and asks you to calculate if the result is significant and how you would make launch decisions based on the result this type of question requires you to understand both epi testing and hypothesis testing as well as how to use hypothesis testing in practice if you are not familiar with epi testing or you need a refresher on the topic i have a great video covering the common ask every testing questions and answers the last type of questions is using sql query to calculate metrics and test statistics for example giving a table containing user behavior data write a sql query to get the average number of likes in control and treatment groups then obtain the test statistic and tell if it's significant or not the query itself is not difficult to write but you will need to have a clear understanding of the formula to calculate the test statistic to solve the problem now you understand the three types of interview questions on hypothesis testing now let's dive into when to use each test and what are the differences between them when i first learned those concepts i feel it was pretty confusing and there seemed to be lots of things to remember i then realized it's easier to understand and explain those concepts if i just make a flow chart so here it is this chart summarizes we want to use a particular test first of all we want to know the metric we want to evaluate if the metric we are interested in follows a bernoulli distribution we need to further check the sample size for those of you who don't know the bernoulli distribution is a distribution with a random variable taking the value 1 with the probability p and the value 0 with the probability 1 minus p a practical example is click through probability the proportion of users who click a button on web page is p and those who don't is 1 minus p similarly clicks rate and the conversion rate can also be considered following a newly distribution another way to understand it is to see if what we want to test is a proportion or not for example percentage of users or pages if we want to compare proportions of two groups such as if there's a change in click-through probability if we change the color of a button we would go this route if the metric does not follow bernoulli distribution for example we want to find out how different two sample means are different from each other then the first thing we want to check is the size of the sample the magic number here is usually 30 30 or above is considered as a large sample and below it is considered a small sample if it's a small sample we need to make sure the probability is normally distributed in order to use a z test or a t-test you may wonder do we care if the proportion is normally distributed for large sample we don't because the central limit theorem tells us that the sample mean follows a normal distribution we don't need to worry about whether or not the population is normally distributed for a large sample if we have a large sample or a small sample from a normally distributed population we need to also consider if the population variance is known to us or not if it is we could use a z-test otherwise we'd choose a t-test that's why in reality the z-test is not used as commonly as t-test because it requires population variance to be known and in lots of cases we don't now it's clear to you one to select a particular test i want to highlight two things to provide more clarity the first thing is the difference between student t distribution and normal distribution we use the t test when the test statistic follows a student t distribution on the null hypothesis how would it be different from a z distribution here's a diagram showing the comparison of t-distribution and z-distribution or standard normal distribution we observe that a t-distribution is more prone to error it's more spread out and thicker in tails than a normal distribution this makes sense because we do not know the standard deviation of the population this also means that the t distribution produces a wider interval than the corresponding standard normal based confidence interval because if we don't know the standard deviation and we estimate it we are less certain about our estimate note that the shapes are different for in lesson 30 and and close to 30. this is related to another concept degree of freedom it is the number of pieces of information that can be freely varied without violating any giving restrictions that is number of independent pieces of information available to estimate another piece of information for example if we have n data points there will be n minus 1 independent values after we know the mean we can see that as n increases the t distribution better approximates normal distribution actually for large sample sizes the t-test gives almost the same p-values and confidence intervals as a z-test the second thing i want to clarify is why we don't use t-tests for proportions we mentioned a z-test or binomial test to compare proportions and we didn't mention a t-test can do so why the reason is that the test statistic doesn't have a t-distribution it does approximately have a z-distribution let me explain in a typical t-test the t-statistics follow the form d over s where d is the difference between means and s is the estimated standard error of d because of central limit theorem when sample sizes are sufficiently large a statistic like d which is the difference between means is very asymptotically normally distributed and the standardized version of d d over sigma of d will be a symmetrically standard normal there is another theorem called slavsky's theorem states that as long as the denominator s converges in probability to that unknown standard error sigma d then d over s should converge to a standard normal distribution the typical one sample and two symbol proportions tests are in the form so we have some justifications for treating them as asymptotically normal but we don't have any justification to consider them as following a t-distribution in practice as long as mp and n1 minus p are not too small specifically when both are larger than 10 the symmetric normality of the proportions test comes in rapidly so theoretically we don't use t-tests to test the proportions and there's no good argument that t-distribution should be better than the z-distribution as an approximation to the distribution of the test statistic but many people do use t-tests for testing proportions academically speaking they are wrong but in practice the approximation obtained by using a t-test on bernoulli data seems to be very good also as we have mentioned earlier as the sample becomes larger using t-test generates almost the identical constant intervals and the p-values compare with a z-test so to summarize using z-tests for testing proportions is theoretically correct while using a t-test is wrong but the results from a z-test and t-test do not have a significant difference especially when the sample is large so it's okay to use a t-test for proportions in reality this is the part one of cracking hypothesis testing problems in data science interviews in part two of the video i will dive into some practical examples and show you how to solve them step by step stay tuned if you're interested in learning how to apply hypothesis test in reality as always guys i appreciate you for taking the time to watch this video let me know if you have any questions i will see you in the next video

Data Science Interview Mastery: How to Solve Sampling and Simulation Problems with Ease!
hey guys welcome back to my channel in today's video i want to dive into a few common ask probability questions in data science interviews i call those type of questions common problem because they involve with fear con and the bias con and use one of them to simulate the other if you are interested in learning what those problems are and how to solve them then keep watching a typical version of the problem is you are giving a bicycle that the probability of getting a head is a p p is unknown but we know that it's larger than zero and less than one sometimes the question made it clear that it's not 0.5 how do you design a strategy to simulate a fear coin using that bias coin a fear coin means the probability of getting a head is the same as that of getting a tail and both of them are 50 percent it's pretty clear that we could not use one toss to simulate a fear coin given that p is unknown so we want to try multiple tosses and find the combination of outcomes that have equal probability we can start with two tosses and see what we can get here we use edge stand for head and t stand for tail so for two tosses we can get four different outcomes hh ht th and tt then we can get the probability of those combinations it's obvious that ht and th have the same probability which is p multiplied by 1 minus p and that is what we want to leverage to simulate a fair coin specifically we toss a buys coin twice in a row if it's hh or tt we discard the result and toss twice again when we see ht we could use it to represent a head of a fear coin and when we see th we use it to represent a tail of that fear con in this way we can guarantee the probability of getting a head is the same as the probability of getting a tail now we have the simplest and the most straightforward solution to the problem let's take a few seconds to think about if there's any potential issues with this approach what happens if p is an extreme value say 0.9 we can get the probabilities of the four combinations because we discard h h and t t we only keep h t and th it means that we will be throwing away the results more than eighty percent of the time and it may take many tosses for us to get a desired outcome in other words the efficiency of this approach is low so how could we improve the efficiency of it can we find a way to use the results of two heads or two tails in fact we can we could keep flipping the bicep cone and combine the outcome together to simulate the head or tail of a fair coin let me elaborate this when we get two consecutive heads or two consecutive tails we can toss two more times and here are all the eight possible outcomes let's try to organize them into pairs and the elements of each pair have the same probability and here are three pairs with this finding we can sum the probabilities of the first element of all pairs and that would be the same as the sum of the second elements of all pairs what does this tell us well it means that we can assign the first group as head of a fair coin and the second group as a tail of that fair because they have the same chance of occurrence that's how we can leverage two consecutive heads and two consecutive tails rather than throwing them away using this method we end up only throwing away four consecutive heads and four consecutive tails all the other outcomes can be utilized intuitively the overall sampling efficiency increases a more scientific way to get the sampling efficiency is to compute the expected number of tosses to get a head or a tail essentially on average how many tosses do we need in order to simulate a head or a tail of a fair coin the less number of tosses the higher the efficiency i will leave this part to you to figure out the exact sampling efficiency and how much we have improved the efficiency by keeping two consecutive heads and tails let's now summarize this strategy when simulating a fear coin with the bisocon we can start with two tosses if the outcome is ht or th we can simply return the result of a head or a tail if the outcome is h tt we need to flip another two times and return head or tail if the outcome is one of these combinations we only discard the outcome if it's a four consecutive heads or four consecutive tails moving on we have a more advanced version of the problem instead of asking you to generate a fear coin from a bias coin the question asks you to generate a range of n numbers with equal probability 1 over n from a biased con for example generating numbers 0 1 to 3 each has a 25 probability as you may have noticed if we use head and tail to represent 0 and 1 respectively we are able to simulate two numbers with equal probability but how do we go beyond two numbers feel free to pause the video and think for a second if you don't have any ideas think about if you can simplify this problem a little bit there are interviews when you ask a hard question especially a technical question you could always try to solve a simpler version first or make some assumptions this can be helpful to break down a seemingly difficult problem into simpler pieces okay so back to the question what if the coin is fair rather than biased could you use it to simulate four numbers for sure you can if it's a fear call you could get four distinct outcomes from two tosses hhtt hd and th and each outcome has exactly the same probability i.e 25 now we can break this problem into two smaller problems one to get a fair coin from a bicycle and the other one is to figure out how many tosses do we need in order to get n different numbers well the good thing is that we've already solved the first problem just two minutes before another interview tip i have for you is try to leverage any problem that you have solved earlier in order to solve the current problem now we only need to know how many tosses are needed to get n different numbers again let's start with a simple example if we toss a fear coin twice we have four outcomes each occurs 25 of the time if we toss a fear coin three times we have eight outcomes which is exactly two to the power of three if we toss a fair coin m times we have two to the power of m results what we want is to simulate n different numbers how many tosses do we need well i guess the answer is clear it should be log base 2 of n now you have the answer to the problem it's not as difficult as it appears right one caveat is that the log base 2 of n may not be a whole number so we need to run it up in cases like this for example if n is 5 we need 3 tosses in a row to get 8 distinct outcomes and we have to discard three outcomes now let's put the solution together first of all we need to use the bias coin to simulate a fear coin we toss two times in a row and we can get hh ht tt and th let's say we choose ht to be the head of the fair coin and th to be the tail of that vericon and both cases have the same probability p multiplied by 1 minus p then we want to simulate a range of n different numbers with this fair coin if n is three we want to toss two times because rounding log base two of three to an integer is two for faircoin we can get four outcomes and we only need three of them we can use hh to represent zero ht to represent one and tt2 represent two we discard the outcome th we can translate it back to the unfair con and hh would be htht of that buys the coin the last problem we'll go through today is to submit a buy coin using a fair coin specifically you are giving a fair coin and you are asked to submit a bias coin with the probability of getting a head 1 over n does it sound like a different problem it's actually very similar to part of the previous problem we just solved namely how to use a fair coin to generate n different numbers and each has probability 1 over n a simple example is when n is 4 meaning the bias coin has a 25 chance of getting a hat we need to toss a fair coin twice and we have four outcomes and each has 25 percent probability we can then choose any one of them to represent the head of the bias the coin and use the other ones to represent the tail this will give us a biased coin with a 25 percent probability of getting a head so how many tosses do we need in general actually we have covered this in the previous question as well we will need log base 2 of n number of tosses and if the number is not an integer we'll have to run it up so if n is 5 we need to toss three times and we'll have to discard three among eight outcomes to get the desired probability you may wonder would abandoning the result impact the probability it will not because we just simply consider it not happening by ignoring the result it's a typical technique used in computational statistics and it's called rejection sampling i hope those questions and answers are helpful if you like this video please give it a thumbs up and subscribe to my channel i make at least one video per week to help you with your interview preparation to land your dream job as always guys i appreciate you for taking the time to watch this video let me know if you have any questions or feedback i will see you soon

Top 5 Statistics Concepts in Data Science Interviews: P-value, Confidence Interval, Power, Errors
hi guys welcome back to my channel in this video i want to dive into five statistical concepts that are so common asked in data science interviews and they are power of a statistical test type 1 error type 2 error confidence interval and p-value sometimes the interviewer will ask you to explain these concepts to a non-technical audience and that requires you to not only have a good understanding of all these terms but also deliver them in a very intuitive way if you ever find it difficult to answer this kind of question this video is definitely for you by the end of this video you will learn how to showcase your knowledge on these five concepts to both technical and non-technical audiences the methods i'm gonna teach you will not only apply to these five concepts but to other concepts as well so if you're ready to dive in with me then keep watching to start off i'd like to share with you a few steps to follow when explaining technical terms to a technical person such as a data scientist you may think this is pretty trivial if the audience is technical then the person is expected to understand everything you say right but the fact is if your answer is disorganized or obscure it's very hard for people even technical people to follow so here are the steps i recommend we can start with talking about where or when a terminology is used then we provide a definition of that terminology even we are explaining it to a technical person this should be easy to understand it should not be obscure like what you see in wikipedia afterwards we can explain the meaning of changes in values if the concept can be represented by numbers basically what does it mean with a larger or smaller value the final step is optional we could talk about the application of the term in practice such as why the concept is widely used why it is important in data science sometimes you are asked to explain technical concepts in layman's terms or to a non-technical audience it requires you to explain things in a very intuitive and understandable way in such cases using examples is a good way to explain a terminology i will show you later what examples to provide for each concept also it's important to avoid introducing more technical terms for example when explaining power of a test you don't want to introduce hypothesis testing null hypothesis or alternative hypothesis this will confuse the audience even more now you've learned the theory let's now put it into practice for the rest of the video i will go through 5 statistical concepts including power type 1 arrow type 2 error confidence interval and p-value to show you how to explain them to both technical and non-technical audiences the first concept is power first let's explain it to a technical person i will follow the steps i shared earlier to give you the answer statistical power is used in a binary hypothesis test it is the probability that a test correctly rejects the null hypothesis when the alternative hypothesis is true to put it in another way statistical power is a likelihood that a test will detect an effect when effect is present the higher the statistical power the better the test is it is commonly used in experiment design to calculate the minimum sample size required so that one can reasonably detect an effect the next terminology is type 1 error type 1 error also known as false positive it is used to categorize errors in a binary hypothesis test it occurs when we mistakenly reject a true null hypothesis it means that we conclude our findings are significant when in fact they have occurred by chance the larger the value the less reliable a test is meaning that we want to minimize the type 1 error of a test type 1 error is commonly used in eb testing to show that we observe differences between two groups but in reality there's no difference the third one is type 2 error type 2 error also known as force negative it is used to categorize errors in binary hypothesis test type 2 error refers to force negative it occurs when we fail to reject a null hypothesis which is in fact false basically we conclude there is not a significant effect when actually there really is the larger the value the less reliable of the test results meaning we want to minimize the type 2 error of the test it is commonly used in eb testing to show that we don't observe differences between two groups but in reality there is a difference we have just explained three concepts in a technical way now let's see how we can explain them to a non-technical audience for example if a person wants to test if he is infected by chrono drivers or not and there are three scenarios we care about the first scenario is that the person is indeed infected by the virus and the test result shows us the same that is the power of a test basically it is a chance that the test result tells us a person is infected when he truly is the second scenario is that the person is not infected but the test result shows here is that is a type 1 error this can be really bad because the person may take some medical treatment that is completely unnecessary the third scenario is that a person is indeed infected but the test result tells us he's not this is a type 2 error it is also very bad because the person may miss the best timing to get treatment that he really needs the next concept is confidence interval let's explain it to a technical person first again i will follow the steps that i mentioned earlier to explain it confidence interval is used when we want to get an idea of whole variable assemble results might be the confidence interval is for the true value but we never know what the true value is and the purpose of having samples and observations is to estimate the true value the conscious interval is a range of numbers it tells us how often it would contain the true value and the probability of it covering the true value is a confidence level a common user value is 95 percent the wider the interval the more uncertain we are about the sample result so the more confidence we want to be and less data we have the wider we make the confidence interval to be enough confident of capturing the true value in short the higher the level of confidence the wider the interval and the less the sample the wider the interval okay that's how we can explain complex interval during an interview i want to highlight a common misconception it considers that the confidence interval answers this question what is the probability that the true value lies within a certain threshold well this is not what context interval is answering because the misconception assumes the true value is a variable and the constant interval is deterministic the correct understanding is just the opposite the true value is determined by nature but is unknown to us it will not change at all the things that can change are the boundaries of the complex intervals which are estimated from the samples and the level of companies we set basically for a specific conflict interval the true value is either a hundred percent within it or not the 95 percent refers to after the 95 percent confidence intervals computed from many samples how likely it would cover the true value now let's try to explain confidence interval to a non-technical person confidence interval measures the level of uncertainty when we try to estimate a value for example we want to know the average height of men in the u.s we can randomly select certain men and measure their heights and let's say we can get a 95 confidence interval and let's say it's 168 to 185 centimeters the constant interval we have means that it is likely to cover the true average height of all men in the u.s but how likely if we repeat the process over and over again we expect the conflict interval we construct to cover the true value 95 of the time the next terminology is p-value similarly let's explain it to a technique audience first p-value is commonly used in hypothesis testing to connect the dots between observation and conclusion it is a conditional probability measures the probability of getting testing results at least as extreme as observed results giving that the null hypothesis is true a low p-value indicates less support for the null hypothesis in practice we often choose 0.05 as a cut of value p-value less than 0.05 denotes strong evidence against the null hypothesis which means the null hypothesis can be rejected and the value larger than .05 denotes weak evidence against the null hypothesis which means the null hypothesis cannot be rejected it is commonly used in every testing when we have a treatment and a control group and we want to test whether a metric is different in those two groups suppose we have done the experiment and obtained the measurements from the two groups the smaller the p-value the more we are convinced there is a difference between the two i have just shared with you how to describe p-value during interview i want to point out one common mistake people make when interpreting p-value very often we have observations and we would like to prove there is a difference between two groups the mistake people make is to define p-value as given the observation the probability of there is at least such a difference between the two groups in other words the belief p-value captures the probability that the null hypothesis is true giving the data observed it may sound reasonable at first but it's almost the opposite of the true meaning of p-value which is that given the null hypothesis is true the probability of obtaining differences at least as large as the data we observed now you understand why the misconception of p-value is wrong let's try to explain p-value in layman's terms we could reuse example when we explain confidence interval and that is we want to get the average height of men in the u.s we randomly select 30 people and get the measurement of their heights but now the question is we want to know if the average value is the same as a fixed value say 175 centimeters the p-value connects the dots between what data we observe and what conclusion we could draw it tells us that assume the true value ie the average height is 175 centimeters how likely we observe the data a very small p-value let's say less than 0.05 means that assume the true average height is 175 the chance that we observe the data is very low or the data we observe is very extreme so we believe the true value should now be 175 centimeters so that's how we can explain p-value to a non-technical person note that we did not introduce any terminology and we use a very simple example to explain it during interviews it can be hard to come up with good examples quickly so i recommend you to prepare some examples for some of the commonly asked concepts if i'm interested in learning more about how to answer real questions in data science interviews stay tuned for more videos to come as always i appreciate you for watching this video let me know if you have any questions or feedback i will see you in the next video

A/B Testing Analysis Made Easy: How to Use Hypothesis Testing for Data Science Interviews!
hi guys welcome back to my channel in today's video we will dive into how to use hypothesis testing to solve real problems specifically how to use hypothesis to analyze results of a b testing i will give you two examples and show you how to solve them step by step this video is part two of cracking hypothesis testing problems in data science interviews in part one of the video we went through a few commonly used hypothesis tests want to use them and what are the differences between them if you need a refresher feel free to check out the video okay let's get started the first question is we run an experiment where we test the color of a button the metric we're looking at is the click-through probability it is calculated as a number of users who click the button over the total number of users there are a thousand users in both control and treatment groups here are the results of the experiment the control group has a 1.1 percent click-through probability while the treatment group has 2.3 percent click-through probability can we conclude there's a significant difference between these two groups would you recommend launching the experiment the practical significance boundary is 0.01 and we choose an alpha of 0.05 let's start with outlining the steps to take to analyze the results first of all we want to decide which hypothesis test to use the diagram we went through in the previous video could serve as a reference next we should be clear on what the null hypothesis of the test is then we could evaluate if the test result is statistically significant by comparing the test statistic with the critical value we also need to check if the result is practically significant by comparing the confidence interval of the estimation with the practical significance boundary finally we can make decisions based on the result now let's go back to the question and analyze the experiment result each user is a click or not clicks a button so it's a benue population in this case n times p hat is 11 in the control group and 23 in the treatment group both can be considered as large samples so we choose a z-test it means that the test statistic ts follows a z-distribution or a standard normal distribution now we'll measure the users who click in each group which we will call x control and x treatment as well as the total number of users in each group which we will call uncontrol and end treatment the estimated probability p of the control group p control hat in this example is 11 over a thousand which is 1.1 percent similarly we can get p treatment head is 2.1 percent remember we want to estimate the difference between p control and p treatment and i'll call this difference d under the null hypothesis p control and p treatment is the same in other words d the true difference is equal to zero and we would expect our estimation d-hat to be normally distributed with a mean of zero we don't know its standard deviation yet and we need to estimate it the test statistic is shown here now i estimate d-head by subtracting the p-control head from the p-treatment head and this comes out to 0.01 to calculate the stand error of d-hat since we have two samples we need to choose a stand error that can give us a good comparison of both we could calculate what is called pulled stand error to obtain the puts and error the first thing we'll calculate is the pulled probability of a click p hat and i'm using a hat here because this is an estimated probability and the probability is a total probability of a click across two groups that is the total number of users who click the button divided by the total number of users then we can calculate the pulled stand error which is given by this formula so the posts and error for our experiment comes out to .00578 now we can get the value of the test statistic which is 2.076 next we can compare it with the critical z-score values of the alpha equals to 0.05 or 95 percent confidence level which is 1.96 if the test statistic is greater than 1.96 or less than the negative of this cutoff then we can reject the null hypothesis and conclude that the difference represents a statistically significant difference in this example it is larger than 1.96 so the test is statistically significant we also want to know if the result is practically significant to help us make the decision to do it we need to calculate the confidence interval of the estimation we already know the center of the condition interval which is 0.012 let's now calculate the width of the context interval which is also called the margin of error for the normal distribution the margin of error m is equal to the z-score of the confidence level times the standard error which comes to 0.0113 so the confidence interval is from .0007 to 0.0233 we can draw a diagram to compare the confidence interval and the practical significance boundary here i've drawn the practical significance boundary as a two dashed lines and zero as this solid line a point estimate which is shown as a solid red circle is greater than the practical significance boundary but the left end of the context interval is less than the practical significance boundary this is a tricky case it means that our best guess the point estimate there is a practical significant change but it's also possible the change is not practically significant so we are not confident the true change is large enough to be worth launching so i would not recommend launching the feature just to mention we could also use the content interval to check statistical significance we could check if it overlaps with zero if it does it's not statistically significant the result is the same as comparing test statistic with a critical value we have just talked about using the z-test to compare two bernoulli populations and how to determine if the difference is statistically and practically significant let's now move forward to the next example we run an experiment to test if adding a new feature will change the average number of posts created per user both control and treatment groups have 30 users the first array represents the number of posts created by each user in the control group and the second array has a number of posts created by each user in the treatment group the control group has a mean 1.4 and the treatment group has a mean two assume variances are similar in the two groups what conclusion can you draw from this experiment shall we launch the feature to all users the practical significance boundary is 0.05 and we choose an alpha of 0.05 let's start analyzing clearly we're not dealing with a bernoulli population and the viruses are unknown based on the diagram we explain in part one we will choose a two-sample t-test to compare the differences between control and treatment we are told that the population variances in the two groups are similar so we could calculate the so-called pulled variance if the vertices in the two groups are different we will need to obtain the unput unequal variance we'll cover it shortly remember our goal here is to measure the difference d between the average number of posts in control mu c and treatment mu t i call the estimate of the d-hat for difference under the null hypothesis the true difference is equal to zero the test statistic of a two-sample t-test with pooled variance is given by this formula as poor as a pooled stand error it can be calculated using a formula like this here we introduce two more parameters sum of square ss and degree of freedom df i will not go through in detail how to get the value of the put stand error but all the numbers are shown here feel free to pause the video to derive it and verify your calculation now we have the value of the pulled stand error we can compare the value of the tested statistic and the critical t-score value of a 95 percent complex level for degree of freedom 58 which is 2.002 the test statistic is larger than 2.002 it means the result is statistically significant next we would construct the content interval of d similar to the previous example we could draw a diagram to compare the content interval with a practical significance boundary and zero in this case both ends of the confidence interval were greater than the practical significance boundary so it's highly possible that the difference of the two means in fact changed by more than the practical significance level so we would recommend launching the experiment we have just covered using t-test to compare two samples with similar variances and sample sizes let's now look into how to deal with the case that two samples have very different viruses or sample sizes watches t-test is used to deal with this scenario it is an adaptation of student's t-test it's specific to the case that when the two standard deviations are not similar specifically when one is more than twice of the other then the unput send error is used we'll calculate the outputs and error instead of the puts and error and it follows this formula sc and st are the sample standard deviation of the control group and the treatment group respectively the convex interval of the estimation can then be obtained using this formula if we compare this scenario with the previous example where two samples have similar variances two things are different one is the standard error and the other one is the degree of freedom the rest are the same the form of the degree of freedom is a bit complicated and you do need to remember it you only need to know that vertices t-test is used to deal with such cases and you could always look up the formula for the calculation i have just walked you through two examples using hypothesis testing in reality hopefully they are helpful to deepen your understanding of the subject as always guys i appreciate you for taking the time to watch this video let me know if you have any questions or feedback i will see you soon

Data Science Onsite Interview: How to Deliver a Killer Presentation and Land the Job - YouTube
hey guys welcome back to my channel in this video i will be discussing something few people enjoy giving presentations specifically giving presentations in data science interviews most people do not really like public speaking however it is an essential skill in almost any job data science is no exception to this as a data scientist you will have to present your ideas and findings to stakeholders which is why many companies require candidates to give presentations when interviewing for a data scientist position typically it ranges from 45 minutes to 1 hour with the majority of the time on presentation and the last 10 to 15 minutes on q a presentations can be nerve wracking but they are a vital part of proving that you are a capable data scientist data scientists have to be sailors that can drive conversations to convince stakeholders to buy into their suggestions and ideas the presentation part of the interview is where you demonstrate your ability to be a driver and not just a supporter it is crucial if you want to land a position especially a senior position but how do you make the most of it in this video i will go over five tips for giving presentations in interviews if you want to deliver a key representation you need to know what information to include what details to highlight and how to carry yourself to make the best impression with the interviewer this video will be covering all that and a bit more so let's get started my first tip is to focus on your impact projects are usually a team effort involving more than just data scientists it takes a lot of collaboration and teamwork to successfully complete a project despite this reality this is not what interviewers tend to care about when you present a project they are interested in you not the rest of the team therefore you want to focus on your role and impact to convey your impact clearly you should describe it in measurable terms use metrics and numbers to explain your achievements on the project the best metrics are those that show the effect you had on the business this includes things like increasing revenue customer acquisition customer retention etc if you know what impact your work has on a business metric this is an ideal way to illustrate your impact in your presentation for example you might say that you can increase any revenue by 10 percent or that on six months project you improve customer retention by 20 specific metrics with specific numbers are the best way to show your impact however sometimes connecting your work to a direct impact on the business is very difficult it may be that the scope of the project was not large enough or that the project is only indirectly affecting a business metric if this is the case you still want to describe your impact using metrics that the interviewer can easily understand you can talk about things like improving team efficiency and productivity by building data pipelines or about how a data modeling or analytics tool you built or methodology you developed was adopted by multiple teams across the company although these findings do not say exactly what impact you have on the business is it clear that you are positively impacting the overall coming improving team productivity is sure to lead to improved business metrics what you should avoid in describing your impact are details that do not clearly explain how you help the business for example simply saying that you improve model performance is not helpful for an interviewer it is unclear exactly how impactful that improvement is stick with metrics that are clear to someone without a technical background and if you have them thesis metrics are the best this tip about focusing on your impact does not only apply to presentations when describing past projects in an interview the impact is also very important you can check out this video on how to describe a past project in a behavioral interview in which i provide a better alternative to the star method my next tip is to use your best stuff this tip might seem painfully obvious at first of course you want to present your best project however what exactly do we mean by best it can be tempting to present a project that you feel was the most successful but your most successful project is often also an easy project why they present your best stuff i'm more so referring to the best stuff about you rather than the most successful project talking about a project with challenges gives you far more opportunities to display your skill thoughtfulness and uniqueness as a candidate challenging projects also tend to be more interesting so it's much easier to capture the interviewer's attention how do you decide which project has changes you can talk about here are some examples of challenges that might make a project more interesting and effective to present they are both technical and non-technical challenges some technical challenges would be problems defining success metrics difficulty obtaining data a large data set that was hard to work with low quality data difficulty processing the data and issues with model training and deployment some non-technical challenges would be other people initially did not like your idea strict deadlines resource constraints and your team hit the block during the project these are just some examples of things that might have created obstacles or being an issue with the project talking about things like these challenges gives you a chance to show a wide range of capability and again keep your presentation far more engaging challenges are not the only thing that makes the project your best you also want a project where you were heavily involved pick a project where you feel comfortable highlighting your actions in the various steps of the process explain things like the project design and implementation feasibility to demonstrate that you understand your job and its implications this shows that you can drive projects because you understand how the entire thing works so when you pick a project to present choose one that captures you at your best this often means choosing to talk about a project that has some problems and obstacles this idea leads us to the third tip list the limitations of the project listing the limitations of a project typically requires a conscious effort on your part because most of us want to do the exact opposite when giving a presentation it's tempting to skip or skim over the issues and limitations that arose during a project however the limitations you face on a particular project are not something that you want to bypass in presentation specifically when discussing data quality and technical feasibility but why it may seem like talking about things that went wrong can hurt you the fact is that simply knowing how to use specific techniques such as a machine learning model is not enough anyone can learn to do that to impress your audiences you need to demonstrate a knowledge of the message advantages and limitations in your application show awareness about what was not ideal in your situation for example you may have faced a limitation with data perhaps there were problems with data collection and your team did not have enough data to do analysis or modeling to overcome this limitation you may have had to do some research to find an external data source or you have proposed and implemented a creative way to leverage existing data what this example shows is that listing the limitations gives you a chance to discuss how you improved or plan to improve therefore awareness of limitations indicates a capability for growth and it demonstrates your problem solving skills it shows that you can identify weaknesses in your plan and work to mitigate them the first three tips should help you decide which project to talk about but now that you have picked the project how do you prepare to present it everyone's master differs but there's one thing that i suggest everyone do and that is my first tip which is to think through the technical details of a project you likely won't have time to explain every technical aspect of a project but you want to ensure that you have a thorough understanding of everything you do choose to talk about you will often be asked follow-up questions in the q a session or during the presentation that will require a solid understanding of the technical details of the project for instance an audience might ask you things like does it make sense to convert a continuous variable to a categorical variable what if the distribution is long-tailed or with the tree-based model do you need to concern about overfeeding if it's a random forest model so make sure that you are comfortable enough with all the technical details of a product you plan to present so that you can answer follow-up questions we have talked about selecting a project to present and preparing to present our final tip looks at the one thing we haven't touched yet the actual presenting stage when preparing a presentation content certain matters and so far that's what our tips have focused on what type of project to select and what aspects of the project to focus on in your presentation however having fantastic content is not the only thing that makes a presentation successful the interviewer will also be evaluating the way you handle yourself when giving a presentation your behavior matters and is one of the things that will evaluate it during a presentation although you want to appear as someone you conceal their ideas and drive conversation you do not want to come across as arrogant or egotistic part of being a good presenter is demonstrating that you are a good listener no one wants to work with a co-worker who refuses to listen to others and consider different perspectives therefore when giving a presentation it is important to show an openness to questions and suggestions or even criticism listen to different opinions even those that disagree with your own and take the time to analyze them you can then either persuade your audience of your own view or admit your limitations in that matter many times it simply means admitting that someone has a good question and thinking of a solution do not dismiss others opinions and concerns because this implies that you will not be a good co-worker during presentation you should carry yourself in a way that shows confidence and capability while also demonstrating that you value different perspectives alright guys that's everything to recap my five tips for delivering a key representation are focus on your impact use your best stuff list the limitations think through the technical details and the behavior does matter i hope you found these tips helpful all of these tips are oriented around one major idea a presentation in an interview is less about presenting a project and more about presenting yourself as a data scientist when preparing and delivering your presentation your focus should be on demonstrating that you are a capable data scientist this means your presentation should go beyond simply explaining the bare facts of the project you want to demonstrate your impact your ability to problem solve your technical skills and the knowledge and the ability to behave professionally and drive the conversation thanks again for watching if you like this video make sure to subscribe to this channel to get updates about future content i will see you soon

AB Testing Case Study | Google and Spotify Data Scientists | Data Science Interview
hey everyone it's dan from datainterview.com it's google and data scientists um in this video what we're going to go through is basically an av experiment um here i have felicia with me she is a data scientist at spotify and we basically decided that you know we're gonna design um you know we're gonna we have this sort of av experiment based on amazon's case study and we flesh document now because we feel that it's really important to kind of you know talk about how do you go about approaching an ada experiment and so we picked out a scenario case based on amazon um as a way to kind of show to um candidates out there in terms of how do you approach an ap experiment so i'd like to just start off by giving an introduction to felicia do you want to perhaps talk about your you know experience as a data scientist uh what are some cool projects you worked on and you know um just maybe some like kind of background information about yourself sure um so hi my name is felicia i was born and raised in new york city and living there currently and working at spotify i've been at spotify for about two and a half years now and prior to that i was in management consulting at accenture for three years right now at spotify i'm on the marketplace economics team so the marketplace mission focuses on the artist rather than solely the listener and the economics team is really cool because it kind of sits cross-functionally across our different marketplace products and it uses causal analysis a b testing machine learning kpi development all the kind of standard data science and econometric practices to understand content substitution understand artist income distribution and most importantly how we can enable artists to actually make a living off their art so a lot of the projects that i've done so far have been focused around that and around artist income distribution if you're interested you can see online we have an external microsite called loud and clear um that we released earlier this year and that helps kind of demystify some of the really confusing royalties um i go on in the music world for artists and that was a really cool project i got to work on because it's very cross-functional with pr and comms and a lot of creative folks which i don't get to work with as often cool yeah that's really cool um so for those who want to you know follow felicia um i will provide her linkedin um profile url in the description of the youtube video uh definitely you know uh you know if you have any sort of questions or anything like that you can definitely follow up on either one of us um you know we're really active on linkedin so uh so with that um you know i'll go ahead and we'll go ahead and basically just you know talk about the uh the amazon case study here so um so this is going to be um two parts as uh so basically you know part one um we're going to go through the problem statement background and hypothesis statement um and if you want to see the rest of the uh this technical design document um definitely check out data tv.com the rest of the video in terms of how we go through the experiment design running experiment launch decision those sort of things will be covered in the actual subscription course itself so i highly recommend that you check out the um the premium content okay so um so felicia let's um you know let's kind of think about um you know let's do like a little bit of a deep dive in terms of this a b testing um amazon case study that we have designed um so um so we figured that this is like a really good um case study because more often than not you know when whenever you're in product data scientists around um you know there's going to be an a b testing question whether it is in the form of like a b testing round or maybe it's sort of embedded within like product data science around um and one of the classic question in a b a b testing is like you know imagine if you have two um if imagine if you have a button that you're testing you're changing the colors of it how would you know you know which color to use um and more often than not candidates will often say you know click through rate and then they'll kind of walk through the typical procedure of a b experiment but there's a lot of sort of subtleties that are often overlooked and so this is where she and i basically designed this document and we're gonna kind of walk through some of the key points that you really want to consider whenever you're talking about you know your um you know whenever you talk about the a-b sperm during during the interview but also when you're actually conducting an experiment itself now before i do a deep dive on this um i just want to caveat a couple things here so first of all um you know neither one of us are data scientists amazon and so um you know an experienced data scientist amazon might have a different sort of approach uh maybe like a disagreement in terms of you know what our success metric is and how we design the experiment and those sort of things um you know the purpose of this exercise isn't to create the most perfect ab experiment there is um but you know we've done our homework and you know we believe that this is uh this is sound and and really it's really for you to be able to take some of the concepts and the techniques that we share for your own repertoire you know when you're doing product data science so um so please just kind of take it as a grain of salt in terms of you know the information that we're going to be sharing um and so yeah so that's that's the main copy here and so now we're going to go through the um you know through the case study itself and i'll start us off by excuse me for the call um i'll start us off by going to the problem statement itself um okay so the problem statement is this um suppose that amazon wants to change the color of the buy now button from orange to green on the item page as you see in this image how would you design an experiment to test this change note that once the button is clicked the user is taken directly to paid to the checkout page um as seen in the second image right here um now let's just just talk about the problem statement itself so basically the change that is being tested is this buy now button um it's you know it's it's something that like users will often interact with right like they click to buy now to purchase something um and in this hypothetical scenario um what what is being tested is the color of it suppose that you were to change this orange green you know should this change be made or not right and so you have to think about how you're going to design an experiment for this and most like naive response would be like okay i'm just going to use click-through rate but you'll soon enough see that click-through rate is somewhat problematic and we're gonna definitely talk about that um now there's another condition to definitely consider which is the fact that you know if you look at this line here um you know you have to keep in mind that once the user clicks to buy now it doesn't mean that they're going to be able to purchase an item right away they're actually taken to a checkout page as you see here and so if you really think about it you know how would you um what is the main action should you be measuring right is it the number of instances where a user clicks to buy now or because they clicked the buy now they ended up in this page and then what we want to track is you know place your order right so so those are a couple things we're definitely going to talk about during the problem statement itself um okay so let's go ahead and jump to the um you know to the background now when you're walking through like an av experiment it's you know it's important to kind of flesh out the background information um you know you don't want to like uh you know you don't want to just start talking about the procedure running the experiment you know you want to make sure you provide some context information in terms of the problem itself so um here are a couple things that you can definitely talk about so um you can definitely talk about the user user journey and this is really important to think about because um because one you want to first of all um you know this is going to help you brainstorm how you're going to design an experiment and secondly when the interviewer wants someone understand whether you have product knowledge right um you know they don't just want you to design an experiment walk through some procedure they want you to look they want you to be able to connect the methodology to the problem itself and so that's why it's really vital that you basically walk through um you know basically the context in this case the context is going to be the product and the business um business aspect of this so um so if you were to think about this you know um you know you definitely want to talk about the user journey so um so if you were to think about this so you know a typical user journey of an amazon shopper is that you know they will search um you know they'll go to amazon.com and then they land on this sort of landing site so they're they you know they visit the website um and then they have the option to like search the items they can browse things um and then if they like a particular item then they can view an item and viewing an item is basically you know this page right here so this is where they can choose whether they're going to add an item to at cart or they they're going to buy it okay and then eventually they're taken to um you know they they the next step is you know click on buy now and then eventually they're taken to a page where they have the option to purchase uh so these are the five concrete steps that a typical shopper and amazon will go through um and felicia if i'm missing anything you know please feel free to chime in yeah um and and what's also somewhat important is kind of think about you know what are the potential scenarios that a user can go through um as they're going through this funnel because not all of the users are gonna go through this funnel right there are going to be some users that might um you know view an item and then they will exit a session right um and when i'm saying exit a session but that what that basically means is that you know they have either closed the browser or they have left the amazon.com app or you know they close a tab so they're no longer using amazon.com um you know they can view an item um and they can you know they can click buy now but but they can exit without any purchase um or you know the another you know one more example might be you know they could view an item and instead of clicking buy now they might add an item to a cart for later and then eventually exit so there's all these different paths that a user could potentially take and it's important to kind of think about that because when you're designing a metric you really want to kind of combine some parameters around you know what are the specific procedures you want to track versus what are the procedures you sort of want to ignore or maybe consider them as like a secondary metric so it's really important to kind of think about all these different paths that a user can potentially take there um and the other aspect is you know more often candidates will say okay you know we're going to you know let's just take random sampling of users and um you know basically distribute them like randomly assign them to either like the controller treatment group right but in most ab experiments you definitely want to kind of um think about like the subset or like subpopulation that you want to test um and in this case it makes is that it makes sense to test the us us shoppers only um but in terms of the devices and browsers and customer type um you can basically just sort of leave it as it is um so so it's also just kind of important to think about you know what is target the user target that you're focusing on and the last thing to consider is you know how are you um exposing this test to the users right so um so should you be randomly assigning any users right or should you be assigning the experiment to users who actually log on and they um they enter the behavior they enter an event that you're that you're testing there in this case it's going to be viewing a page and the reason why you want to be very specific about that is because what if you assign a user um who hasn't used um amazon.com for a long time right then that's that's a bit problematic because you know this user might have turned and you're never going to be able to collect any data about it right so ideally you want to assign an experiment to users who satisfy certain conditions in this case it is going to be users who log on and they actually view a page and that's when the exposure actually kicks in so this is something to consider whenever you're thinking about which sub-population of users you're going to actually test there okay so felicia do you want to go ahead and perhaps talk about the hypothesis statement now sure um and just to make a quick point on what you said before i think that's why understanding the user journey is so important one it does demonstrate the product sense like you mentioned and second it really helps you understand that custom exposure targeting of where in that funnel is the user actually going to even see the button because if they're not seeing it then we don't really want them to be counted however the way that most experimentation platforms are built if you work for a company that has an experimentation platform unless you actively specify that you only want to target that set of users or that level of exposure it'll include all users and it'll dilute the effects of your results um and then a quick point on the um the geo targeting for example i think you mentioned that in this case will um do a target to the us and i think that was because the buy now button is in english right so typically what we would do is um and we do said spotify will maybe ship to us or a couple english-speaking countries we'll see um what the effects are there and then we'll possibly extrapolate that to other countries um as well and roll it out to them later yeah definitely that's a that's a really good point um in fact that's something that i was doing when i was working for the grasshopper team um at google where um uh you know we we basically started by testing only the u.s users and then from there if the chain was you know was uh statistically significant with some positive lift then we consider whether to roll it out for other geography as well it just makes it a lot more kind of scalable a lot more efficient you know rather than trying to roll out to all globally worldwide because there's a lot of sort of risk in that you know what i mean so starting with what perhaps a smaller market uh you know starting with like a one particular market and then expanding it to different market it's a lot more of like a risk-averse procedure as well okay so do you want to go ahead and just talk about the hypothesis statement sure um yeah the hype um having a clear hypothesis statement is really really important um before setting up an a b test because it is the fundamental way that we then measure the impact of our tests and so a way that um we structured at spotify just a good way to structure hypothesis statement is kind of think of it as in we believe that in this case making a color change to this button for a subset of users like the users we mentioned will lead to more purchases or um and we will know that that is true when we see an increase in the user level click count so you know a clear statement that says what are we doing which is then what we're testing who are we testing it on what do we want to see and how will we know that we saw it so um definitely make sure that it is specific and concrete and measurable and so in order to actually test hypothesis we need to come up with a good success metric and the ideal success metric directly measures our desired outcome and and is closely and directly connected to what we're actually trying to learn from the experiment um so when you're picking a success metric it's good to make sure that it's observable in the short term it's sensitive to changes um and it's relevant for business in the long term um and sometimes it can be tempting to look at really important metrics like retention or turn kind of like we talked about um but it's important to also note that those are often pretty insensitive metrics and are very difficult to move and so sometimes we need to go a little more granular and think about metrics that will actually observe a change in that then hopefully down the line will lead to better retention um or engagement and so and of um through assessing the metric that um we both came up with was the average number of clicks within the first session per user and then in the appendix you can see a little bit more on how we define click-through rate and how we uh specify here and the reason that we wanted to do the first session per user um but once again so one other point that i just want to mention when you're thinking about the success metric of the experiment is that you always want to make sure that the metric that you pick out is relevant for business in the long term um you know obviously the north star of a company like amazon.com you know would be sales revenue right because i said it's an e-commerce so they really want to target that but you do not necessarily want to make the sales revenue as your main kpi of an experiment because uh sales revenue over course a month in one year um that's more like a long-term thing right so you really need to think about you kind of need to balance things out so you have to think about okay i'm running a big experiment it's only going to be one to two weeks and ideally i want to be able to make a decision very quickly so i need to figure out figure out a metric that is ideally short term and very sensitive you know highly sensitive to the test that we're actually implementing and the third thing is that it needs to be correlated with the long-term metric and that is very important when it comes to almost any av experiment whether you know you're designing experiment for facebook google or any of the sort of product based kind of companies you always may think about you know these three um you know properties when you're designing experiment and and i'll go ahead and uh you know let you sort of finish your thought on that one actually in terms of the uh success metric yeah no and um i appreciate you kind of um refocusing us on those three um elements that are very important and you know in that way when you are doing a b testing um in the real world you're constantly having these inner conversations with product managers stakeholders other data scientists um because there's not always just one right answer and there's pros and cons to each of the metrics um and so we the two of us definitely had discussions about what's the best one to choose would it be click-through rate would it be um more of a revenue focused metric um and so definitely it's important when you're actually doing av tests to talk to stakeholders talk to other data scientists see what the best practices are at the company and for similar projects definitely definitely and and what on that note you know why don't we actually kind of talk about you know our decision in terms of why we use the average number of clicks within the first session per user and of course as i mentioned the caveat you know you can ask the same a b experiment questions to 10 different data scientists from entry level to senior principal or whatever and they'll kind of come up with somewhat different sort of approach in terms of what the metric is and so it is really important to not think too much about okay there's only one golden and perfect answer okay rather what interviewers are often looking for is do you have a structure in which you can approach these problems right so to always sort of think about that um and don't think that is okay i have to get i have to have the most perfect response there is um okay so so let's go to the appendix and let's think about um let's kind of walk through our decisions here so um so do you want to perhaps start off start us off and then from there i can kind of you know pick it up uh where um you know pick it up from where you move on sure yeah so um yes um we could talk about first the click-through rate which is um the actual um kind of metric that we uh chose and um um click-through rate typically is measured by number of clicks per uh item page visits and the reason that you and i we decided to actually subset it to just the user's first session um is because otherwise um click-through rate can tend to um violate um hypotheses of indep independent assumptions um yeah yeah and i think i think that's a very crucial thing that most oft you know people often sort of overlook right so they sort of use click-through rate because it's convenient right uh but here's why it's a little bit problematic there so click-through rate is fine if so consider this table right here so um so here in this case we have three unique users right so a b and c we see that user a click user b did not click user c click so you know it's fairly simple all we need to do is just take the proportion of it right and and then we can use something like the t-test for comparing proportions um and then we're sort of done right well here's why this is a bit problematic the thing is that in the e-commerce case a user a single user can produce multiple sessions and multiple events so but the thing is that you know for a lot of these um statistical tests like t-tests or z-tests or whatever or even like almost all of the non-parametric tasks unless you have some kind of corrections for multiple observations for users or unless you're using like general generalized linear model where you have like a mix effect that accounts for multiple events per user for the most part you cannot just use these um these common and use details because you know for instance like if you looked at user a this person um produced four events over the course of an experiment and had three um of these basically page views a b and c up to this first session click twice and then um visit it the second time visit the page item b and then click and so because you have multiple observations per user you need to think about two things you have to first of all think about how you're going to normalize this um basically kind of aggregate it somehow whether you're using some some average or something like that you also have to think about the constraint because when a user enrolls an experiment how long are you observing this user for is it time based do you want to observe how the user behaves over the course of the next 24 hours after enrolling in the experiment or do you want to observe within perhaps the first user session so i go on amazon.com i interact and i accept amazon.com and that's the only thing that matters to experiment so those are often the two elements that candidates will often um not think too much about they'll say like you know oh let's just look at the average user per session or average clicks per session or something like that right but you always have to think about what is the constraint of this um so so in this case what we felt was that average number of clicks within the first session per user makes sense because the number of clicks here will account for these multiple visits so these are multiple page visits and the second part within the first session is important because we can constrain it to the first session and not necessarily have to wait until the user visits the second time because we want to make quick decisions you know and what if the user visits the first time and then doesn't come back until three four weeks later right then that's gonna be a little bit problematic and so that's why it's important to try to combine it into um the first um you know the first user session yeah and and i think no those are really great points i think just like statistically speaking um if you saw the you know green buy but buy now button um today and then like you use it again in a few days there's no way to say that you weren't influenced by it the first time and that way those two observations are not independent and then you're violating the um statistical properties of what you need for a hypothesis test so that is correct yeah yeah um so let's complete the rest of the excuse me um i bought the statement which is the uh guardrail metrics yeah so guardville metrics um just a quick note on that probably if you're doing um a case study for an interview um you might not need to mention guardrail metrics or go on them in detail guardrail metrics are sort of complementary to the success metrics so society success metrics are what are what is defined in your hypothesis but guard ball metrics are metrics that help you actually mitigate the risk of your test and they ensure that key metrics are not adversely impacted so success metrics we are looking for something you know to improve or something negative to decrease guardrail metrics it's almost the other way around we want to make sure that we're not negatively impacting um anything that's important to us with this test and having unintended negative side effects or possibly cannibalizing another success metric um so for example like the average order value per user that shouldn't go down and so we can implement this guardball metric um um and it and make sure that even if we let's say have a great success met success metric if our hypothesis test is statistically significant uh positively um we have to make sure that our guard male retreat didn't go down too much um otherwise we can't launch it because just because it does a good thing if it on the flip side does a bad thing then it's not worth it for us to launch experiments so that's kind of the the purpose of guardrail metrics yeah about them yeah really good point and i also see that you made a point about the non-inferiority margin you want to perhaps kind of briefly talk about it for those who are not familiar with this yes yes so i purposely didn't say anything about it at the moment because i don't think we went through mde yet and i think we have it a little bit lower but i'll i'll just touch briefly on it so for hypothesis we need to say i'm sure we want the average number of clicks um to increase right but we need to set a specific effect size and when when we go down the document a little we'll um explain why we need to say okay maybe we want it to increase by one to two percent so that will be called our um minimum detectable factor mde and in the way that we have an mde for our success metric we have a non-inferiority margin for a guard or guard male metric so our metric should not increase or decrease by more than for example one to two percent so it's kind of the flip side of the minimal defec detectable effect size yup yup that makes perfect sense um okay so that is the um end of basically the preview for this and you know if you like this content and you definitely want to see the rest of the video where we talked about the experiment design the running experience launch decision and you want the full access to the document definitely check out datantd.com and make sure you subscribe to the monthly subscription course um along with this you definitely get access to this to the slack community group where you can pair up and find mock interview buddies you can definitely post questions and instructors like myself will definitely be able to address any questions you have um you get access to um ab testing course the product sql course um and you know 40 uh 40 product cases and much more and so definitely check out data entity.com if you're doing any prep but also you know if you just want to learn data science um you know some of the information that i put that out put out is also really useful as well so so make sure you check it out

A/B Testing in Data Science Interviews by a Google Data Scientist | DataInterview
if you are preparing for a data science interview AB testing is a mustn concept whether it's for Google meta and Uber and so forth AB testing is a very popular Topic in interview questions because data scientists in those companies will use Ab test as a way to figure out whether the change that they have made on those platforms are due to random chance or because of the actual change that they have implemented and so what we are going to do in this video is I'll provide a walk through of an AB test based on a real life example and along the way I'll pepper in some hints that can be helpful in terms of acing interview questions on AB test hey everyone I'm Dan the founder of data interview.com X Google and PayPal data scientist in this video we're going to do a deep dive on AB testing based on a real life example and what I will do is I'll walk through the procedure of setting up an a test and I'll talk about a couple things that you should definitely mention when whenever you're walking through an a AB testing case now let's get started the first thing that you need to realize is that when you are walking through the AB testing procedure there are essentially seven steps that you need to consider the first thing to do is basically understanding the problem the problem statement this is where you try to make sense of the case problem that you need to solve by asking clarifying questions to the interviewer and also figuring out what is this success metric and what is a user journey and we will definitely do a deep dive on this topic in a second the second thing is that you want to Define your hypothesis testing and what this basically means is that you set up what your null hypothesis and alternative hypothesis is and and you want to set up some parameter values for your experiment such as the significance level and statistical power the third step is designing the experiment itself and so this is where you talk about what is the randomization unit and which user type you're going to actually Target for this experiment and various other things that you definitely need to consider when you're designing the experiment the next step is to run the experiment itself and this is where you need to think about the instrumentation that is required to actually collect the data and analyze the result now once you've collected data the next thing that you need to do even before you actually interpret the result and decide to launch is basically do some sanity check or validity checks because if your experiment design was flawed or if there's some bias that was implemented into the data collection itself then you have flawed result and so you might end up making poor DEC decision so this is where doing sending a check even before you think about the interpretation and launch decision is very crucial once you have done the sending check the next step is to basically interpret the result in terms of what is the lip that you saw the P Val in compress interal and lastly now that you have the statistical result along with the business context this is where you make a decision in terms of whether you're going to launch the change or not now with all of the steps covered now let's do a deep dive on an actual case problem suppose that an online clothing store called fashion web store wants to test a new ranking algorithm to provide products more relevant to customers how would you design an experiment so in order to tackle this problem that requires AB testing we want to first of all understand the nature of this product so we know that this is an e-commerce store that sells Goods such as basically clothing Goods such as clothes shoes bags and other type of merchandise and what this store uses are basically a product recommendation system an algorithm where once the user searches some keyword let's just say clothing for instance it's going to generate a result with product that's could be relevant for the customer based on let's just say their profile transaction history and so forth and so what we want to test in this example is that if we change the recommendation system maybe we're providing more relevant products to the users thereby it should boost the revenue sales of this e-commerce store so we have some general sense about what this problem involves and one thing I want to mention is that I've worked with the number of clients on AB testing interview questions one thing I've noticed is that clients will often skip this problem statement part and jump right into basically thetical methodology proposing what the experiment design is you don't want to engage the interview question that way you always want to start with the business context and then and segue over to the sisal methodology now as you're fleshing out the business goal of this problem one thing that you want to do is basically clarify what is the user Journey or the product experience that you're trying to change so this e-commerce store has the following user funnel a user will visit meaning they land on this Landing site and then they will search an item which basically produces a result based on the recommendation and then there a user is going to browse couple items and then eventually they might might click an item and then they'll purchase it which is the ultimate success that we seek to achieve through the change now thinking about the user journey is important because later down the road when you're thinking about what is a success metric and what is a target user population you want to think about at what stage do you want to consider the user to be participant for the experiment and this is something we're going to talk about very soon now one pro tip I definitely want to mention is that when you have an a testing interview round whether it's for meta Google go through the individual product basically the core product the core features of that platform and then create an outline of what that user journey is once you have done this this is going to be really helpful when you are in the actual interview setting and you're asked to design an AB test based on that particular product once you have staish the user Journey the next thing to do is basically Define the success metric so what what is it that you need to move in order to basically be certain that the change that you're applying is actually better for the platform overall now when you think about the success metric you want to think about a couple qualities so the first thing that you need to think about is that is it measurable so is it the type of user Behavior you can actually collect through the instrumentation or the platform and the next quality you want to think about is is your metric attributable and what this basically means is that can you establish a clear linkage that the cause the treatment that you applied to the platform is has led to the effect that you saw the change in the metric in this case the next quality that you want to think about is is your metric sensitive and you essentially have a metric that serves as a proxy that yes the there is a a genuine difference in terms of the user experience in terms of the old algorithm versus a new algorithm and so statistically speaking you you want to find a metric that has low variability and a bad metric for in this case let's just say time span on the website so the total time spent on the uh on the website for a given user might have high variability and so you cannot clearly tell whether there is an actual difference in terms of the how the user is engaging the e-commerce given that there are some underlying changes to the ranking algorithm the fourth thing is that AB experiments needs to be very quick it's very it's a very iterative process as a way to improve a product very quickly and so you want to ensure that what you're measuring is timely you don't want to wait weeks and months to observe a user behavior and then eventually make the change because that's a very costly way of running an experiment so you want to think about what is that short-term behavior that can serve as a proxy for long-term desired Behavior So based on these four qualities ultimately the success metric that we want to use for this case is the revenue per day per user now once you establish the prom statement clearly the next thing to do is establish your hypothesis testing so this is where you state your no hypothesis and alternative hypothesis so the no hypothesis in this case is going to be the average revenue per day per user between the Baseline and the variant ranking algorithms are the same and the alternative hypothesis in this case is going to be that the average revenue per day per user between the Baseline and the variant ranking algorithms are different once you've State the hypothesis statements the next thing that you want to do is you want to set the significance level or your Alpha in this case so the significance level is basically the decision threshold if the probability of observing a particular event is very low then it is deemed statistically significant and so in this example case the significance level that we want to set is 005 and that's the usual value that is often set in in online experiments the next value that we want to set is the statistical power the statistical power usually is set as80 and what 080 basically means is is the 80% probability of detecting effect given that the alternative hypothesis is true and lastly what you want to set is your practical significance basically the minimum detectable effect and typically for a large online platform with millions of users the MD is 1% lived once you've set up the hpth testing the next thing that you want to do is design the experiment itself so the first thing that you want to consider as you design the experiment is you want to consider what is the randomization unit in this case we're going to randomly assign at at the user level um basically randomly assign them in terms of the control group or the treatment group once you have defined the randomization unit you need to think about which population of the users you want to Target and this is something that I talked about earlier in terms of the user funnel right so you have a user that visits you have a user that searches they browse they will visit an item and then they'll eventually purchase so at what level do you want to allow the user to participate in the experiment so in this case we actually want to Target users who have actually started searching something why because this is where the algorithm actually kicks in and then they're actually exposed to the treatment in this case which is either the uh the old algorithm or the new algorithm the third thing that you want to Define is your sample size the general rule of thumb is the following formula which is that n is approximately equal to 16 * the variance / by the Delta Square where Delta essentially represents the difference of the key metric between the treatment and the control and this formula is based on your the assumption that your significance level is 05 and your statistical power is 80 once you have determined your sample size the next thing that you want to do is determine the duration of your experiment and the typical duration of the experiment is going to be 1 to two weeks you don't want to run the experiment less than one week because you want to account for the day of the week effect meaning that there could be some underlying difference in terms of the how the user engages the website during the weekdays versus the weekends once you have designed the experiment the next thing that you want to do is you want to run the experiment and this is where you use instrumentation some experiment platforms as a way to collect the data and track your result now it is very important that while you're running the experiment you do not Peak at the P value meaning you don't make any decision in terms of whether you're going to launch or not while the experiment hasn't been completed yet the reason is that when you're peaking there's a chance that when you have low sample sies for instance there's a lot of variability in terms of where that lift goes and so you might falsely conclude that there is an underlying difference when there is so once you have determined what the experiment time is given your statiscal power and your sample size you have to ensure that you weigh it out otherwise you're going to increase the chance that you falsely reject the null hypothesis given that it is actually true after you have run the experiment the next thing that you want to do is you want to perform validity checks so this is where you conduct sanity checks including instrumentation effect are there any bugs or glitches that could potentially affect the experiment results another potential issues that you want to look at are external factors so maybe you run the experiment during the holiday or when competion launch something very important or it could be some general economic conditions like covid or recessions and when you run an experiment when some external disruptions happen this could potentially impact your experiment result so ideally you want to run an experiment that avoids periods like this the next thing that you want to do is you want to check for selection bias and what this basically means is that you want to assume that the underlying distribution between the control and the treatment group even before they're exposed to the treatment condition in this case is that they're homogeneous and so one way to confirm that the distributions are the same is to basically run an AA test the next thing that you want to check is a simple ratio mismatch and what it basically means is that if you're randomly assigning a user into the control or the treatment group then out of all the part partipants of the experiment 50% of them should be in the control and 50% of them should be in the treatment but there are cases where because of some flaws with the randomization algorithm that the ratio is actually not 50 to 50% it might be 49 to 51% so in order to ensure whether this could pose a potential issue later down the road you want to use a Kai squore test as a way to ensure that the ratio between the two samples is sound the next item you you want to check is the nty effect what this basically means is that if you made some change to the website itself a user might have reacted simply because there's a novelty behind being exposed to something new one way to detect a novelty effect is basically look at it by user segment look at the underlying difference of the success metric in terms of new visitors versus the recurrent visitors and if you see that there is a change between the two then there is a presence novelty effect so you may actually want to run the experiment where you segment it by the new visitor group versus the recurrent group ver you've conducted the validating check and there's no issue with the experiment now you can actually interpret the result and when you interpret the result you want to look at the direction of the success metric is a left negative or positive and you want to consider the P value because it helps you establish whether the lift that you saw is statistically significant or not and you also want to consider the the confidence interval in this case so based on the experiment that we ran for this example what we see is that the average revenue per day per user in the control group is $25 whereas in the treatment group it's $26.10 this produce the following lift so in terms of the absolute difference is $110 in terms of relative lift it is an increase of 4.4% and the lip we saw is statistically significant because we seen that the P value 01 is is less than the Staal significance at 05 and we also see that the confidence interval at the significance level of 05 is between 3.4 and 5.4% lift so the initial interpretation based on what we see is the following there is statistical significance to reject the null hypothesis and conclude that the average revenue per day per user between the Baseline and the varant ranking algorithms are different with this result we can now consider whether we want to launch or not now when you think about whether you want to launch it or not and in this step there are three factors that you want to consider the first factor is the metric tradeoff so you might have a case where the success metric might have improved but the guard R make metrics or the secondary metrics might have declined and so you have to think about what are the pros and cons of launching it considering that the guard one metrics might have declined the next Factor you want to consider is the cost of launching if you see that the cost of building this out basically rolling out to all of the users and the cost of maintaining this change is highly costly then maybe this isn't something that you want to actually launch the last Factor you want to consider is a risk of committing false positive or your typon error rate for instance if you falsely conclude that there is an effect when there isn't and you've made a change then it might have a negative consequence to the user you might might end up providing poor experience to users and they might turn and ultimately that is going to negatively affect the bottom line of the product so these are three important business factors that you want to consider now you want to kind of go back to the interpretation of the result that you got from the experiment and along with the business context and the statistical result ultimately you want to decide whether you're going to launch it or not so what we're going to do is we're going to look at a couple example cases where we'll look at a possible range of lift along with the com tabl and then think about what is a sound decision based on the result that we have seen so in this first case what we see is that the lift is placed at a positive value but it's still less than practical significance and you also see that the lower bound and the upper bound of the confidence interval are less than the Practical significance in this case which is going to be positive 1% so in this case what you want to consider is that maybe you want to change CH the algorithm or scrap the idea all of it together in the next Cas what we see is that the lift and the bounds the comperable is uh practically significant so this provides a strong support that we should make a launch in the third case we see that the entire interval is less than zero so it's in a negative territory in this case you want to consider perhaps iterating the idea or scrapping it all together and the next example is you have a positive direction in the expected LIF but then the bounds are in the negative territory and the positive directory and you can see that it's a very wide bound now what is very important to note is that the upper bound as interval is practically significant it's greater than 1% so there is still some likelihood that you might see a lip with practical significance so what you want to consider in this case is actually to rerun the experiment with increased physical power and this is going to help improve the Precision of that lift that you're saying and in the last case you see that the lift itself you see that the expected lift is practically significant but you see that the lower bound of this is not practically significant but it's still in the positive territory and the best thing to do in this case is to rerun the experiment with increased statistical power just to be absolutely sure that the underlying change is practically significant So based on all of these considerations in terms of the business context along with various statistical outcomes ultimately what the decision you want to make is that you want to launch this new algorithm as a way to provide a more relevant product recommendations to the users thereby improving Revenue overall so there you have it guys this is the endtoend process of how to walk through an AB test how to actually address an AB testing interview question I hope you found this video really helpful if you need any help in terms of mock interview coaching or um courses that comes with a testing courses and various business case problems and SL Community Access definitely check out datant tv.com and if you have any questions along the way uh feel free to drop a comment down below or feel free to send me an email at Dan dat nv.com I'll see you in the next video


A/B testing at Airbnb - Che Sharma - The Data Scientist Show #068
they ran it as an experiment and it didn't move any metrics and everyone got very sad and they kind of ended up like disbanding the effort disbanding the team and I looked at it and it was like they ran it for like a few days or a week it needed to run for like a month and a half like it was nowhere close to enough data like there was no way you were gonna detect anything like this is sort of what I was getting into these failure modes is that there can be a huge issue where you'd internalized that something didn't work but it's really just that you didn't handle all these failure modes and now bad institutional knowledge had been created hello everyone welcome to the data scientist show today we have Che Sharma Che was the forced data scientist in Airbnb he worked on fraud detection a b testing after that he joined webflow as an early employee in 2021 he became the founder and CEO of Apple a national a b experimentation platform designed for modern data and product team to run more trustworthy and advanced experiments their customers include mural twitch and DraftKings he studied electrical engineering college and has a master's degree in statistics from Stanford University today we'll talk about a b testing best practices a b testing for machine learning models and Chase career Journey if you enjoyed the show subscribe to the channel and give me a five star review welcome to the show Jay it's great to be here daliana excited to chat yeah actually in Amazon I worked at AV testing for over three years they analyzed hundreds of experiments so this topic is really close to my heart and I'm excited to get into it yeah I love it yeah I always admire videos that's Jeff Bezos was such a big fan of experimentation we didn't have the same like Airbnb but it was very nice yeah I was gonna ask because I know Airbnb is a design Focus company so what was it like to be the first data scientist in Airbnb it was awesome I mean everybody is such a special company I have so much admiration for that company and for Brian chesky and everything they built it's definitely an incredible culture from a data perspective it was kind of an interesting environment because on one hand it's such a fruitful space two-sided Marketplace is Rich with data problems International two-sided Marketplace but on the other hand it's a design lead company and that means the leadership is not kind of naturally leading towards data for as a first-class citizen to decision making and so so for an early data team before we had a warehouse before we had much Tech to solve both the technology the problems before a snowflake world and to also kind of surmount this cultural issue of people not reaching the data it was a lot to overcome and a lot of kind of what led me to build EPO was seeing just how transformative experimentation was for that Journey like you know that was for anyone that was there as early as I was before we started investing in tooling and everything we all saw it was experimentation that brought data into the mainstream yeah and so what are some interesting results of the experiments or anything counter-intuitive you learned through running experimentation there yeah the big thing you realize right away is just how unexpected the outcomes can be you've launch all these things and you think they're all great you've done so much research you've done all these focus groups U.S research sessions you know it looks so polished it looks beautiful and it seems so logical and yet you put it out there and the metrics don't move or they move the opposite direction that you think you really start to learn a lot about you know what exactly is your business and whatever drivers of it you know one example is when I first joined Airbnb in 2012 uh we launched this product called neighborhood guides and so the idea was suppose you're visiting San Francisco and you say you're going to stay in the mission you would be showing these like five page 10 page glossy Guides of like here is 13 Bakery you should get a morning bun there here is the lowest Park to good place that people love give you this kind of guide of what to do in the mission and it was this big thing Brian teskey thought a lot of it he got up and did a bunch of PR there's still press releases today from there era of like all the pr that was done around it because the company was so confident in that product and then down the road we did an A B experiment to see how valuable this thing was and lo and behold there was a huge metric lift to remove it all together to get it completely out of the product and it was very instructive because the thing we realized is that we aren't really TripAdvisor we're not some like content system that monetizes via ads and time on site we monetize by people making bookings booking airbnbs and a big 10 page glossy photograph guide was a huge distraction people who would otherwise book are now being sent down as content rabbit hole and then a few minutes in their coffee is done their dog starts barking they gotta go do something else and they're gone and that booking is gone so yeah that was a very instructive thing and then on the positive side there were some experiments which are examples of great product thinking such as we started demoting the Airbnb hosts who are not being great on the marketplace for example they would just reject everyone they would sit up there with attractive looking listings but they would always just reject everyone or cause a lot of friction to book we started steadily demoting them in the ranking and eventually removing them all together and that was a huge list in bookings just for guests to be able to go on the marketplace and only see hosts who actively are going to say yes a lot that was a huge lift a lot of bookings came from that and kind of even more fun one was it's probably my favorite one because it just demonstrates why infrastructure is so important when I was there five years 2012 to 2017 but I found a way the biggest impact was making it so that when you click on an airme listing it opens in a new tab and that one little thing bookings more than anything else and it's funny because it seems so obvious in retrospect but I tell you the design team hated it and a design lead company was not so easy to make happen but you give an experiment infrastructure that lets anyone try stuff out and then you show the metrics and it just gets very hard to bite so that was really good for big Nets yeah I love the stories because I remember when I was the Amazon I was seeing the experiments when you do some small changes on the font you can generate thousands of dollars I think hundreds of millions of increase something you don't understand and then if you look at all the experiments less than 50 probably even fewer got launched that means most of times our product intuitions are wrong so for example opening a new tab it's interesting I'm booking Airbnb right now I really love within this feature but I also have 200 tabs open right now yeah so when you work with designers or product managers how do you interpret the results in their language to first convince them to run the test and then tell them what a launch decision looks like based on the statistical analysis definitely well the decision to write an experiment usually the first thing is to just like convey how uncertain everything is and so that like anything else if you're early in your experimentation journey and most people are skeptical I would start with the early adopters the people are curious about this usually it's an AI team or a growth team are the ones who run experiments before anyone else for a variety of reasons we can get into but I suppose the company runs a lot of experiments it's already established and you're trying to get some new employees on board but a great thing to talk about is like previous experiments you've run kind of unintuitive answers you've seen it can be from previous jobs as well kind of just show them how this stuff works and then just say look all we're going to do is do the same amount of work but just wrap it in this experiment and then we'll have so much learning to talk about on the other side yeah in terms of how to interpret it and make decisions that's a there's a lot to go into there and it gets into just what are you trying to accomplish you know why are we even doing this product like what are you hoping to happen getting people to clarify that can be like one of those sneaky benefits of running experimentation programs is just to enhance the rigor of your thinking yeah definitely want to talk more about rigorous thinking later so a lot of times people think oh a b testing is easy you just look at a metrics you can compare which metrics it's higher or lower but there are a lot of challenges in a b testing a lot of nuances so what makes experimentation difficult yeah experientation is funny because it's both easy to do it right you're basically divide people into equal groups and you see who outperforms another but in practice it can be a little bit more finicky and there can be some structural things you have to overcome and there's a big reason why the armies and Netflix is not invest a ton into their infrastructure to make it happen uh so the first thing is there's a concept an experimentation called statistical power and so the idea is you need a certain amount of data a certain amount of sample size to be able to even notice that something's happening I think people are sort of familiar with this idea like if you are trying to take an average people know that if it's an average of like three things it's like not much of an average compared to an average of 1 000 things the same principle applies just at a kind of deeper level with experimentation that you need a lot more data to detect things and it's one of those where if it's a very large effect size you might need less if it's a smaller effect size you might need more but in the end you just need enough data so you have to run this thing called a power analysis to figure it out this becomes mathematical equation that needs some inputs and that can be a little bit more hard because there are really off the shelf great off-the-shelf ways to do it it's but it's super important especially as you're starting out because you're probably at more of a threshold where it may or may require waiting a long time to get enough data to finish an experiment so that's step number one is sample power analyzes and disco power another thing is when you are randomizing into 50 50 groups how are you doing it and is it a robust way for example you might think it's 50 50 but it's actually like 51 to 49 it's slightly different and it might seem like this trivial thing but I tell you every time there has been a meaningful difference in the splits it's usually due to an actual issue where that one percent is a highly biased type like they're low latency users or your users out of certain app versions that are broken or something like that and so you have to run this thing called a sample ratio mismatch test to figure out are the slight deviations from your expected randomization is that due to chance or is that due to a real issue to look into and then when you get into the actual implementation one of the things that happens reliably is you start running experiments is you quickly realize your execution jobs for example like once you get to a point you're running experiments there's a lot of different use cases a lot of different settings and contexts that people are using your app and it might be wildly successful on some but failing at others for example Airbnb we found all the time that we launched these experiments which would look generally pretty positive but they would be failing on Internet Explorer because at that stage instead of swore had a bunch of issues especially the older versions or it might be working everywhere except for East Asia because how you handle time zones and stuff like that so you'll quickly realize that diversity of your user base in the use cases and that'll be something where it's not that the idea was bad it's that you haven't really executed for the diversity that your application is serving there's a lot more to that in terms of like using statistics properly like a lot of times people use very simple statistical methods like t-tests and basic methods which are kind of these fixed sample size tests which are easy to implement they have off-the-shelf tools for it but they actually require some pretty strict adherence you cannot stop experiments early you're theoretically not even supposed to look at them until it's done in practice that's a very hard rule to follow in an org where you're collaborating with business users and so if you want to avoid the sort of false positive rate like inflations that happen with it then you end up meeting to under you need to follow the the principles very closely so in the end there's quite a lot of things I I think I'm only like a third of the way through the list of potential failure modes of an experiment yeah but this is the goal of infrastructure is to take care of a lot of that for you and make sure you don't have to know what you don't know yeah and data science is special data science are working on product and growth it's in an interesting position because a lot of times they don't report to a data science manager they report to a product manager or report to engineering manager especially when they just join a new company or new team some data science find it challenging to influence their teammates to set up the best practices so what are some advice on setup experimentation best practices yeah well I think a starting point is to always realize that any effort is done by a collection of humans and so your ability to get people on the same page is going to lean on the strength of your relationships so yeah like anything else you do in the data world there's basically no way to under invest in human relationships so start there then the second thing is we have to bring people along the good news is that there's quite a lot of technical content out there on experimentation and statistics airme Netflix Amazon booking everyone's writing quite a lot about it and people look up to those Brands so that's another good way to to look at it we publish quite a lot of technical content as well for this purpose to arm people as they navigate their org to be able to get people on the same page as some of these Concepts and then a big part is setting expectations it's people there can be a bit of a whiplash of going from a world where you just kind of ship everything and throw it over the wall to one where you're carefully measuring it in a follow-up and that measurement might show that it was a wild success they thought it was it can be good for people to just understand what is going to come after and understand that the first launch might not be the wild success and there might be some follow-ups and then the last mode this is probably the thing I see most often messed up among companies they start running experiments and they immediately want to take big swings they want to go after the most public thing the most visible highly contentious project because that's the one that ends up having the most doubt where people want to make sure it works and that can be a bit of failure mode I think of all the things I just said before about why an experiment might fail that has nothing to do with idea the worst thing is a practice is if you do this big political fight to get people to run experiments and then you just end up in this cloud of dowel about did we even run the experiment properly so I instead say it's better to run a lot of small lightweight experiments first build the muscle build on low stake stuff build on stuff that people understand and might not move a metric or not among there you're probably gonna come across some wins you'll probably have something to celebrate and then by the point you go after something more public or visible you have confidence in the infrastructure you have some basic knowledge of how this stuff works and what the check and the failure emotes yeah I like that maybe start with changing some font or the color of the button when when I work with companies or some as part of Apple we often do quite a lot of support and what we like to do is give them a list of 10 experiments which is just green light right away and it's usually very small stuff it's like adding badges or yeah again adjusting fonts adjusting layout very slightly it's stuff that theoretically some front-end engineer can just do by themselves it doesn't require yeah before I worked on AP testing I wouldn't believe changing some color even just one shade of a button or from a square the two like around it would change Revenue but I've seen it myself so it's really fascinating to work in the field totally I mean every company's got crazy stories like that I know I wasn't at Microsoft and working with I'm Bing but I'd heard that Bing did something interesting Ronnie cohabi's book but like you know being it turns out if you like take these sort of text that's part of what the ad is going to say and you have it like wrap around instead of dot dot or something like that then it was like a 10 boost in Revenue it was some like absurd amount yeah and so these small changes can lead to Big effects the one thing I would say is that when we say button colors and font changes I think people just sort of roll their eyes and be like what are you even doing here the key thing is small changes can have big effects what it means you care about metrics in general so it should matter but more importantly it can be to big learnings I mean think of that listing a new tab thing of course it's great that it boosted metrics and it's awesome but the big takeaway is that not losing search context matters if you're Midway through a search and you get distracted it's not like you need to pick up where you left off it's very helpful to have those tabs open it's kind of helpful to have to be able to pick up where you left off and you can see a lot of product work coming out of that direction yeah so in Airbnb you also worked on search ranking you worked on fraud detection so what made you want to later work on a b testing or were there any connection there yeah I think the big thing with experimentation was we just saw the whole culture of Airbnb and again this is a place that's led by designers like start to just really lean on experimentation and it led to just some incredible moments for example most companies in hypercraft mode they grow very fast but at a decreasing rate so air for me we grew like 3X 3X 2.8 x 2.6 x both steadily decreasing but we actually reinflected the growth we actually made that growth rate go up again and because we ran the experiments we had the receipts of why and it was a bunch of the work that was happening on this kind of search Marketplace team you know so that was just very telling like it's pretty rare in a tech company to both reinflect the growth that way and have that sort of clarity on like what exactly what it was doing and that led to a place where suddenly that all every team every was being measured out of how many bookings they drove and how do you know how many bookings that team drove while they ran experiments and so it just really dramatically changed the culture in a way that I thought was awesome and I think what's most compelling about that culture change the thing that kind of really got me to Philadelphia was it just enabled this entrepreneurial Spirit or something you could try stuff out without having to win big political battles but you could put something out there put in front of customers see how they like it and yeah that ended up being a great fit for Airbnb you didn't have to be a data person to love the idea that like here's an atmosphere a culture where people are empowered to try stuff out you don't have to like win over like high-powered people with your charisma or whatever you can get in front of customers and see what they think yeah very few things that data a data team can do that will like change a culture like experimentation yeah and I also talked to a friend working as a machine learning engineer at meta and he said he wished more Engineers know more about a b testing because a lot of people focusing on offline metrics like accuracy precision and recall but when you launch a model in production it's completely different you have to think about do people actually use their model do they click on that do they like it customer satisfaction so what are some advice on testing machine learning models yeah I test everywhere test off it and I basic advice I mean you said it perfectly like me on search ranking we saw that like every model we put out there looked great on the offline simulations on this sort of local development setting something like 20 of them actually move the metric there's just a bunch of reasons why that'd be different I mean basically your offline data isn't quite representative it's your online data you have people reacting to your thing that can lead different situations especially in fraud world where you have adversarial contacts the very different world so the basic answer is in a way you probably want to experiment infrared before you even have ml infra because like you can do things like linear regressions and make slight improvements linear regressions and that can move metrics and then you would know if you have an A B test thing so I would say every time you are adjusting a model in production whether that's adding a new feature trying it if your architecture but in the case of next-gen AI General AI stuff if you're using like an open source model versus an open AI model or something that's more tailor-made to a vertical versus otherwise a b testing is how you're going to know whether it's the juice is worth squeeze whether it has Roi whether the metrics are moving and thus it's worth paying up for seven model yeah so when people are testing their machine learning models so what kind of metrics you recommend when they run a b tests well the big thing is this is you want to use business metrics one of the things that happens with AI and ml teams is that there's quite a lot of political backing and the excitement when you first get the model up the first time you move to a search drinking model as opposed to some heuristic or the first time you get a gen AI feature and that can be exciting in some XML team but if you want to justify further investment once you're talking about iterating on the existing model that's something that can be very hard for an organization to feel the difference think of something like your me search ranking or pricing Airbnb or recommendations or whatever your new model might have taken an enormous amount of effort in feature engineering and offline evaluations and stuff to a business user or to a customer it looks identical in product it's just like you're searching models now we're turning a different set of 20 things first the original set of 20 things your pricing recommendations is also just spitting a number out yeah so it can be very hard for an organization to feel like over there I have this team that is like materially changing the product and like noticeable visual ways and over here I have this ml team which costs a lot does high expertise work and is can feel like it's just moving through musical chairs around but when you run the experiments if you do it against business metrics if you do it against stuff that is the language of a business user the language of a CFO then you end up in this magical world where you can make your cases against all the other teams if this is why this stuff is worth investing in this is why we need more tools to be more productive this is why ML and AI is a direction is like very important for the company so this is a big thing where ml teams can be trapped in this world where everything makes sense from an operator perspective but it's just hard to put in conversation with any other investment that a company might make and Aviation is really what brings you out of the darkness there yeah and I think in our previous conversation you mentioned we should put a b testing in part of the ml Ops process yeah 100 I mean this is something where if you just look at any kind of production AI practice whether it's at Netflix or meta or Google or whatever they are running a b tests after every model iteration and so it just becomes part of ml Ops this is something that I think most people kind of intimately know but it doesn't get set as much as just a b experimentation is ml Ops it's just the last stage of a model's life cycle from data set curation to offline training offline evaluation and then production eval AKA AB experiments yeah and you learn so much about your customer that helps you build better model and collect better data as well so after Airbnb you become an already employee in another company again so what made you want to make the change yeah I really loved that kind of 100 200 person stage aired me when I joined was 200 people or so and by time I left it was like five thousand it was just so big it was very very big it's just a different world and so I feel like had learned a lot about what can a data team do to drive impacts quickly and I was excited to try it out again at different places so I did some Consulting I worked to some places and then eventually with the webflow I was the second data scientist there and that was cool because it was a SAS business your business actually workplace isn't going to Assassin's business yeah one of the things you've learned as a data team is that your job and the makeup of your team changes by the business model SAS metrics are very different than Marketplace metrics marketing-led growth is different product like growth and so it was cool to bring it that Playbook into a different setting and it I ended up rebuilding the same Airbnb exploitation infrastructure there and that that was definitely taught me a lot about a kind of zero to one on that type of infra and I was supporting the marketing R quite a lot for life cycle metrics and optimizing a lot of their campaigns oh nice so for folks who are not really familiar with Marketplace business versus SAS business can you tell give us some examples what are in terms of metrics what kind of metrics do we use for those type of businesses totally so marketplaces are typically the one metric that ruled them all is transactions Marketplace clearing in the case of me it's like people making a booking in case of eBay people making a purchase marketplaces they usually monetize some ve off a transaction and that ends up being the the main metrics you want to lift and that has really nice properties because it's the many marketplaces clear fairly fast so your metrics bear fruit very quickly they align customer goals with company goals very well so you can nakedly pursue that metric and know that you're also helping the customer for the most part and so it from a data team's perspective it's sort of on easy mode in my mind because like you your value system of establishing a data empirical based value system of saying we care about this metric going up and thus we should do all these things it's very easy in the marketplace once you get to a SAS company typically the most important metric is AR annual recurring Revenue number of subscriptions that sort of stuff and so weblow was a monthly subscription swipe a credit card type of deal and it's great as a business model because recurring subscription money is a nice stable thing you can grow nice businesses and it's very easy to turn into some large outcomes but as a data team it can be a little challenging because a customer's desire like signing up for a subscription that they don't end up canceling is only correlated with customers benefiting from using the product so in the case of web low like the reason people use webflow is to make a website and so we I was establishing a lot of metrics around did someone get a website up and running I was getting traffic because that actually fulfilled the promise of webflow and if someone's upgraded their subscription but they did not get a website live they tended to churn over time like it was very rare that they actually continue staying subscribed so you could get some money from them but that wasn't the customer value thing and so that was the pivot and making these types of metrics and figuring out what represents customer value when the money metric is only correlated that's a bit of a challenge for the data team it's a very interesting challenge I think it's I have a lot of thoughts on as I might go about it but a very different mode than a Marketplace business yeah yeah thanks for sharing that and you're the second data scientist so how do you scale experimentation there yeah that was much much more challenging so at Airbnb we had the Bennett we had a couple benefits one that there was a lot of just straight up phds like lying around you know they're very high expertise people and who are used to statistical methods and probabilistic uncertainty and stuff like that and even if they weren't there's a lot of Engineers who came from Facebook or came from Google or something and so even without a lot of Prior experimentation having happened there was still just a lot of latent desire for it and people understood just how to do it in contrast at webflow I was I'm pretty sure I was the first person there who had any experimentation background and so there was a huge education Gap there were no phds lying around there was no kind of big growth culture or AI culture or anything like that to work off of and so we were standing up the growth team running our these first sets of experiments and so I built the urban system there learned a lot about kind of what extends what that's in the good news is that core architecture extends the SAS businesses which is awesome well the problem I ran into was all the support all of the questions of how do I interpret this thing did I set it up properly how long do I run it what how do I interpret these stats and so I started really coming entering into my head that a great experimentation infrastructure would not just deliver capabilities it would deliver confidence it would deliver like a feeling like I know what to make of this and I trust it and so that's really what's kind of built into EPO is to say how can you have the Army culture without having to hire all those phds you have a system that not only has all the power they missed some but it have a lot of very intentional design so that you as a business user kind of know should I trust this report was it ready to read and if it's ready to read how should I make a decision yeah I can really resonate with the confidence part because when I was at Amazon we have a tool it shows a direction I think it shows some information how to interpret the results but as the experimentation team we still host regular office hours and product managers and Engineers who come to get advice from us and we even have training called bar raisers I was a bar raiser so we can give us more confidence yeah thanks whether you can launch decision or not and sometimes when they it's interesting when someone has a really good results they never question the data quality and when they have a bad results oh can you check whether randomization works are there any missing data Etc and the browser you have to explain things to them and convince them this is not a good decision so when a company doesn't have like imagine a lot of phds or in this case bar raisers how can you provide the rigorous answer to someone using experimentation tool when they see a bunch of metrics and different colors yeah I think that there's a couple of directions here so the first is we just kind of go like one by one through all the failure common failure months of experimentation and just try to come up with intentional design that's going to work if you're not a bar racer for example those common traffic and balance checks sample ratio is best checks so we automate those and we'll send you an alert proactively in your slack that says like there's an issue here other things in terms of the statistical power conversation we have these out of the box like statistical power tools but even if you don't use them every experiment in eppo has a progress bar which is kind of a mental model that is that works for business people it's like I don't have to really know the details of how you came up with the idea that we are 75 done I understand over 75 done and I need to wait so what's great is with EPO is like you can just lean on our kind of off-the-shelf power analysis tools to say like you want to see a certain effect size and thus and we can observe your data and see that you need to wait longer or suppose you're just saying I want to run it for two weeks they can enter that in and so you're enabling kind of this self-serve guidance of how long do I need to wait and then another big thing I'm very passionate about is experimentation but has to live on as a data platform it has to inherit all the same dependencies of data platform all your other Data Systems and that means it sits on top of the warehouse and all those pipelines better work now as you and I both know especially in a kind of well any company but especially startups the data pipelines often don't work sometimes they break and sometimes there's like data deviations data movement data drift and so we've we take that stuff seriously so a lot of experimentation tools will wipe their hands up and say that's part of some other system's fault we basically build in some lightweight observability tools and non-way detection tools into EPO to say like we have not noticed any data in past few days read the report it's not ready you need to fix the pipeline or there's some kind of anomaly under the hood other things that I've seen people get tripped up on are outliers every business I've ever worked in has outliers all of them yeah and so these are these kind of these data points that are like many deviations for the mean in a way that's anomalous another way of saying it is I don't know if anyone if your listeners have read listen to the Black Swan by the same celeb yeah the general gist is the most distributions in the real world are follow power laws which have really heavy tails and lead to these crazy outliers that will sway an entire experiment so just the removal of that data point will actually completely change your interpretation of the entire experiment and so this is something that happened at webflow happened to Airbnb and what we do is we out of the box we'll windsorize things kind of clip it at the 99.92 percentile and just kind of take away these like incredibly impactful outliers by just kind of capping the value and then we use a statistical method called Cupid which has a lot of benefits in terms of speeding up experiments but it also helps with this outlier problem and so there's a long list of stuff but what we try to say is like experimentation is collaboration of engineering business users and data what is data's responsibility is to supply the correct data and the correct definitions so we create a kind of a metric Library type of experience where data people can input the correct definitions and maintain them easily govern them as the company grows for engineers they are setting up these experiment with sdks and hopefully you're enabling in your company Engineers through run experiments at the end all on their own and so they need to be able to navigate systems and make decisions even without a death decrease and then business users they're usually socializing stuff all around the org they want to celebrate their team's accomplishments they want to explain their learnings so they need interfaces that not only that they understand not only that this PM understands but that PM can communicate to others can put up on the board in the middle of the product meeting and have everyone get the right takeaway so a long sequence of gelier modes that we try to mitigate Pro upbeat proactively and communicate around while also enabling each member of this collaboration to do their best most powerful work yeah and I like that in apple you have a split buy feature where you can filter the samples and dive deeper into one segment and see the impact on new users and old users so in my experience we have to write custom queries so every time when the product manager have a new assumption because they always have a lot of new assumptions and you dive deeper into Data so I feel this will improve data science productivity but I have a question so when we recommend in the decision making process we recommend team to only focus on the two three metrics we want them to think about the metrics they have defined before they start experiments and sometimes when they discover something interesting after the experiments they want to change their goal posts and we call this a cherry picking metrics so how do you think people should best use this feature and not to cherry pick metrics yeah it's a real challenge I think this is part of what I'm what I was getting at where there's some benefits of experimentation of making people encode they're thinking of how they make decisions you know I think this is something where infrastructure can really play a role where there are some things where you do have to check all these metrics we call them guard rail metrics right if you are yeah I'll just use the airme example again like of course you want to improve bookings but you don't want to spike customer support tickets or cancellations or just something that is like a downstream bad thing if you are some income business you course want to increase the number of purchases but you don't want to increase the rate of returns and you want to make sure your margin is good so there'll be a bunch of these metrics which truly matter to the business and you do want to track all of them and so it can be a little bit of a shape of a try to improve this primary metric but make sure the others like don't move at least don't get worse but what people often do is try to come up with some way down the list like not that important metric and call that as a basis of Victory our attitude at EPO I've finally been able to build this system that I would I wanted for this which is you can create this decisioning space that allows a center of excellence to be a little bit opinionated where every experiment might have a bespoke primary metric the main thing they're trying to move and then they'll have the guard rail metrics but all the other metrics you're trying to read they are going to be in a separate section as you do all these split buys like you can look at those numbers but they're not going to be on the primary decisioning space they're going to be in another section of the app so basically what we want to do is we want to let people do investigations get curious find explanations but not pollute the decisioning space with it and so this is how infrastructure can help but of course this is also a cultural organizational problem and I think as an org if you sit down and say Here's generally how we think experimentation decisions should go and you socialize that could be a good way to enforce some systemic rigor some idea of here's our opinion about how experiments should be done yeah we very much encourage both infra to do its job and organizations to also be somewhat opinionated about what they want in their decisioning process yeah yeah I like that so you think this feature so we buy to dive deeper into a current experimentation to get curious to learn something it can help you form new assumptions and then with a new assumption you can run a new experiment to test it separately but not pollute your current analysis exactly exactly and it's especially true once you're socializing and stuff around the org you're sending it to your Chief product officer or something like that's where you can see some Shenanigans right that's where people just sneak in all sorts of stories if you have infrastructure that delivers somewhat opinionated reports just the consistency of that can really help here because then it's like you're sending me these numbers in slack can you show me the the thing show me the report and that kind of puts me on some guardrails I think that's part of how you can get there yeah yeah another thing I worked on when I was at Amazon was to help data scientists engineers from different team to onboard their own metrics because the main metrics around purchase but think about audiobooks and Prime video they care about different things and in beginning it takes some time for them to onboard metrics on the the central a B test experimentation so do have something that make it easy for customers from different industry to onboard their own metrics totally totally I mean metric management is like one of the big big things that we tend to excel at and I think part of it is that there's some Nuance to metrics in a large organization right so you call that one thing which is there are multiple business units who have their own goals and they have a set of core metrics and they want to be aware of how all the other teams are affecting their metric for example if you're Airbnb there might be a business travel team and they care very very closely they look very very closely at the business travel bookings that's their metric and then you'll have some other thing that's about like family bookings and they want to know how many family bookings there are and so you need the ability to have different businesses encoder metrics but and so we definitely make that a very easy thing for every team to kind of add their metrics and certify them and lock them down well there's a separate problem experimentation that I feel like everyone feels but haven't been able to articulate much which is there are core important metrics that really matter that need to be locked down and usually you put the definitions in GitHub you Version Control them they only a center of excellence really gets to change them some certification structure but then there's all these other metrics which are just sort of ad hoc and exploratory like you don't even know if you're going to use this metric definition for too long it just is sort of relevant for this one experiment or for this one quarter or maybe you are considering spinning up a business travel team and you want them to operate with a certain metric but it hasn't been fully greenlit as like a anything more than a tiger team and so what can often happen is if you're the only metrics you can view in infrastructure are the lockdown certified GitHub control thing you just start to see that Library balloon it just yeah all these little ad hoc things which have very short lifetimes end up going through a very long certification process often taking multiple days or a week or two just to be able to review the result in the platform so I feel like good infrastructure needs to acknowledge that there are some core lockdown things and then there are some exploratory ones that eventually graduate into the lockdown things and you need a path for the exporter ones to be spun up fast and the lockdown ones to be not touched too often and to be tightly governed and controlled yeah and in my past experience a b testing sounds like something data scientists do but it's actually a team sport we rely on Engineers to writing code to kind of embed their algorithm into the experimentation and some challenges I'm seeing is sometimes because they don't know a lot about statistics sample sometimes the way they filter their sample before the experiment get triggered causing some issues in randomization and then Downstream data scientists analyzed the results and the report to product manager and there's also some knowledge Gap there that make decisions so I'm curious who's using apple is it Engineers data scientists or product managers and how do you bridge those skill gaps yes all three I mean one I think one of the big things we have said about this industry is that experimentation is collaboration business experimentation is a collaboration tool not just like a Dev tool and I think the part that goes into this intentional design where if you think of experiment and experiment as like a kind of canvas that gets added to as these layers get added on sometimes there's a long planning process and you have PMS like writing up hypotheses and writing up notion docs and outlining how they're going to make a decision and that all gets encoded in before you've set up anything sometimes it's just an engineer just spinning something up and just using it off the shelf metrics but if you just have this surface area that's meant to be collaborated on and highly public you can have these multiple starting points but the PMS the data people with Engineers everyone's socialized to like how to interact with this thing yeah this whole conversation reminds me so many stories and experiences I had I think another thing about a b experimentation in a large organization is every team tests their experiment in Silo even if you're in the same team one product manager testing someone learning something the other product manager might not know this so I used to look at all the results and then write a report share the learning and also the failure to the entire company so T different teams have visibility of what other teams are testing and they can get inspired by that I'm curious in apple do you have something like that to help teams learn from each other yeah 100 it's one of the things that comes with centralization there's some companies that they grow up and they get into experimentation but they let every pod run with their own infrastructure and have this like issue where you no longer get that pooling of knowledge in Awareness where you no longer have this place where you can say like here's all the experiments going on and I can like understand how that business unit is affecting my metric or how what have we learned about experimenting this area historically there's less of that ability to like index and build up a knowledge base so I think to be able to centralize across multiple businesses business units across multiple teams you kind of need to architecture do it which kind of gets into how you model metrics and what you choose to centralize what you choose to decentralize yeah the Apple we made this conscious decision that we want to strongly centralize analysis and metrics because no business units should be redefining what revenue is or redefining what a purchase is but decentralizing setup because people might have a variety of flagging tools they might have a variety of setup tools maybe they're randomizing completely offline in an Excel sheet and just kind of uploading it into some redis cluster or something and randomizing using that there are a variety of ways in which you can get there maybe they're using a machine Learning System and that is handling random randomization so decentralizing the the setup while centralizing the analysis was a kind of way you get there in centralization and then once you centralize the analysis that's the natural place where it kind of reports get curated Knowledge Learning Happens and knowledge gets disseminated and it's a really natural place to build a knowledge base off of where now you can like index all those findings you can disseminate them share them out in slack or email or notion or whatever and if anyone was ever to join the company and they wanted to see has anyone tried experimenting on the onboarding flow or whatever but you now have a place you can figure that out yeah and I think one thing about experimentation is it helps data and product team learn a lot of things about their customers another thing is to finally help them make decisions however most of the experimentation doesn't they won't have significant results that's another thing I learned only in a small portion of them yeah well and then the product manager will lean on you to help them still make a decision so when the result is inconclusive how do you make decisions because we cannot just test the same thing over and over again right and sometimes you just don't get that many users and you don't reach the successful power totally totally and this is something when I first joined webflow they had to run an experiment before but they had didn't have the knowledge of what's required to do it and so they like they use the someone way of randomizing using like the last digit of the user ID with which is a fine way to start you know but when they did the analysis it was in some ways sort of a sad story there was this product that I thought was like actually a pretty good product I had some good research behind it and I was an ex-web flow user and I thought it would be pretty compelling they ran it as an experiment and it didn't move any metrics and everyone got very sad they kind of ended up like disbanding the effort disbanding the team assigning them to other stuff and when I joined it had happened about a month before I joined they asked me to take a look at it and I looked at it and I was like this thing had like they ran it for like a few days or a week it needed to run for like a month and a half like it was nowhere close to enough data like there was no way you were gonna detect anything like this is sort of what I was getting into these failure modes is that there can be a huge issue where you'd internalize that something didn't work but it's really just that you didn't handle all these failure modes and now bad institutional knowledge had been created so yeah it's both important to be able to look into the past to like see what's happened and see what's been tested and see whether it worked but it's also important that if your infrastructure is good you could introspect was it even a good experiment was it set up properly like what were not just that you tested on onboarding Flow but what were the screenshots of what you put in there and was that actually well founded um so yeah this is a big part of what a good kind of knowledge generating system should be doing yeah and I think you mentioned something very interesting the experiment was terminated too early and I know some companies want to have a holdout to test the long-term effect I think there's a pros and cons of it because if you're testing a customer online Behavior sometimes people clear their cookies or different type of logins maybe share accounts so what do you think about running those long-term experiments having a holdout set yeah it's a great topic I'm a big believer in holdouts there are great gold standards for understanding a program's entire set of effects so if you launch like 10 experiments and you want to see what is the total added effect these holdouts are great for that they're really good for understanding team's entire impact but the problem is that only a few companies are really going to be able to accomplish them and it has very little to do with the infrastructure like EPO we can help people set up holdouts we have a holdout feature but the the problem that happens is one if you don't have large data volume and you do some sort of one percent one percent hold out or something or like 2.5 2.5 hold out like that is just not going to add up to a lot of data in the end yeah it's just going to be so small that you're not really going to text anything the second problem is what you're mentioning where like holdouts basically don't work on cookie based assignments like anything where cookies get wiped where your metrics at a user level and you have the identity resolution like it it's very hard for it to work there and then the third problem is that holdouts if you're going to have some version of your code that's going to be kept aside for a long stretch of time that's a lot of feature Flags you're keeping open and it can be very hard to kind of cast a version of the application in Amber and have it still workbook free yeah because you need no use of doing a comparison if like that version of application is just has bugs in it and so that can be very hard for a team to accomplish you see some of the larger organizations like meta who like are very config driven in their development and everything like they are able to do these holdouts but for a lot of other companies it can be hard to accomplish so we encourage people to use holdouts if they are ready to get everyone get a large effort a large engineering effort on board to do it but if you are unable to do it there's some other options of how you might get similar goals accomplished yeah so have a whole house is good to measure long-term effects and for short-term effects I know some companies and teams are running Motown Bandit testing so how do you decide when to run a B test versus mab yeah Bandits are super interesting so for anyone in your audience who doesn't know what the Machamp Bandit are the gist is an A B test you keep these even splits like 50 50 for you know a month and then you're going to make a decision to launch one of them with a multi-around bandit you automate the decision making such that suppose a weak interior into the test you notice that one of the variations are doing better than the other you start shifting traffic to the winner and then so long as that winner keeps winning you continue to shift the traffic over there and so it has this nice benefit that you can start reaping the rewards earlier so you don't have to wait for the one month to start pushing treatments to the winter it also has this benefit and that is sort of automated in the world where people feel uncomfortable with making decisions on experiments it feels like this get the humans out of it sort of thing but the problem is that bandits in practice are actually very finicky most people use multiple metrics and they make a decision it's not just the one thing they have these trade-offs and once you are in many metric world then Bandits don't converge so well even if they're somewhat more unstable and then the other issue is that they're not really optimal for the business other than a few certain situations for example another way to think about EB tests is that they're an explore and exploit process you are massively exploring for the one month in which you're doing 50 50 and then you're going to exploit it indefinitely forever forward you're going to use that winner and get all the metrics improved a bandit is basically saying we want to start exploiting during the exploration phase and that's important if in certain circumstances where there's a big time opportunity cost for example if you have a Black Friday deal yeah there's no benefit to your winner after Black Friday once Black Friday is Con gone it doesn't matter what was the winner because all your benefits are up until then another thing would be a news headline suppose you have three sets of copy for a news headline once that news has passed a the World Cup or something and World Cup's done then there are no more benefits of that news more optimal news headline and so those are circumstances for which Bandits are great they're really good because you want to start reaping your rewards with enough time to benefit from them but the vast majority of testing out there is not of that shape it's actually something where you can reap the rewards of winter for indefinitely and in that case you better it's just much better to do the Explorer stage optimally and 50 50 splits are way better than anything else for the exploitation yeah and now a lot of people are talking about AI large language models so what does AI mean for data scientists I I think it means an incredible amount for us I'm there for a variety of reasons one is that data folks kind of are naturally positioned to be able to be a big part of making those products these are even though a lot of the foundational models are kind of giving you a lot off the shelf the type of data you're feeding into them if you do any amount of local training or is kind of a data process that you're gonna be naturally part of but the second big thing is that hey we talked about machine learning before and why people run so many experiments why experimentation is part of the ml Ops workflow the same thing is true of AI products where the second you put an AI product out there you're gonna start running a lot of ab experiments and you're going to start driving a very metrics-oriented point of view so suddenly you're going to start deciding that products are successful by whether they move these metrics or not which naturally makes a lot of people reach out to the data team for consultation for investment for advice and once metrics are a key part of decision making then suddenly it's worth investing in data quality it's worth investing in data scientist productivity a lot of stuff comes out of that so it's going to mean quite a lot and if I'm to really prognosticate into the future put on my tin foil hat so to speak I think this is going to do a lot for data teams at an even deeper level where the things we have noticed so far about gen AI it's early days but the things we've noticed so far is that we are going to speed up the process of ideation because chat GPT is very good at brainstorms it's very good at generating ideas if you want to just spit out 10 ideas you can just ask chappie to you do it we are also going to speed up the development of creative assets suddenly you can just ask a Runway to give you like 10 100 different creative assets along with the and you have 100 different things to evaluate and we're also going to see the speed of software development rapidly speed up because GitHub co-pilot is going to enable people to develop much more quickly and for kind of people with less sophisticated software backgrounds to do some out of programming and so in every which way we're going to start to see just the cost of implementing ideas go to zero just go down tremendously and if the cost of implementing ideas goes down a ton then the bottleneck becomes evaluating ideas yeah how do you pick the winners of that if you have 100 creative assets which one's the winner if you have five different potential growth experiments going out like which ones are driving value and which ones are actively detrimental this again just pushes on the data team because that that we're talking about evaluation that is basically what the data team lives for mm-hmm yeah and I think now I hear two voices one is people saying oh data scientists are gonna be out of jobs because of AI tools and then there's that another voice saying with different new tools there are different problems to solve there are new opportunities and you still need data scientists to customize solution Define the business problem so what's our take on this but it's hard to say exactly where tools and people are gonna jump into here but I I do believe that data as an evaluation process is going to be a much bigger thing that was true we're I think we are going to see some AI tools that are going to make it that maybe you're not typing SQL very much maybe you're leaning on some other tools to actually generate these queries but one of the things is that I think across the board we're just gonna we might see leaner teams but the magnitude the importance of the function is just going to increase yeah maybe we'll have linear team but there are more data teams because we don't have the barrier to entry anymore it's so easy to set it up because there's perfect things to measure totally because one of the things that I don't want to ever say AI will never be able to do this but something I think that AI will struggle with is defining correct canonical metrics because if you think of what does it take to do the entire transformations to come up with what exactly is an Airbnb booking confirmation yeah it's a lot of very bespoke local stuff right AI is very good at taking general knowledge and applying it stuff that's out there on internet if it's something like this particular code base in the history of this code base and especially code across services so it's not even in one code base it had a bunch of very bespoke decisions that created weird artifacts that have to be untangled by your data Engineers right that is something I think AI might have trouble with and that's something where I think for a long time data teams will be doing now everything from the moment of creating canonical tables clean artifacts that can be reused that is something where I think tools and AI will plug into at least of all EPO like you know a big part of what we're doing is sitting on top of those artifacts and giving you an experimentation culture so I I see data teams becoming more and more in the Weeds on developing clean artifacts and more consultative high-level consultative on everything that follows yeah so you worked at two companies in their early stage when you were a data scientist and I'm sure you worked with Engineers leaders product leaders so you're not just doing data science you have to communicate and sell your Solutions so how do you develop those communication skills yeah it's so important so important I think the one of the things about working in data is you should figure out are you on are you in the business of building production systems like an ml engineer or something like that or a data infrastructure engineer are you on that's what a path in which case your career might look more like an IC engineer type or are you in the path of influence are you trying to change decisions in the business are you trying to get people to invest in a certain area to drive a roadmap stuff that might look more like a PM or a marketer if you're doing marketing related work or Etc if you're in the path of influence it's good to just embrace it and not continue to treat yourself as like an engineer like I see a lot of kind of like analytics engineer types who like they they give that title and they lean into the engineer word and not into the analyst word where they need like it it's not enough to just spin up a dashboard and just say hey everyone self-serve off the dashboard the dashboard was created to drive a certain agenda and you need to pursue that agenda which means winning hearts and Minds how do you win hearts and Minds well there's the best ways to invest in relationships the best way to invest in relationships is to help be a sincere helper help people do their jobs make it so that every time you're collaborating with them they know you are here to help you're going to help them do their job you're going to be an asset which comes in a lot of different ways but the good thing about being a data person is that it's very easy to be helpful for people people always need more data yeah and you mentioned analytics engineer in Apple's website also saw you have statistics engineer so how is that what was that role and how is it different from a statistician yeah we made it up yeah it was a moment of branding because we feel an echo that there's a certain type of engineer if you're one of them that was the best place in the world to work and what it is we do some of the most advanced statistical methods in the world I think even in this young stage we've been able to drive the the agenda of experimentation practices at very high power places but we are also a software application where infrastructure which means that it's not enough to know the statistical methods and prototype them in a Jupiter notebook you have to submit production code you need to create the capabilities for the whole world and so what we needed were people were who were able to submit production code but also no statistics well and that de facto means either you're a great software engineer who knows and like stats or you're a great statistician who knows enough coding to contribute in a way I feel like it's a bit of a throwback when data science first emerged at places like LinkedIn and Facebook and places like that it had a whole lot of people who like knew stats but were fundamentally still Engineers I feel like as the tooling got better and we didn't have to like manually write mapreduce jobs in Java whatever and it became something more SQL oriented it started to become more data analyst oriented which is good because again if you want to drive a roadmap and an agenda that's a different skill set but this is a bit of a throwback to the overall the data science I was saying like what are the people who are great at building and also deeply understand the methods because yeah if you're one of those people like EPO is just an incredible place for you yeah and so you joined two early stage startups so how do you know they're going to be successful later were you lucky or did you have a strategy yeah air meal is definitely luck that one was I was early in my career so I didn't know too much about how to evaluate companies much less something that turned into like the Behemoth that it became that's a funny story about Airbnb the reason for others that I joined is I had a friend who worked there and we were going to meet up at a bar in San Francisco yeah and so I thought it was just being hit so I just shut up there and and then he shows up on a bus that has been decorated in themes like the Beatles it's called The Magical Mystery bus and she comes off of it with all of Airbnb into this bar and everyone's just in this ridiculous mood and it basically Takes Over the Bar and it was just like everyone was such a special culture it was so adventurous and quirky and weird and uh just like very empathetic and fun and so just hearing the culture and them hearing about the problems you know at that point you know airme had just gotten big enough where fraudsters were coming after him which is you know a big moment of becoming big league and so they needed someone who could create all these Machinery models to predict fraud and so it was a nice moment a cool technical problem and a great culture and so I joined it and then became to realize how powerful it is through work in the travel industry to uh work in a place that has big International Network effects and there's asset light and all these benefits now joining other startups I think if you were to join now I think the big thing you have to figure out is are you joining a company that has product Market fit or not if it doesn't have product Market fit and I'm kind of paraphrasing the the founder of post hog who admire who for his tweets about this but if a company does not have a product Market fit they should be keeping pretty lean being exploratory getting their way to product Market fit with a small type team that is capital efficient once you are at product Market fit that is when you can be someone more aggressive and pursue a growth strategy and that is about seizing as much of the market as you can and so when you look at these companies you have to like one figure out where they are in their Journey if they do have product Market fit then you better figure out like what is the size of the market they're operating under how big can this thing be and kind of what is my role in it so I I yeah that's some kind of simple things about how to evaluate companies yeah thanks for sharing that and when you think about your previous experiences what are some mistakes you made in your career let's see I think I think part of the things in my career was that early on in Airbnb I came there and I was having a lot of fun and working some of these problems but there was probably much more I could have done to try to rise up management tracks I think for example in the data world I think there's this kind of issue where there's not really such a thing as like a principal IC data scientist but it's not like Engineering in that way and so to kind of rise up a data team usually involves embracing management to some extent and so I was actively against managing at that stage I was like I really don't so I I think that was probably a mistake in retrospect is I should have just embraced that as a career possibility besides that it's hard to really freeze anything as a mistake because I think the one thing I've done pretty successfully in my career is that when I know something's not working I tend to move off of air pretty quickly and that can be whether the team internally in a company or whether it's the company itself and so long as you are moving off of your mistakes quickly it's hard to really call them mistakes yeah and what was the important feedback you got could be someone from someone you work with or a manager or your current employee yeah I think some types of feedback I got a lot of it was stuff that can seem small at the time but turn into a much bigger thing down the road for example when I was at Airbnb I was kind of working on this side project called the knowledge repo and it was this kind of collaborative notebook tool that was a way to take the sort of Jupiter notebooks that are markdowns and add a peer review process on top of it and render it in a web app so that became a knowledge base and I got encouragement from my manager to write about it do a blog post and eventually to open source it and it's the sort of thing it doesn't really rise you up any sort of career track because it doesn't there's no hear me bookings outcome that comes out of it but it ended up being huge for my career I think one of the most underrated things people can do is just write more about your job in public in blogs because all sorts of opportunities can find you when you write about topics that you know well that you become an expert in and then of course open sourcing a tool that has since gone on to have this incredible lifetime was a really big thing a similar thing happens with at Airbnb we like many other companies are logging events our event streams had a lot of dirtiness and they would break all the time and it caused a bunch of nightmares for everyone in the data world I was kind of poking around the edges of this project I was trying to essentially create what we now call data contracts things that are like you develop a schema of what a good event looks like and if some engineer breaks the schema in their local development it would break the build but actually it's not that and so that was another place where a manager encouraged me this actually would be really impactful if it got if it became a thing and so I started you know investing a lot in that and became part of that team so it was kind of nudges from managers that like of the various things you're poking around here's one that might be really important to you or really important to Airbnb that was really good guidance yeah thanks for sharing that and it's interesting you mentioned previously you're very against about becoming a manager and now you're a CEO of I know right how life changes how life changes yeah so how do you how did you handle this change it's been a big learning process I at this point I'm older and wiser right I was like 22 or something when I first had Airbnb so the biggest part of it is just having the Reps of having managed a bunch of people by now um so I think that's a lot easier but I think there's a lot of just if you treat being a good manager and not like just something you just read a book and you're done and it's just an ongoing craft of improving every day then but you can kind of make progress towards doing something that make sure your reports are having fulfilling careers that they experience growth they're satisfied they feel fulfilled mm-hmm are you hiring right now we are definitely hiring so we are firmly in a hot Market where we're kind of in a pole position to to accomplish everything we need to accomplish so we are hiring sales reps we're hiring Engineers or hiring designers pretty much across the board there's this incredible demand for experimentation technology right now like it's never been the case that Airbnb experimentation framework was on the market but something you can buy and bring into your own into your own company and much less a kind of more advanced version of it that like handles some of this guided support that kind of helps people feel confident in their decisions and socialize and evangelize so yeah it's very exciting times I feel experimentation has gone mainstream in a certain way like it's no longer the something that only like the things are doing it's now like basically the moment you have sample size it's imperative to do it and so it's exciting time to be in space yeah and for people who want to check out Apple where can they try it yeah just come to getepo.com www.geteppo.com so you can kind of see see our philosophy you can take it for a spin the preview and of course just get in touch I'm on LinkedIn I'm on Twitter if you just want to talk about experimentation topics in general we love doing knowledge sessions we have a statistics reading group we do every week where we talk about the latest and greatest in kind of stats papers I think people would be fascinated by just how much thinking is going on at these PHD programs in terms of like causal in France and how do you make these statistical programs more robust more powerful delivering insights faster while still maintaining complete rigor so we're constantly in that world talking about those topics so come check us out at getepo.com or Reach Out LinkedIn yeah great I'll put in a show notes so what do you think about the future of experimentation I think it the AI thing is just kind of the tip of the iceberg where we are now in a place where it's easier than ever to implement ideas and with the down Market people are just much more conscious of the ROI of everything um so it's this kind of went from a whole bunch of angles it's becoming more imperative to measure and evaluate and just today experimentation is just this incredible thing that gives you a clean read on evaluation if you think of all the other ways in which you might try to measure something a randomized controlled trial is just the purest form controls for all the other products going out all the other Concepts in marketing and IT like gives you this clean read now I think in the future as the cost of ideas goes down whether via gen AI or whatever else like we're going to see much more Dynamic experimentation systems stuff that like looks a little bit like personalization delivering dynamically experiences to the right people I think these type of systems are is definitely what EPO will be delivering so that's my bet and where everything's headed and how do you think data scientists should prepare for the future of experimentation and all together with the shift in Ai and large language model space yeah I think it's definitely a good time to start reading about these methods and something we have observed everywhere and at some point where we just we'll just see if there's some study but if you understand statistics and probability you understand what a t-test is and what it assumes and how it works and ideally you go from there and understand sequential methods and all a bunch of statistics like your salary is probably gonna be like 30 higher okay data teams are benefits so much from this types of inference knowledge and it's really important because again in a world where the cost of like the the value added known SQL really well might go down it could that'll make too many bets but if nothing else because there's a lot of SQL Engineers out there if not because AI is going to push into that differentiated knowledge and causal inference will be really good so I would definitely recommend investing in that or one of the more differentiated areas like data engineering or something like that cool I'm about to wrap up is there anything else you want to add no thanks for bringing me on it's been a pleasure talking to you and hearing about your Amazon background talking experimentation great thank you so much for having what it's gonna say Oh thanks thank you so much for coming to the show I learned a lot from our conversation and I'm excited to see how Apple grows in the future yeah thank you daliana it's been a pleasure being here foreign

The ultimate guide to A/B testing | Ronny Kohavi (Airbnb, Microsoft, Amazon)
I'm very clear that I'm a big fan of test everything which is any code change that you make any feature that you introduce has to be in some experiment because again I've observed this sort of surprising result that even small bug fixes even small changes can sometimes have surprising unexpected impact and so I don't think it's possible to experiment too much you have to allocate sometimes to these high risk High reward ideas we're going to try something that's most likely to fail but if it does win it's going to be a home run and you have to be ready to understand and agree that most will fail and it's amazing how many times I've seen people come up with new designs or a radical new idea and they believe in it and that's okay I'm just cautioning them all the time to say hey if you go for something big try it out but be ready to fail eighty percent of the time welcome to Lenny's podcast where I interview world-class product leaders and growth experts to learn from their hard-winning experiences building and growing today's most successful products today my guest is Ronnie kohavi Rani is seen by many as the world expert on a b testing and experimentation most recently usvp and Technical fellow of relevance at Airbnb where you led their search experience team prior to that he was corporate vice president and Microsoft where he led Microsoft's experimentation platform team before that he was director of data mining and personalization at Amazon he's currently a full-time advisor and instructor he's also the author of the go-to book on experimentation called trustworthy online controlled experiments and in our show notes you'll find a code to get a discount on taking his live cohort based course on Maven in our conversation we get super tactical about a b testing Ronnie shares his advice for when you should start considering running experiments at your company how to change your company's culture to be more experiment driven what are signs your experiments are potentially invalid why trust is the most important element of a successful experiment culture and platform how to get started if you want to start running experiments at your company it also explains what actually is a p-value and something called twyman's Law plus some hot takes about Airbnb and experiments in general this episode is for anyone who's interested in either creating an experiment driven culture at their company or just fine-tuning one that already exists enjoy this episode with Ronnie kohavi after a short word from our sponsors this episode is brought to you by mixpanel get deep insights into what your users are doing at every stage of the funnel at a fair price that scales as you grow mixpanel gives you quick answers about your users from awareness to acquisition through retention and by capturing website activity add data and multi-touch attribution right in mixpanel you can improve every aspect of the full user funnel powered by first party behavioral data instead of third-party cookies mixed panels built to be more powerful and easier to use than Google analytics explore plans for teams of every size and see what mixpanel can do for you at mixpanel.com friends slash Lenny and while you're at it they're also hiring so check it out at mixpanel.com friends slash Lenny this episode is brought to you by round round is the private Network built by Tech leaders for Tech leaders round combines the best of coaching learning and authentic relationships to help you identify where you want to go and accelerate your path to get there which is why their weightless tops thousands of tech execs round is on a mission to shape the future of technology and its impact on society leading in Tech is uniquely challenging and doing it well is easiest when surrounded by leaders who understand your day-to-day experiences when we're meeting and building relationships with the right people we're more likely to learn find new opportunities be dynamic in our thinking and Achieve our goals building and managing your network doesn't have to feel like networking join around to surround yourself with leaders from Tech's most Innovative companies build relationships The Inspired take action visit round.tech slash apply and use promo code Lenny to skip the waitlist that's round dot Tech slash apply thank you Ronnie welcome to the podcast thank you for having me so you're known by many as maybe the leading expert on a b testing and experimentation which I think is something every Product Company eventually ends up trying to do often badly and so I'm excited to dig quite deep into the world of experimentation and a b testing to help people run better experiments so thank you again for being here great goal thank you let me start with kind of a fun question what is maybe the most unexpected a B test you've run or maybe the most surprising result from an A B test that you run yeah so I think the the opening example that I use in my book and in my class is the most surprising public example we can talk about and this is this was kind of an interesting experiment somebody proposed to change the way that ads were displayed on Bing the search engine and he basically said let's take the second line and move it promote it to the first line so that the title line becomes larger and when you think about that and there's you know if you're going to look in my uh book or in the class There's an actual diagram of what happened uh the screenshots but if you think about it it's just realistically it looks like an idea like why would this be such a reasonable interesting thing to do uh and indeed when we went back to the backlog it was on the backlog for months and language there and many things were rated higher but the point about this is it's trivial to implement so if you think about return on investment we could get the data by having some Engineers spend a couple of hours implementing it and that's exactly what happened somebody uh Ed Bing who kept seeing this in the backlog and said my God we're spending too much time discussing it I could just Implement that he did he spent a couple of days implementing it and as is you know the common uh thing at Bing he launched the experiment uh and a funny thing happened we had an alarm big escalation something is wrong with the revenue metric now this alarm fired several times in the past when there were real mistakes where somebody would log Revenue twice or you know there's some data problem but in this case there was no bug we that simple idea increased Revenue by about 12 percent and this is something that just doesn't happen we can talk later about employment's law but that was the first reaction which is this is too good to be true let's find the bug uh and we did and we looked for several times and we replicated the experiment several times and there was nothing wrong with it this thing was worth a hundred million dollars at the time when Bing was a lot smaller and the key thing is it didn't hurt the user metric so it's very easy to increase Revenue by doing theatrics that you know displaying more ads is a trivial way to raise revenue but it hurts the user experience and we've done the experiments to show that in this case this was just a home run that improved Revenue didn't significantly hurt the the guardrail metrics and so I was we were like in awe of you know what a trivial change that was the biggest Revenue impact of being in all its history and that was basically shifting in two lines right switching two lines in the search results right and this was just moving the second line to the first line now you then go and run a lot of experiments to understand what happened here is it the fact that the title line has a bigger font sometimes different color so we ran a whole bunch of experiments and this is what usually happens we have a breakthrough you start to understand more about what can we do and there was suddenly a shift towards okay what are other things we could do that would allow us to improve Revenue we came up with a lot of follow-on ideas that helped a lot but to me this was an example of a tiny change that was the best Revenue generating idea in Bing's history and and rated properly right nobody gave this the the priority that in hindsight it deserves and that that's something that happens often I mean we are often humbled by how bad we are at predicting the outcome of experiments this reminds me of a classic experiment at Airbnb while I was there and we'll talk about Airbnb in a bit the search team just ran a small experiment of what if we were to open a new tab every time someone clicked on a search result instead of just going straight to that listing and that was one of the biggest wins in search yeah and by the way I I don't know if you know the history of this but I tell about this in class we did this experiment way back around 2008 I think and so this predates Airbnb and I remember it was heavily debated like why would you open something in a new tab the users didn't ask for it uh it was a lot of pushback from the designers and we ran that experiment and again it was one of these highly surprising results that made it that we learned so much from it so we first did this it was done in the UK for opening Hotmail and then we moved it to MSN so it would open search in Utah and all the set of experiments were highly highly beneficial we published this and I have to tell you when I came to Airbnb I talked to our joint friend Ricardo about this and it was sort of done it was very beneficial and that it was semi-forgotten which is one of the things you learned about institutional memories when you have winners make sure to address them and remember them so it was an Airbnb done for a long time before I joined that listings opened in a new tab but other things that were designed in the future were not done and I reintroduced this to the team and we saw big improvements shout out to Ricardo or mutual friend who helped make this conversation happen there's this like holy grail of experiments that I think people are always looking for of like one you know hour of work and it creates this massive result I imagine this is very rare and uh don't expect this to happen I guess in your experience how often do you find kind of one of these gold nuggets just lying around yeah so again this is a topic that's uh near and dear to my heart everybody wants these amazing results and you know I show them in chapter one in my book multiple of these you know small efforts huge gain but as you said they're very rare I think most of the time the winnings are made sort of this inch by inch and there's a graph that I show in my book a real graph of how Bing ads has managed to improve their revenue per thousand searches over time and every month you can see a small Improvement and a small Improvement sometimes a degradation because of legal reasons or other things we you know there were some concern that we were not marking the ads properly so you have to suddenly do something that you know is going to hurt Revenue but yes I think most results are inch by inch you improve small amounts lots of them I think they're the best example that I can say is a couple of them that I can speak about one is at Bing the relevance team hundreds of people all working to improve being relevant they have a a metric we'll talk about oh we see the overall evaluation Criterion but they have a metric that their goal is to improve it by two percent every year it's a small amount and that two percent you can see here's a point one and here's a 0.15 here's a 0.2 and then they add up to around two percent every year which is amazing another example that I am allowed to speak about uh from Airbnb is the fact that we ran some 250 experiments in my tenure there in search relevance and again small improvements added up so this became overall a six percent Improvement to revenue you know so when you think about six percent it's a big number but it became it came out not of one idea but many many smaller ideas that each gave you a small gain and in fact I again there's another number I'm allowed to say of these experiments 92 percent failed to improve the metric that we were trying to move so only eight percent of our ideas actually were successful at moving the key metrics there's so many threads I want to follow here but let me follow this one right here you just mentioned of 92 percent of experiments failed is that typical in your experience running seeing experiments run a lot of companies like what should people expect when they're running experiments what percentage do they expect to fill well first of all I published three different numbers for my career so overall at Microsoft about 66 two-thirds of ideas fail right and don't take the 66 is accurate like you know it's about two-thirds and Bing which is a much more optimized domain after we've been optimizing it for a while the failure rate was around 85 so it's harder to improve something you've been optimizing for a while and then at Airbnb uh this 92 number is you know the highest failure rate that I've observed now I've quoted other sources that you know it's not that I worked at groups that were particularly bad uh booking uh Google ads other companies published numbers there are around 80 to 90 percent failure rate of ideas here this is where it's important of experiments it is important to realize that when you have a platform it's easy to get this number you look at how many experiments were run and how many of them launched not every experiment maps to an idea so it's possible that when you have an idea your first implementation you start an experiment boom it's egregiously bad because you have a bug in fact 10 percent of experiments tend to be aborted on the first date those are usually not that the idea is bad but that there is an implementation issue or something we haven't thought about that forces on a board you may iterate and pivot again and ultimately if you do two or three or four pivots or bug fixes you may get to a successful launch but those numbers of 80 to 92 percent failure rate are of experiments very humbling I know that every group that starts to run experiments they always start off by thinking that somehow they're different and their successor is going to be much much higher and they're all humbled you mentioned that you had this uh pattern of clicking a link and opening a new tab is a thing that just worked at a lot of different places yeah are there other versions of this you collect kind of a list of like here's things that often work when we want to move yeah are there some some you could share I don't know if you have a list in your head like you give you two resources uh one of them is a paper that we wrote called rules of thumb and what we tried to do at that time at Microsoft was to just look at thousands of experiments that run and extract some patterns and so that's that's one paper that uh we can then put in the nodes perfect um but there's another more more accurate I would say uh resource that's useful that I recommend to people and it's a site called goodui.org and good ui.org is exactly the site that tries to do what you're saying at scale so guys the name is Jacob linovsky he asks people to send them results of experiments and he derives he puts them into patterns there's probably like 140 patterns I think at this point and then for each pattern he says um well who hasn't helped how many times and by how much so you have an idea of you know this worked three out of five times and it was a huge win in fact you can find that open a new window in there I feel like you feed that into chat GPT and you have basically a product manager creating a roadmap uh tool in general by the way this is all about a lot of that is institutional memory right which is can you document things well enough so that the organization remembers the successes and failures and learns from them I think one of the mistakes that some company makes is they launch a lot of experiments and never go back and summarize the learnings so I've actually put a lot of effort in this idea of institutional learning of doing the quarterly meeting of the most surprising experiments by the way surprising is another uh question to people uh often are not clear about what is a surprising experiment to me a surprising experiment is one where the estimated resolve beforehand and the actual result differ by a lot so that absolute value of the difference is large now you can expect something to be great and it's flat well you learn something but if you expect something to be small and it turns out to be great like that ad title uh promotion then you've learned a lot or conversely if you expected something will be small and it's very negative you can learn a lot by understanding why this was so negative and that's interesting so we focus not just on the winners but also surprising losers things that people thought would be a no-brainer to run and then for some reason it was very negative and sometimes it's that negative that gives you and so I'll actually you know I'm just coming up with one example of that that I should mention we were running this experiment at Microsoft to improve the windows indexer and the team was able to show on offline test that it does much better at indexing and you know they showed some relevance is higher and all these good things and then they ran it as an experiment you know what happened surprising result indexing the relevance was actually high but it killed the battery life um so here's something that comes from Left Field that you didn't expect it was consuming a lot more CPU on laptops it was killing the laptops and therefore okay we learned something let's document it let's remember this so that you know we now take this other factor into account as we design the next iteration what advice do you have for people to actually remember these surprises you said that a lot of it is institutional what do you recommend people do so that they can actually remember this when people leave say three years later document that you know right you know we had a large deck internally of these successes and failures and we encourage people to look at them the other thing that's very beneficial is just to have your whole history of experiments and do some ability to search by keywords right so I'm I have an idea type a few keywords and see if from the thousands of experiments that ran and by the way these are very reasonable numbers at Microsoft just to let you know when I left in 2019 we were on a rate of about 20 to 25 000 experiments every year so every working day we were starting something like a hundred new treatments big numbers so when you're running in a group like Bing which is running thousands and thousands of experiments you want to be able to ask has anybody did an experiment on this or this or this and so that searching capability is in the platform but more than that I think just doing the quarterly meeting of the most successful most interesting sorry not just successful most interesting experiments is very key and that also helps the flywheel of experimentation this will get segue to something I wanted to touch on which is there's often a I guess a weariness of running too many experiments and being too data driven and the sense that experimentation just leads you to these micro optimizations and you don't really innovate and do big do big things what's your perspective on that and then just can you be too experiment driven in your experience I'm very clear that I'm a big fan of test everything which is any code change that you make any feature that you introduce has to be in some experiment because again I've observed this sort of surprising result that even small bug fixes even small changes can sometimes have surprising unexpected impact and so I don't think it's possible to experiment too much I think it is possible to focus on incremental changes because some people say well you know if we only tested 17 things around this and you have to think about it's not just it's like in stock you need a portfolio you need some experiments that are incremental that move you in the direction that you know you're going to be successful over time if you just try enough but some experiments have you have to allocate sometimes to these high risk High reward ideas we're going to try something that's most likely to fail but if it does win it's going to be a home run and so you have to allocate some efforts to that and you have to be ready to understand and agree that most will fail most of these high enough it's amazing how many dimes I've seen people come up with new designs or a radical new idea and they believe in it and that's okay I'm just cautioning them all the time to say hey if you go for something big try it out but be ready to fail eighty percent of the time right and one true example that again I'm able to talk about because we put it in my book is we were at Bing trying to change the landscape of search and one of the ideas the Big Ideas was we're going to integrate with social so we hooked into the Tweeter fire hose feed and we hooked it to Facebook and we spent a hundred percent years on this idea and it failed you don't see it anymore it existed for about a year and a half and all the experiments were just negative to flat and you know it was an attempt it was fair to try it I think it took us a little long to fail to decide if that this is a failure but at least we had the data we had hundreds of experiments that we tried none of them were a breakthrough and I remember sort of mailing uh chilu with some statistics showing that you know it's time to abort it's time to fail on this uh and you know he decided to continue more and it's a million dollar question you know you continue and then maybe the breakthrough will come next month or uh do you abort and a few a few months later we we aborted that reminds me of at Netflix they tried a social component that also failed at Airbnb early on there was a big social attempt to make like here's your friends have stayed at these airbnbs completely not had no impact so maybe that's one of these learnings that we should talk yeah this is hard this is hard and uh but that's again that's the value of experiments which are this Oracle that gives you the data you may be excited about things you might believe it's a good idea But ultimately the Arbiter the Oracle is the controlled experiment it tells you whether users are actually benefiting from it whether you and the users the company and the users there's obviously a bit of overhead and downsides running an experiment setting all up and making sure you know analyzing the results is there anything that you ever don't think is worth a b testing first of all there are some necessary ingredients to a b testing and I'll just say I'll write not every domain is a minimal thing to be testing right you can't a B test mergers and Acquisitions right it's something that happens once you either acquire you don't acquire so you do have to have some necessary ingredient you need to have enough units mostly users in order for the statistics to work out so yeah if you're too small it may be too early to a B test but what I find is that in software it is so easy to run a b testing and it is so easy to build a platform I don't say it's easy to build a platform but once you build the platform the incremental cost of running an experiment should approach zero and we got to that at Microsoft where after a while the cost of running experiments was so low that nobody was questioning the idea that everything should be experimented with now I don't think we were there at Airbnb for example the platform at Airbnb was much less mature and required a lot more analysts in order to interpret the results and to find issues with it so I do think there's this trade-off you're willing to invest in the platform it is possible to get the marginal cost to be close to zero but when you're not there it's still expensive and there are there may be reasons why not to run AB thefts you talked about how you may be too small to run a b tests and this is a constant question for startups is when should we start running a b tests do you have kind of a heuristic or rule of thumb of just like here's a time you should really start thinking about running an image a million dollar question that everybody asks so I actually we'll put this in the notes but I gave a talk last year what I called it is practical defaults and one of the things I show there is that unless you have at least tens of thousands of users the math the statistics just don't work out for most of the metrics that you're interested in in fact you know I gave an actual practical number of a retail site with some conversion rate trying to detect changes that are at least you know five percent beneficial which is something that startups should focus on they shouldn't focus on the one percent they should focus on the five and ten percent then you need something like 200 000 users right so start experimenting when you're in the tens of thousands of users you'll be only be able to detect large effects and then once you get to 200 000 users then the magic starts happening then you can start testing a lot more then you have the ability to test everything and make sure that you're not degrading uh and getting value out of response so you ask for rule of thumb 200 000 users you're magical below that start building the culture start building the platform start integrating so that as you scale you you start to see the value love it coming back to this kind of concern people have of experimentation keeps you from innovating and taking big bets I know you have this framework uh overall evaluation Criterion and I think that helps with this can you talk a bit about that the oec or the overall evaluation criteria is something that I think many people that start to dabble in a b testing Miss and the question is what are you optimizing for and it's a much harder question that people think because it's very easy to say we're going to optimize for money Revenue but that's the wrong question because you can do a lot of bad things that will improve Revenue so there has to be some countervailing Metric that tells you how do I improve Revenue without hurting the user experience okay so let's take a good example uh with search you can put more ads on the page and you will make more money there's no doubt about it you will make more money in the short term the question is what happens to the user experience and how is that going to impact you in the long term so we've run those experiments and we were able to map out you know this number of ads causes this much increase to churn this number of ads causes this much increase to the time that users take to find a successful result and we came up with an oec that is based on of these metrics that allows you to say okay I'm willing to take this additional money if I'm not hurting the user experience by more than this much right so there's a trade-off there one of the nice ways to phrase this as a constraint optimization problem I want you to increase Revenue but I'm going to give you a fixed amount of average real estate that you can use right so you can for one query you can have zero ads for another query you can have three ads for a third query you can have wider bigger ads I'm just going to count the pixels that you take the vertical pixels and I will give you some budget and if you can under the same budget make more money you're good to go right so that to me turns the problem from a badly defined let's just make more money right any page can start Plastering more ads and make more money short term but that's not the goal the goal is long-term growth and revenue then you need to insert these other criteria and what am I doing through the user experience one way around it is to put this constraint another one is just to have these other metrics again something that we did to look at the user experience how long does it take the user to reach a successful click what percentage of sessions are successful these are key metrics that were part of the overall evaluation Criterion that we've used I can give you another example by the way from you know the hotel industry or Airbnb that we both worked at um you can say I want to improve conversion rate but you can be smarter about it and say it's not just enough to convert a user to buy or to pay for a listing I want them to be happy with it several months down the road when they actually stay there right so that could be part of your oec to say what is the rating that they will give to that listing when they actually stay there and that's a that causes an interesting problem because you don't have this data now you're gonna have it three months from now when they actually stay so you have to build the training set that allows you to make a prediction about whether this user whether Lenny is going to be happy at this cheap place or where they know I should offer him something more expensive because Lenny likes to stay at nicer places where the water actually is hot and comes out of the faucet that is true okay so it sounds like the courts of this approach is basically have a kind of a drag metric that makes sure you're not hurting something that's really important to the business and then being very clear on what's the long-term metric we care most about to me the key here the key word is lifetime value which is you have to define the oec such that it is causally predictive of the lifetime value of the user right and that that's what causes you to think about things properly which is am I doing something that just helps me short term or am I doing something that will help me in the long term once you put that model of lifetime value people say okay what about retention rates you can measure that what about the time to achieve a task we can measure that and those are these countervailing metrics that make it make the oec useful and to understand these longer term metrics what I'm hearing is use kind of models and forecasts and predictions or would you suggest sometimes use like a long-term holdout or some other approach like what do you find is the best way to think see these well so there's two ways that I like to think about it one is you can run long term experiments for the goal of learning something so I mentioned that at Bing we did run these experiments where we increased the odds and decreased the odds so that we will understand what happens to key metrics the other thing is you can just build models that use some of our background knowledge or use some you know data science to look historical I'll give you another good example of this when I came to Amazon one of the teams uh that reported to me was the email team that it was not the transactional emails when you buy something you get an email but was the team that sent these recommendations you know here's a book by an author that you bought here's a product that we recommend and the question is how do we give credit to that team and the initial version was well whenever a user comes from the email and purchases something on Amazon we're gonna give that email credit well it turned out this had no counter available metric the more emails you send the more money you're going to credit the theme and so that led to spam literally a really interesting problem the team just ramped up the number of emails that they were sending out and claimed to make more money in their Fitness function uh improved and and then so then we backed up and then we said okay we can either phrase this as a constraint satisfaction problem you're allowed to send user and email every X days or which is what we ended up doing is let's model the cost of spamming the users okay what's that cost well when they unsubscribe we can't mail them okay so we did some data science study on the side and we said what is the value that we're losing from an unsubscribe right we came up with a number it's a few dollars but the point was now we have this counter availity metric we say here's the money that we generate from the emails here's the money that we're losing on long-term value what's the trade-off and then when we started to incorporate those formula more than half the campaigns that were being sent were negative hmm okay so it was a huge Insight uh at Amazon about how to send the right campaigns and this LED and this is what I like about these discoveries this fact that we integrated the unsubscribe led us to a new feature to say well let's not lose their future lifetime value through email when they unsubscribe let's offer them by default to unsubscribe from this campaign so when you get an email uh you know there's a new book by the author the default to unsubscribe would be unsubscribe me from author emails and so now the the negative the countervailing metric is much smaller and so again this was a breakthrough in our ability to send more emails and understand based on what users were unsubscribing from which ones are really beneficial I love the surprising results we all love them I mean this is this is The Humbling reality and you know people talk about the fact that a b testing sometimes leads you to incremental I actually think that many of these small insights lead to fundamental insights about you know which areas to go some strategies we should take some things we should develop helps a lot this makes me think about how every time I've done a full redesign of a product I don't think ever has it ever been a positive result and then the team always ends up having to claw back what they just hurt and try to figure out what they messed up is that your experience too absolutely yeah in fact I I've published uh some of these in in LinkedIn posts showing a large set of you know big launches that redesigns that dramatically failed uh and it happens very often so the right way to do this is to say yes we want to do a redesign but let's do it in steps and test on the way and adjust so you don't need to take 17 new changes that many of them are going to fail start to move incrementally in a direction that you believe is beneficial just on the way the worst part of that those experiences I find is it took like I don't know three six months three to six months to build it and by the time it's launched it's just like we're not gonna unlaunch this everyone's been working in this Direction all the new features are assuming this is going to work and you're basically stuck right I mean this is the sun cost fallacy right we invested so many years in it let's launch this even though it's bad for the user no that's terrible yeah yeah so uh this is this is the other advantage of of recognizing this humble reality that most ideas fail right if if you believe in that statistics that I published then doing 17 changes together is more likely to be negative do them in smaller increments learn from it's called o-fat one factor at a time do one factor learn from it and adjust of the 17 maybe you have four good ideas those are the ones that will launch and be positive I generally agree with that and always try to avoid a big redesign but it's hard to avoid them completely there's often team members that are really passionate and like we just need to rethink this whole experience we're not going to incrementally get there have you found anything effective in helping people either see this perspective or just making a larger bet more successful by the way I I'm not opposed to large redesigns I try to give the team the data to say look here are lots of examples where big redesigns fail try to decompose your redesign if you can't decompose it the one factor that I do a small set of factors at a time and learn from these smaller changes what works and what doesn't now it's also possible to do a complete redesign I'm just as you said yourself they be ready to fail right I mean do you do you really want to work on something for six months or a year and then run the A B test and realize that you've hurt revenues or other key metrics by several percentage points and a data driven organization will not allow you to launch what are you going to write in your annual review yeah but nobody ever thinks it's going to fail I think no we got this or we've talked to so many people but I think organizations that start to run experiments are humbled early on from the smaller changes yeah right you're right nobody I'll tell you a funny story when I came from Amazon to Microsoft I joined the group and for one reason or another that group disbanded a month after I joined and so people came to me and said look you just joined the company you're a partner level you figure out how you can help Microsoft and I said I'm going to build an experimentation platform because nobody in Microsoft is running experiments and 50 more than 50 of ideas at Amazon that we tried fail in the classical response was we have better PMS here right there was this complete denial that it's possible that 50 of ideas that Microsoft is implementing in a three-year development cycle by the way this is how long it took office to release it was a classical every three years we release and the the data came about showing that uh yeah you know Bing was the first to truly Implement experimentation at scale and we share with the rest of the companies the surprising results and so when office was and this was you know credited to chilu and uh Sacha Nadella they were ones that says Ronnie you know you try to get office to run experiments we'll give you the air support uh and it was hard but we did it you know it was it took a while but office started to run experiments and they realized that many of their ideas were failing he said that there's a site of a failed redesigns was that is that in your book or is that a site that you can point people to to kind of help build this case but I teach this in my class but I I think I've posted this on LinkedIn and answered to some questions I'm happy to put that in the notes okay cool we'll put that in the show notes because I think that's the kind of uh data that often helps convince a team maybe we shouldn't rethink this entire onboarding flow from scratch maybe we should kind of iterate towards and learn as we go this episode is brought to you by EPO EPO is a Next Generation a b testing platform built by Airbnb alums for modern growth teams companies like DraftKings zapier click up twitch and Cameo rely on EPO to power their experiments wherever you work running experiments is increasingly essential but there are no commercial tools that integrate with a modern grow team stack this leads to wasted time building internal tools or trying to run your own experiments through a clunky marketing tool when I was at Airbnb one of the things that I loved most about working there was our experimentation platform where I was able to slice and dice data by device types country user stage EPO does all that and more delivering results quickly avoiding annoying prolonged analytic cycles and helping you easily get to the root cause of any issue you discover Apple lets you go beyond basic click-through metrics and instead use your North Star metrics like activation retention subscription and payments EPO supports tests on the front end on the back end email marketing even machine learning clients check out app people at geteppto.com that's getepo.com and 10x your experiment velocity is it ever worth just going let's just rethink this whole thing and just give it a shot to break out of a local Minima or local Maxima essentially so I think what you said is fair I mean I I do want to allocate some percentage of resources to Big bets as you said we've been optimizing this thing to Hell could be completely redesigned it it's a very valid idea you may be able to break out of a local Minima what I'm telling you is 80 of the time you will fail so be ready for that right what people usually expect is my redesign is going to work no you're most likely going to fail but if you do succeed it's a breakthrough I like this 80 rule of thumb is that just like a simple way of thinking about it eighty percent yeah rule of thumb and you know you've had uh you know I've heard people say it's 70 or or 80 but it's in that area where I think you know when you talk about how much to invest in the known versus the high risk High reward that's usually the right percentage that most organizations end up doing this allocation right you interviewed treas I think he mentioned that uh you know that Google is like 70 percent you know the searching ads and it's a 20 for some of the apps and new stuff and then it's the 10 for infrastructure yeah I think the the most important point there is if you're not writing an experiment 70 of stuff you're shipping is hurting your business well it's not hurting it may it's flat too negative some of them are flat uh and by the way flat to me if something is not that Sig that's a no ship because you've just introduced more code there is a maintenance overhead to shipping your stuff I've heard people say look we already spent all this time the team will be demotivated if we don't ship it and no that's wrong guys right you know let's make sure that we understand that shipping this project has no value is complicating the code Base maintenance costs will go up you don't ship on flat unless it's a sort of a legal requirement right when legal comes along and says you have to do X or Y you have to ship on flat or even negative and that's understandable but again I think that's something that a lot of people make the mistake of saying legal told us we have to do this therefore we're going to take the hits no legal gave you a framework that you have to work under try three different things and ship the one that hurts the least I love that reminds me when Airbnb launched the Rebrand even that they ran as an experiment with the entire homepage redesigned the new logo and all that and I think there's a long-term holdout even and I think it was positive in the end from what I remember speaking of Airbnb I want to chat about Airbnb briefly I know there's and you're limited in what you can share but uh it's interesting that Airbnb seems to be moving in this other direction where it's becoming a lot more top-down Brian Vision oriented and Brian's even talked about how he's less motivated through an experiments he doesn't want to run as many experiments as they used to things are going well and so you know it's hard to argue with the success potentially you worked there for many years you ran the search team essentially I guess just what was your experience like there and then roughly what's your sense of how things are going where it's going so as you as you know I'm restricted from talking about Airbnb I will say a few things that I am allowed to say one is in my team in search relevance everything was a b tested so while Brian can focus on some of the design aspects that people who are actually doing you know the neural networks and the search everything was a b tested to help so nothing was launching without an A B test we had targets around improving uh certain metrics and everything was done a B test now other teams some did some did not I will say that you know when you say things are going well I think we don't know the counter factual I believe that head Airbnb kept people like Greg really which was pushing for a lot more data driven and had Airbnb run more experiments it would have been in a better State than today but it's the counter factual we don't know it's a really interesting perspective yeah there may be such an interesting natural experiment of a way of doing things differently there's like de-emphasizing experiments and also they turned off paid ads during covet and I think I don't know where it is now but it feels like it's become a much smaller part of the growth strategy who knows if they've ramped it up to back to where it is today but I think it's going to be a really interesting case study looking back I don't know 5-10 years from now it's a one-off experiment where it's hard to assign value to some of the things that Airbnb is doing I personally believe it could have been a lot bigger and a lot more successful if it had run more controlled experiments but I can't speak about some of those that I ran and that showed uh that some of the things that were initially untested were actually negative and could be better all right mysterious one more question Airbnb you were there during kovid which was quite a wild time for Airbnb we had sunshine on the podcast talking about all the craziness that went on when travel basically stopped and there was a sense that Airbnb was done and travel is not going to happen for years and years what's your take on experimentation in that world where you have to really move fast make crazy decisions make big decisions what did you what was like during that time so I I think actually in a state like that it's even more important to run a b tests right because what you want to be able to see is if we're making this change is it actually helping in the current environment you know there's this idea of external generalizability is it going to work out now during covet is it going to generalize later on these are things that you can really answer with the controlled experiment so sometimes it means that you might have to replicate them six months down when covid say uh is not as impactful as it is saying that you have to make decisions quickly to me I'll point you to the success rate like if if in peacetime you're wrong two-thirds to 80 of the time why would you be subtly right in Wartime right until the time so uh I I don't believe in the idea that because bookings went down materially the company should certainly you know not be data driven and do things differently I think if Airbnb stayed the course did nothing the revenue would have gone up in the same way in fact if you look at one investment one big investment that was done at the time was online experiences and the initial data wasn't very promising and I think today it's a footnote yeah whatever another case study for the history books Airbnb experiences I want to shift a little bit and talk about your book which you mentioned a couple of times it's called trustworthy online controlled experiments and I think it's basically the book on a b testing let me ask you just uh what surprised you most about writing this book and putting it out in in the reaction to it I was pleasantly surprised that it sold more than what we thought and more came more than what Cambridge predicted so when when first we were approached by Cambridge after a tutorial that we did to write a book I was like I don't know this is too small of an inch area yeah um and I uh you know they were saying so you'll be able to sell a few thousand copies and help the world and uh I found you know my co-authors which were great and you know we wrote a book that we thought is not statistically oriented has fewer formulas than you normally see and focuses on the Practical aspects and on trust which is the key the book as I said you know was more successful it sold over 20 000 copies in English it was translated to Chinese Korean Japanese and Russian and so it's it's great to see that we helped the world become more data driven with experimentation and I'm happy because of that and I was pleasantly surprised by the way all proceeds from the book are donated to Charities on if I'm pitching the book here uh I there is no financial gain for me uh from having more copies sold I think we made that decision which was a good decision all proceeds go with the charity amazing I didn't know that we'll link to the book in the show notes you talked about how trust like it's trust is in the title you just mentioned how important trust is to experimentation a lot of people talk about how do I run experiments faster you focus a lot on trust why is trust so important in writing experiments so to me the experimentation platform is the safety net and it's an oracle so it serves really two purposes the safety net means that if you launch something bad you should be able to abort quickly right safe deployments say velocity there are some names for this but this is one key value that the platform can give you the other one which is the more standard one is at the end of the two-week experiment we will tell you what happened to your key metric and do many of the others surrogen and debugging and guardrail metrics trust builds up it's easy to lose and so to me it is very important that when you present this and say this is science this is a control experiment this is the result you better believe that this is trustworthy and so I focus on that a lot I think it allowed us to gain the organizational trust that this is really and the nice thing is when we we built all these checks to make sure that the experiment is correct if there was something wrong with it we would stop and say hey something is wrong with the experiment and I think that's something that some of the early implementations in other places did not do and it was a big mistake I mentioned this in my book so I can mention this here optimizely in its early days were very statistically naive they sort of said hey we're real time we can compute your P values in real time uh and then you can stop an experiment when the p-value is statistically significant that is a big mistake that inflates your what's called type one error or the false positive rate materially so if you think you've got a five percent type one error or you aim for that p-value less than 0.05 using real-time sort of p-value monitoring to optimize the offered you would probably have a 30 percent error rate so what this led is that people that started using optimizely thought that the platform was telling them they're very successful but when they actually started to see while it told us this is positive Revenue but I don't see this over time like by now we should have made double their money uh so their questions started to come up around the trust in the platform there's a very famous post that some of you wrote about how optimized they almost got me fired by a person who basically said look I came to the organ I said we have all these successes but then I said something is wrong and he tells them how he ran an AAA Test when there is no difference between the A and the B and optimizely told them that it was statistically significant too many times optimizely learned optimizely you know several people pointed I pointed this out uh in my Amazon review of the book uh that the optimize the authors wrote early on I said hey you're not doing the statistics correctly other you know Ramesh Johari at Stanford pointed this out became a consultant of the company and then they fixed it but to me that's a very good example of how to lose trust they allowed a lot of trust in the market they lost all this trust because they built something that had very much inflated erroring that is uh pretty scary to think about you've been running all these experiments and they weren't actually telling you accurate results what are signs that what you're doing may not be valid uh if you're starting to run experiments and then just how do you avoid having that situation what kind of tips can you share for people trying to run experiments you know there's a whole chapter of that in my book but I'll say maybe one of the things that is the most common occurs by far which is a sample ratio mismatch now what is the sample ratio mismatch if you design the experiment to send 50 of users to control and 50 of users per treatment supposed to be a random number uh or you know a hash function if you get something off from 50 percent it's a red flag so let's take a real example uh let's say you're running an experiment and it's large it's got a million users and you got 50.2 so people say well I don't know it's not going to be exactly the same as 50.2 reasonable or not well there's a formula that you can plug in I have a spreadsheet available for those that are interested and you can tell here's how many users are in control here's how many users have in treatment my design was 50 50 and it tells you the probability that this could have happened by chance now in a case like this you plug in the numbers it might tell you that this should happen one in half a million experiments well unless you've run half a million experiment very unlikely that you would get a 50.2 versus 49.8 split and therefore something is wrong with the experiment right now people I remember when we first implemented this check we were surprised to see how many experiments suffered from this right and there's a paper that was published like 2018 and where we share that and Microsoft even though we'd be running experiments for a while is around eight percent of experiments that suffered from the sample ratio mismatch and it's a big number I think about this you're running 20 000 experiments a year so many of them eight percent of them are invalid and we somebody has to go down and understand what happened here we know that we can't trust the results but why so over time you begin to understand there's something wrong with the pi data pipeline there's something that happens with Bots Bots are a very common factor for causing uh sample ratio mismatch um so there's a whole that paper that was published by my team talks about how to diagnose sample ratio mismatches in the last about a year and a half it was amazing to see all these third-party companies Implement sample ratio mismatches and all of them were reporting oh my God you know six percent eight percent ten percent uh so yeah they were it's sometimes fun to go back and say how many of your results are in the past or invalid before you had this sample ratio mismatch test yeah that's frightening is the most common reason this happens is you're signing users in in kind of the wrong place in your in your code So when you say most common I think the most common is Bots somehow they hit the controller the treatment in different proportions because you changed the website the bot may fail to parse the page and try to hit it more often that's a classical example another one is just a data pipeline um we've had cases where we were trying to remove bad traffic under certain conditions and it was skewed because of the control and treatment I've seen people that start an experiment in the middle of the site on some page but they don't realize that some campaign is pushing people uh from the side so there's multiple reasons it is surprising how often this happens and I'll tell you a funny story which is when we first added this test to the platform we just put a banner saying you have a sample ratio mismatch do not trust these results and we noticed that people ignored it okay we're starting to present results that had this banner and so we blanked out the scorecard we put a big you know red can't see this result you have a sample ratio mismatch click ok to expose the results and why we do we need that okay we need that okay button because you want to be able to debug the reasons and sometimes the metrics help you understand why you have a sample ratio mismatch so we blanked out the scorecard we have this button and then we started to see that people press the button is still presented the results of experiments with sample ratio of this method so we ended up with an amazing compromise which is every number in the scorecard was highlighted with a red line so that if you took a screenshot other people could tell you that a sample ratio mismatch freaking product managers this is this is uh intuition people just say wow my Instagram was small therefore I can still present the results people want to see success I mean this is a natural bias and then we have to be very conscientious and fight that bias and say when something looks too good to be true investigate which is a great segue to something you mentioned briefly uh something called toyman's law yeah can you talk about that yeah you know the the general statement is if any figure that looks interesting or different is usually wrong uh it was first said by this person in the UK who worked in radio media but I'm a big fan of it and uh you know my main claim to people is if the result looks too good to be true if you suddenly moved your you know your normal movement of an experiment is under one percent and you suddenly have a 10 movement hold the celebratory there like it was just your first reaction right let's take everybody to fancy dinner because we just improved Revenue by millions of dollars hold that dinner investigate see because there's a large probability that something is wrong with the result and I will say that nine out of ten when we call out time is law it is the case that we find some flaw in the experiment now there are obviously outliers right that first experiment that I shared where we promoted that made long ad titles that was successful but that was replicated multiple times in double and triple checked and everything was good about it many other results that were so big turn out to be false so I'm a big I'm a big big fan of Foreigner's law there's a deck I could also give this in the note where I shared some real examples uh of toymouth love amazing I want to talk about rolling this out of companies and things that run you run into that fail but before I get to that I'd love for you to explain just p-value I know that people kind of misunderstand it and this might be a good time just help people understand what is it actually telling you that p-value say 0.05 I don't know if this is the right forum for explaining p-values because the definition of a p-value is simple what it hides is very complicated so I'll say one thing which is many people assign one minus p-value as the probability that your treatment is better than control so you run an experiment you got a p-value of 0.02 they think there's a 98 probability that the treatment is better than the control that is wrong okay so rather than defining B values I want to cautiously caution everybody that the most common interpretation is incorrect p-value assumes it's a conditional probability or an us or assumed probability it assumes that the null hypothesis is true and we're Computing the probability that the data we're seeing it matches the hypothesis this null hypothesis in order to get the probability that most people want we need to apply Bayes Rule and invert the probability from the probability of the data given the hypothesis to the probability the hypothesis given the data for that we need an additional number which is the probability the prior probability that the hypothesis that you're testing is uh successful or not that's an unknown what we do is we can take historical data and say look people fail two-thirds of the time or eighty percent of the time and we can apply that number and compute that we've done that in a paper that I will give in the notes so that you can assess the number that you really want that what's called a false positive risk so I think that's something for people to internalize that what you really want to look at is this false positive risk which tends to be much much higher than the five percent that people think right so if you're I think the classical example in the Airbnb where the failure rate was very very high is that when you get a statistically significant result let me actually pull the node so that I know how to have the actual number if you're at Airbnb where the success rate of or Airbnb search where the success rate is only eight percent if you get a statistically significant result with the p-value less than 0.05 there's a 26 percent chance that this is a false positive result right it's not five percent it's 26 so that's the number that you should have in your mind and that's why when I worked at Airbnb one of the things we did is we said okay if you're less than 0.05 but above 0.01 rerun replica when you replicate you can combine the two experiments and get a combined p-value uh using something called Fisher's method or Snuffer's method and that gives you the joint probability and that's usually much much lower so if you get 2.05 or something like that then the joint the probability that you've got them is much much lower wow I have never heard it described that way makes me think about how like even data scientists in our teams are always just like this isn't perfect like we're not 100 sure this experiment is positive but on balance if we're launching positive experiments we're probably doing good things it's okay if sometimes we're wrong by the way it's true on balance you're probably better than 50 50. but people don't appreciate how much that 26 that I mentioned is high and the reason that I want to be sure is that I think it leads to this idea of the learning the institutional knowledge what you want to be able to say is share with the organ success and so you want to be really sure that you're successful so by lowering the p-value by forcing teams to work with the p-value maybe below 0.01 and do replication on hires then you can be much more successful and and the false positive rate will be much much lower fascinating and also shows the value of keeping track of just what percent of your experiments are failing historically at the company or within that specific product say someone listening wants to start running experiments they say they have tens of thousands of users at this point what would be the first couple steps you'd recommend well so if they have somebody in the org that has previously been involved experiment that's a good way to consult internally uh I think the the key decision is whether you want to build or buy there's a whole series of eight sessions that I posted on LinkedIn where I invited guest speakers to talk about those problems or if people are interested they can look at how what the vendors say and what agency said about build versus buy question and it's usually not a zero one it's usually a both you build some and you buy some and it's a question of do you build 10 or do you build a 90 I think for people starting the third party products that are available today are pretty good this wasn't the case when I started working so when I started building running experiments at Amazon we were building the platform because nothing existed same at Microsoft I think today there's enough vendors that provide good experimentation platforms that are trustworthy that I would say not a good way to consider using one of those so you're at a company where there's resistance to experimentation and a b testing whether it's a startup or a bigger company what have you found Works in helping shift that culture and how long does that usually take especially at a larger company my general experiences with Microsoft where you know we we went with this beachhead of Bing we were running a few experiments and then we were asked to focus on being and we scaled experimentation and build a platform at scale at Bing once Bing was successful and we were able to share all these surprising results I think many many more people in the company were amenable and it's all it was also the case that helped a lot that you know there's the usual cross-pollination people from Bing move out to other groups and that helped these other groups say hey there's a better way to build software so I think if you're starting out find a place find a team where experimentation is easy to run and by that I mean they're launching often right don't go with the team that launches every six months or you know office used to launch every three years go with the team that launches frequently you know they're running on Sprints uh they launch every week or two sometimes they launch that I mean being used to launch multiple times a day and then make sure that you understand the question of the oec is it clear what they're optimizing for right there are some groups where you can come up with a good oec some groups are harder you know I remember one funny example was the microsoft.com website which this is not MSN this is microsoft.com has like multiple different constituencies that are trying to determine this is a support side and this is some uh the ability to sell software through this site and it's and and warn you about you know safety and updates it has so many goals I remember when the team said we want to run experiments and I said I brought the group in and some of the managers and I said do you know what you're optimizing for it was very funny because the the they surprised me they said hey Ronnie we read some of your papers we know there's this term called oec we decided that time on site is our oec and I said wait a minute some of your main goals as a support site is people more spending more time on the support side a good thing or a bad thing and then half the room thought that more time is better and half the room thought that more time is worse so I know he sees bad if directionally you can't agree on it um that's a great tip along these same lines and you're a big fan of platforms and building a platform to run experiments versus just one-off experiments can you just talk briefly about that to give people a sense of where they probably should be going with their experimentation approach yeah I mean so I think the motivation is to bring the marginal cost of experiments down to zero so the more you self-service right go to a website set up your experiment Define your targets Define the metrics that you want right people don't appreciate that the number of metrics starts to grow really fast if you're doing things right and being you could Define 10 000 metrics that you wanted to be in your scorecard big numbers so it was so big and people said it was computationally inefficient we broke them into templates so that if you were launching a UI experiment you would get this set of 2000 if you're doing a revenue experiment you would get this set of 2000 if you're doing uh so the point was build a platform that can quickly allow you to set up and run an experiment and then analyze it I think you know one of the things that I will say at Airbnb is the analysis was relatively weak and so lots of data scientists were hired to be able to compensate for the fact that the platform didn't do enough and so and this happens in other organizations too where there's this trade-off like if you're building a good platform invest in it so that more and more automation will allow people to look at the analysis without the need to involve a data scientist we published a paper again I'll give it in the notes with this you know sort of a nice Matrix of six axes and how you move from crawl to walk to run to fly and what you need to build on those sixes so if you know one of the things that I do sometimes when I can solve is I go into the organ and say where do you think you are on these six axes and that should be the guidance for what are the things you need to do next this is going to be the most epic show notes episode we've had yet maybe a last question we talked about how important trust is through any experiments and how even though people talk about speed trust ends up being most important still I want to ask you about speed is there anything you recommend for helping people run experiments faster and get results more quickly that they can Implement yeah so I'll say a couple of things one is if your platform is good then when the experiment finishes you should have a scorecard soon after they made fix a day but it shouldn't be then you have to wait a week for the data scientist to me this is the number one way to speed up things now in terms of using the data efficiently there are mechanisms out there under the title of variance reduction that help you reduce the variance of metrics so that you need less users so that you can get results faster some examples that you might think about are capping metrics so if your Revenue metric is very skewed maybe you say well if somebody purchased over a thousand dollars let's make that a thousand dollars at Airbnb one of the key metrics for example is nights booked well it turns out that some people book tens of nights they're like an agency or something hundreds of nights you may say okay let's just cap this it's unlikely that you know people book more than 30 days in a given month so that various reduction technique will allow you to get statistically significant results faster and uh a third technique is called Cupid which is an article that we published again I can give it into notes which uses the pre-experiment data to adjust the result and we can show that you get the result is unbiased but with lower variance and health hence it requires fewer users Ronnie is there anything else you want to share before we get to our very exciting lightning round uh no I think we've asked a lot of good questions uh hope people enjoy this I know they will lightning round lightning craft here we go I'm just gonna roll right into it what are two or three books that you've recommended most to other people there's a fun book called calling which is despite the sort of uh name which is a little extreme I think for the title it actually has a lot of amazing insights uh that I love and it sort of embodies in my opinion a lot of the twyman's law of showing that things that are too extreme your meter should go up and say hey I don't believe that so that was that's my number one recommendation there's a slightly older book that I love called hard facts dangerous half-truths and total nonsense uh by the Stanford professors from The Graduate School of Business very interesting to see many of the things that we grew up with as sort of well understood turn out to have no uh justification and then some with stranger book which I love sort of on the verge of psychology it's called mistakes were made but not by me uh about all the fallacies and and that we fall into uh in The Humbling results from that the titles of these are hilarious and there's a common theme across all these books next question what is a favorite recent movie or TV show so I recently saw a short series called Chernobyl on the disaster it I thought it was amazingly well done uh yeah highly recommend it you know based on true events uh you know as usual there's some freedom for the artistic uh movie they it was kind of interesting at the end they say this woman in the movie wasn't really a woman it was a bunch of 30 data scientists not data scientists 30 scientists that in real life presented all the data to the leadership of what to do I remember that uh fun fact I was born in Odessa Ukraine which was not so far from Chernobyl and I remember my dad told me he had to go to work they called him into work that day to clean some stuff off the trees I think Ash from the explosion or something it was like far away where I don't think we were exposed but uh but yeah we were in the vicinity that's pretty scary my wife thinks I've yeah every every time something's wrong with me she's like oh that must be a Chernobyl Chernobyl thing okay next question favorite interview question you like to ask people when you're interviewing them so it depends on the interview but I'll I'll give you when I do a technical interview which I do less of but uh one question that I love that is amazing how many people it throws away for languages like C plus plus is tell me what the static qualifier does and for for multiple you know you could do it for a variable you can do it for function uh and it is just amazing that I would say more than 50 percent of people that interview for engineering job cannot get this and get it awfully wrong definitely the most technical uh answer to this question yeah yeah I love it okay what's a favorite recent product you've discovered that you love blink cameras so uh a bling camera is this small camera you stick in two double a batteries and it lasts for about six months they claim up to two years my experience is usually about six months but it was just amazing to me how you can throw these things around in the yard and see things that you would never know otherwise you know some animals that go by we had a skunk that we couldn't figure out how he was entering so I threw five cameras out and I saw where he came in um where'd he come in he came in under a hole in the fence that was about this High I cannot I have a video of this thing just squishing underneath we never would have assumed that it came from there from the neighbor but yeah it's these things have just changed and when you're away on a trip it's always nice to be able to say you know I can see my house everything's okay you know one point we had a false alarm and the cops came in and had this amazing video of how they're entering the house uh and pulling the guns out you gotta share that on Tick Tock that's good content wow okay done cameras we'll set those up in my house ASAP yes what is something relatively minor you've changed in the way your teams develop product that has had a big impact on their ability to execute I think this is something that I learned at Amazon which is a structured narrative so Amazon has some variance of this which sometimes but then they go by The Neighborhood six page or something but when I was at Amazon I still remember that email from Jeff which is no more PowerPoint I'm gonna force you to write a narrative I took that the heart and many of the features that the team presented instead of a PowerPoint you start off with a structured document that tells you what you need the questions you need to answer for your idea and then review them as a team and Amazon these were like paper base now it's all you know based on word or Google Docs where people comment and I think the impact of that was amazing I think the ability to give people honest feedback and have them appreciate and have it stay after the meeting was you know in these notes on the document just amazing final question have you ever run an A B test on your life either your dating life your family or kids and if so what did you try so there aren't enough units remember I said unique 10 000 or something you run through a B test I do I will say a couple of things one is I try to emphasize to my family and friends and everybody this idea called the hierarchy of evidence when you read something there's a hierarchy of trust levels if something is anecdotal don't trust it if there was an experiment it was observational give it some bit of trust as you get more up and up to a natural experiment and control experiments and multiple control experiments your trust levels should go up so I think that that's a very important thing that a lot of people miss when they see something in the news is where does it come from I have a talk that I I've shared of all these observational studies that people made that were published and then somehow a control experiment was run later on and proved that it was directionally incorrect so uh I think there's a lot to learn about this idea of the hierarchy of evidence and share it with our family and kids and and friends and there's another I think there's a book that's based on this like how to read a book well Ronnie the experiment of us recording a podcast I think is a hundred percent positive p-value 0.0 thank you so much for being here thank you so much for inviting me in for great questions amazing I appreciate that uh two final questions where can folks finding online if they want to reach out and is there anything that listeners can do for you finding me online is easy it's LinkedIn and what can people do for me you know understand the idea of control experiments there's a mechanism to make the right data driven decisions use science uh you know learn more by reading my book if you want again all proceeds go to charity and if you want to learn more there's a class that I teach every quarter on Maven we'll put in the notes uh how to find it and uh some discount for people who managed to stay all the way to the end of this uh podcast yeah that's awesome I'll include that at the top so people don't miss it so there's gonna be a code to get a discount on your course Ronnie thank you again so much for being here this was amazing thank you so much bye everyone [Music] thank you so much for listening if you found this valuable you can subscribe to the show on Apple podcast Spotify or your favorite podcast app also please consider giving us a rating or leaving a review as that really helps other listeners find the podcast you can find all past episodes or learn more about the show at lennyspodcast.com see you in the next episode

A/B testing and growth analytics at AirBnb, metrics store-Nick Handel-the data scientist show#037
i think that if you can find a way to separate you know your internal beliefs your bias the bias of the people around you yeah from the analysis that you're doing um then it allows you to make points that people trust people believe in and it's really really hard because you know so often we want we want to like make the point that benefits our team or benefits you know there's so many different reasons hello everyone welcome to the data scientist show today we have nick handel nick joined airbnb during the early days as a senior data scientist focusing on growth he led the launch of the data side of airbnb trips and later built a team that designed airbnb's end-to-end machine learning platform big head he was recognized as 30 under 30 by forbes in 2018 today as the co-founder and ceo of transform he is building the first centralized metric store that empowers data analysts to deliver insights we'll talk about data science in product roles building data science tools and ml platforms and of course his career journey if you have been joining the show subscribe to the channel and give me a five star review so let's start with your career journey how did you get into data science yeah thanks so much it's great to be here um i forgot to say welcome to the show it's great to be here thank you for having me um so yeah let's see how did i get into data um yeah so i was always really interested in math as a kid um and so i you know did some math competitions i um you know was kind of a little bit accelerated and took some took a bunch of different classes and so when i got to college i didn't know what i wanted to do you know i think a lot of kids in college are kind of in that position especially in the beginning and so i figured that the most generic thing that i could study was math i figured if i studied math i could go and do anything and i could you know switch careers early on if i didn't like what i was doing and so i started off there and i would say that you know there's some kind of natural relationship between uh applied math and wanting to do data right yeah and so um ended up getting a summer internship at blackrock um in 2011 and i quickly realized that the most fun most interesting part of that job for me was just working with data and um and so i was on a team that was doing lots of kind of portfolio analysis on on some of the portfolios that uh you know bigger funds uh had blackrock manage and i really like doing that analysis but i really wanted to just go deeper yeah so i yeah that was kind of the beginning cool thanks for sharing that and uh can you share a little bit about what's the day-to-day like um as a quantitative analyst in blackrock yeah so i think it really depends on on what kind of team you're on um blackrock is a a really data-driven organization and so you know most of the parts of the company use data in some way and one of blackrock's biggest products is this platform called aladdin which is basically a data and data analysis platform um and so you know it probably depends on what part of the organization you're in i was originally on a part of a group called multi-asset client solutions and basically we just managed portfolios of a bunch of different data asset types and so a lot of the work that i was doing initially was kind of just doing analysis on the performance of those portfolios and reporting that information out to clients that was my internship and then i really wanted to kind of get into you know deeper math and do some programming too my very first kind of venture into uh into programming was actually at blackrock so i had taken a few computer science classes as a part of my math degree but um we we ended up basically just trying to automate this thing that a bunch of uh portfolio managers were doing at blackrock and i did that in vba and so it was a very terrible first experience programming but um i think a lot of a lot of uh you know data analysts start with excel and then try vba or maybe try you know something else right um and so that was my first experience and then from there um joined this team called um called market advantage which was um a it was a risk parity fund which you know somebody can go and look that up and learn more about that okay um but it was basically a bunch of different you know quantitative signals for how different assets we're going to perform and so behind the scenes was basically a bunch of you know a bunch of machine learning and a bunch of analysis to try and figure out what kinds of signals we should build and so that was i didn't realize it at the time but i was doing data science and that was actually kind of before the word data science existed right because that was 2012 2013 i think that was you know right when that article got published that it was like the the sexiest job right right and so that was my yeah that was my first kind of experience um as you know doing data science work yeah and then um how did you get into airbnb what was the transition like from the finance industry to tech yeah so i've had a few kind of career transitions that you know they weren't they weren't really that um kind of planned out you know they just they just happened and i got really lucky and ended up finding the right people and and so in 2014 i left blackrock and i knew that i wanted to do something related to startups i you know was in san francisco this was like you know so many cool startups were everywhere and i wanted to um have some experiences there and so i started working on some startup ideas with friends um but uh quickly realized i just didn't have the set of experiences that i really needed to be successful and i i remember i went to a meetup at airbnb and just met some really great people and then decided that i wanted to interview and then i met more great people and i was just like yeah i can't you know i can't pass this up this is such a great opportunity and the company was growing so fast and they had just raised um their around that was at a 10 billion valuation um and so it was just like it was a hot tech company that i you know i was really interested in joining yeah and uh what was the your first project airbnb oh yeah so it was funny i joined the growth team and my very first project was doing analysis on the the emails that the company was sending which you know i thought it was funny at the time because i probably was subscribed to like three you know email um email uh like groups or whatever yeah i like i just unsubscribed from everything and so i was like okay this is this is a real challenge this is like not a space that i know a ton about i think that the very first thing that i was in charge of doing was basically there was an experiment that had been run about three weeks before i joined and they had basically a bunch of assignment uh assignment data sets so like you know this user saw uh this version of the email this user saw this version of the email and they wanted me to do some analysis on it but they basically just had a fairly messy data warehouse and um and some you know data that said who got assigned to the experiment and so it was it was you know just a bunch of sql like write a bunch of sql and figure out what happened um and yeah i ended up kind of leading to more and more experiments and um i got this was another one of those moments where i just got lucky and was kind of in the right place um the company was just releasing some tooling for data scientists to do experimentation analysis and so i started tinkering around with that and that actually you know just to not to like fast forward fully to today was kind of the beginning of the inspiration for the the company that i'm working on now yeah yeah i think a lot of companies we have internal tools to do a b testing i used to work on a team amazon that runs a lot of a b testing for internal teams can you tell us a little bit more about the data science tools the experimentation tools you'll build there yeah so the very first um the very first tool that we used was um it was very very basic and it it was called uh it was called experiment reporting framework um so erf and airbnb has since built you know a ton of iterations on top of it and made it a really really amazing tool but basically what it did was it allowed you to express a sql query um so basically say you know this is where the data is in my data warehouse and then express an aggregation on top of that so you know i want to sum this i want to take the like you know mean of this i want to divide this thing by this thing um and then it would basically join those data sets to the data sets that have the experimentation assignments in them and it was this you know big data pipeline that would run and do that for all of the different yeah experiments that the company was running and then it would serve those uh results to the user in like a very very root you know rudimentary version of an experimentation dashboard and then that tool kind of iterated over time and um and just became better and better and you know some of the like really really big things that they added on to it were um you know just much better kind of analysis of how the experiment was doing over time so how you were doing on converging to like uh you know to something that was statistically significant and then you know lots of guardrails that helped business users kind of see the data and not misinterpret it right um and then what else um there was a lot of really useful stuff around being able to slice by different dimensions so you know you see that a result is not statistically significant but you really have a hypothesis that like hey maybe it's good on mobile or something that and so then you slice by mobile and you see that oh it's really good on android but it's not good on iphones or something like that and then you have to dig in and say oh there was a bug on the iphones yeah and so you know we need to go fix the bug rerun the experiment and this is the kind of tooling that just it has such a profound impact if you've used it right but it's just not generally accessible right yeah so few companies actually have data data tools that are capable of doing this right um so many data analysts are kind of in that position that i was in when i really just you know first joined airbnb where i just have a bunch of data over here a bunch of data over here and i need to join it and make a bunch of graphs and like figure out how to interpret it yeah yeah thanks for sharing that um and from my own experience i know when we have this type of tool dashboard like you mentioned you need to put on guardrails or some alarms to kind of remind people of best practices so can you share what are some common mistakes you see people make when they use the tool or when they interpret results yeah i mean there are so many mistakes that you can make specifically with with product experimentation yeah so there are so many mistakes um and you know i made a lot of them um the i i kept this tracker of every experiment i ran at airbnb and i think in the first two years it was like 150 or 160 so i mean we really got to launching a lot of experiments towards the end yeah yeah i mean we were doing a few a week and you know some of them took uh some of them took months and some of them took you know two days um so we kind of we saw a bunch of different failure modes um i think you know some of the more interesting ones were um cross-device tracking it's really really hard right so if a user sees one version of the experiment on one device and then comes back to a different device and you're doing like an offline right yeah a logged out user experience right so like uh an experience on search or login or signup or you know anything that you can do logged out then you might realize that well they were just a cookie on this device and so i gave them experiment you know treatment a yeah and then they were a different cookie in this device so i gave them treatment b um and so you'd kind of find out that you kind of ruined your experiment because you showed them both sides of it yeah so that's a big one that's that's called um uh uh mixed group and um you know we we kind of built tooling to protect from that i think the other ones are there's some really hard stuff around mobile devices because uh the way that mobile devices send events um you know it basically buckets events to try and save uh battery on your phone yeah and so for example if somebody's uh if like an app crashes or they exit the phone or exit the app or they you know turn their phone off or something like that um they might do something and then that event gets uh bucketed on the phone to be sent off to you know to airbnb or whatever other company but it never actually goes and so then you don't know that that thing happened yeah um and it's you know you just end up missing data and so that's a big one i think there's a lot around time i did i ran some experiments where they had big novelty effects and then those novelty effects wore off yeah so for the audience who are not familiar with novelty in fact yeah yeah so um novelty effect is basically when you know let's say you have some new feature and you put a a big button on it that says new yeah right and um and then everyone clicks that button initially but the feature is not very good right right like people don't come back and so you launch this thing and you say well you know tons of people are going to click this thing because it's new it's interesting and so your data is going to spike and you know you're going to see a bunch of people clicking that button but then you know they're not coming back they're not using it and so you might see that um that like the kind of longer term impacts are much lower than the shorter term impacts um so that's that's one interesting thing um and then there's there's also there's a funny word but cannibalization yeah is like a funny thing and you know it's an interesting thing in uh in experimentation where you uh will run an experiment and let's say somebody was gonna buy this week anyways like you know i don't know amazon they they needed a um you know a blender or something like that and so um you know you then run an email that that sends them a deal on a blender and then they go and they buy a blender but they would have just bought a blender you know anyway they had some time later that week anyways and so there's some kind of you know experiments where you'll see a spike in in purchases in the beginning but it's actually just pulling purchases forward or something like that right and so that's another case where you see some novelty effects and so then you have to do this analysis where um you know you basically look at from the moment that somebody's exposed kind of how did they interact with it and then do statistical tests where you're actually looking at like from the moment that every single person was exposed rather than doing these very kind of broad statistical tests of like everyone who's exposed to this experiment um you know did it was it like statistically significant um because you end up kind of blending users who were exposed a day ago with users who were exposed 30 days ago right okay so you're doing kind of a cohort level cohort level stud study instead of do i think the standard a b testing is you have a cutoff for treatment control you just treat everything the same yeah yeah yeah definitely the the standard way is you basically just continually assign people and then at some point you stop the experiment and you run a statistical test and you say is it significant um and so what i'm talking about yeah is doing this kind of cohort analysis yeah see how it changes right that's that's really interesting and uh so you run 150 experiments of over a year have you done any meta-analysis to look at them together yeah definitely um you know it's a combination of like formal meta-analysis right of like kind of going in and exploring those um and so you know we ran some interesting analysis not just on my experiments but on all of the data analysts experiments and i'm trying to you know i don't have a ton of memory of like what the really interesting things were but i i definitely remember some parts of it where we found that um that really only about a third of the product features that we um tried actually launched and i think that is probably the best statistic for understanding why we run product experiments yeah right and it wasn't just that we only launched a third it was that a third actually had roughly the inverse effect of what we expected which is you know if you think about a product team just shipping everything that they think of rather than running experiments then you know you kind of end up in a place where they're shipping a third of the things are good a third of the things are bad they're kind of just staying in the same place yeah and so that kind of speaks to the power of experimentation where if you know which one-third of those things are good one you'll be better able to kind of find the interesting things um that you know that you should go and explore and try next um but two you just won't launch the bad things yeah at all right you know net effect in the long term hopefully lead to better business outcomes or whatever people are working on so yeah i think have that data point is definitely um important for product managers to realize oh not every you know something might be counter-intuitive that's why you need to look at the data and i like to follow up with the uh you talk about like mobile like cookies or sessions i think there's always a challenge for data scientists to decide should i run myself sessions or like customer id especially when you want to combine mobile results to desktop or maybe sometimes you run mobile just separately you don't care whether they have seen something on the desktop so is the choice standardized across airbnb or different team can have their own choice of what is the unit of experimentation so the way that we built our our metrics framework um is we built it initially around user ids and what we called uh uh unique visitor ids um and unique visitor ids are basically um it's like a coalesce of of user and visitor yeah and so if we can match a device to a user then we match it to a user um if we can't then we leave it as as kind of like a cookie or a visitor and so pretty much every experiment in the early days was run against a unique visitor or a user id and it it just depends on if they're logged in so if you were running an experiment on the home page right you can see the home page logged out so um you know so that would be something that was uh run at the visitor level if you were running an experiment on the checkout page well you have to log in in order to you know check out for a listing and so we knew you know who was kind of on that page and so we would run those experiments at the user level and in general user level experimentation because you're asking the person you're basically saying like you're already identified we know who you are and so this is you know the the user id yeah it just makes everything in you know easier and so we ended up running i would say about probably 50 50 between those two and it just it just depends um but the preference was always if you knew who the user was you would use the user id yeah so it depends on the scenario and the use case yeah yeah yeah thanks for sharing that and do you remember what is like uh most counter-intuitive or some interesting finding from those experiments so i i'm actually i'm remembering you asked me about the what i did in my first few weeks i'm remembering that the kind of story there um i've written a little bit about this um and it's on the the transform blog okay um but the basically it was a timing experiment and so we i think we tested sending an email um you know 24 hours after they abandoned their cart or like 72 hours or a week hour i remember those emails yeah of course yeah you probably started receiving them in you know late 2014 and they were really effective yeah you know we thought that this would be a valuable email and you know we were we were kind of right it was it was valuable and it was a good email to send and it you know turns out it's better to send it sooner um but what was really interesting was we just saw this huge drop-off around uh 48 hours drop off of checking conversion conversion okay um around 48 hours and so we said well that's really weird that it would be so significant right around 48 hours yeah we looked into a bunch of different things but basically found that we were logging every user out at 48 hours oh and if they're logged out they have to go remember their password and figure out how to log back in um and so we ended up going to the trust and safety team and saying like do we really need to log people out after 48 hours can we you know just do a like quick verification that they've been on this device for a long time like can we make some rules that basically make it so we know this isn't an account takeover and whatnot and so they agreed to let us run experiments where we took the log out time from 48 hours to seven days and that alone i think airbnb in 2014 um you know probably booked 38 39 million nights and that experiment alone contributed i think about 1.2 million nights incremental uh to airbnb's growth and that was and we knew that because we were running experiments so we had statistically significant you know results that showed how many bookings came out of that um and so that was i think that wasn't counterintuitive it was just like an opportunity that we just did not see and when we saw it it was like oh my gosh this is we can literally change a number in the code base from 48 hours to you know uh to one week yeah and it's like a one-line code change and it's worth such a significant amount of money um and what's interesting is we kind of chased that down and we said how do we make it really easy to log in right um and we ended up running some experiments this was probably the most counter-intuitive we took the uh login with facebook button people were getting confused between login and sign up they didn't know if they had an account but it turns out it doesn't really matter if you just press continue with facebook uh or continue with google yeah well you know that routes it to google and google knows if you have an account or not and so we changed some of the language there and made it a little less you know put a little bit less burden on the user to figure out if they already had an account or not um and that experiment also had a really really significant uh impact and i think that was an experiment where i didn't realize how simple like you know it's just changing a string from login with facebook to continue with facebook and that was another one that was worth hundreds of thousands of nights booked so that was actually counterintuitive when that happened it was like the whole team was just shocked yeah thanks for sharing that i love those anecdotes the fix or the change are seemingly easy but i'm sure you spend a lot of time doing the deep dive sometimes i feel like data scientists are just like sherlock holmes expec except our tools are you know data science tools or programming languages do you have any framework uh processes when you try to understand the experiment doing those deep dives because every every um experiment the case is very different um so how do you go down the rabbit hole to find out you know what you could improve that is such a hard that's such a hard question to answer you know i mean there so there's really good tools put the information that you need to know um in front of you right and and they do a lot of work to choose the right information to put in front of you and so when i think about our metric framework at airbnb it did a lot of the work for the user um and so it it you know really helped the user um to kind of define different metrics different dimensions um and then you know surface them to the experiment reporting framework and i think that the experiment reporting framework did a a pretty good job of kind of highlighting things that were interesting or statistically significant yeah um and i think that there are you know even better tools nowadays for analysis that you know are doing kind of automated like diving into different dimensions to try and find you know interesting cuts and whatnot yeah um but i think a lot of it was just kind of curiosity and and you know working with the team um one thing i found was being really close to the engineers really talking to the engineers about you know what what's your hypothesis like why do you think this happened and you know sometimes it's behavioral and sometimes it's you know uh people that are making different decisions right yeah but a lot of the time it turns out that you know there's probably a bug in this corner or there's you know the like log out time is 48 hours like i'm pretty confident that that came from an engineer who's like oh yeah well i think that that you know is 48 hours in the code base or something like that and so i think just being really close to the different people who will have different information right like product managers tend to have really good insights into the users right because they're doing user research they're talking to design yeah and engineers tend to have really good insights into the code base and so if as a data person you can be connected to the product people connected to the engineers and maybe even you know business people who are kind of you know thinking about um thinking about a bunch of other different things it'll help you to have better ideas uh of like what to actually dig into what to slice by what kinds of questions you should be asking yeah thanks for sharing that and also i think one thing data scientist also product manager struggle with is you you can always slice the data you know if you torture the data hard enough it will always give you some type of hypothesis some type of story so how do you avoid those random hypothesis that actually are meaningless yeah yeah i mean it's so hard because um a good like a really good product manager won't do that right a good like a really good product manager won't force you to tell their story they'll allow you to kind of dig in um and you know i've certainly worked with people who really want to make the point that they want to make yeah and they they don't understand why you don't want to make the same point too right um and i think i think that as as data people you know we kind of we kind of need to be switzerland in some of like we need to be neutral right um in some of these uh arguments and it's really hard because i think oftentimes we have opinions too right we think that we should build this we're trying to make our own points and i think that if you can find a way to separate you know your internal beliefs your bias the bias of the people around you yeah from the analysis that you're doing um then it allows you to make points that people trust people believe in and it's really really hard because you know so often we want we want to like make the point that benefits our team or benefits you know there's so many different reasons we want a promotion we want to have the impact i think that over time you know it's like trust right you have to build trust and the way that you build trust is you uh you separate those two things i think it's okay to voice your opinions and say hey i really believe that you know this is an interesting finding in the the data and i think that it means this and like i think we should do this because of that um but if you can separate this is an interesting insight in the data from i think these other things then i think that's really powerful and i remember some of um some of the slides that i made early on for product managers for business people were not like that and i remember kind of getting this feedback and and making some changes and trying to like you know it's in a different part of the slide it's a different color text like this is my opinion this is what the data says yeah like if those two things are separate that's pretty it's pretty powerful and i think people around you will realize you know that that you're unbiased and and that you're really trying to present information in an unbiased way and i think it actually provides a framework to other people to try and provide their opinions outside of the data yeah um yeah i i like that what is the what does the data say and what's my opinion because some people say oh data scientists you just present the results you're completely unbiased but i think it's also important to be biased that's the whole point we hire a human instead of a robot doing the test exactly and that might sound silly but a lot of great ideas actually comes from some intuition especially if you know a lot about users or human behaviors and not everything is reflected in data so i think it's important to have an opinion but separate those two that help you have the awareness you know just for yourself to evaluate your opinion also like your mention and trust from the stakeholders so thanks for sharing that and you're also the founding member of experimentation council working on design better experiments and uh i like that you talked about where we need to be neutral um like switzerland so when it comes to building a experimentation tool a tool always seems like neutral but i think uh every two also have this um some hints on opinions subconsciously would affect the user right what metric are you going to put there what is the first metric data scientist going to see so my question is when you build those experimentation tools do you try to influence data scientists on best practices say don't peak early don't over slicing your data or you're just completely be there as a tool and you let them make the decision yes like absolutely um i think that i think that the tools need to reflect the kind of culture that you're trying to build right um if you just design tools that just put a bunch of numbers up there yeah that's the kind of culture you know you're going to build you're going to have a bunch of people who know a bunch of numbers and they're going to form all of their own opinions and kind of their own beliefs and as a data person you know you kind of know the nuances of the data and so oftentimes you know really good data analysts will find ways to kind of present the data with those nuances um in a way that allows for uh kind of less technical people business people to kind of join them on the journey of the various kind of points that they're trying to make with the data yeah so some some really good examples of this um so the experimentation council airbnb it was it was meant to improve uh the experimentation framework but beyond that it was meant to improve the culture of experimentation um and we did that by you know one requesting various features that allowed us to kind of improve the culture of experimentation and address problems um so examples of some of the things that we advocated for um we made it so that as a company we had i think there were eight eight or ten kind of core metrics and they were added to every experiment in the company yeah so you know new user signups nights booked customer service contact rate and those were the ones that really mattered to airbnb yeah um but there were you know lots of lots of other ones um that we chose to kind of exclude because they weren't as important and so the nice thing there was everyone had to be uh judged against the like most important metrics in the company you couldn't launch an experiment and just be like i'm not gonna include that one exactly like i know it's not gonna be good yeah um and so that was the first thing and then the second thing that we did was we made every data analyst say uh what the target metrics were for the experiment like um and i think we allowed people to pick three uh maybe maybe more than that even yeah i think early on we wanted like one or two right um and they kind of just expanded um and so that was really useful because that allowed us to say it allowed us to tell the rest organization this was what we were trying to move right right up front which is part of the scientific method recommended but not everyone always does that you know sometimes people just launch experiments and then dig and dig and dig until they find something interesting um and and then the other thing was we made it really really easy to add a lot of metrics and i think that this was controversial some people have opinions that you know you should only look at like the metrics you're trying to move or something like that i don't believe that at all i i believe that you should look at as much data as you can possibly look at because if you have a dashboard of 100 metrics and you can see which you know 30 of them went up and which 20 of them went down and which ones were neutral like you get this whole picture of how this experiment how these new interactions are altering users experiences their behavior their kind of interactions with the product the you know um even the interactions within like your company yeah um and that was fascinating and it just led to so many interesting insights um but with that came people misinterpreting all kinds of things and seeing some metric that was you know up by seven percent but it wasn't even remotely statistically significant and so we tried to um we added the ability we basically made it so that things that were going up or things that were going in positive directions were green things that were going in negative directions were red um and we made it so that once something hit a certain uh power level and we would actually take it and say hey rather than showing you know plus point eight percent or something we'll just show a big zero and say neutral and you have to hover over it to see the actual value okay um and so that was that was an interesting one um and then the other thing was there are ways to um kind of protect from peaking right right yeah and so there's some interesting kind of you know statistical things that you can do there yeah but yeah we we did all of it oh interesting yeah so you did uh have some design thinking when you design this data science tool and try to recommend best practices and for the teams who for example if they launch the experiment that i have metrics where the core one of the core uh metrics were significant and negative do you go yelling yeah yeah i mean we um there it it it required a conversation yeah so actually the the sign up and login one was a good example so you know there's good and bad right so some of our like login experiments led to more customer service tickets but then we did the analysis and we said well there are more customer service tickets because there are more bookings and more people are traveling on airbnb and like this is good yeah you know this is good and so you know we would go and have that conversation and the customer service you know the team that was responsible for customer service tickets would say we don't like this but this is good you know this is growth in the business so this is you know launched the experiment um another example is um really um great product manager on the team uh lenny ruchitsky who now runs the newsletter came to me at one point and said hey some of these experiments you're running around login are actually impacting my metrics and he was working on the instant book feature which allowed kind of people to just rather than contacting the host instantly book and so he was working to try and increase the percentage and it turns out that one of the features that the hosts have is the ability to say only people who have previous bookings can instant book my listing well if they're not logged in we don't know that and so we don't show that it's instant bookable yeah and so by increasing how many people were logging in um we were making it so that more people could see instant booking in the product and then choose to instantly book listings which then positively impacted his metrics and uh and so at one point he reached out to me and he said hey i like this experiment how do we get this launched and i was like i'm glad you asked cause you know i'm talking to these other teams and they don't want to launch it so like how do we you know how do we work through this and then i think that that kind of sparks interesting um product conversations where you at least have that good founding you know foundational data to make the decision like everyone knows the number but you have to have the hard conversation around like is this good for the business right that's a great story so are those experiments visible to all product managers or sometimes you have to randomly talk to somebody in the kitchen to find out yeah so so this is definitely visible yeah visible to tall oh okay that's cool and then um do you coordinate because sometimes the feature you're experimenting might affect other team do you have the conversation hey i want to run my person that's another one of those hard lessons we learned i think at one point um i remember this oh we were we were um we were working on a banner that would you know tell people to like log in or something like that yeah or and you know some other team was working on another banner and some other team was working on another banner and i remember some version of the product where if you just were so fortunate enough to get into these three experiments you'd have like two banners from the top and one banner from the bottom like this team did not coordinate yeah um so yeah it you should coordinate especially when features are overlapping um there there are interesting things where some companies um and i believe netflix uh does this or they did this in the past um they'll actually only launch a small number of experiments at the same time okay and um they will uh basically have like a queue of experiments because they really want to make sure they don't uh overlap right airbnb was kind of the opposite every single team was launching experiments all the time um and you just kind of hoped that the net effect of all of these other experiments wouldn't impact your experiment right so do you when you estimate the net effect do you just look at the feature for example are they like above the fold the location of it or are you thinking about the signing check out pipeline from the life cycle perspective um well we would have all that data so we would you know we would look at those numbers which is that's the benefit of having tons and tons of metrics and being able to calculate them all yeah which you know we can talk about transform at some point but like that's what we do yeah um and and that that's kind of the root of you know the inspiration uh for a lot of what we're working on but basically if you have a lot of metrics and you can look at all of them and you have that information then you might look at the ones that you expected to move or that your team cares about but you kind of have to look at all of the other ones and everyone else can too so at the very least you know you'll know what's happening in those other parts of the product um i would say it was you know most experiments had some effect on other parts of the product that people didn't didn't expect and like if you were just doing analysis on 10 metrics you probably wouldn't pick that up but if you're doing analysis and you have 150 metrics in in your kind of experiment reporting tool then you can't ignore it and you know you need to go have the hard conversations with different teams or you know you learn something interesting that leads to a valuable next experiment so yeah thanks for sharing that and what are some other things that um are a little bit surprising but also kind of common in the context of a b testing experimentation i would say just the biggest thing that i think surprises people is how often they're wrong you know um and i kind of already said that but it's it's kind of a profound realization when you realize you know it's really just that you know our intuitions are only so good and and i think as data people we understand that yeah but i don't think that a lot of businesses really understand that and i think that people just kind of assume that if you build good stuff and you you know sell it then like everything will be fine yeah um but i think what we found was basically just that in net it'll be okay but you can build a really really good business if you have a you know good product as a foundation and then you have the ability to just ask every possible question when you're running experiments um trying to think what else is is kind of like really surprising or i think the the the other surprising thing is when you have really good tooling um because nobody can ignore the numbers and because they are so easy to produce and to go to the dashboard and look at um it is just like it is you know so hard to ignore them that everyone just kind of becomes a part of the experimentation culture um and that's probably the most fun part is when you you know when you see like a you know business development manager paying attention to product experiments and saying like ah that's that's really cool like i've got some ideas too you know and just knowing that they can look at the data and actually ask questions and and have ideas is pretty cool yeah and that that really happened like we had a great culture of experimentation at airbnb and that actually did happen like you know i remember a business development manager working on a partnership deal and then like they really wanted to know how their partnership was was working out and so we ran some experiments with the partnership yeah and we just like found great data and it was cool because they could then actually um you know pull that those numbers out and go back to the the conversation with the partner and kind of talk about like how to make the partnership even better yeah and that was you know that's something that we talk about trying to impact products um you know positively but that's that's really cool because that's like parts of the company that you just wouldn't even expect to care about experiments or you know want to talk about them yeah so um how do you foster that good experimentation culture especially outside the data science team i think that it was it was a combination well so i mean you know i'll keep saying tooling like tooling it has to come from the tooling um but i think that the other thing was you know executive support um we had a few executives you know the vp of engineering was was very supportive of running experiments um we had a director of product on kind of growth that was you know just wanted to see the numbers but only wanted to see the statistically significant ones and those like that's pretty good when you have those kinds of leaders who um you know have conversations with the ceo and and uh the like executive team yeah and say like these numbers i'm confident about them and and you know can actually go back and kind of talk about their work um then you know i think that they get interested in it because it allows them to have a ton just have a ton of confidence when they go and talk to you know the executives who they're reporting to right and i think that the executives really like it because the their reports are coming in with a ton of confidence saying hey you know we ran these experiments we expect the this to happen to the business and um and sometimes it's hard because you know a really senior person will really want to launch something yeah and it'll end up being bad but i think that a a good culture of experimentation is when um that conversation happens and the like clearly the right thing to do is is to not launch it and the executive is like willing to back down like willing to be wrong in the face of data yeah yeah and i think that you know maybe a part of that was that we didn't really shame being wrong it was kind of just like a part of being curious was being wrong so yeah yeah even if a experiment uh failed it's not like a failure you learn something from it yeah and let's talk a little bit about the data project you worked on for airbnb trips so yeah what did you do there yeah so um for a number of years airbnb was interested in building out experiences as a product so you know the the company is um was you know really invested in just the idea of kind of local travel experiences right and obviously staying in you know somebody else's uh home or some kind of you know guest unit or something like that is a great way to have a local experience um and you know what was cool was in the early days we got travel credit and we you know i was on the growth team so i worked with i worked on various efforts that like meant i got to travel to like tokyo and like cool places and and so you know stayed with people who were just super generous and kind and like wanted to show me the places around their you know where they lived so i had some of these experiences and and so i kind of i knew that i think all a lot of employees and a lot of people who traveled on airbnb in those early days kind of felt how special that was um but we thought that you know we could kind of extend the experience beyond just like where you're staying to you know i want to go and learn about some interesting part of where i am yeah and that ended up um leading to um a team forming around kind of experiences initially it was really small kind of eight ten people and they did a bunch of user research they kind of you know tried to figure out what a prod product could look like around this um and then around i think it was kind of early 2016 the company decided we're going to launch this at the end of 2016. and i got you know tap on the shoulder saying like hey we are going to need a data person over there at some point soon it's really unclear you know like when yeah and i just said this seems really cool this you know there are like 10 people on this team um and so they ended up recruiting a few different folks to go and uh join them and they already had a data engineer on the team who was doing a good job kind of pulling data and helping them to kind of um answer questions around like where are people traveling what might be good cities to try and launch this in etc um and so i got to kind of go and be a tech lead on this team and it was really small and originally it was i think when i joined it was probably about 14 15 people um so um the you know ceo brian chesky was paying a ton of attention to this team so they were we would like he would come to our stand-ups and you know um so it was just it was a flashy part of the company and it was it was really fun um and we just had to build all kinds of new stuff but we wanted to do it in a way that was we were going to have entirely new data sets right like the data sets weren't going to be the same data sets as the rest of the business we were going to launch an entirely new product and so we wanted to do data really well um and so we ended up going and talking to the data tools and data infra team and because they were so um i had already worked a ton with them on experimentation and you know now two of them are my co-founders so we had a good relationship um and so you know they basically said hey this is you know executive level like this is the most important thing according to the company that we launch experiences by the end of the year so um what tools do you need and there were already some really cool tools um that were being built out at the time um super set is is a you know open source um a bi tool that was being built out and so we ended up being one of the really early adopters of that inside the company um and then we also built kind of new logging infrastructure we ended up you know kind of asking for some like extensions to airflow and some cool stuff inside yeah the company related to airflow and we yeah we you know built an entire data stack um kind of that was connected but separate from the rest of airbnb's data stack yeah and what was really cool was we made a bunch of decisions um that then ultimately the rest of airbnb kind of adopted right so it was kind of this experiment to see what would work of like some of this novel tooling and then the rest of the company you know ended up adopting a lot of that yeah that's really cool so here what you did was basically um collecting data and think about what it will be the schema what tools we're going to use and then how can we build a pipeline so we can provide a good data for people to do analysis on top of it yeah yeah yeah exactly and the the most important thing i think that we worked on was uh this new logging infrastructure that we had you know um the the like least appealing sounding things are sometimes the most important and just having good clean consistent event logs was magical i mean we could do such interesting analysis like yeah i think about it now and i'm like i remember my first two years at airbnb we had this thing called air events um and before that there was when i first joined there was a thing called vlog and that was kind of how we did our logging and it was the beginning of this tool that um we called jitney and it just just made everything so much easier like we had a ton of kind of schema enforcement um they're actually interesting new tools popping up in this space um i think uh avvo is a is an interesting one where people are kind of really taking event logging seriously now which is great yeah i remember when i was debugging some a b testing experiments um digging into the event logs and find out some bugs myself so i think there's a lot of value for someone understands data science basically the downflow of this data product to design the schema of the data so we understand how data science would analyze it but in your case data airbnb trips is a new product at that time maybe you don't even know how those data are going to be analyzed what is going to be the north star matrix for it so it's kind of like a chicken egg problem so how did you make that decision gosh it was what was so hard about it was we didn't really have any data yeah and we were building the pipelines and it's like you know the pipelines are empty right nothing is really happening there's some test data we've got a little bit of data from some like you know user research we did where we kind of tried to log things in cert certain ways and um and you also you know you don't know two things one is you don't know what kind of analysis you want to do so you just collect everything that was our strategy and it works you know i mean it's a lot of work because you have to but we also had time ahead of this launch to be like let's collect everything let's know every detail about around how they interact with this app yeah um and so we ended up adding you know hundreds of events um um to the like corner of the application that experiences was in and then um you know i think that the other um the other thing that we did was we were just really really thoughtful around the entities that existed in our logs um so you know the like what do we call a user id what do we call you know um the like the guests like country of origin what do we call the language that like the guest speaks um and just really really ensuring that you know there were structs around uh those there was like structure around that information yeah so that you know the you never ended up with like english spelled like four different ways in your like event data and then trying to like clean all of those up and catch everything um and so yeah we were just really thoughtful around yeah yeah and later on did you take any feedback from you know data analysts or data scientists when they analyze the data what does the iteration process look like when you design a data scheme yeah i mean this was one of the more flawless things about the launch i think we um we launched and you know i remember we had tier one tier two and tier three uh event logs and i think we launched with you know we were expecting to get all the tier ones most the tier twos and not the tier threes and the engineers were so excited to see data we were so excited to see data we'd been operating this world without data that we basically got every event log that we could possibly imagine wanting yeah in by the launch deadline um because everyone was just so excited to see data and so i mean airbnb at this point already had a culture of data which you know ultimately was won through you know hard hard battles that data scientists before yeah before this had to fight um but yeah it basically meant that um we weren't really missing much at least like that's great data was just kind of there so we got to do all kinds of analysis and the other cool thing was you know we ended up getting to launch a lot of um a lot of kind of machine learning um you know type work relatively quickly after that launch so yeah is that at that point when you started to do more machine learning at airbnb yeah so you know i did i did some machine learning at blackrock right and then i didn't really know a ton about it i you know it was kind of in in the like early formation in my head of like what is this thing you know what why is it useful um and then i ended up working with the marketing team on some attributions some like ltv predictions when i was on the growth team and when we got to experiences we were like this product is going to have to be so personalized relative to you know homes homes you know maybe you like this decor or maybe you like you know maybe you have a family so you want like some extra bedrooms or something like that um but the level of personalization that was kind of we expected to be required for you know do you want to go on a cooking experience do you want to go to an exercise class do you want to like go learn about beekeeping like totally different things yeah and so that was um that ended up being you know really really important and so started thinking about experimentation a lot more then yeah yeah thanks for sharing i'm sorry yeah uh yeah before i move on to talk more about um you leading the machine learning platform i want to uh just in my mind i just feels like the experience that you built this um data infrastructure this data um provide this data and you launched and you know like in your words flawless and the events or so useful i think that's such a great case study so looking back now um i think it's it was also very challenging because if you think about the industry there's no comparable products there's nothing for you to look for um so for folks who are building some other new um data product what is something you think they can learn from this experience besides you know logging all the as many events as possible well that's that's definitely the main thing is just collect a lot collect so much data more data than than you know you think you need and i was i think people have been talking about this recently i think i saw something on twitter where people talking about you know the challenges around events and um the the really really hard thing about events is like if you don't log them you don't get them right the data just disappears and then you go and you try and answer the question and you're like i can't answer that question yeah i and you know let me go and add the event and then i'll cut back to you in a month once i actually have data and so i think our attitude there was you know we just want knowledge we want to log everything and be able to answer that question but i also think that it's you know surprising because of the position that most people are in but i think maybe it shouldn't be surprising we were starting from scratch we designed an application and said this is what it's going to look like and then we're going to launch it and we had you know the resources to go and build out every single event in this app but most the position that most people are in is you know they've built a product and then they've iterated on it for a long long time they've removed features they've added features they've added new pages they've merged pages they've like and so you know trying to keep track of all of the different events in that situation is far harder than it is to just start with a blank slate and pick out all the events that you want to be able to track um and so i kind of think that the only the like main lesson there is if you're if you're really trying to clean up your event data you kind of just have to to to you know get the resources to do or get the like space from the rest of you know your day-to-day work to go and do a full audit and just say i need i need like a week and i'm going to make a spreadsheet that just has every single event we log yeah and i'm going to tell you which ones are good and which ones are bad and i'm going to like you know need you to go find some engineers who are willing to work on this right to make our data much much better so that i'm not kind of in this you know this mode where every time i go to try and answer a question it's like do we have the data is it wrong is it you know um and then the way that you can go and you can sell you know the the like engineering manager on this is just tell them like if we do this i'm not gonna have to go to your team and ask an engineer to go and add an event break the schema yeah do everything again exactly yeah and and that's really really time consuming so if if you can kind of get that space but i mean this is true for all tech debt right this is like if you can get the space to just go and actually fix it in a good way um yeah so i think i think that's you know that's probably the the best tip is try and get that space and do that audit we did that a few times before i did that with our email data when i first started working on email yeah um just to say like you know do we know everything do we know every open click unsubscribe yeah you know et cetera and that ended up being really useful yeah yeah cool and how do you gain the knowledge of all the event type how do you know i'm not missing something yeah you just i mean you get the prototype or you get the app or whatever it is and you just click on every button like i mean you kind of you kind of have to think about it as like a tree right and you have to go down every path to get to every single corner and um and so you know if it helps to like have a white board and dry out the tree yeah and then you know what you want is is like every single state change every single action that happens um and you can you know you can still prioritize these later and say hey every click those are really important yeah um but i don't necessarily need like page scrolls you know or something like that yeah but just in case i'm still gonna log page scrolls but like can we just do it yeah it's tier three but like um and it just if you're if you find a really data-driven engineer who gets excited about this they'll want to do it you know they'll want to know like they'll you'll ask for page scrolls and then they'll be like yeah do people ever scroll to the bottom of this page you're like well we'll never know you know unless you log it yeah thanks for sharing that um and later on you led effort to build big head the end-to-end machine learning platform so um instead of having data scientists build their own machine learning models what was the motivation behind this what was the problem you're trying to solve yeah so i had worked on uh machine learning a few different times and it was you know mostly relatively simple machine learning models um and one of the things that we realized as a company was we had three huge machine learning models that were really really important and those were the you know the basically the trust and safety model yeah like is this a concerning booking that was really really important we invested a huge amount in that yeah there was uh search like show me the good listings right um and then there was pricing what's a good price you know for for my airbnb on any given night yeah and those three models had entire teams built around them and of those teams you know there were maybe two or three people working on the machine learning model and then seven people working on the infrastructure in order to support it and so as a company what they realized you know what the kind of the infrastructure and engineering leadership realized was hey we've got 21 you know engineers across these three teams yeah building infrastructure for machine learning and then every other part of the company that's trying to build out machine learning applications is struggling and so the the idea was okay you know growth team is struggling to build out machine learning applications um uh or to build out you know machine learning applications for uh referrals or for our like marketing spend um and you know product people are struggling to build machine learning models for you know when do we send this email everything just everything yeah and so that hypothesis was okay we could build a lot more you know interesting machine learning applications if we centralized a team to build good infrastructure and so we basically had the charter um i decided i wanted to try and be a product manager um it was you know i really loved doing data work and being kind of hands-on later i went back to do data work and i still do data work so it's like i really enjoyed it but it just felt like there was this high leverage role of actually building tools for data scientists where i was a data scientist i know what the job is i know what the problems are and i can you know talk to these people and relate to them and because of that you know i could probably build pretty great tools for them um and so i decided you know i should um move over to to the product team and so there were a few engineers and and we basically decided that you know we were going to try and build out a few different pieces of infrastructure and um i'm this these are like the fuzzy memories of you know things that i did five years ago but um there were basically you know a few different pieces and it followed the life cycle of building a machine learning model so we built out a feature store uh we built out an environment to go and you know develop models with where you were connected to you know an eight gpu machine on you know aws or something like that if you want to do deep learning yeah or just a you know really really like a machine with two terabytes of ram if you wanted to just download a ton of data and do it locally and so we built out this kind of notebook environment and then we built a tool that made it so that you could take the notebooks that you're building and build a production-worthy machine learning model off of those notebooks and then we built a system that allowed you to um basically take that uh you know deployable machine learning model and host it and turn it into an point for serving yeah to some kind of production application um and then we talked a lot about building monitoring of those models but it turns out building those four took so much time that we didn't actually spend a ton of time on the monitoring tool but we built some basic monitoring of those kinds of models and so those were the five parts of a system that we called bighead yeah thanks for sharing that and when you build a tool you want it works on every use cases in airbnb so how how do you make sure that um it will scale for different type of use cases yeah so that's really hard and one of the things that we realized was these three applications the trust and safety the pricing and search they were probably not going to come use our tooling for a while because they had just spent six years building tools you know from the earliest days of the company to be able to serve those models and so um we ended up we ended up basically um just really building for the smaller applications initially and trying to kind of find the new applications i mean so that meant you know just trying to support new parts of the company um in kind of what they were building and you know i think that was probably more successful and it meant that we got to start people off you know using the kind of tools that we wanted um you know to kind of build out and for the like initial applications it also meant that we basically just built for their applications yeah um and so we ended up just building tools around kind of their applications oh got it so it's not like i'm just gonna build an application agnostic tool i'm gonna focus on a few use cases to begin with and then generalize out yeah do you remember what are the um initial use cases that you focused on yeah i mean we definitely worked with the growth team just because i had some connections there and so we were trying to support their um i think it was their ad bidding model that was one and then the pricing team eventually ended up you know asking us to support some of their models yeah um so those were some of the early ones we worked with the trust and safety team for some of their kind of newer smaller models like um i think rather than trying to take the really big model that was really important we took smaller ones like yeah i think there were they were working some models that would predict like account takeovers and those kinds of things so yeah some smaller models initially yeah i think for um machine learning platforms there's always some issues when data scientists want to use a version of a programming language or a certain package in my support so how do you handle those type of situations we just made it really flexible um so you know we just hosted a jupiter environment that allowed them to to kind of pull whatever packages they want and then there was a ton of compute behind the scenes so they could do crazy things yeah um and then we had this thing the the way that we served machine learning models um basically made it so that uh we took it was it was you know probably it sounds crazy but we basically took um some like python code wrapped it up in an environment and then served it in this like java environment which saying that makes no sense but that's what we did and it worked really well yeah and you mentioned when you were building it you were not trying to make the three big teams to use this eventually did they adopt this tool so the pricing team definitely did you know while i was while i was there i think i actually don't know you know it's been it's been about four years now i would i have to imagine that you know that those teams are using at least parts of that infrastructure yeah right now because it's it's pretty mature so yeah and uh after you build a tool was it hard for you to gain adoption from different teams yeah it really was i mean i think building building tooling is really really hard i mean building all kinds of products is really hard but building tooling i think is particularly hard um and it's because you're really kind of expecting somebody to change something that's so critical to them right like and there's so many problems but um you're expecting them to change an experience that like they do every single day and they need it to work and it needs to be clean and consistent and like it needs to be a good experience yeah and so it takes a lot of polish to replace tools if somebody truly doesn't have a tool like if they need to dig a hole and you give them a shovel then like that's great yeah but finding opportunities like that is pretty rare most the time they have a shovel to dig a hole and you're trying to figure out how to like you know help them help them dig the hole faster right yeah so yeah yeah so what was your pitch or what is your approach to gain adoption um for them it was basically just we'll build what you want like you are limited you have limited engineering resources yeah we'll build the infrastructure that you want to do to support machine learning applications right and internally that's a much easier pitch than you know externally because internally it's like well we're all on the same team so you're going to build some stuff and then you know i will use it and then you'll find ways to generalize that um externally it's like do i want to trust this external vendor to you know build the right tools for me or do i want to trust my own teams like do i hire people or do i go talk yeah yeah but you also have limited resources for your own team when you build a platform and i imagine at one point you probably won't be able to cater to every request from different teams so how do you at that point make the decision um am i gonna yeah am i two more generalized or more specialized so this is you know people talk about one of my favorite decisions at airbnb is that we were really intentional about doing things that didn't scale yeah and that's like a thing that airbnb has repeated a lot you know that's on podcasts yeah and all over the place and i think my biggest probably my biggest and most important learning from airbnb is that people think about problems of scale way too early yeah and so we just didn't have that problem you know not in the first year of building tooling yeah and i think you know this this is related to startups like obviously you want to think about how you scale a business like you want to think about that but if you're willing to do things that don't scale the thing that you would ultimately build to allow that business to scale or to allow that tool to you know to scale or to allow your team to scale within an organization might not be the same thing that you would build initially because if the product changes or you know the way that you interact with customers changes then like you just wasted all this effort thinking about how to make this thing scale that doesn't even matter anymore right so yeah yeah um i think that's uh that's a great point because like you mentioned sometime i want to build a tool we think about we want to build something or create a model that gonna be useful for every use cases then maybe in the end you're not catered to even just one use case and then you might have some problem after you build this seemingly perfect product um so what are some other important decisions you made when you build this platform i think that building things building things in really kind of flexible ways was you know like i mentioned we built things that you know solved very specific problems but we built them in ways that allowed us to kind of extend them and make them more flexible in the future and there's a kind of you know gentle like there's a kind of you know i don't know like a song and dance there where you kind of you're trying to solve a very specific problem but as you solve it you're trying to kind of write the code and design the product in a way that it doesn't make it so that you can't solve other problems right it just means that you solved that problem but you're like kind of leaving all of your doors open and i think that that's a really important um approach to kind of product development for for tools basically right um and so we did a lot of that you know and examples are we had this like you know jupiter notebook environment um where you know we could back it up with whatever machines aws could give us and but it was it could have been more than that like there could have been you know other environments that allowed you to host like r studio or i don't know like you know different environments um to do kind of analytical work on those machines yeah previously i had a william pennar talked to him yeah that's cool feature store at tecton so um can you tell me a little bit about the feature store you build yeah so um the feature store we built you know it was we started working on it before we had ever heard of feature source anywhere else and um the impetus for it and i think a lot of people kind of came to this idea at the same you know at the same time yeah the impetus for us was just that we talked to different teams and they were using the same features um and they had slightly different ways of calculating them they were using different tools et cetera but it ended up being um it ended up being you know basically just that we wanted to be able to define this logic once and then reuse it um and it i think the thing that we were kind of surprised by is that there were very slight differences in the logic for a lot of the features that we were calculating and people just didn't want to change the logic to be consistent with other teams yeah they didn't want to use the logic that other teams were using and so we didn't actually end up having any reuse of features yeah we ended up having a lot of um just a lot of kind of forking a feature and like modifying it in some ways okay which i think actually really surprised us but um i think you know the difference one of the interesting things about machine learning is that every model performs like slightly better or slightly worse with a different feature right and so you're going to use the one that makes your model perform better it doesn't really matter to you if it's like a correct definition of a feature yeah you know it's like it doesn't matter that it's consistent and so we ended up just um we ended up just you know making it really easy to fork features and you know use them amongst different teams but really what it did was it it made it easy for people to um backfill data sets because we had a lot of data for training and then it also made it really easy for people to um to you know stream new data sets uh for um for their kind of online serving of their machine learning models yeah but it i think a lot of what we were doing is actually very similar um probably more rudimentary than like the feature stores that exist today um but yeah it was you know and i think we actually talked to some of the people at uber who are working on the michelangelo platform that then became kind of techton at various points so yeah yeah thanks for sharing that and as a data scientist i think i might want to reuse the exact feature my i created earlier oh yeah or in my team but i can relate to that i might not trust you know it's not i don't trust the data scientist but they might have different assumptions they're tuning different parameters so the feature might not be exactly the same so the fact that you make it very easy to fork and so i can mod just modify a little bit i think it still can save a lot of time yeah yeah yeah definitely um so you basically worked on the two most important use cases in data science experimentation machine learning and you build tools for them and now you um founded your own company so what is your motivation behind that yeah so i was you know at airbnb until 2018 um i i joined this startup after airbnb called branch and it was a microlender and so i was doing machine learning and data analysis and all kinds of things but at a much much smaller scale yeah and you know now i'm really thankful because it it kind of highlighted to me how different the world that i was existing in at airbnb was from the rest of the world yeah um you know i was the i was the first data person on the team okay and there were some engineers who were interested in data and had done some data work but i was kind of the first dedicated data person and we we ended up hiring a few really great data people but i just realized that like some of the tools that we built were really special and the kinds of tools that people could buy were just not even close and you know i i think that they weren't even close because you know because it's really really hard to build generalized versions some of these tools like building a building a feature store within a company or building a metrics repository that backs your experimentation platform is easy if you're the only user of it you can build something for yourself and then you know use it and a few other people at your company use it but you're on a specific set of infrastructure and it works a specific way and um the like main realization that i had was just that um you know this could be really really valuable but the hard part was going to be generalizing it yeah and so in 2019 i was i was honestly i was kind of feeling burnout from just you know i was i guess at that point about seven years of you know doing data work and um and so i took some time off and as a part of that i just started thinking about ideas and it was kind of similar actually to just before i joined airbnb i took some time off and i was like i was like could i start a company right now and um and i just you know i just kind of as i thought about it i just built up more and more confidence that we could actually take some of the things that we had built some of the the really useful parts of the experimentation platform and and metrics um you know tooling and that we can actually build generalized versions of those tools um and you know luckily we we're in this world where i think that it's become much more normal to have a very stable cloud data warehouse which i think is a requirement for uh building a really great metrics repository and then i think the other part of this is just that data analysts and data data scientists have uh now have tools that allow them to do a lot of data engineering work yeah right and it's the problem like it's the i think it's satisfying but i think it's also like not it's not ultimately the work that derives value right and so i think people want to have a lot of kind of power at in their hands of like i want to do data engineering work but i want to do it fast and i want to be able to do a lot of it and i want to build really great clean data for my analysis and that's really satisfying because then you know what you're building right but they really really don't want to like tinker around and write thousands of lines of sequel and then like try and stitch together cron jobs on like you know a hadoop system that like just doesn't work and so we're past that now right we have you know like airflow i think was a really really big part of getting us getting us past that and you know now we have all of these great tools like you know we've got five tran and stitch to like pull the data into the warehouse and we've got bigquery and redshift and snowflake and then we've got dbt and you know from there we've basically got like the tools that we need to do foundational work with data and i think that there's this you know new and exciting kind of set of applications that we're just not pursuing today yeah um and i think that it's it's in part because we have all of these tools but it's still extremely manual to go in and construct data sets for these different applications and i think you know the the way that i would think about this is if you know for everyone everyone who's like you know listening to this and and like thinks about the data pipelines that they have just like think about all of the different all of the different kinds of you know expressions of logic that you have right all of the different joins all of the different filters and you know um all of the work that you do to kind of get that those data sets in place there is so much repetition in that work yeah right i mean it's hundreds thousands of lines of sql to just generate hundreds thousands of tables and data warehouses and i think ultimately every single time we get new questions like we kind of just need to like go and make more data and extend you know that dag of different data sets that we're building and so um those were like some of the really big realizations that i had yeah and um we had built some great tooling around metrics so kind of moving on to transform and like why we started working on this um you know i i started brainstorming with um two of my favorite co-workers from airbnb uh james and paul and james was the product manager for the data tools team in 2014 and then eventually the director product for all of infrastructure and then paul was a tech lead on data infrastructure and engineering manager on data infrastructure um he played a huge role in kind of maintaining airflow and airbnb's public you know presence and kind of you know building air flow out yeah and so i started talking to them a lot and we realized you know just these problems are really really prevalent and um you know the more we talk to other companies we realized hey airbnb kind of solve some of these problems but in a very like big company kind of way like we've got you know all these tools we can build all this stuff yeah and most people just can't do that and so i started working on transform towards the end of 2019 uh we kind of formally started the company you know in 2020 and james and paul joined and we raised some money and started hiring some people and you know the core of it was just that we believe that metrics are this like ultimately the unit the kind of thing that we are trying to build in order to do a bunch of analysis and beyond that they're kind of the language for data like yeah when you go and you get a request for data your business user you know your your kind of business partner like whoever it is that you're working with they're probably asking for data in the format of like i'd like to see revenue you know by country for this new product category we launched and if you think about that like revenue that's a metric yeah you know um product category that's a dimension like uh you know product uh or like by country like that's a another dimension right and so we're filtering by a dimension we're grouping by a dimension and these are all reused and so we should be able to consolidate the logic for these different semantic objects yeah and uh and not redefine them a bunch of different times right so if i'm a data analyst using transform so i can easily construct those common metrics but what if there's some metric that's very specific to my team am i able to create that within transform yeah so um at the core um this is the thing that i'm really excited about so actually it was about a week and a half ago um yeah about a week ago we launched an open source project called metric flow oh cool you can source it yeah we open sourced it um and i'm just so excited about this because you know the a feature store is you know there's a ton of semantic information in there like that's that is a basically a semantic layer for uh machine learning yeah and you know the metrics work that i did on experimentation was basically semantic layers and so i kind of realized like i've been working on the same thing for like eight years yeah and we've learned a ton over time airbnb you know worked on um airbnb has now has a tool called minerva which was built off of some of the early metrics work and um it's fantastic i mean it's one of the most used tools inside of airbnb and so this you know this kind of platform at its core is a semantic layer and that's not a new thing you know semantic layers have existed for a long long time uh there are tools called you know like business objects isn't is an older version of this um looker in some ways is you know is a semantic layer yeah wrapped up with a bi tool power bi has a semantic modeling tool and what's really unique about transform is one it's a very powerful semantic layer and you know what does that mean it means you can define like more metric types with less configuration it can do more things it can build metrics to uh for different applications that other players can't experimentation is an example um there are a lot of metrics layers out there uh now you know it's it's turning into a category and i look around at them and i don't think that those people have ever worked on experimentation because they're not gonna be able to build metrics for product experimentation yeah um and and so we just you know have kind of built up this experience and and we just thought about it and we said this is core infrastructure this is a huge technical project this will thrive in the open um and you know we started thinking about that about two years ago but decided we wanted to build uh you know kind of closed off because it allowed us to just have a ton of focus we didn't we didn't need like you know communities community opinions yet we wanted to build the foundation get it to a place where um you know we could really kind of be really really proud and know that it was the best foundation that we could put forward right and then you know really kind of open it up and try and get some help with generalizing it and broadening it so yeah it's it's out in the open it's really exciting yeah that's really cool and when you build a reporting to for uh the experimentation airbnb we talked about you adopt some design thinking you try to influence the user to have you know use best practices when it comes to decision making so now with transform your customers are from different industries they have their own use cases um are you also having some suggestions on the tool for example oh you're creating this metric but this metric might give you i don't know just example high variance maybe you need to be careful do you have those type of design yeah so there it's it's riddled with opinions we have so many opinions on the space right like you work on something for this long and i was thinking about this between the original metrics repo that i worked on in airbnb an evolution of that that you know i was an end user of so i i was kind of providing feedback on and then you know um this semantic layer inside of airbnb um that was you know that was basically behind our feature store um and then i i worked on another feature store actually at that startup branch yeah um and it was kind of like a metric store also another version of transform i'm now on my sixth version of of like a semantic layer so i've been building basically these same things for like eight years so i have a ton of opinions um when i maybe just to kind of set a foundation the way that i think about what a you know what a metrics store is um there are kind of four pieces there is semantics how do i define what an entity is yeah what a metric is what is a dimension and this is really really important because if you do this wrong you know you capture a bunch of unnecessary information you don't capture the right information and that prevents you from pursuing certain applications or you capture information in a really kind of non-dry way and it becomes very very hard to manage and so you know i would say that probably the best way to look at our opinions is to go to go to our you know documentation but yeah there's a lot there you know in the way that we uh take in a data source and kind of say you know who are the entities that exist within the data source like it's a transactions table and so the primary key is transactions and the foreign keys are users and you know products and so those are you know foreign keys that go off to you know dimensional tables that have more dimensions on those things um so there's there's kind of this like you know ontology of information that is stored in these uh in this like in the structure of the project and what that enables us to do is then be really really flexible so next really important thing is querying how do i query these metrics and so our query interfaces allow you to ask for you know i want this metric slice by this dimension i want this metric sliced by these dimensions yeah i want you know this very complicated metric like a rolling cumulative window with filters et cetera and and to actually be able to express that in like a very clean way and query it and then get um you know what ultimately might be like 100 lines of sql to write uh back to you uh really really quickly in kind of whatever applications you're trying to use um and then the other pieces are around kind of performance and governance and performance is important just because when you ask for a metric you don't want to wait that long so how do you generate efficient sql do you cache data so there's some interesting stuff there and then governance i think is probably the piece where um it's it's hard to be opinionated because everyone is opinionated about governance right um and everyone wants to govern their their data in different ways but with metrics i think that there are there are kind of best practices and we try to support those and so it's around how do you manage a life cycle of metrics yeah right so definition iteration archival you know um all those kinds of workflows yeah yeah thanks for sharing that um so now like you mentioned there are more and more tools to empower data scientists but it's also sometimes kind of confusing for us to choose what tools to use so um in in your based on your experiences what type of tools that you think is going to be like retiring from our job and what what type of tools will be emerging and we probably need to start to pay attention to yeah so i know you're biased but it's okay i think that the i definitely have opinions and i think most of my opinions i probably used to have more opinions around you know compute frameworks and tools like that yeah and i think most of my opinions nowadays are around the tools that we use to consume data and i think that the biggest trend that i've seen is that um historically people have built these really big monolithic bi tools yeah right like you know i think that there are a bunch of different examples in the past but bi tools that are expected to be everything and do everything with data and so it's like every data set that you produce ultimately just goes to that one bi tool and gets distributed the rest of your business but they don't really expose interfaces that um you know and business users really like or that data analysts really like they don't you know necessarily do as much as they could with executive reporting or with forecasting or with anomaly detection or with product experimentation yeah or with like any number of of applications that people have with data and so um the thing that i'm really excited about is just i think that we're moving away from this world of the monolithic bi tool being the one place where data goes right to more custom applications that serve their end users much much better and i think that the reason why we get to do this is that data is being seen as more valuable to more you know in more applications and to more end users yeah um and so you know that means that um that means that we get to basically um have more tools for specific end users yeah so i like the idea of data analysts having really flexible environments to kind of go and do analysis um so there's you know interesting things happening in the kind of um like what has historically been that like notebook type experience like deep note and hex are really interesting tools and and then you know there's kind of this evolution of like the like analytics you know first kind of querying interface um like i think mode is an example of a tool that you know data users really like and i think you know business users have historically had like harder times querying from those kinds of tools that try to be more powerful for data analysts but at the same time now we're building tools uh that business users like a lot more so better self-serve environments um and i think there's you know sigma is an interesting bi tool that's kind of popping up and trying to to do that yeah and so i'm just excited that you know we're building each of these these tools that have more specific end users and i hope you know obviously i'm really biased but i hope that what the metrics layer can do is provide consistency to all of these different tools yeah when i'm a data analyst and i want to you know go and see revenue by product category by day like i should be able to go look in my you know analytical environment and i should be able to interact with like a python api to go and pull that data and at the same time my business user should be able to go to like google sheets and select revenue and product category and day and then get back a data set that they can then go and pivot in like the environment that they like yeah so i'm really excited about that because i think it means more choice for end users and ultimately the right tool to use is the tool that makes you more productive and more impactful yeah whether you're a data scientist or a business you know stakeholder yeah that's very exciting and when i read the description of transform you now label it as a metric store for data analyst so i i haven't used it yet so bear with me if i'm asking about the features that you already have so data analysts when they um talk to their stakeholders eventually it's going to be a report so the next step would be they may they might want to you know visualize it create some type of a dashboard and also there are more low code no code machine learning tools they might also want to use some sales some metrics for a region to build a machine learning model so do you see this metric store go into the direction into a feature store for machine learning or are you going to add some similar features like tableau kind of dashboard into it to kind of complete the loop of the analyst's work cycle so i think neither um so why not feature store the problems there are very different there are challenges around kind of point in time correctness typically in machine learning you want your data to be accurate as of the moment that you would have run your model which if you're trying to backfill and kind of you know yeah train your model you want specific moments of like how many you know how many transactions did this user have at this moment you know on like the you know at 7 22 p.m on this day and like that's a really hard problem and that's that's actually the opposite of what data analysts want what data analysts want is they want all of the information they don't want you know the like information at a specific moment in time okay um i mean maybe they do so snapchat snap a snapshot like once a month they don't need it to be like a data stream yeah yeah and so they're just kind of different questions and the most important thing there is that they are very different pieces of technology to build yeah um there are overlapping concepts and ideas but they're very different technologies yeah the other thing is you know people don't necessarily want their features to be exactly the same between teams people definitely want their metrics to be the same yeah you know um if you can't you can't have 20 different teams in a company or even two different teams in a company saying you know i like to calculate revenue like this and i like to calculate revenue like this that is a nightmare and nobody wants that and every data scientist has deal with that yeah um and then the other thing is like you know related to will we become you know a bi tool um i think that we're working very hard to avoid that and the way that we're working hard to avoid that is by exposing different interfaces for people to query data from and so you know we have some some really big ones right now like uh there's a command line interface to go and do development so you go and you change the definition of your metrics in in your kind of you know transform um in in metric flow the underlying framework and you can go and test it you can run queries against your warehouse you can see the sql that metric flow generates you can also you know go and uh connect to vi tools so for example you can connect mode hex you know different applications uh tableau for example to transform and from those interfaces you know you can pull data out of transform um and so there are a few different options but you know tableau has um a web data connector that allows you to kind of pull data out of transform and then in motor hex uh you can go and you can write sql and uh the sql basically is all the same sql that you'd write against your snowflake or redshift or bigquery um but it also generates uh the metric definitions within the sql uh that you're running yeah so um that basically means that you have all the power of your underlying data warehouse but you also have consistent metric definitions yeah so you can basically just say hey give me revenue by day and then go and join it to this other table and do all this other analysis with it and so then it's in the workflows that you already have in your bi tools but you can pull consistent metrics into them right yeah i think you definitely solve a problem because i remember every time i create some reports uh the biggest problem there's always different type of data sources how do you consolidate that and different data sources have different you know logic so if we have something consistent and also we can we can trust i think um that would be uh great news for data analysts and also um i think it's it's good that you sounds like you're very clear about what you don't want to do so you want to just focus on focus is important yeah um there's a lot to build that being said there are there are applications for metrics that are not you know business intelligence that i think that people don't pursue today because they don't have the ability to generate metric logic very easily and i think being able to generate metrics for product experimentation you know we've we talked about it a bunch it's really really important yeah and it's really really hard and you know i was i was explaining how important it is to be able to track you know tens hundreds of metrics against your experiments the reason people don't do that is because joining you know 80 different tables around your warehouse to assignment data sets and calculating 100 different metrics is like a month worth of work and nobody has time to do that yeah exactly especially if there's a specific metric just for my experiment for my team that nobody have used before then i have to go there validate the data quality and then create all the new queries nobody has created before yeah but imagine if everyone could just publish their queries it's you know i think one really interesting trend is people talk about like distributed or centralized data teams yeah right yeah so what's great about centralized data teams is you know everything happens in one place so it's very easy to kind of keep things consistent uh what you know what's great about distributed teams is you get a bunch of people who are really close to their you know and business partners to the people who they're doing work with well what if we could kind of have both of those like a hybrid exactly by allowing people to be close to their business users but to contribute to the central definitions of metrics and to allow everyone to consume those same definitions um i you know i think about some of those experiments we were talking about and the really interesting thing here is like um i did so much analysis on you know customer service contact rate yeah i to this day do not know how airbnb defines that metric right but i spent probably hundreds of hours doing analysis on that metric yeah and i know that the analyst who is responsible for it defined it correctly yeah and every single time i did analysis they asked me where did it come from and i pointed at our metric definition and they said okay cool that's how i defined it so we just we trusted each other because we had these centralized definitions yeah we weren't you know in a room hashing out the differences between our hundreds of lines of sql right mostly you know yeah and like you mentioned the when you make launch decisions you always have the company-wise core metrics and those things you do want to be centralized so um you know it's not like everybody create their different um you know what is it the customer the complaint related metric oh customer customer service ticket contact ratio yeah but you do also want to have maybe some specific metrics just for your own experiment that only your team defines so yeah that's very exciting so now looking back at your journey from um quan to data data scientist to product manager now ceo what are some um important lessons you learned what is some kind of mindset shift i talked about some of the lessons around kind of removing my bias you know around doing data work and i think that was a really really important one that i learned early on in my in my time doing data analysis i think um you know the more i got into building tools and and working on tools i think that that removing bias became more important because you have these opinions around you know how you like to work but yeah ultimately the way that you need to build tools is you need to go and ask other people like right how do you want to work with data like what's what's wrong with your workflows um and so i think a big part of that is just listening yeah i you know the honestly the the like the thing that i just try and constantly um you know constantly try and maintain in my career is um just being being nice to people honestly like it's it's an amazing you know it's one it feels good and two i think that you know people root for you and want to work with you and want to they want to teach you things they want to learn from you and it just makes i think everything in life a lot easier yeah and so that's you know that's something that was kind of i think has generally been pretty important in my career and just something that i've i've tried to uh maintain and i think a big part of that is to you know operate within integrity yeah and so i you know have always just tried to be really honest and and also you know tried to recognize um like what that actually means so you know examples are when i first moved into managing people i found that that desire to be nice you know and airbnb has this culture of you know be a host host yeah and it's it's really really important and it's true um but i think that in sometimes in some ways it kind of got in the way of airbnb's culture to some extent like it prevented people from giving feedback in ways that i think would have been beneficial yeah and so i think that if you can you know now i i try and rely on some of these lessons but i think that if you can really kind of have good intent when you go and give feedback to somebody it's actually one of the nicest things that you can do yeah right um it might be hard for the moment but it's nice in the long term yeah it's really hard and i think especially when you're learning how to give feedback you know i think trying to be honest with people and and like let's just do sandwiches do follow the formula i don't and honestly i get feedback that you know that i'm too direct sometimes yeah i think that the you know the formula is interesting um but i find i don't know i feel like in some ways the formula kind of takes away from the sometimes yeah it feels less authentic because so many people using that they give you something positive and then you know the butt is coming i know right and then they finish it with something nice yes and then just also take away the nice thing they're saying in the end right like yeah i don't feel good i knew that you just had to say something nice there yeah but you know i think a big part of that is also trying to um trying to receive feedback really well yeah and you know i think that culture is made by a group of people and at transform we one of the first things we do did actually i remember getting some emails my first investor update ever to our investors said hey you know the three founders got together we talked about things and anyways here are our values and here's the kind of culture we want to build yeah and everyone was responded to us and was just like really like that's one of the first things you did that's kind of cool most companies don't do that yeah um and i just remember thinking like what do most companies do to start because this seems really really important yeah um and you know those those values have evolved we did a team off site um a little while back and and kind of changed them and we you know went from six down to four and kept two and added two new ones um but some of those are just direct products of i think you know the kinds of people who we tried to hire which i think are kind of reflective of the environment that we want to work in yeah and so they're things like you know um lead with empathy is is the first one and it's the first one because we're leaving with empathy in life yeah um and then you know there's some other ones in there around kind of growth and and kind of the passion that we bring to our work and yeah and whatnot and that are just honestly things that i feel like are really important but i didn't i didn't force those things on people it was that's what that's the group that we hired and yeah that's the thing that we all realized was important and it just has a you know it has a really significant impact if you can find a group of people who believe those the same the same things as each other you know yeah thanks for sharing that yeah what are some mistakes you made in your career um okay so i think the one of the ones that we talked about that i think was really significant was just kind of tangling my own bias and my own beliefs with the data and i think that that made me less effective as a data scientist early on because people couldn't tell the difference between what i was arguing for and what the data said yeah right and so you know i almost made the data less impactful by kind of tangling those two things um and so that's that's definitely one i and yeah you learned that and then you started to separate you do separate the two okay be really intentional about that say this is what the data says and now this is what i believe and this is why i believe it right and i think data scientists should have opinions and should say these things yeah but i think that at the same time they need to you know clearly articulate those the differences i think that's a really big one i think that you know when i was initially starting to manage i think i mentioned this one you know just a minute ago i think that i was a little too nice you know i was i was i was kind of trying too hard to build a friendly relationship and not a relationship where there was a lot of mutual trust around yeah the fact that i was helping i was doing my best to kind of help somebody grow um and i think that that is something that i've been you know been working on a lot and i think that yeah i think that i've improved but you know there's a lot a lot more improvement there that you know i can kind of keep working on yeah you know i was really early on in my career i was really bad at um at just kind of like figuring out who to communicate things to and yeah like it's really hard as a data person you know you know all about numbers and you love analysis and you love thinking about that but then you're like i did some analysis what do i do with this yeah you know and i think that that is a really underrated skill for data analysts because in some ways like you're trying to influence people with data right and so you need to be able to find out you need to figure out how to distribute that data and who to send it to and how they'll receive that message and you know there's a lot of kind of interpersonal interactions there that go beyond like you know how proficient in python or sql am i right like because it's a lot more complicated than that um and so you know that was something early on that that i was you know just i realized early on how important it was and and started working on that um yeah and then maybe just you know kind of to extend that one right now something i'm working on is is trying to be better at writing trying to like write more um i have so many ideas in this kind of you know metric store space and i have a lot of opinions about things that are not you know um things that we're building that are kind of missing the point um and i really want to kind of get the industry focused on the point of this tool yeah and so i'm yeah i'm figuring out kind of how to how to talk more about that yeah so what is the one example of the industry is missing or we already covered that well there are some very specific ones related to our product and kind of the technical implementation of it um but i think that probably the most important one that i think that the industry is kind of missing is that the reason why urban beef found this tool minerva so valuable and the reason it found metrics repo so valuable was because it generated data pipelines it took what you know would otherwise be a huge amount of data engineering work stuffed it into a few lines of you know yaml configuration yeah and made it so that a data analyst a data scientist could go and do this and it would then programmatically write thousands of lines of sql and do it in a very performant and efficient way um which ultimately produced you know metrics to a lot of people um and i think that would when people look at metric stores today they think a lot and and i think part of this is because of the you know early blog posts that other people in the industry wrote about this they think about like a jdbc that allows you to write sql yeah and allows you to kind of connect your bi tool the fact is airbnb derived an unbelievable amount of value out of its metrics repo and minerva by 2020 and it didn't even expose an external api to pull data until 2020. and even now you know airbnb is only really kind of just getting to the place where they're connecting tools like superset and tableau um you know really well with with these tools and so i think that people are kind of missing the point of what this is supposed to do and and think that it's a very thin layer around the data warehouse that just makes the consistent metrics in your bi tool but the reality is is it it's really meant to solve the just the burden of doing a huge amount of kind of you know data construction work yeah thanks for sharing that and uh do you have any mentors oh yeah so many um you know the fun thing about starting a company is you get to meet a lot of really smart people who want to invest or kind of advise in your company um and so there are some really great people there um i mean you know my mentor since like birth or my my parents and my brother um so my brother is is um you know i i kind of i followed him at various points in my career mm-hmm um so what is your brother doing now uh he's he's a product manager at at uh at block on cash app yeah and so he's a growth product manager um and so i followed him to school we both went to ucla yeah um but he went he's older so he went there first and you know and then i think now i just yeah i learn a lot from him um and definitely you know my dad did a phd in finance and so i learned a lot of kind of business and you know legal and all kinds of interesting things from him and then you know outside of family um yeah i mean you know a lot of kind of people who i've met in my career right yeah um for example what is the most important thing you learn from your brother um he's incredibly patient and i'm incredibly impatient so um that's probably the biggest one yeah um yeah i am i think you have to be a little impatient you have to be a little impatient to be a founder but you also have to realize that you're impatient right otherwise people just think you know you're a jerk yeah but i feel like from the um the the story where you collected all the events i think there's a lot of patience there because i'm sure that they hold on hey can we launch this faster um yeah i mean interesting that yeah i think yeah i think that that probably was true yeah um there are times in my in my career where i've had to like learn how to be patient that's probably another lesson um you know outside of that i think my two co-founders now honestly are are like they were mentors to me before yeah so my co-founder james was my manager at airbnb okay he was my boss yeah so we knew that we liked working with each other and i learned so much from him he's you know he is such a thoughtful leader and he was managing you know 20-something pms um across various parts of airbnb's infrastructure when when i was reporting to him and he's just such a high empathy person so like thoughtful so kind um and so you know i've learned a lot of lessons from him especially now that we work you know um closely together on transform and i kind of get to see him make really hard decisions um what is the biggest lesson you learned from him i think a lot of lessons are in empathy i think with him i think you know empathy with uh co-workers or with users everyone co-workers users everyone he just he has this way of just kind of putting himself in somebody else's shoes and then you know he'll say well you know they're probably thinking this and they're probably feeling this because of this and like you know we should probably ask them this question to like clarify that yeah try and understand how they're feeling about that and i'm just like i didn't even think about that now i feel like i feel like i'm like you know not thinking enough about like from their perspective what's happening so i learned a lot um a lot there yeah who who else um i worked with a few really great great data data scientists um you know i think that there were some some really great data scientists that you know in the kind of early days of my my work just were like people who i could talk to and there was one product manager at airbnb actually and uh she was one of the first products managers i worked with she was like you know so driven like she was like we're gonna ship on the deadline you know like a really great like focused product manager but at the same time she like was really really good at you know communicating and like updating people and being flexible when like she needed to be flexible and i feel like she picked her battles really well yeah and because of that she was like very influential um and so i feel like yeah i learned a lot from her also yeah thanks for sharing that so it's like you can be very focused but it doesn't mean you have to be a jerk you can communicate and then also know what are the deadlines you have to stick to and what are the things you can be flexible yeah yeah um and uh if you were to mentor someone what's your advice to them i think empathy is the biggest one just like try to understand the people that you're working with try to you know try to really really understand them because ultimately the way that you know you will be more influential and and you'll kind of have the ability to have a broader impact yeah is is to build those relationships and yeah yeah thanks for sharing that um so before we wrap up what is something in your life or in your career that you're excited about ooh i just got engaged oh so congrats yeah like um yeah three weeks ago so nice um are you planning wedding already yeah yeah yeah kind of i mean you kind of yeah it takes a long time yeah nice um and uh um what is uh something exciting in the near future for transform okay so well i mean you asked me a week after like launching my pride and joy you know the thing that i love you know most at work is is metric floats yeah i'm just i'm really excited about it because it is i do believe that it's the most powerful semantic layer ever built um and it's i'm really really proud of so many of the decisions that we made in it and i think that it's a foundation that you know we will build on for years so that's the thing that i'm most excited about um and yeah i'll be i'm uh going to push out more of our kind of road map next week yeah to to some folks and i'm really hoping that people will contribute and that companies will you know pick it up and play around with it um or even individuals so yeah yeah um and for folks who want to follow your journey your writing transform or engagement photos so where can i find you um so on most social media accounts i'm nick underscore handle okay um that's your handle handle my handle right and it's my last name is h-a-n-d-e-l uh like the the famous composer yeah you know that's twitter and instagram if you want to see the engagement photos and um and then beyond that you know i a lot of my writing i'm trying to push to the transform blog so that's uh yeah blog.transform.com and um yeah if you reach out to you know transform and you want to demo i'm happy to to give a demo so if anyone wants to see transform i would love to show them that's awesome well thank you so much for coming to the show nick i learned a lot and really enjoyed our conversation thank you so much this was a ton of fun thank you

